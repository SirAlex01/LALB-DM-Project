{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX2Kz1jxRkkt",
    "outputId": "c3e515bc-98a8-485a-a09f-9de23aae41a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.local/lib/python3.10/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.local/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.local/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install sentencepiece\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from datetime import datetime\n",
    "#from google.colab import drive\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from enum import Enum\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "prefix_path = './DMPROJECT/'\n",
    "USE_BRNN = True #set to true if using BRNN, false for transformer\n",
    "USE_TRANSFORMER = False\n",
    "USE_FCN = False\n",
    "USE_CONV = False\n",
    "USE_COMPOSITION = False\n",
    "USE_BICONV = False\n",
    "#assert sum([USE_BRNN, USE_TRANSFORMER, USE_FCN, USE_CONV]) == 1, \"Exactly one model must be selected (set one flag to True).\"\n",
    "\n",
    "EMBEDDINGS_DIM = 256 #Embeddings size for phonetic embeddings, word embeddings and transformer token embeddings\n",
    "\n",
    "#Default brnn is use EOS true and use SOS false\n",
    "USE_SOS_IN_X = False if USE_TRANSFORMER else True\n",
    "USE_EOS_IN_X = False\n",
    "\n",
    "#Default brnn is use EOS true and use SOS false\n",
    "USE_SOS_IN_Y = False\n",
    "USE_EOS_IN_Y = True\n",
    "\n",
    "USE_ONE_HOT_ENCODING = False #One-Hot instead of tokenization+embedding\n",
    "EMBED_FOR_GRAPH = False\n",
    "\n",
    "#Embeddings usage\n",
    "USE_WORD_EMB = False\n",
    "\n",
    "#Type\n",
    "USE_TFIDF = False #use tfidf features\n",
    "USE_NLLWithKL = False #use NLLLwithKL loss\n",
    "USE_RANKED_LOSS_CONTRIB = False #use ranking in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYZPD2IHUgaK",
    "outputId": "793df5fa-92a1-4ce0-908d-0ce28c2ddcd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 17 for reproducibility!\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 17  # You can change this to any fixed number\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure deterministic behavior for CUDA (if using GPU)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n",
    "\n",
    "# Ensure reproducibility in cuDNN operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Disabling this may slow down training but ensures reproducibility\n",
    "np.random.seed(SEED)                      # NumPy RNG\n",
    "torch.manual_seed(SEED) \n",
    "#PLEASE BE DETERMINISTICK\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "# Set environment variables for deterministic behavior\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # Ensures CUDA operations remain deterministic in newer PyTorch versions\n",
    "\n",
    "print(f\"Random seed set to {SEED} for reproducibility!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU device: 0\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n",
      "Total CPU RAM: 2015.66 GB\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Total GPU RAM: 79.15 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Total RAM in GB\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(f\"Total CPU RAM: {total_memory:.2f} GB\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(gpu_index).total_memory / (1024 ** 3)\n",
    "\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total GPU RAM: {total_gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA GPU not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "87I6xVc5gWP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMfIbj4oyexW"
   },
   "outputs": [],
   "source": [
    "if EMBED_FOR_GRAPH:\n",
    "    USE_SOS_IN_X = False\n",
    "    USE_EOS_IN_Y = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASfy4nGDQ3ZA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDA LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "euP9xvH7Qwbc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DMPROJECT/signs_LB.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#LB\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_lb \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/DMPROJECT/signs_LB.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DMPROJECT/signs_LB.csv'"
     ]
    }
   ],
   "source": [
    "#LB\n",
    "df_lb = pd.read_csv('/content/drive/MyDrive/DMPROJECT/signs_LB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er725Pt6SZbO"
   },
   "outputs": [],
   "source": [
    "signs = df_lb['sign'].unique()\n",
    "signs = sorted(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wyo8XZDoRoM"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each sign in df_lb\n",
    "sign_counts_lb = df_lb[\"sign\"].value_counts()\n",
    "sign_counts_lb = sign_counts_lb.head(70)\n",
    "\n",
    "# Create a bar plot for the signs in df_lb\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "sns.barplot(x=sign_counts_lb.index, y=sign_counts_lb.values, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Signs in df_lb\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzm3_1MAa915"
   },
   "source": [
    "##EDA LA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79aiz3ZEdaSg"
   },
   "outputs": [],
   "source": [
    "df_la = pd.read_csv('/content/drive/MyDrive/DMPROJECT/signs.csv')\n",
    "df_la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXcXXpnVdt9t"
   },
   "outputs": [],
   "source": [
    "signs = df_la['sign'].unique()\n",
    "signs = sorted(signs)\n",
    "len(signs)\n",
    "signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tywOPnT7ersE"
   },
   "outputs": [],
   "source": [
    "df_sites = pd.read_csv('/content/drive/MyDrive/DMPROJECT/sites_data.csv')\n",
    "df_la_sites = df_sites.merge(df_la, on=\"document_name\")\n",
    "\n",
    "sites_to_remove = [\"Syme\", \"Pyrgos\", \"Psykhro\", \"Papoura\", \"Mycenae\", \"Melos\", \"Kythera\", \"Kea\", \"Haghios Stephanos\", \"Gournia\"]\n",
    "df_la_sites.drop(df_la_sites[df_la_sites[\"site\"].isin(sites_to_remove)].index, inplace=True)\n",
    "df_la_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHE8AfGMfVEM"
   },
   "outputs": [],
   "source": [
    "#Distribution signs per site\n",
    "freq_df = df_la_sites.groupby(\"site\")[\"sign\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Display frequency table\n",
    "unique_sites = df_la_sites[\"site\"].unique()\n",
    "\n",
    "for site in unique_sites:\n",
    "    site_df = df_la_sites[df_la_sites[\"site\"] == site]  # Filter data for the site\n",
    "    sign_counts = site_df[\"sign\"].value_counts()  # Count occurrences\n",
    "\n",
    "    # Get top 20 most frequent signs\n",
    "    top_20_sign_counts = sign_counts.head(50)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "    sns.barplot(x=top_20_sign_counts.index, y=top_20_sign_counts.values, palette=\"viridis\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Sign\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Top 20 Most Frequent Signs for Site {site}\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG6hZRhvk6nC"
   },
   "outputs": [],
   "source": [
    "site_signs = {site: set(df_la_sites[df_la_sites[\"site\"] == site][\"sign\"].unique()) for site in unique_sites}\n",
    "\n",
    "# Find the intersection of signs across all sites\n",
    "common_signs = set.intersection(*site_signs.values())\n",
    "\n",
    "# Filter the dataframe to only include rows with common signs\n",
    "df_common_signs = df_la_sites[df_la_sites[\"sign\"].isin(common_signs)]\n",
    "\n",
    "# Count the occurrences of these common signs\n",
    "common_sign_counts = df_common_signs[\"sign\"].value_counts()\n",
    "\n",
    "# Create a bar plot for the common signs\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "sns.barplot(x=common_sign_counts.index, y=common_sign_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Common Signs Across All Sites\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfyjS2kjpmd9"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each sign in df_lb\n",
    "sign_counts_la = df_la[\"sign\"].value_counts()\n",
    "sign_counts_la = sign_counts_la.head(70)\n",
    "\n",
    "# Create a bar plot for the signs in df_lb\n",
    "plt.figure(figsize=(16, 8))  # Adjust the figure size\n",
    "sns.barplot(x=sign_counts_la.index, y=sign_counts_la.values, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Signs in df_lb\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ca5j5iS0qat",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDA LB LA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxsWQtvr0tRn"
   },
   "outputs": [],
   "source": [
    "SEQUENCES_LA = os.path.join(prefix_path, \"sequences.csv\")\n",
    "SEQUENCES_LB = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_LA = pd.read_csv(SEQUENCES_LA)\n",
    "df_LB = pd.read_csv(SEQUENCES_LB)\n",
    "\n",
    "\n",
    "def compare(sequence_LA, sequence_LB):\n",
    "    sequence_LA = sequence_LA.split(\"-\")\n",
    "    sequence_LB = sequence_LB.split(\"-\")\n",
    "    i, j = 0, 0\n",
    "\n",
    "    score = 1\n",
    "    correspondences = 0\n",
    "\n",
    "    while i < len(sequence_LA) and j < len(sequence_LB):\n",
    "        if sequence_LA[i] == sequence_LB[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "            correspondences += 1\n",
    "        elif sequence_LA[i][0] == sequence_LB[j][0]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "            score -= 0.1\n",
    "            #correspondences += 1\n",
    "        else:\n",
    "            score -= 0.5\n",
    "            if len(sequence_LA) < len(sequence_LB):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "    score -= 0.1 * (len(sequence_LA)-i + len(sequence_LB)-j)\n",
    "    return score, correspondences\n",
    "\n",
    "dic = defaultdict(lambda: set())\n",
    "\n",
    "# Iterate over each sequence in LA\n",
    "for index_LA, row_LA in df_LA.iterrows():\n",
    "    if row_LA[\"complete\"] and row_LA[\"sequence\"] not in dic:\n",
    "        for index_LB, row_LB in df_LB.iterrows():\n",
    "            if abs(row_LA[\"length\"] - row_LB[\"length\"]) <= 2:\n",
    "                score, correspondences = compare(row_LA['sequence'], row_LB['sequence'])\n",
    "                if score > 0.5 and correspondences >= 2:\n",
    "                    dic[row_LA['sequence']].add((row_LB['sequence'], score, row_LB[\"complete\"]))\n",
    "                    print(row_LA['sequence'], row_LB['sequence'], score, correspondences)\n",
    "\n",
    "for key, value in dic.items():\n",
    "    dic[key] = sorted(list(value), key=lambda x: x[1], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO-UEBvqCLfd"
   },
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CM84L7pqoj0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ALEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abbP_0CqtI2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "SEQUENCES_LA = os.path.join(prefix_path, \"sequences.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "seq_la = pd.read_csv(SEQUENCES_LA)\n",
    "\n",
    "# Print total number of sequences\n",
    "total_sequences = len(seq_la)\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "\n",
    "complete_sequences = seq_la['complete'].sum()\n",
    "print(f\"Sequences which are marked as complete: {complete_sequences}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSxP_IEKMxhN"
   },
   "outputs": [],
   "source": [
    "seq_la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6abhniMLNsL0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def reconstruct_LA_documents():\n",
    "    # Load the signs.csv file\n",
    "    signs_path = os.path.join(prefix_path, \"signs.csv\")\n",
    "    sequences_path = os.path.join(prefix_path, \"sequences.csv\")\n",
    "    signs_df = pd.read_csv(signs_path)\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    # Dictionary to hold reconstructed documents\n",
    "    documents = {}\n",
    "\n",
    "    # Group by 'document_name' and sort by 'sign_number'\n",
    "    grouped_signs = signs_df.sort_values(by=['document_name', 'sign_number']).groupby('document_name')\n",
    "    grouped_sequences = sequences_df.sort_values(by=['document_name', 'sequence_number']).groupby('document_name')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_signs:\n",
    "        documents[document_name] = []\n",
    "\n",
    "        # gather all sequences from the appropriate document\n",
    "        seq = grouped_sequences.get_group(document_name)['sequence'].tolist() if document_name in grouped_sequences.groups else []\n",
    "        for i in range(len(seq)):\n",
    "            seq[i] = seq[i].split(\"-\")\n",
    "\n",
    "        # Initialize variables\n",
    "        seq_idx = 0\n",
    "        seq_len = 0\n",
    "        stack = []\n",
    "        for sign in group['sign']:\n",
    "            # if the sequence is actually complete, we can put it in the document collection\n",
    "            if len(seq) > 0 and sign == seq[seq_idx][seq_len] and seq_len == len(seq[seq_idx])-1:\n",
    "                seq_len = 0\n",
    "                stack = []\n",
    "                documents[document_name].append(\"-\".join(seq[seq_idx]))\n",
    "                if seq_idx < len(seq)-1:\n",
    "                    seq_idx += 1\n",
    "            # we are completing the current sequence\n",
    "            elif len(seq) > 0 and sign == seq[seq_idx][seq_len]:\n",
    "                seq_len += 1\n",
    "                stack.append(sign)\n",
    "            # the symbol does not correspond to the current sequence, so we push the new symbol (and all those recognized from the end of the previous sequence)\n",
    "            else:\n",
    "                stack.append(sign)\n",
    "                documents[document_name] += stack\n",
    "                stack = []\n",
    "                seq_len = 0\n",
    "\n",
    "        documents[document_name] = \" \".join(documents[document_name])\n",
    "    return documents\n",
    "reconstruct_LA_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmQXz9kaLwe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = reconstruct_LA_documents()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with improved settings\n",
    "\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # Convert the document dictionary into a list of strings (concatenated words per document)\n",
    "    document_names = list(documents.keys())\n",
    "    corpus = list(documents.values())\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        token_pattern=r\"[a-zA-Z\\-\\*\\d]+\",  # Keep special characters including ?\n",
    "        min_df=1,  # Include all words\n",
    "        sublinear_tf=True,  # Prevent extreme weighting\n",
    "        lowercase=False  # Keep original case\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert TF-IDF matrix to a DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=document_names, columns=feature_names)\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "# Example dictionary (your processed documents)\n",
    "documents = reconstruct_LA_documents()\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidf_result = compute_tfidf(documents)\n",
    "\n",
    "# Display the results\n",
    "print(tfidf_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PArR6E3SElyC",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## UGARITIC/LB TASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7HM6nHTzj0w",
    "outputId": "a30635f1-5499-4263-b83c-7740085987ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: colorlog in ./.local/lib/python3.10/site-packages (6.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ortools in ./.local/lib/python3.10/site-packages (9.12.4544)\n",
      "Requirement already satisfied: absl-py>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ortools) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./.local/lib/python3.10/site-packages (from ortools) (1.23.5)\n",
      "Requirement already satisfied: pandas>=2.0.0 in ./.local/lib/python3.10/site-packages (from ortools) (2.3.1)\n",
      "Requirement already satisfied: protobuf<5.30,>=5.29.3 in ./.local/lib/python3.10/site-packages (from ortools) (5.29.5)\n",
      "Requirement already satisfied: immutabledict>=3.0.0 in ./.local/lib/python3.10/site-packages (from ortools) (4.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->ortools) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.0.0->ortools) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=2.0.0->ortools) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->ortools) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: enlighten in ./.local/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: blessed>=1.17.7 in ./.local/lib/python3.10/site-packages (from enlighten) (1.21.0)\n",
      "Requirement already satisfied: prefixed>=0.3.2 in ./.local/lib/python3.10/site-packages (from enlighten) (0.9.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/lib/python3/dist-packages (from blessed>=1.17.7->enlighten) (0.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: treelib in ./.local/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from treelib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rapidfuzz in ./.local/lib/python3.10/site-packages (3.13.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install colorlog\n",
    "!pip install ortools\n",
    "!pip install enlighten\n",
    "!pip install treelib\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v60bw2n1bA3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 03:32:43.451254: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-03 03:32:44.161860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "from functools import wraps\n",
    "from inspect import signature\n",
    "from collections import defaultdict, namedtuple\n",
    "from collections.abc import Callable, Hashable\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from prettytable import PrettyTable as pt\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from colorlog import TTYColoredFormatter\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "import torch.sparse\n",
    "import itertools\n",
    "from ortools.graph.python import min_cost_flow as mcf\n",
    "import types\n",
    "import enlighten\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from treelib import Tree\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from rapidfuzz.process import cdist\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMzlfXi5ZBXi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## To RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VByz0YR6e0wl"
   },
   "source": [
    "### ArgLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L0EpEUWAe3Il"
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_properties(*names):\n",
    "    \"\"\"Add properties as methods to classes.\"\"\"\n",
    "\n",
    "    def decorator(cls):\n",
    "        for name in names:\n",
    "            # NOTE The keyword is necessary.\n",
    "            setattr(cls, name, property(lambda self, name=name: getattr(self, f'_{name}')))\n",
    "        return cls\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def set_properties(*names, **values):\n",
    "    \"\"\"Set a private variable and use it as a property for an instance.\"\"\"\n",
    "\n",
    "    def decorator(self):\n",
    "        for name in names:\n",
    "            setattr(self, f'_{name}', values[name])\n",
    "\n",
    "        return self\n",
    "\n",
    "    return decorator\n",
    "\n",
    "#Decorator wraps old init witht he properties of *name\n",
    "\n",
    "def has_properties(*names):\n",
    "\n",
    "    def decorator(cls):\n",
    "        cls = add_properties(*names)(cls)\n",
    "        old_init = cls.__init__\n",
    "\n",
    "        @wraps(old_init)\n",
    "        def new_init(self, *args, **kwargs):\n",
    "\n",
    "            func_sig = signature(old_init)\n",
    "            bound = func_sig.bind(self, *args, **kwargs)\n",
    "            bound.apply_defaults()\n",
    "            all_args = bound.arguments\n",
    "            self = set_properties(*names, **all_args)(self)\n",
    "            old_init(self, *args, **kwargs)\n",
    "\n",
    "        cls.__init__ = new_init\n",
    "        return cls\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIa0iX34efK-"
   },
   "source": [
    "### DevMisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FPIdQz0IbF-v"
   },
   "outputs": [],
   "source": [
    "class Map(dict):\n",
    "\t\"\"\"\n",
    "\tAbandoned the original way of doing it:\n",
    "\t\tself.__dict__ = self\n",
    "\tThis introduces a self-reference and makes gc fail to collect garbage\n",
    "\tin time. As a result, pytorch can't properly empty cache if a tensor is\n",
    "\tstored in it.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __getattr__(self, key):\n",
    "\t\ttry:\n",
    "\t\t\treturn self[key]\n",
    "\t\texcept KeyError:\n",
    "\t\t\traise AttributeError\n",
    "\n",
    "\tdef __setattr__(self, key, value):\n",
    "\t\tself[key] = value\n",
    "\n",
    "\tdef update(self, *args, **kwargs):\n",
    "\t\t'''\n",
    "\t\tReturn self.\n",
    "\t\t'''\n",
    "\t\tsuper(Map, self).update(*args, **kwargs)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef apply(self, func, ignored=set()):\n",
    "\t\tfor key in self:\n",
    "\t\t\tif key in ignored:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif isinstance(self[key], Map):\n",
    "\t\t\t\tself[key].apply(func, ignored=ignored)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself[key] = func(self[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sVEjpkRKeTSS"
   },
   "outputs": [],
   "source": [
    "_CachedItem = namedtuple('_CachedItem', ['persist', 'value'])\n",
    "_CACHE = dict()\n",
    "_USE_CACHE = True\n",
    "\n",
    "def cache(full=True, persist=False):\n",
    "    global _USE_CACHE\n",
    "\n",
    "    def descriptor(func):\n",
    "        func_sig = signature(func)\n",
    "        def decorator(self, *args, **kwargs):\n",
    "            if not _USE_CACHE:\n",
    "                return func(self, *args, **kwargs)\n",
    "\n",
    "            bound = func_sig.bind(self, *args, **kwargs)\n",
    "            bound.apply_defaults()\n",
    "            items = [(k, v) for k, v in bound.arguments.items() if not isinstance(v, dict)]\n",
    "            arg_key = frozenset(items)\n",
    "            if full:\n",
    "                key = (id(self), func.__name__, arg_key)\n",
    "            else:\n",
    "                key = (id(self), func.__name__)\n",
    "            if key in _CACHE:\n",
    "                return _CACHE[key].value\n",
    "            else:\n",
    "                ret = func(self, *args, **kwargs)\n",
    "                _CACHE[key] = _CachedItem(persist, ret)\n",
    "                return ret\n",
    "\n",
    "        return decorator\n",
    "\n",
    "    return descriptor\n",
    "\n",
    "def clear_cache():\n",
    "\tglobal _CACHE\n",
    "\t# First pass to get keys to be removed.\n",
    "\tto_remove = list()\n",
    "\tfor k, item in _CACHE.items():\n",
    "\t\tif not item.persist:\n",
    "\t\t\tto_remove.append(k)\n",
    "\t# Now remove them.\n",
    "\tfor k in to_remove:\n",
    "\t\tdel _CACHE[k]\n",
    "\n",
    "def set_cache(flag):\n",
    "\tglobal _USE_CACHE\n",
    "\tassert flag in [True, False]\n",
    "\t_USE_CACHE = flag\n",
    "\n",
    "####################################### structured cache #################################\n",
    "\n",
    "class _StructuredCache:\n",
    "\t'''\n",
    "\tAssuming that the return is a Map object, this cache will selectively keep some\n",
    "\tattributes while removing the rest.\n",
    "\tYou can also dynamically select what to cache, useful when you want to perform analysis.\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# What should be kept.\n",
    "\t\tself._to_keep = dict()\n",
    "\t\t# What should be cached.\n",
    "\t\tself._to_cache = defaultdict(set)\n",
    "\t\t# What is cached. Note that each instance method has its own cache, keyed by the object's unique id.\n",
    "\t\tself.clear_cache()\n",
    "\t\t# The mapping from the object id to the object.\n",
    "\t\tself._id2obj = dict()\n",
    "\n",
    "\tdef keep(self, name, ret):\n",
    "\t\t'''Only keep what should be kept.'''\n",
    "\t\tif self._to_keep[name] is None:\n",
    "\t\t\treturn ret\n",
    "\t\telse:\n",
    "\t\t\treturn Map(**{k: ret[k] for k in self._to_keep[name] if k in ret})\n",
    "\n",
    "\tdef __contains__(self, key):\n",
    "\t\treturn key in self._to_keep\n",
    "\n",
    "\tdef register_keep(self, name, *to_keep):\n",
    "\t\t'''Record what to keep for each function.'''\n",
    "\t\tassert name not in self\n",
    "\t\tif len(to_keep) == 0:\n",
    "\t\t\tself._to_keep[name] = None # NOTE None means keeping everthing.\n",
    "\t\telse:\n",
    "\t\t\tself._to_keep[name] = set(to_keep)\n",
    "\n",
    "\tdef register_cache(self, name, *keys):\n",
    "\t\t'''Register cache for all instances of the same registered function with ``name``.'''\n",
    "\t\tself._to_cache[name].update(keys)\n",
    "\n",
    "\tdef cache(self, name, obj, ret):\n",
    "\t\tid_ = id(obj)\n",
    "\t\tif id_ not in self._id2obj:\n",
    "\t\t\tself._id2obj[id_] = obj\n",
    "\t\telse:\n",
    "\t\t\tassert self._id2obj[id_] is obj # Make sure it's the same object.\n",
    "\t\tfor k in self._to_cache[name]:\n",
    "\t\t\tassert k not in self._cache[name][id_]\n",
    "\t\t\tself._cache[name][id_][k] = ret[k]\n",
    "\n",
    "\tdef get_cache(self, name, *keys):\n",
    "\t\t'''Get all caches generated from the same function.'''\n",
    "\t\tret = list()\n",
    "\t\tfor id_ in self._cache[name]:\n",
    "\t\t\tobj = self._id2obj[id_]\n",
    "\t\t\tcache = self._cache[name][id_]\n",
    "\t\t\tret.append((obj, {k: cache[k] for k in keys}))\n",
    "\t\treturn ret\n",
    "\n",
    "\tdef clear_cache(self):\n",
    "\t\tself._cache = defaultdict(lambda: defaultdict(defaultdict))\n",
    "\n",
    "_SC = _StructuredCache()\n",
    "def sc(name, *to_keep):\n",
    "\tglobal _SC\n",
    "\n",
    "\tdef descriptor(func):\n",
    "\n",
    "\t\t@wraps(func)\n",
    "\t\tdef decorator(self, *args, **kwargs):\n",
    "\t\t\tret = func(self, *args, **kwargs)\n",
    "\t\t\tassert isinstance(ret, Map)\n",
    "\t\t\t_SC.cache(name, self, ret)\n",
    "\t\t\treturn _SC.keep(name, ret)\n",
    "\n",
    "\t\treturn decorator\n",
    "\n",
    "\t_SC.register_keep(name, *to_keep)\n",
    "\treturn descriptor\n",
    "\n",
    "def sc_clear_cache():\n",
    "\tglobal _SC\n",
    "\t_SC.clear_cache()\n",
    "\n",
    "def sc_register_cache(name, *keys):\n",
    "\tglobal _SC\n",
    "\t_SC.register_cache(name, *keys)\n",
    "\n",
    "def sc_get_cache(name, *keys):\n",
    "\tglobal _SC\n",
    "\treturn _SC.get_cache(name, *keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QcT12Yotfu-Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tensor(data, dtype=None, requires_grad=False, use_cuda=True):\n",
    "    use_cuda = os.environ.get('CUDA_VISIBLE_DEVICES', False) and use_cuda # NOTE only use cuda when it's not overriden and there is a device available\n",
    "\n",
    "    # If data is a tensor already, move to gpu if use_cuda\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        if use_cuda:\n",
    "            return data.cuda()\n",
    "        return data\n",
    "\n",
    "    if dtype is None: # NOTE infer dtype\n",
    "        dtype = 'f'\n",
    "        if isinstance(data, np.ndarray) and issubclass(data.dtype.type, np.integer):\n",
    "            dtype = 'l'\n",
    "\n",
    "    # NOTE directly declare data. I believe it's faster on cuda, although I'm not entirely sure\n",
    "    requires_grad = requires_grad and dtype == 'f'\n",
    "    assert dtype in ['f', 'l']\n",
    "    #if use_cuda:\n",
    "    #    module = getattr(torch, 'cuda')\n",
    "    #else:\n",
    "    #    module = torch\n",
    "    #if dtype == 'f':\n",
    "    #    cls = getattr(module, 'FloatTensor')\n",
    "    #    dtype = 'float32'\n",
    "    #elif dtype == 'l':\n",
    "    #    cls = getattr(module, 'LongTensor')\n",
    "    #    dtype = 'int64'\n",
    "    #ret = cls(np.asarray(data, dtype=dtype))\n",
    "    # Set PyTorch data type and device\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "    if dtype == 'f':\n",
    "        torch_dtype = torch.float32\n",
    "    elif dtype == 'l':\n",
    "        torch_dtype = torch.int64\n",
    "\n",
    "    # Create tensor properly\n",
    "    ret = torch.tensor(np.asarray(data, dtype=dtype), dtype=torch_dtype, device=device)\n",
    "\n",
    "    ret.requires_grad = requires_grad\n",
    "    return ret\n",
    "\n",
    "def get_zeros(*shape, **kwargs):\n",
    "    if len(shape) == 1 and isinstance(shape[0], torch.Size): # NOTE deal with 1D tensor whose shape cannot be unpacked\n",
    "        shape = list(shape[0])\n",
    "    return get_tensor(np.zeros(shape), **kwargs)\n",
    "\n",
    "def get_eye(n):\n",
    "    return get_tensor(np.eye(n))\n",
    "\n",
    "def counter(iterable, *args, max_size=0, interval=1000, **kwargs):\n",
    "    total = 0\n",
    "    for i, item in enumerate(iterable, *args, **kwargs):\n",
    "        yield item\n",
    "        total += 1\n",
    "        if total % interval == 0:\n",
    "            logging.debug(f'{total}')\n",
    "            sys.stdout.flush()\n",
    "        if max_size and total == max_size:\n",
    "            logging.info(f'Reached max size')\n",
    "            break\n",
    "    logging.debug(f'Finished enumeration of size {total}')\n",
    "\n",
    "def freeze(mod):\n",
    "    for p in mod.parameters():\n",
    "        p.requires_grad = False\n",
    "    for m in mod.children():\n",
    "        freeze(m)\n",
    "\n",
    "def sort_all(anchor, *others):\n",
    "    '''\n",
    "    Sort everything (``anchor`` and ``others``) in this based on the lengths of ``anchor``.\n",
    "    '''\n",
    "    # Check everything is an numpy array.\n",
    "    for a in (anchor, ) + others:\n",
    "        assert isinstance(a, np.ndarray)\n",
    "    #  Check everything has the same length in the first dimension.\n",
    "    l = len(anchor)\n",
    "    for o in others:\n",
    "        assert len(o) == l\n",
    "    # Sort by length.\n",
    "    lens = np.asarray([len(x) for x in anchor], dtype='int64')\n",
    "    inds = np.argsort(lens)[::-1]\n",
    "    # Return everything after sorting.\n",
    "    return [lens[inds]] + [anchor[inds]] + [o[inds] for o in others]\n",
    "\n",
    "def pprint_cols(data, num_cols=4):\n",
    "    t = pt()\n",
    "    num_rows = len(data) // num_cols + (len(data) % num_cols > 0)\n",
    "    for col in range(num_cols - 1):\n",
    "        t.add_column(f'Column:{col+1}', data[col * num_rows: (col + 1) * num_rows])\n",
    "    t.add_column(f'Column:{num_cols}', data[(num_cols - 1) * num_rows:] + [''] * ((num_rows - len(data) % num_rows) % num_rows))\n",
    "    t.align = 'l'\n",
    "    print(t)\n",
    "\n",
    "def check(t):\n",
    "    if (torch.isnan(t).any() | torch.isinf(t).any()).item():\n",
    "        breakpoint()\n",
    "\n",
    "def canonicalize(shape, dim):\n",
    "    if dim < 0:\n",
    "        return len(shape) + dim\n",
    "    else:\n",
    "        return dim\n",
    "\n",
    "def divide(tensor, dim, div_shape):\n",
    "    prev_shape = tensor.shape\n",
    "    dim = canonicalize(prev_shape, dim)\n",
    "    if -1 not in div_shape:\n",
    "        total = reduce(mul, div_shape, 1)\n",
    "        assert total == prev_shape[dim]\n",
    "    new_shape = prev_shape[:dim] + tuple(div_shape) + prev_shape[dim + 1:]\n",
    "    return tensor.view(*new_shape)\n",
    "\n",
    "def merge(tensor, dims):\n",
    "    prev_shape = tensor.shape\n",
    "    dims = [canonicalize(prev_shape, dim) for dim in dims]\n",
    "    for a, b in zip(dims[:-1], dims[1:]):\n",
    "        assert b == a + 1\n",
    "    total = reduce(mul, [prev_shape[d] for d in dims], 1)\n",
    "    new_shape = prev_shape[:dims[0]] + (total, ) + prev_shape[dims[-1] + 1:]\n",
    "    return tensor.view(*new_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5VBhRVn6zdKG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from MUSE\n",
    "'''\n",
    "\n",
    "\n",
    "# From https://stackoverflow.com/questions/2183233/how-to-add-a-custom-loglevel-to-pythons-logging-facility.\n",
    "def addLoggingLevel(levelName, levelNum, methodName=None):\n",
    "    \"\"\"\n",
    "    Comprehensively adds a new logging level to the `logging` module and the\n",
    "    currently configured logging class.\n",
    "\n",
    "    `levelName` becomes an attribute of the `logging` module with the value\n",
    "    `levelNum`. `methodName` becomes a convenience method for both `logging`\n",
    "    itself and the class returned by `logging.getLoggerClass()` (usually just\n",
    "    `logging.Logger`). If `methodName` is not specified, `levelName.lower()` is\n",
    "    used.\n",
    "\n",
    "    To avoid accidental clobberings of existing attributes, this method will\n",
    "    raise an `AttributeError` if the level name is already an attribute of the\n",
    "    `logging` module or if the method name is already present\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> addLoggingLevel('TRACE', logging.DEBUG - 5)\n",
    "    >>> logging.getLogger(__name__).setLevel(\"TRACE\")\n",
    "    >>> logging.getLogger(__name__).trace('that worked')\n",
    "    >>> logging.trace('so did this')\n",
    "    >>> logging.TRACE\n",
    "    5\n",
    "\n",
    "    \"\"\"\n",
    "    if not methodName:\n",
    "        methodName = levelName.lower()\n",
    "\n",
    "    #if hasattr(logging, levelName):\n",
    "    #    raise AttributeError('{} already defined in logging module'.format(levelName))\n",
    "    #if hasattr(logging, methodName):\n",
    "    #    raise AttributeError('{} already defined in logging module'.format(methodName))\n",
    "    #if hasattr(logging.getLoggerClass(), methodName):\n",
    "    #    raise AttributeError('{} already defined in logger class'.format(methodName))\n",
    "\n",
    "    # This method was inspired by the answers to Stack Overflow post\n",
    "    # http://stackoverflow.com/q/2183233/2988730, especially\n",
    "    # http://stackoverflow.com/a/13638084/2988730\n",
    "    def logForLevel(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(levelNum):\n",
    "            self._log(levelNum, message, args, **kwargs)\n",
    "\n",
    "    def logToRoot(message, *args, **kwargs):\n",
    "        logging.log(levelNum, message, *args, **kwargs)\n",
    "\n",
    "    logging.addLevelName(levelNum, levelName)\n",
    "    setattr(logging, levelName, levelNum)\n",
    "    setattr(logging.getLoggerClass(), methodName, logForLevel)\n",
    "    setattr(logging, methodName, logToRoot)\n",
    "\n",
    "\n",
    "addLoggingLevel('IMP', 25)\n",
    "\n",
    "\n",
    "class LogFormatter(TTYColoredFormatter):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):  # , color=False):\n",
    "        fmt = '%(log_color)s%(levelname)s - %(time)s - %(elapsed)s at %(filename)s:%(lineno)d - %(message)s'\n",
    "        super(LogFormatter, self).__init__(\n",
    "            fmt,\n",
    "            log_colors={\n",
    "                'DEBUG': 'white',\n",
    "                'INFO': 'green',\n",
    "                'IMP': 'cyan',\n",
    "                'WARNING': 'yellow',\n",
    "                'ERROR': 'red',\n",
    "                'CRITICAL': 'red,bg_white'},\n",
    "            *args,\n",
    "            **kwargs)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def format(self, record):\n",
    "        # only need to set timestamps once -- all changes are stored in the record object\n",
    "        if not hasattr(record, 'elapsed'):\n",
    "            record.elapsed = timedelta(seconds=round(record.created - self.start_time))\n",
    "            record.time = time.strftime('%x %X')\n",
    "            # if self.colored:\n",
    "            prefix = \"%s - %s - %s at %s:%d\" % (\n",
    "                record.levelname,\n",
    "                record.time,\n",
    "                record.elapsed,\n",
    "                record.filename,\n",
    "                record.lineno\n",
    "            )\n",
    "            message = record.getMessage()\n",
    "            # If a message starts with a line break, we will keep the original line break without autoindentation.\n",
    "            if not message.startswith('\\n'):\n",
    "                message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n",
    "            record.msg = message\n",
    "            record.args = ()  # NOTE avoid evaluating the message again duing getMessage call.\n",
    "        x = super(LogFormatter, self).format(record)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_logger(filepath=None, log_level='INFO'):\n",
    "    \"\"\"\n",
    "    Create a logger.\n",
    "    \"\"\"\n",
    "    # create console handler and set level to info\n",
    "    console_handler = logging.StreamHandler()\n",
    "    # create log formatter\n",
    "    colorlog_formatter = LogFormatter(stream=console_handler.stream)\n",
    "    console_handler.setLevel(getattr(logging, log_level))\n",
    "    console_handler.setFormatter(colorlog_formatter)\n",
    "\n",
    "    # create logger and set level to debug\n",
    "    logger = logging.getLogger()\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(log_level)\n",
    "    logger.propagate = False\n",
    "    logger.addHandler(console_handler)\n",
    "    if filepath:\n",
    "        # create file handler and set level to debug\n",
    "        file_handler = logging.FileHandler(filepath, \"a\")\n",
    "        file_handler.setLevel(log_level)\n",
    "        log_formatter = LogFormatter(stream=file_handler.stream)\n",
    "        file_handler.setFormatter(log_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    # reset logger elapsed time\n",
    "    def reset_time():\n",
    "        log_formatter.start_time = time.time()\n",
    "    logger.reset_time = reset_time\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def log_this(log_level='DEBUG', msg='', arg_list=None):\n",
    "    \"\"\"\n",
    "    A decorator that logs the functionality, the beginning and the end of the function.\n",
    "    It can optionally print out arg values in arg_list.\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        new_msg = msg or func.__name__\n",
    "        new_arg_list = arg_list or list()\n",
    "        def log_func(msg): return logging.log(getattr(logging, log_level), msg)\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            log_func(f'*STARTING* {new_msg}')\n",
    "\n",
    "            if new_arg_list:\n",
    "\n",
    "                func_sig = signature(func)\n",
    "                bound = func_sig.bind(*args, **kwargs)\n",
    "                bound.apply_defaults()\n",
    "                all_args = bound.arguments\n",
    "\n",
    "                arg_msg = {name: all_args[name] for name in new_arg_list}\n",
    "                log_func(f'*ARG_LIST* {arg_msg}')\n",
    "\n",
    "            ret = func(*args, **kwargs)\n",
    "            log_func(f'*FINISHED* {new_msg}')\n",
    "            return ret\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def log_pp(obj):\n",
    "    '''\n",
    "    Log ``obj`` with better indentations.\n",
    "    '''\n",
    "    logging.info(('\\n' + str(obj)).replace('\\n', '\\n' + ' ' * 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6kMrdDBXMgTJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plain(value):\n",
    "    '''Convert tensors or numpy arrays to one scalar.'''\n",
    "    # Get to str, int or float first.\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        assert value.numel() == 1\n",
    "        value = value.item()\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        assert value.size == 1\n",
    "        value = value[0]\n",
    "    # Format it nicely.\n",
    "    if isinstance(value, (str, int)):\n",
    "        value = value\n",
    "    elif isinstance(value, float):\n",
    "        value = float(f'{value:.3f}')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return value\n",
    "\n",
    "\n",
    "class Metric:\n",
    "\n",
    "    def __init__(self, name, value, weight, report_mean=True):\n",
    "        self.name = name\n",
    "        self._v = value\n",
    "        self._w = weight\n",
    "        self._report_mean = report_mean\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.report_mean:\n",
    "            return f'{plain(self._v)}/{plain(self._w)}={plain(self.mean)}'\n",
    "        else:\n",
    "            return f'{plain(self.total)}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Metric(name={self.name}, report_mean={self.report_mean})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Metric):\n",
    "            assert self == other, 'Cannot add two different metrics.'\n",
    "            assert self.report_mean == other.report_mean\n",
    "            return Metric(self.name, self._v + other._v, self._w + other._w, report_mean=self.report_mean)\n",
    "        else:\n",
    "            # NOTE This is useful for sum() call.\n",
    "            assert isinstance(other, (int, float)) and other == 0\n",
    "            return self\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def rename(self, name):\n",
    "        '''This is in-place.'''\n",
    "        self.name = name\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._v\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self._w if self.report_mean else 'N/A'\n",
    "\n",
    "    @property\n",
    "    def report_mean(self):\n",
    "        return self._report_mean\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        if self.report_mean:\n",
    "            return self._v / self._w\n",
    "        else:\n",
    "            return 'N/A'\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self._v\n",
    "\n",
    "    def clear(self):\n",
    "        self._v = 0\n",
    "        self._w = 0\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    def __init__(self, *metrics):\n",
    "        # Check all of metrics are of the same type. Either all str or all Metric.\n",
    "        types = set([type(m) for m in metrics])\n",
    "        assert len(types) <= 1\n",
    "\n",
    "        if len(types) == 1:\n",
    "            if types.pop() is str:\n",
    "                self._metrics = {k: Metric(k, 0, 0) for k in keys}\n",
    "            else:\n",
    "                self._metrics = {metric.name: metric for metric in metrics}\n",
    "        else:\n",
    "            self._metrics = dict()\n",
    "\n",
    "    def __str__(self):\n",
    "        out = '\\n'.join([f'{k}: {m}' for k, m in self._metrics.items()])\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Metrics({\", \".join(self._metrics.keys())})'\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Metric):\n",
    "            other = Metrics(other)\n",
    "        union_keys = set(self._metrics.keys()) | set(other._metrics.keys())\n",
    "        metrics = list()\n",
    "        for k in union_keys:\n",
    "            m1 = self._metrics.get(k, 0)\n",
    "            m2 = other._metrics.get(k, 0)\n",
    "            metrics.append(m1 + m2)\n",
    "        return Metrics(*metrics)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return super().__getattribute__('_metrics')[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f'Cannot find this attribute {key}')\n",
    "\n",
    "    def get_table(self, title=''):\n",
    "        t = pt()\n",
    "        if title:\n",
    "            t.title = title\n",
    "        t.field_names = 'name', 'value', 'weight', 'mean'\n",
    "        for k in sorted(self._metrics.keys()):\n",
    "            metric = self._metrics[k]\n",
    "            t.add_row([k, plain(metric.value), plain(metric.weight), plain(metric.mean)])\n",
    "        t.align = 'l'\n",
    "        return t\n",
    "\n",
    "    def clear(self):\n",
    "        for m in self._metrics.values():\n",
    "            m.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XMQcRjGPM743"
   },
   "outputs": [],
   "source": [
    "_manager = enlighten.get_manager()\n",
    "_stage_names = set()\n",
    "\n",
    "def clear_stages():\n",
    "    global _manager\n",
    "    global _stage_names\n",
    "    _manager = enlighten.get_manager()\n",
    "    _stage_names = set()\n",
    "\n",
    "def _check_name(name):\n",
    "    assert name not in _stage_names\n",
    "    _stage_names.add(name)\n",
    "\n",
    "\n",
    "def _reset_pbar(pbar):\n",
    "    pbar.count = 0\n",
    "    pbar.start = time.time()\n",
    "\n",
    "\n",
    "@has_properties('name', 'num_steps', 'parent')\n",
    "class _Stage:\n",
    "\n",
    "    def __init__(self, name, num_steps=1, parent=None):\n",
    "        _check_name(name)\n",
    "\n",
    "        self._pbars = dict()\n",
    "        self.substages = list()\n",
    "        if self.num_steps > 1:\n",
    "            self.add_pbar(name, total=self.num_steps)\n",
    "\n",
    "    def update_pbars(self):\n",
    "        for pbar in self._pbars.values():\n",
    "            if pbar.total == pbar.count:\n",
    "                _reset_pbar(pbar)\n",
    "            pbar.update()\n",
    "\n",
    "    def reset_pbars(self, recursive=False):\n",
    "        for pbar in self._pbars.values():\n",
    "            _reset_pbar(pbar)\n",
    "        if recursive:\n",
    "            for substage in self.substages:\n",
    "                substage.reset_pbars(recursive=True)\n",
    "\n",
    "    def add_pbar(self, name, total=None, unit='samples'):\n",
    "        if name in self._pbars:\n",
    "            raise NameError(f'Name {name} already exists.')\n",
    "        pbar = _manager.counter(\n",
    "            desc=name,\n",
    "            total=total,\n",
    "            unit=unit,\n",
    "            leave=False)\n",
    "        pbar.refresh()\n",
    "        self._pbars[name] = pbar\n",
    "\n",
    "    def add_stage(self, name, num_steps=1):\n",
    "        stage = _Stage(name, num_steps=num_steps, parent=self)\n",
    "        self.substages.append(stage)\n",
    "        return stage\n",
    "\n",
    "    # def adjoin_stage(self, stage):\n",
    "    #     assert isinstance(stage, _Stage)\n",
    "    #     self.substages.append(stage)\n",
    "    #     return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\"{self.name}\"'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Stage(name={self.name}, num_steps={self.num_steps})'\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        missing = list()\n",
    "        for name, pbar_meta in state_dict['_pbars'].items():\n",
    "            try:\n",
    "                pbar = self._pbars[name]\n",
    "            except KeyError:\n",
    "                missing.append(f'pbar:{name}')\n",
    "                continue\n",
    "            pbar.count = pbar_meta['count']\n",
    "            pbar.refresh()\n",
    "        for s1, s2 in zip(self.substages, state_dict['_stages']):\n",
    "            s1.load_state_dict(s2)\n",
    "        if missing:\n",
    "            raise RuntimeError(f'Missing {missing}')\n",
    "\n",
    "    def state_dict(self):\n",
    "        ret = dict()\n",
    "        # NOTE pbar itself cannot be serialized for some reason.\n",
    "        ret['_pbars'] = {name: {'count': pbar.count} for name, pbar in self._pbars.items()}\n",
    "        stage_ret = list()\n",
    "        for s in self.substages:\n",
    "            stage_ret.append(s.state_dict())\n",
    "        ret['_stages'] = stage_ret\n",
    "        return ret\n",
    "\n",
    "\n",
    "@has_properties('step', 'substage_idx')\n",
    "class _Node:\n",
    "    \"\"\"A wrapper of stage that contains step and substage_idx information.\"\"\"\n",
    "\n",
    "    def __init__(self, stage, step, substage_idx):\n",
    "        self.stage = stage\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.stage.name\n",
    "\n",
    "    def is_last(self):\n",
    "        last_step = (self.step == self.stage.num_steps - 1)\n",
    "        if self.stage.substages:\n",
    "            last_substage = (self.substage_idx == len(self.stage.substages) - 1)\n",
    "            last = last_substage and last_step\n",
    "        else:\n",
    "            last = last_step\n",
    "        return last\n",
    "\n",
    "    def next_node(self):\n",
    "        \"\"\"Return whether next node will increment the step.\"\"\"\n",
    "        if self.stage.substages:\n",
    "            new_substage_idx = self.substage_idx + 1\n",
    "            incremented = False\n",
    "            if new_substage_idx == len(self.stage.substages):\n",
    "                new_substage_idx = 0\n",
    "                incremented = True\n",
    "            new_step = self.step + incremented\n",
    "            return _Node(self.stage, new_step, new_substage_idx), incremented\n",
    "        else:\n",
    "            return _Node(self.stage, self.step + 1, None), True\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.stage}: {self.step}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Node(stage={str(self.stage)}, step={self.step}, substage_idx={self.substage_idx})'\n",
    "\n",
    "\n",
    "class _Path:\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        self._nodes = list()\n",
    "        self._nodes_dict = dict()\n",
    "        self._schedule = schedule\n",
    "        self._get_first_path(self._schedule)\n",
    "        self._finished = False\n",
    "\n",
    "    def _add(self, node):\n",
    "        # Check that this is a valid extension of the original path.\n",
    "        if len(self._nodes) == 0:\n",
    "            safe = True\n",
    "        else:\n",
    "            last_node = self._nodes[-1]\n",
    "            safe = last_node.stage.substages[last_node.substage_idx] is node.stage\n",
    "        assert safe\n",
    "        # Add it.\n",
    "        self._nodes.append(node)\n",
    "        self._nodes_dict[node.stage.name] = node.step\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = ' -> '.join([str(node) for node in self._nodes])\n",
    "        return ret\n",
    "\n",
    "    def _get_first_path(self, stage_or_node):\n",
    "\n",
    "        def helper(stage_or_node):\n",
    "            if isinstance(stage_or_node, _Stage):\n",
    "                stage = stage_or_node\n",
    "                if stage.substages:\n",
    "                    self._add(_Node(stage, 0, 0))\n",
    "                    helper(stage.substages[0])\n",
    "                else:\n",
    "                    self._add(_Node(stage, 0, None))  # None means there is no substage.\n",
    "            else:\n",
    "                assert isinstance(stage_or_node, _Node)\n",
    "                node = stage_or_node\n",
    "                if node.stage.substages:\n",
    "                    new_node = _Node(node.stage, node.step, node.substage_idx)\n",
    "                    self._add(new_node)\n",
    "                    child_node = _Node(new_node.stage.substages[new_node.substage_idx], 0, 0)\n",
    "                    helper(child_node)\n",
    "                else:\n",
    "                    self._add(node)\n",
    "\n",
    "        helper(stage_or_node)\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._finished\n",
    "\n",
    "    def next_path(self):\n",
    "        \"\"\"Note that this is in-place. It returns the nodes incremented.\"\"\"\n",
    "        # First backtrack to the first ancestor that hasn't been completed yet.\n",
    "        assert not self._finished\n",
    "        i = len(self._nodes)\n",
    "        while i > 0:\n",
    "            i -= 1\n",
    "            last_node = self._nodes[i]\n",
    "            if not last_node.is_last():\n",
    "                break\n",
    "        # Now complete it.\n",
    "        if last_node.is_last():\n",
    "            self._finished = True\n",
    "            affected_nodes = self._nodes[1:]\n",
    "        else:\n",
    "            affected_nodes = self._nodes[i + 1:]  # NOTE Everything that is last will be incremented.\n",
    "            self._nodes = self._nodes[:i]\n",
    "            next_node, incremented = last_node.next_node()\n",
    "            if incremented:\n",
    "                affected_nodes.append(next_node)\n",
    "            self._get_first_path(next_node)\n",
    "        return affected_nodes\n",
    "\n",
    "    @property\n",
    "    def leaf_node(self):\n",
    "        return self._nodes[-1]\n",
    "\n",
    "    def get_step(self, key):\n",
    "        return self._nodes_dict[key]\n",
    "\n",
    "\n",
    "class _Schedule(_Stage):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name, num_steps=1)\n",
    "        self._path = None\n",
    "\n",
    "    def _build_path(self):\n",
    "        self._path = _Path(self)\n",
    "\n",
    "    def update(self):\n",
    "        affected_nodes = self._path.next_path()\n",
    "\n",
    "        for node in affected_nodes:\n",
    "            node.stage.update_pbars()\n",
    "\n",
    "    def as_tree(self):\n",
    "        tree = Tree()  # NOTE Store the tree structure for treelib.\n",
    "        tree.create_node(repr(self), id(self))\n",
    "\n",
    "        def helper(stage):\n",
    "            for substage in stage.substages:\n",
    "                tree.create_node(repr(substage), id(substage), parent=id(stage))\n",
    "                helper(substage)\n",
    "\n",
    "        helper(self)\n",
    "\n",
    "        sys.stdout = io.StringIO()\n",
    "        tree.show()\n",
    "        output = sys.stdout.getvalue()\n",
    "        sys.stdout = sys.__stdout__\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def current_stage(self):\n",
    "        return self._path.leaf_node\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._path.finished\n",
    "\n",
    "    def get_step(self, key):\n",
    "        return self._path.get_step(key)\n",
    "\n",
    "    def fix_schedule(self):\n",
    "        self._build_path()\n",
    "\n",
    "    def reset(self):\n",
    "        self._build_path()\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.clear_best()\n",
    "        self._schedule = _Schedule(name)\n",
    "        self._metrics = Metrics()\n",
    "\n",
    "    def schedule_as_tree(self):\n",
    "        return self._schedule.as_tree()\n",
    "\n",
    "    @property\n",
    "    def schedule(self):\n",
    "        return self._schedule\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self._metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._schedule.reset()\n",
    "\n",
    "    def add_stage(self, name, num_steps=1):\n",
    "        return self._schedule.add_stage(name, num_steps=num_steps)\n",
    "\n",
    "    def clear_best(self):\n",
    "        self.best_score = None\n",
    "        self.best_stage = None\n",
    "\n",
    "    def check_metrics(self, epoch):\n",
    "        log_pp(self._metrics.get_table(title=f'Epoch: {epoch}'))\n",
    "\n",
    "    def clear_metrics(self):\n",
    "        self._metrics.clear()\n",
    "\n",
    "    def update_metrics(self, metrics):\n",
    "        self._metrics += metrics\n",
    "\n",
    "    def state_dict(self):\n",
    "        ret = {'_schedule': self._schedule.state_dict(), 'best_score': self.best_score,\n",
    "               'best_stage': self.best_stage, '_metrics': self._metrics}\n",
    "        return ret\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        #self._schedule.load_state_dict(state_dict['_schedule'])\n",
    "        self.best_score = state_dict['best_score']\n",
    "        self.best_stage = state_dict['best_stage']\n",
    "        self._metrics = state_dict['_metrics']\n",
    "        round_num, last_step = extract_round_and_last_step(state_dict['_schedule'])\n",
    "        #logging.critical(state_dict['_schedule'])\n",
    "        #logging.critical(f\"{round_num}, {last_step}\")\n",
    "        #self.skip_to_checkpoint_progress(round_num, last_step)\n",
    "        while not (self.get('round') == round_num):\n",
    "            self._schedule.update()\n",
    "            #logging.critical(f\"{self.current_stage.name}, {self.current_stage.step}\")\n",
    "        # Skip to last M-step inside that round if needed\n",
    "        while not (self.current_stage.name == \"M step\" and self.current_stage.step == last_step):\n",
    "            self._schedule.update()\n",
    "            #logging.critical(f\"{self.current_stage.name}, {self.current_stage.step}\")\n",
    "        self._schedule.update()\n",
    "        #logging.critical(f\"{self.current_stage}, {self.get('round') }\")\n",
    "\n",
    "\n",
    "    def update_best(self, score, mode='min', quiet=False):\n",
    "        \"\"\"Update the best score and best stage.\n",
    "\n",
    "        Args:\n",
    "            score: score for the current stage\n",
    "            mode (str, optional): take the maximum or the minimum as the best score. Defaults to 'min'.\n",
    "            quiet (bool, optional): flag to suppress outputting the best score. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            updated (bool): whether the best score has been updated or not\n",
    "        \"\"\"\n",
    "        score = plain(score)\n",
    "        updated = False\n",
    "\n",
    "        def should_update():\n",
    "            if score is None:\n",
    "                return False\n",
    "            if self.best_score is None:\n",
    "                return True\n",
    "            if mode == 'max' and self.best_score < score:\n",
    "                return True\n",
    "            if mode == 'min' and self.best_score > score:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        updated = should_update()\n",
    "        if updated:\n",
    "            self.best_score = score\n",
    "            self.best_stage = str(self.current_stage)\n",
    "        if self.best_score is not None and not quiet:\n",
    "            logging.info(f'Best score is {self.best_score:.3f} at stage {self.best_stage}')\n",
    "        return updated\n",
    "\n",
    "    def update(self):\n",
    "        self._schedule.update()\n",
    "\n",
    "    @property\n",
    "    def current_stage(self):\n",
    "        return self._schedule.current_stage\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._schedule.finished\n",
    "\n",
    "    def get(self, key):\n",
    "        return self._schedule.get_step(key)\n",
    "\n",
    "    def fix_schedule(self):\n",
    "        self._schedule.fix_schedule()\n",
    "\n",
    "    def reset_pbars(self):\n",
    "        self._schedule.reset_pbars(recursive=True)\n",
    "\n",
    "\n",
    "def extract_round_and_last_step(schedule_state_dict):\n",
    "    try:\n",
    "        outer_stage = schedule_state_dict['_stages'][0]\n",
    "        round_count = outer_stage['_pbars']['round']['count']\n",
    "\n",
    "        # Assuming the last executed step is inside substages\n",
    "        substages = outer_stage['_stages']\n",
    "        last_step = None\n",
    "        for sub in substages:\n",
    "            if 'M step' in sub['_pbars']:\n",
    "                last_step = sub['_pbars']['M step']['count']\n",
    "                break\n",
    "\n",
    "        return round_count, last_step\n",
    "    except (KeyError, IndexError) as e:\n",
    "        raise ValueError(\"Invalid schedule state_dict format\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qubc_YVfkZ2"
   },
   "source": [
    "### CognateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4l3bbdigfnnB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CognateSet:\n",
    "    '''\n",
    "    Similar to the concept of synset in wordnet. Each cognate set contains words that are cognates with each other.\n",
    "    If A is cognate to B1 and B2, then all of them will be stored here.\n",
    "    Note that this doesn't gurantee that B1 and B2 are semantically similar. The word \"bank\" might be cognate to\n",
    "    two totally different words in another language. In other words, while the cognate relation across languages are\n",
    "    perserved here, nothing can be definitely said wrt the relation within the same language.\n",
    "    '''\n",
    "    IDX = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data = defaultdict(set)\n",
    "        self.idx = CognateSet.IDX\n",
    "        CognateSet.IDX += 1\n",
    "\n",
    "    def add(self, lang, *words):\n",
    "        words = [w for w in words if w != '_']\n",
    "        if words:\n",
    "            self._data[lang].update(words)  # '_' is a placeholder.\n",
    "\n",
    "    def is_in(self, word, lang):\n",
    "        if lang not in self._data:  # Do this to avoid spurious keys for defaultdict.\n",
    "            return False\n",
    "        return word in self._data[lang]\n",
    "\n",
    "    def __contains__(self, lang):\n",
    "        return lang in self._data\n",
    "\n",
    "    def items(self):\n",
    "        return self._data.items()\n",
    "\n",
    "    def __getitem__(self, lang):\n",
    "        if not lang in self:\n",
    "            raise KeyError\n",
    "        else:\n",
    "            return self._data[lang]\n",
    "\n",
    "    def to_df(self):\n",
    "        data = list()\n",
    "        for l, s in self._data.items():\n",
    "            for w in s:\n",
    "                data.append((w, l, self.idx))\n",
    "        return pd.DataFrame(data, columns=['word', 'lang', 'idx'])\n",
    "\n",
    "\n",
    "class CognateDict:\n",
    "    '''\n",
    "    A big dictionary that stores every cognate set. This works by storing many CognateSet's, and each\n",
    "    word is then mapped to one set.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, langs):\n",
    "        self._cs = dict()\n",
    "        self._langs = langs\n",
    "        # For each language, map a word to the CognateSet's it is in.\n",
    "        self._keys = {l: defaultdict(list) for l in langs}\n",
    "\n",
    "    @property\n",
    "    def langs(self):\n",
    "        return self._langs\n",
    "\n",
    "    def add(self, *cognate_sets):\n",
    "        for cognate_set in cognate_sets:\n",
    "            for lang, words in cognate_set.items():\n",
    "                for w in words:\n",
    "                    self._keys[lang][w].append(cognate_set.idx)\n",
    "            self._cs[cognate_set.idx] = cognate_set\n",
    "\n",
    "    def find(self, word, lang):\n",
    "        if word not in self._keys[lang]:\n",
    "            raise KeyError(f'Word {word} in language {lang} not in the dictionary.')\n",
    "\n",
    "        ret = defaultdict(set)\n",
    "        for idx in self._keys[lang][word]:\n",
    "            cs = self._cs[idx]\n",
    "            for l, words in cs.items():\n",
    "                ret[l].update(words)\n",
    "        return ret\n",
    "\n",
    "    @cache(persist=True, full=False)\n",
    "    def to_df(self):\n",
    "        dfs = [cs.to_df() for cs in self._cs.values()]\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def get_wordlist(self, lang):\n",
    "        df = self.to_df()\n",
    "        return sorted(set(df[df['lang'] == lang]['word']))\n",
    "\n",
    "\n",
    "class CognateList:\n",
    "    '''\n",
    "    List of cognates, possibly with noncognates as well. This is the main class used for the stream object.\n",
    "    '''\n",
    "\n",
    "#    def __init__(self, cognate_path, lost_lang, known_lang, max_size=0, only_valid=True):\n",
    "#        self.all_langs = set([lost_lang, known_lang])\n",
    "#\n",
    "#        cognates = list()\n",
    "#        with Path(cognate_path).open(encoding='utf8') as fcog:\n",
    "#            header = fcog.readline().strip().split(\"\\t\")\n",
    "#            # transliterated_linear_b, greek\n",
    "#            header_langs = [header[0], header[1]]\n",
    "#\n",
    "#            for line in counter(fcog, max_size=max_size):\n",
    "#                tokens = line.strip().split('\\t')\n",
    "#                is_valid = tokens[3] == '1'\n",
    "#                tokens = [tokens[0], tokens[2]]\n",
    "#                if is_valid or not only_valid:\n",
    "#                    cog = CognateSet()\n",
    "#                    for l, t in zip(header_langs, tokens):\n",
    "#                        if l in self.all_langs:\n",
    "#                            cog.add(l, *t.split('|'))\n",
    "#                    cognates.append(cog)\n",
    "#\n",
    "#\n",
    "#        self._cognates = cognates\n",
    "#        self._cognate_dict = CognateDict(self.all_langs)\n",
    "#        self._cognate_dict.add(*cognates)\n",
    "\n",
    "    def __init__(self, cognate_path, lost_lang, known_lang, max_size=0):\n",
    "        self.all_langs = set([lost_lang, known_lang])\n",
    "\n",
    "        cognates = list()\n",
    "        with Path(cognate_path).open(encoding='utf8') as fcog:\n",
    "            header_langs = fcog.readline().strip().split(\"\\t\")\n",
    "            for line in counter(fcog, max_size=max_size):\n",
    "                tokens = line.strip().split('\\t')\n",
    "                cog = CognateSet()\n",
    "                for l, t in zip(header_langs, tokens):\n",
    "                    if l in self.all_langs:\n",
    "                        cog.add(l, *t.split('|'))\n",
    "                cognates.append(cog)\n",
    "        self._cognates = cognates\n",
    "        self._cognate_dict = CognateDict(self.all_langs)\n",
    "        self._cognate_dict.add(*cognates)\n",
    "\n",
    "    def get_wordlist(self, lang):\n",
    "        return self._cognate_dict.get_wordlist(lang)\n",
    "\n",
    "    def has_cognate(self, w, lang):\n",
    "        cs = self._cognate_dict.find(w.form, w.lang)\n",
    "        return lang in cs\n",
    "\n",
    "    def is_cognate(self, w1, w2):\n",
    "        cs = self._cognate_dict.find(w1.form, w1.lang)\n",
    "        if w2.lang in cs:\n",
    "            return w2.form in cs[w2.lang]\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THVkwEMUesYG"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QMDkd0R4d_ku"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_VOCABS = dict()\n",
    "_COG_LIST = None\n",
    "\n",
    "\n",
    "def get_vocab(lang):\n",
    "    return _VOCABS[lang]\n",
    "\n",
    "\n",
    "def get_vocab_size(lang):\n",
    "    return len(get_vocab(lang))\n",
    "\n",
    "\n",
    "def get_words(lang):\n",
    "    return get_vocab(lang).words\n",
    "\n",
    "\n",
    "def get_forms(lang):\n",
    "    return get_vocab(lang).forms\n",
    "\n",
    "\n",
    "def is_cognate(w1, w2):\n",
    "    global _COG_LIST\n",
    "    return _COG_LIST.is_cognate(w1, w2)\n",
    "\n",
    "\n",
    "def has_cognate(w, lang):\n",
    "    global _COG_LIST\n",
    "    return _COG_LIST.has_cognate(w, lang)\n",
    "\n",
    "\n",
    "def clear_vocabs():\n",
    "    global _COG_LIST\n",
    "    global _VOCABS\n",
    "    _COG_LIST = None\n",
    "    _VOCABS = dict()\n",
    "\n",
    "\n",
    "def build_vocabs(path, lost_lang, known_lang, max_size=0):#, only_valid=True):\n",
    "    global _COG_LIST\n",
    "\n",
    "    assert _COG_LIST is None\n",
    "    cog_list = CognateList(path, lost_lang, known_lang, max_size=max_size)#, only_valid=only_valid)\n",
    "\n",
    "    for lang in [lost_lang, known_lang]:\n",
    "        if lang in _VOCABS:\n",
    "            raise ValueError(f'There already is a vocab for {lang}')\n",
    "        _VOCABS[lang] = _Vocab(cog_list.get_wordlist(lang), lang)\n",
    "    _COG_LIST = cog_list\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, order=True)\n",
    "class Word:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n",
    "\n",
    "\n",
    "@has_properties('lang')\n",
    "class _Vocab:\n",
    "\n",
    "    def __init__(self, wordlist, lang):\n",
    "        assert len(wordlist) == len(set(wordlist))  # Make sure they are all unique.\n",
    "        self._build(wordlist)\n",
    "\n",
    "    def _build(self, wordlist):\n",
    "        self._id2word = list()\n",
    "        self._form2id = dict()\n",
    "        for w in wordlist:\n",
    "            w = Word(self.lang, w, len(self._id2word))\n",
    "            self._id2word.append(w)\n",
    "            self._form2id[w.form] = len(self._form2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def words(self):\n",
    "        return np.asarray(self._id2word)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def forms(self):\n",
    "        return np.asarray([word.form for word in self.words])\n",
    "\n",
    "    def cognate_to(self, lang):\n",
    "        global _COG_LIST\n",
    "        return np.asarray([w for w in self.words if _COG_LIST.has_cognate(w, lang)])\n",
    "\n",
    "    def get_word_from_form(self, form):\n",
    "        return self._id2word[self._form2id[form]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEsmopy9rRv0"
   },
   "source": [
    "#### Test Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dtlv--YRgPHj"
   },
   "outputs": [],
   "source": [
    "#clear_vocabs()\n",
    "#USE_RAW = False\n",
    "#file_name = \"cognates.cog\" if not USE_RAW else \"linear_b-greek.cog\"\n",
    "#lost_lang = \"transliterated_linear_b\" if not USE_RAW else \"linear_b\"\n",
    "#cog_path = os.path.join(prefix_path, file_name)\n",
    "#known_lang = \"greek\"\n",
    "#build_vocabs(cog_path, lost_lang, known_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HNPIID47N7EM"
   },
   "outputs": [],
   "source": [
    "#get_words('transliterated_linear_b')[1].id_seq, get_words('transliterated_linear_b')[1].char_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MBPS9HBnhxlZ"
   },
   "outputs": [],
   "source": [
    "#lb_words = get_words(lost_lang)\n",
    "#greek_words = get_words(known_lang)\n",
    "## \t\n",
    "#lin_b_test = \"si-mi-te-u\" if not USE_RAW else \"\"\n",
    "#for w in lb_words:\n",
    "#    if w.form == lin_b_test:\n",
    "#        print(w)\n",
    "#        print(has_cognate(w, known_lang))\n",
    "#        break\n",
    "#for g in greek_words:\n",
    "#    if g.form == \"\":\n",
    "#        print(g)\n",
    "#        print(has_cognate(g, lost_lang))\n",
    "#        print(is_cognate(g, w))\n",
    "#        print(is_cognate(w, g))\n",
    "#        break\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "i2l5zpz_oYym"
   },
   "outputs": [],
   "source": [
    "#w = Word(\"transliterated_linear_b\", \"ko-wo\", 0)\n",
    "#w, w.char_seq, w.id_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf9HEKgIa0aQ"
   },
   "source": [
    "### Charsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uRqXIVjOazRX"
   },
   "outputs": [],
   "source": [
    "\n",
    "PAD_ID = 0\n",
    "SOW_ID = 1\n",
    "EOW_ID = 2\n",
    "UNK_ID = 3\n",
    "EOS_ID = 4\n",
    "\n",
    "PAD = '<PAD>'\n",
    "SOW = '<SOW>'\n",
    "EOW = '<EOW>'\n",
    "UNK = '<UNK>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "START_CHAR = [PAD, SOW, EOW, UNK, EOS]\n",
    "\n",
    "_CHARSETS = dict()\n",
    "\n",
    "\n",
    "def register_charset(lang):\n",
    "    global _CHARSETS\n",
    "\n",
    "    def decorated(cls):\n",
    "        assert lang not in _CHARSETS\n",
    "        _CHARSETS[lang] = cls\n",
    "        return cls\n",
    "\n",
    "    return decorated\n",
    "\n",
    "\n",
    "def get_charset(lang):\n",
    "    '''\n",
    "    Make sure only one charset is ever created.\n",
    "    '''\n",
    "    global _CHARSETS\n",
    "    cls_or_obj = _CHARSETS[lang]\n",
    "    if isinstance(cls_or_obj, type):\n",
    "        _CHARSETS[lang] = cls_or_obj()\n",
    "    return _CHARSETS[lang]\n",
    "\n",
    "\n",
    "def _recursive_map(func, lst):\n",
    "    ret = list()\n",
    "    for item in lst:\n",
    "        if isinstance(item, (list, np.ndarray)):\n",
    "            ret.append(_recursive_map(func, item))\n",
    "        else:\n",
    "            ret.append(func(item))\n",
    "    return ret\n",
    "\n",
    "\n",
    "class BaseCharset(object):\n",
    "\n",
    "    _CHARS = u''\n",
    "    _FEATURES = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self._id2char = START_CHAR + self.__class__._CHARS\n",
    "        self._char2id = dict(zip(self._id2char, range(len(self._id2char))))\n",
    "        self._feat_dict = {}\n",
    "        for f in self.features:\n",
    "            self._feat_dict['char'] = None\n",
    "            self._feat_dict[f] = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2char)\n",
    "\n",
    "    def char2id(self, char):\n",
    "        def map_func(c): return self._char2id.get(c, UNK_ID)\n",
    "        if isinstance(char, str):\n",
    "            return map_func(char)\n",
    "        elif isinstance(char, (np.ndarray, list)):\n",
    "            return np.asarray(_recursive_map(map_func, char))\n",
    "            # return np.asarray([np.asarray(list(map(map_func, ch))) for ch in char])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def id2char(self, id_):\n",
    "        #def map_func(i): return self._id2char[i] *** remove next line and restore this\n",
    "        def map_func(i): return self._id2char[i] if i < len(self._id2char) else \"<LOGOGRAM>\"\n",
    "        if isinstance(id_, int):\n",
    "            return map_func(id_)\n",
    "        elif isinstance(id_, (np.ndarray, list)):\n",
    "            return np.asarray(_recursive_map(map_func, id_))\n",
    "            # id_.tolist()\n",
    "            # if id_.ndim == 2:\n",
    "            #     return np.asarray([np.asarray(list(map(map_func, i))) for i in id_])\n",
    "            # elif id_.ndim == 3:\n",
    "            #     return np.asarray([self.id2char(i) for i in id_])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_tokens(self, ids):\n",
    "        if torch.is_tensor(ids):\n",
    "            ids = ids.cpu().numpy()\n",
    "        chars = self.id2char(ids)\n",
    "\n",
    "        def get_2d_tokens(chars):\n",
    "            tokens = list()\n",
    "            for char_seq in chars:\n",
    "                token = ''\n",
    "                for c in char_seq:\n",
    "                    if c == EOW:\n",
    "                        break\n",
    "                    elif c in START_CHAR:\n",
    "                        c = '|'\n",
    "                    token += c\n",
    "                tokens.append(token)\n",
    "            return np.asarray(tokens)\n",
    "\n",
    "        if chars.ndim == 3:\n",
    "            a, b, _ = chars.shape\n",
    "            chars = chars.reshape(a * b, -1)\n",
    "            tokens = get_2d_tokens(chars).reshape(a, b)\n",
    "        else:\n",
    "            tokens = get_2d_tokens(chars)\n",
    "        return tokens\n",
    "\n",
    "    def process(self, word):\n",
    "        # How to process chars in word. This function is language-dependent.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return self._FEATURES\n",
    "\n",
    "\n",
    "@register_charset('greek')\n",
    "class ElCharSet(BaseCharset):\n",
    "\n",
    "    _CHARS = list('fhy')\n",
    "    _FEATURES = ['']\n",
    "\n",
    "\n",
    "@register_charset('transliterated_linear_b')\n",
    "class LinbLatinCharSet(BaseCharset):\n",
    "    # _CHARS is a list (check .split(\"-\"))\n",
    "    _CHARS = 'a-e-i-o-u-da-de-di-do-du-dwe-dwo-ja-je-jo-ka-ke-ki-ko-ku-ma-me-mi-mo-mu-na-ne-ni-no-nu-nwa-pa-pe-pi-po-pu-pte-phu-qa-qe-qi-qo-ra-re-ri-ro-ru-rya-rai-ryo-sa-se-si-so-su-ta-te-ti-to-tu-tya-twe-two-wa-we-wi-wo-za-ze-zo-ha-ai-au-*18-*19-*22-*34-*35-*47-*49-*56-*63-*64-*65-*79-*82-*83-*86'.split(\"-\")\n",
    "    _FEATURES = ['']\n",
    "\n",
    "\n",
    "@register_charset('linear_b')\n",
    "class MinoanCharSet(BaseCharset):\n",
    "\n",
    "    _CHARS = list('')\n",
    "    _FEATURES = ['']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_charset('greek')._CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAqaPwXaAHNE"
   },
   "source": [
    "#### Test Charset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MA_uDAew5rN9"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(known_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "radj284b-djZ"
   },
   "outputs": [],
   "source": [
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['<EOW>', '<UNK>', 'h', '']))\n",
    "#print(len(ch))\n",
    "#print(ch.get_tokens([i for i in range(len(ch))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x4C6yMttAKjm"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(lost_lang)\n",
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['<EOW>', '<UNK>', 'te', 'po']))\n",
    "#print(len(ch))\n",
    "#print(ch.get_tokens([i for i in range(len(ch))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pCKN374-oGaq"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(lost_lang)\n",
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['ko', 'wo', '<EOW>']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKVUfb_Ks5wT"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WgizMl8hs49N"
   },
   "outputs": [],
   "source": [
    "def pad_to_dense(a, dtype='f'):\n",
    "    '''\n",
    "    Modified from https://stackoverflow.com/questions/37676539/numpy-padding-matrix-of-different-row-size.\n",
    "    '''\n",
    "    assert dtype in ['f', 'l']\n",
    "    dtype = 'float32' if dtype == 'f' else 'int64'\n",
    "    maxlen = max(map(len, a))\n",
    "    ret = np.zeros((len(a), maxlen), dtype=dtype)\n",
    "    for i, row in enumerate(a):\n",
    "        row = np.asarray(row, dtype=dtype)  # force dtype conversion\n",
    "        ret[i, :len(row)] += row\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@has_properties('lang')\n",
    "class WordlistDataset(Dataset):\n",
    "    \"\"\"This is for one language.\"\"\"\n",
    "\n",
    "    def __init__(self, words, lang, indices=None):\n",
    "        assert isinstance(words[0], Word), \"words list must contain instances of class Word\"\n",
    "        if indices is not None:\n",
    "            palle = []\n",
    "            indices = set(indices)\n",
    "            for w in words:\n",
    "                if w.idx in indices:\n",
    "                    palle.append(w)\n",
    "            #logging.critical(f\"WORDS: {len(palle)}, {len(words)}\")\n",
    "            self._words = palle\n",
    "        else:\n",
    "            self._words = words\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._words)\n",
    "\n",
    "    @cache(persist=True, full=True)\n",
    "    @cache(persist=True, full=True)\n",
    "    def __getitem__(self, idx):\n",
    "        word = self._words[idx]\n",
    "        return Map(word=word, form=word.form, lang=self.lang, char_seq=word.char_seq, id_seq=word.id_seq)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        return collate_fn([self[i] for i in range(len(self))])\n",
    "\n",
    "\n",
    "class VocabDataset(WordlistDataset):\n",
    "\n",
    "    def __init__(self, lang, indices=None):\n",
    "        super().__init__(get_words(lang), lang, indices=indices)\n",
    "\n",
    "\n",
    "def _get_item(key, batch):\n",
    "    # raised an exception as arrays have different lengths\n",
    "    return np.array([record[key] for record in batch], dtype=object)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words = _get_item('word', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "    char_seqs = _get_item('char_seq', batch)\n",
    "    id_seqs = _get_item('id_seq', batch)\n",
    "    lengths, words, forms, char_seqs, id_seqs = sort_all(words, forms, char_seqs, id_seqs)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    id_seqs = pad_to_dense(id_seqs, dtype='l')\n",
    "    id_seqs = get_tensor(id_seqs[:, :max_len])\n",
    "\n",
    "    lang = batch[0].lang\n",
    "\n",
    "\n",
    "    return Map(\n",
    "        words=words, forms=forms, char_seqs=char_seqs, id_seqs=id_seqs, lengths=lengths, lang=lang)\n",
    "\n",
    "\n",
    "def _prepare_stats(name, *rows):\n",
    "    table = pt()\n",
    "    table.field_names = 'lang', 'size'\n",
    "    for row in rows:\n",
    "        table.add_row(row)\n",
    "    table.align = 'l'\n",
    "    table.title = name\n",
    "    return table\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'cognate_only')\n",
    "class LostKnownDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.known_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for known_batch in super().__iter__():\n",
    "            lost_batch = self.datasets[self.lost_lang].entire_batch\n",
    "            num_samples = len(known_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@has_properties('lost_lang', 'known_lang', 'cognate_only')\n",
    "class LostKnownDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False, evaluation_mode=False, indices=None):\n",
    "        self.datasets = dict()\n",
    "        #logging.critical(f\"WORDS: {len(indices)}\")\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang, indices=indices)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang, indices=indices)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "        #logging.critical(f\"{self.datasets[self.lost_lang]}, {self.datasets[self.known_lang]}\")\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "        #logging.critical(f\"{batch_size}, {len(self.datasets[self.lost_lang])}, {len(indices)}\")\n",
    "        super().__init__(self.datasets[self.known_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for known_batch in super().__iter__():\n",
    "            lost_batch = self.datasets[self.lost_lang].entire_batch\n",
    "            num_samples = len(known_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LawEQT2Ft5cp"
   },
   "source": [
    "#### Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7FocIHeotmsz"
   },
   "outputs": [],
   "source": [
    "#train_data_loader = LostKnownDataLoader(lost_lang, known_lang, 2, cognate_only=False)\n",
    "#eval_data_loader = LostKnownDataLoader(lost_lang, known_lang, 2, cognate_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ByeSYq7ht-CH"
   },
   "outputs": [],
   "source": [
    "#create_logger(filepath='./log', log_level=\"DEBUG\")\n",
    "#log_pp(train_data_loader.stats('sdrogo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLgdHwYEJqXm"
   },
   "source": [
    "### Model Submodules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RSNAT4IE60Ix"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTMState(object):\n",
    "\n",
    "    def __init__(self, states):\n",
    "        self.states = states\n",
    "\n",
    "    @classmethod\n",
    "    def from_pytorch(cls, states):\n",
    "        hs, cs = states\n",
    "        _, bs, d = hs.shape\n",
    "        hs = hs.view(-1, 2, bs, d) #changes shape to num_layers/2, 2, bs, d\n",
    "        cs = cs.view(-1, 2, bs, d)\n",
    "        nl = hs.shape[0]\n",
    "        #sums on the direction 2 after unbinding num_layers/2 so final is num_layers/2, bs, d\n",
    "        states = [(h.sum(dim=0), c.sum(dim=0)) for h, c in zip(hs.unbind(dim=0), cs.unbind(dim=0))]\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @classmethod\n",
    "    def stack(cls, iterator_states, dim):\n",
    "        nl = len(iterator_states[0])\n",
    "        hs = [list() for _ in range(nl)]\n",
    "        cs = [list() for _ in range(nl)]\n",
    "        for states in iterator_states:\n",
    "            for i, state in enumerate(states.states):\n",
    "                h, c = state\n",
    "                hs[i].append(h)\n",
    "                cs[i].append(c)\n",
    "        states = list()\n",
    "        for i in range(nl):\n",
    "            h = torch.stack(hs[i], dim)\n",
    "            c = torch.stack(cs[i], dim)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @classmethod\n",
    "    def zero_state(cls, num_layers, shape):\n",
    "        states = list()\n",
    "        for _ in range(num_layers):\n",
    "            h = get_zeros(*shape)\n",
    "            c = get_zeros(*shape)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.states[0][0].shape\n",
    "\n",
    "    def clone(self):\n",
    "        new_states = [(s[0].clone(), s[1].clone()) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def dim(self):\n",
    "        return self.states[0][0].dim()\n",
    "\n",
    "    def unsqueeze(self, dim):\n",
    "        new_states = [(s[0].unsqueeze(dim), s[1].unsqueeze(dim)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def view(self, *sizes):\n",
    "        new_states = [(s[0].view(*sizes), s[1].view(*sizes)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def unbind(self, dim):\n",
    "        n = self.states[0][0].shape[dim]\n",
    "        ret = [list() for _ in range(n)]\n",
    "        for s in self.states:\n",
    "            h, c = s\n",
    "            hs = h.unbind(dim)\n",
    "            cs = c.unbind(dim)\n",
    "            for i, (h, c) in enumerate(zip(hs, cs)):\n",
    "                ret[i].append((h, c))\n",
    "        ret = tuple(LSTMState(s) for s in ret)\n",
    "        return ret\n",
    "\n",
    "    def expand(self, *sizes):\n",
    "        new_states = [(s[0].expand(*sizes), s[1].expand(*sizes)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def contiguous(self):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0].contiguous()\n",
    "            c = s[1].contiguous()\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def detach_(self):\n",
    "        for s in self.states:\n",
    "            s[0].detach_()\n",
    "            s[1].detach_()\n",
    "\n",
    "    def size(self):\n",
    "        return self.states[0][0].size()\n",
    "\n",
    "    def cat(self, other, dim):\n",
    "        states = list()\n",
    "        for s1, s2 in zip(self.states, other.states):\n",
    "            h = torch.cat([s1[0], s2[0]], dim)\n",
    "            c = torch.cat([s1[1], s2[1]], dim)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0] * other\n",
    "            c = s[1] * other\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        states = list()\n",
    "        for s1, s2 in zip(self.states, other.states):\n",
    "            h = s1[0] + s2[0]\n",
    "            c = s1[1] + s2[1]\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.states[-1][0]\n",
    "\n",
    "    def get(self, ind):\n",
    "        return self.states[ind]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0][key]\n",
    "            c = s[1][key]\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        for s1, s2 in zip(self.states, item.states):\n",
    "            s1[0][key] = s2[0]\n",
    "            s1[1][key] = s2[1]\n",
    "\n",
    "\n",
    "class UniversalCharEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, langs, char_emb_dim, universal_charset_size, mapping_temperature=0.0):\n",
    "        super(UniversalCharEmbedding, self).__init__()\n",
    "        self.langs = langs\n",
    "        self.charsets = {l: get_charset(l) for l in langs}\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.universal_charset_size = universal_charset_size\n",
    "        self.mapping_temperature = mapping_temperature\n",
    "        # U in the paper\n",
    "        self.char_emb = nn.Embedding(self.universal_charset_size, self.char_emb_dim)\n",
    "        # Wx and Wy in the paper\n",
    "        self.char_weights = nn.ModuleDict({\n",
    "            l: nn.Embedding(len(self.charsets[l]), self.universal_charset_size)\n",
    "            for l in self.langs})\n",
    "        self.device = next(self.parameters()).device\n",
    "\n",
    "    # takes embeddings of the input char sequence char_seq\n",
    "    def forward(self, char_seq, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        char_seq = char_seq.to(self.device)\n",
    "        return char_emb[char_seq]\n",
    "\n",
    "    # projects embedded sequence into the space of characters of language lang\n",
    "    # calculates input_ * U^T * Wy^T for language y\n",
    "    def project(self, input_, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return input_.matmul(char_emb.t())\n",
    "\n",
    "    # calculates Wx * U\n",
    "    @cache(full=True)\n",
    "    def get_char_weight(self, lang):\n",
    "        mapping = self.mapping(lang)\n",
    "        char_emb = mapping.matmul(self.char_emb.weight)\n",
    "        return char_emb\n",
    "\n",
    "    # calculates Wx for language x\n",
    "    @cache(full=True)\n",
    "    def mapping(self, lang):\n",
    "        weight = self.char_weights[lang].weight\n",
    "        if self.mapping_temperature > 0.0:\n",
    "            # NOTE use log_softmax first for more numerical stability\n",
    "            weight = torch.log_softmax(weight / self.mapping_temperature, dim=-1).exp()\n",
    "        return weight\n",
    "\n",
    "    # computes similarity between characters of the two languages\n",
    "    def char_sim_mat(self, lang1, lang2):\n",
    "        # Wx\n",
    "        x = normalize(self.mapping(lang1), dim=-1)\n",
    "        # Wy\n",
    "        y = normalize(self.mapping(lang2), dim=-1)\n",
    "        # mat = Wx * Wy^T\n",
    "        mat = x.matmul(y.t())\n",
    "        return mat\n",
    "\n",
    "    # this function computes similarity scores between the universal embeddings of the characters of the languages\n",
    "    def char_softmax(self, lang1, lang2):\n",
    "        # w1 = Wx * U\n",
    "        w1 = self.get_char_weight(lang1)\n",
    "        # w2 = Wy * U\n",
    "        w2 = self.get_char_weight(lang2)\n",
    "        mat = w1.matmul(w2.t())\n",
    "        # mat = Wx * U * U^T * Wy^T = w1 * w2^T. Shape: nx, ny\n",
    "        l1_l2 = mat.log_softmax(dim=-1).exp() # shape becomes: nx, ny (notice softmax on last dim)\n",
    "        l2_l1 = mat.log_softmax(dim=0).exp().t() # shape becomes nx, ny (notice softmax on first dim and .t())\n",
    "\n",
    "        return l1_l2, l2_l1\n",
    "\n",
    "    # this computes a mapping between the chars of the two languages in both directions:\n",
    "    # for each character of the two words, the top k (k=3) similar characters of the other language\n",
    "    # are retrieved and put into a dictionary. Also the values of similarity are returned\n",
    "    def char_mapping(self, l1, l2):\n",
    "        l1_l2, l2_l1 = self.char_softmax(l1, l2)\n",
    "        # get topk most similar items\n",
    "        def get_topk(a2b, a_cs, b_cs):\n",
    "            # s = values of similarity, idx = indexes of most similar items\n",
    "            s, idx = a2b[4:].topk(3, dim=-1) # notice: first 4 token (PAD, SOW, EOW, UNK) are excluded\n",
    "            a = a_cs.id2char(np.arange(4, len(a2b)).reshape(1, -1)).reshape(-1)\n",
    "            b = b_cs.id2char(idx.cpu().numpy())\n",
    "            d = {aa: ' '.join(bb) for aa, bb in zip(a, b)}\n",
    "            return d, s\n",
    "        l1_l2 = get_topk(l1_l2, self.charsets[l1], self.charsets[l2])\n",
    "        l2_l1 = get_topk(l2_l1, self.charsets[l2], self.charsets[l1])\n",
    "        return l1_l2, l2_l1\n",
    "\n",
    "    # calculates weight * Wy * U for language y\n",
    "    def soft_emb(self, weight, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return weight.matmul(char_emb)\n",
    "\n",
    "    def get_start_emb(self, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return char_emb[SOW_ID]\n",
    "\n",
    "\n",
    "class MultiLayerLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0):\n",
    "        super(MultiLayerLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        cells = [nn.LSTMCell(input_size, hidden_size)] + \\\n",
    "                [nn.LSTMCell(hidden_size, hidden_size) for _ in range(self.num_layers - 1)]\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "\n",
    "    # initializes all LSTM cells with the mean of the hidden states of the batch and of the cell states of the batch\n",
    "    # NOTE: unused in the code, as it works by default with a single layer and therefore passes the single state directly\n",
    "    #       to the forward. I believed it is needed in case we wanna use a multi-layer decoder\n",
    "    def init_state(self, bs, encoding):\n",
    "        states = list()\n",
    "        for _ in range(self.num_layers):\n",
    "            state = (encoding[0].mean(dim=0), encoding[1].mean(dim=0))\n",
    "            states.append(state)\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def forward(self, input_, states):\n",
    "        assert len(states) == self.num_layers\n",
    "\n",
    "        new_states = list()\n",
    "        # computes new states for the decoder\n",
    "        # input_ becomes each time the output of the previous layer\n",
    "        for i in range(self.num_layers):\n",
    "            new_state = self.cells[i](input_, states.get(i))\n",
    "            new_states.append(new_state)\n",
    "            input_ = new_state[0]\n",
    "            input_ = self.drop(input_)\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '%d, %d, num_layers=%d' % (self.input_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "\n",
    "class GlobalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_src_size, input_tgt_size, dropout=0.0):\n",
    "        super(GlobalAttention, self).__init__()\n",
    "\n",
    "        self.input_src_size = input_src_size\n",
    "        self.input_tgt_size = input_tgt_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.Wa = nn.Parameter(torch.Tensor(input_src_size, input_tgt_size))\n",
    "        #self.Wf = nn.Parameter(torch.Tensor(256, input_tgt_size))  # Project FastText to dt\n",
    "\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "\n",
    "    @cache(full=False)\n",
    "    # simply multiplies Wa and h_s\n",
    "    def _get_Wh_s(self, h_s):\n",
    "        bs, l, _ = h_s.shape # bs x sl x ds\n",
    "        # shape of Wa is ds x dt\n",
    "        # There is some weird bug with dropout layer if dropout rate is zero\n",
    "        Wh_s = self.drop(h_s).reshape(bs * l, -1).mm(self.Wa).view(bs, l, -1)\n",
    "        return Wh_s\n",
    "\n",
    "    def forward(self, ctx_t, h_s, mask_src):#, fasttext_embs):\n",
    "        bs, sl, ds = h_s.size()\n",
    "        dt = ctx_t.shape[-1] # bs x dt\n",
    "\n",
    "        Wh_s = self._get_Wh_s(h_s)  # bs x sl x dt\n",
    "\n",
    "        # combine decoder out with fasttext embeddings\n",
    "        scores = Wh_s.matmul(self.drop(ctx_t).unsqueeze(dim=-1)).squeeze(dim=-1)  # bs x sl\n",
    "        scores = scores * mask_src + (-9999.) * (1.0 - mask_src)\n",
    "        almt_distr = nn.functional.log_softmax(scores, dim=-1).exp()  # bs x sl\n",
    "\n",
    "        #fasttext_ctx = fasttext_embs.matmul(self.Wf)  # [bs, dt]\n",
    "\n",
    "        # Shared Wh_s from earlier (projected h_s): [bs, sl, dt]\n",
    "        #scores_ft = Wh_s.matmul(fasttext_ctx.unsqueeze(-1)).squeeze(-1)  # [bs, sl]\n",
    "        #scores_ft = scores_ft * mask_src + (-9999.) * (1.0 - mask_src)\n",
    "        #ft_contribute = nn.functional.log_softmax(scores_ft, dim=-1).exp()\n",
    "        return almt_distr#, ft_contribute\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'src=%d, tgt=%d' % (self.input_src_size, self.input_tgt_size)\n",
    "\n",
    "# none: only sum\n",
    "# absolute: give a list of norms, each tensor will be scaled by the corresponding inputed norm\n",
    "# relative: give a list of fractions and a multiplier, each tensor will be scaled following paper's formula\n",
    "#           ex. ratios = [1.0, 0.7] and multiplier = 1\n",
    "class NormControlledResidual(nn.Module):\n",
    "\n",
    "    def __init__(self, norms_or_ratios=None, multiplier=1.0, control_mode=None):\n",
    "        super(NormControlledResidual, self).__init__()\n",
    "\n",
    "        assert control_mode in ['none', 'relative', 'absolute']\n",
    "\n",
    "        self.control_mode = control_mode\n",
    "        self.norms_or_ratios = None\n",
    "        if self.control_mode in ['relative', 'absolute']:\n",
    "            self.norms_or_ratios = norms_or_ratios\n",
    "            if self.control_mode == 'relative':\n",
    "                assert self.norms_or_ratios[0] == 1.0\n",
    "\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "    def anneal_ratio(self):\n",
    "        if self.control_mode == 'relative':\n",
    "            new_ratios = [self.norms_or_ratios[0]]\n",
    "            for r in self.norms_or_ratios[1:]:\n",
    "                r = min(r * self.multiplier, 1.0)\n",
    "                new_ratios.append(r)\n",
    "            self.norms_or_ratios = new_ratios\n",
    "            logging.debug('Ratios are now [%s]' % (', '.join(map(lambda f: '%.2f' % f, self.norms_or_ratios))))\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        if self.control_mode == 'none':\n",
    "            output = sum(inputs)\n",
    "        else:\n",
    "            assert len(inputs) == len(self.norms_or_ratios)\n",
    "            outs = list()\n",
    "            if self.control_mode == 'absolute':\n",
    "                for inp, norm in zip(inputs, self.norms_or_ratios):\n",
    "                    if norm >= 0.0:  # NOTE a negative value means no control applied\n",
    "                        outs.append(normalize(inp, dim=-1) * norm)\n",
    "                    else:\n",
    "                        outs.append(inp)\n",
    "            else:\n",
    "                outs.append(inputs[0])\n",
    "                norm_base = inputs[0].norm(dim=-1, keepdim=True)\n",
    "                for inp, ratio in zip(inputs[1:], self.norms_or_ratios[1:]):\n",
    "                    if ratio >= 0.0:  # NOTE same here\n",
    "                        norm_actual = inp.norm(dim=-1, keepdim=True)\n",
    "                        max_norm = norm_base * ratio\n",
    "                        too_big = norm_actual > max_norm\n",
    "                        adjusted_norm = torch.where(too_big, max_norm, norm_actual)\n",
    "                        outs.append(normalize(inp, dim=-1) * adjusted_norm)\n",
    "                    else:\n",
    "                        outs.append(inp)\n",
    "            output = sum(outs)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rOSgQ9c0KPxN"
   },
   "outputs": [],
   "source": [
    "@has_properties('lang')\n",
    "class Trie:\n",
    "    '''\n",
    "        A trie that efficiently computes the log probs for every word.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lang):\n",
    "\n",
    "        words = get_words(lang)\n",
    "        self._max_length = max(map(len, words))  # NOTE EOW has been taken care of by __len__\n",
    "        self._prepare_weight()\n",
    "        self.clear_cache()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._eff_weight = self._weight\n",
    "        self._eff_max_length = self._max_length\n",
    "\n",
    "    def _prepare_weight(self):\n",
    "        rows = list()\n",
    "        cols = list()\n",
    "\n",
    "        words = get_words(self.lang)\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        self._word2rows = defaultdict(list)\n",
    "        # here the char_seq is transformed into a one-hot encoded vector\n",
    "        for row, word in enumerate(words):\n",
    "            # OUR MODIFICATION\n",
    "            # padding the sequence with EOW to preserve original length in the mapping wrt the starting sequence\n",
    "            char_seq = word.char_seq  # already a NumPy array\n",
    "            padded_length = self._max_length - len(word)\n",
    "            if padded_length > 0:\n",
    "                # NumPy padding using concatenation\n",
    "                pad = np.full(padded_length, EOW, dtype=char_seq.dtype)\n",
    "                char_seq = np.concatenate([char_seq, pad])\n",
    "\n",
    "            for i, c in enumerate(char_seq):\n",
    "                cid = charset.char2id(c)\n",
    "                self._word2rows[word].append(len(rows))\n",
    "                rows.append(row)\n",
    "                # for each row a sparse vector in defined with a 1 only in the position defined by the cid\n",
    "                # the vector associated to each row has length len(charset)\n",
    "                cols.append(len(charset) * i + cid)\n",
    "        # rows and cols now define the indexes of the sparse tensor with the values one\n",
    "        data = np.ones(len(rows))\n",
    "        # NOTE This is ugly, but it avoids this issue in 0.4.1: https://github.com/pytorch/pytorch/issues/8856.\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len(words), self._max_length * len(charset)))\n",
    "        self._weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def _sample(self, words):\n",
    "        all_words = get_words(self.lang)\n",
    "        word_indices = list()\n",
    "        old_to_new = np.zeros([len(all_words)], dtype='int64')\n",
    "        self._eff_id2word = list()\n",
    "        self._eff_word2id = dict()\n",
    "        self._eff_max_length = max(map(len, words))\n",
    "        # creates new indices for the sample of words\n",
    "        # also takes the indexes of the characters corresponding to the sampled words\n",
    "        # from weight matrix\n",
    "        for w in words:\n",
    "            word_indices.extend(self._word2rows[w])\n",
    "            old_to_new[w.idx] = len(self._eff_id2word)\n",
    "            self._eff_id2word.append(w)\n",
    "            self._eff_word2id[w] = len(self._eff_word2id)\n",
    "        old_to_new = get_tensor(old_to_new)\n",
    "        #logging.debug(f\"Old to new: {old_to_new}, {len(old_to_new)}\")\n",
    "        #logging.debug(f\"words: {[w.idx for w in self._eff_word2id.keys()]}\")\n",
    "\n",
    "        indices = get_tensor(word_indices, dtype='l')\n",
    "        # here the indexes of the sampled words' characters are retrieved\n",
    "        old_rows, cols = self._weight._indices()[:, word_indices].unbind(dim=0)\n",
    "        # the new sparse vector will follow the new indexing of the words\n",
    "        rows = old_to_new[old_rows]\n",
    "        # sample the data corresponding to the characters of the sampled words\n",
    "        data = self._weight._values()[word_indices]\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        # OUR MODIFICATION\n",
    "        # Filter out padding beyond eff_max_length\n",
    "        max_col = self._eff_max_length * len(charset)\n",
    "        keep = cols < max_col\n",
    "\n",
    "        cols = cols[keep]\n",
    "        rows = rows[keep]\n",
    "        data = data[keep]\n",
    "\n",
    "        \n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            torch.stack([rows, cols], dim=0),\n",
    "            data,\n",
    "            (len(words), self._eff_max_length * len(charset)))\n",
    "        self._eff_weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def analyze(self, log_probs, almt_distr, words, lost_lengths):\n",
    "        self.clear_cache()\n",
    "        self._sample(words)\n",
    "\n",
    "        assert self._eff_max_length == len(log_probs)\n",
    "\n",
    "        tl, nc, bs = log_probs.shape\n",
    "        charset = get_charset(self.lang)\n",
    "        assert nc == len(charset)\n",
    "\n",
    "        # V x bs, or c_s x c_t -> bs x V\n",
    "        valid_log_probs = self._eff_weight.matmul(log_probs.view(-1, bs)).t()\n",
    "\n",
    "        sl = almt_distr.shape[-1]\n",
    "        pos = get_tensor(torch.arange(sl).float(), requires_grad=False)\n",
    "        mean_pos = (pos * almt_distr).sum(dim=-1)  # bs x tl\n",
    "        mean_pos = torch.cat([get_zeros(bs, 2, requires_grad=False).fill_(-1.0), mean_pos],\n",
    "                             dim=-1)\n",
    "\n",
    "        reg_weight = lost_lengths.float().view(-1, 1) - 1.0 - mean_pos[:, :-2]\n",
    "        reg_weight.clamp_(0.0, 1.0)\n",
    "        # here we should go through alignment between characters at distance 2 instead of 1 for the tgt length\n",
    "        # assuming that 1 LinB char -> 2 AncGreek chars\n",
    "        rel_pos = mean_pos[:, 2:] - mean_pos[:, :-2]  # bs x tl\n",
    "        rel_pos_diff = rel_pos - 1\n",
    "        margin = rel_pos_diff != 0\n",
    "        reg_loss = margin.float() * (rel_pos_diff ** 2)  # bs x tl\n",
    "        reg_loss = (reg_loss * reg_weight).sum()\n",
    "        #logging.debug(reg_loss)\n",
    "        out = Map(reg_loss=reg_loss, valid_log_probs=valid_log_probs)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL TRIE\n",
    "@has_properties('lang')\n",
    "class Trie:\n",
    "    '''\n",
    "        A trie that efficiently computes the log probs for every word.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lang):\n",
    "\n",
    "        words = get_words(lang)\n",
    "        self._max_length = max(map(len, words))  # NOTE EOW has been taken care of by __len__\n",
    "        self._prepare_weight()\n",
    "        self.clear_cache()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._eff_weight = self._weight\n",
    "        self._eff_max_length = self._max_length\n",
    "\n",
    "    def _prepare_weight(self):\n",
    "        rows = list()\n",
    "        cols = list()\n",
    "\n",
    "        words = get_words(self.lang)\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        self._word2rows = defaultdict(list)\n",
    "        # here the char_seq is transformed into a one-hot encoded vector\n",
    "        for row, word in enumerate(words):\n",
    "            for i, c in enumerate(word.char_seq):\n",
    "                cid = charset.char2id(c)\n",
    "                self._word2rows[word].append(len(rows))\n",
    "                rows.append(row)\n",
    "                # for each row a sparse vector in defined with a 1 only in the position defined by the cid\n",
    "                # the vector associated to each row has length len(charset)\n",
    "                cols.append(len(charset) * i + cid)\n",
    "        # rows and cols now define the indexes of the sparse tensor with the values one\n",
    "        data = np.ones(len(rows))\n",
    "        # NOTE This is ugly, but it avoids this issue in 0.4.1: https://github.com/pytorch/pytorch/issues/8856.\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len(words), self._max_length * len(charset)))\n",
    "        self._weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def _sample(self, words):\n",
    "        #logging.debug(f\"LEN WORDS: {len(words)}\")\n",
    "        all_words = get_words(self.lang)\n",
    "        word_indices = list()\n",
    "        old_to_new = np.zeros([len(all_words)], dtype='int64')\n",
    "        self._eff_id2word = list()\n",
    "        self._eff_word2id = dict()\n",
    "        self._eff_max_length = max(map(len, words))\n",
    "        # creates new indices for the sample of words\n",
    "        # also takes the indexes of the characters corresponding to the sampled words\n",
    "        # from weight matrix\n",
    "        for w in words:\n",
    "            word_indices.extend(self._word2rows[w])\n",
    "            old_to_new[w.idx] = len(self._eff_id2word)\n",
    "            self._eff_id2word.append(w)\n",
    "            self._eff_word2id[w] = len(self._eff_word2id)\n",
    "        old_to_new = get_tensor(old_to_new)\n",
    "        indices = get_tensor(word_indices, dtype='l')\n",
    "        # here the indexes of the sampled words' characters are retrieved\n",
    "        old_rows, cols = self._weight._indices()[:, word_indices].unbind(dim=0)\n",
    "        # the new sparse vector will follow the new indexing of the words\n",
    "        rows = old_to_new[old_rows]\n",
    "        # sample the data corresponding to the characters of the sampled words\n",
    "        data = self._weight._values()[word_indices]\n",
    "        charset = get_charset(self.lang)\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            torch.stack([rows, cols], dim=0),\n",
    "            data,\n",
    "            (len(words), self._eff_max_length * len(charset)))\n",
    "        self._eff_weight = get_tensor(weight)\n",
    "        #logging.debug(f\"WEIGHT shape {weight.shape}\")\n",
    "\n",
    "\n",
    "    def analyze(self, log_probs, almt_distr, words, lost_lengths):\n",
    "        self.clear_cache()\n",
    "        self._sample(words)\n",
    "\n",
    "        assert self._eff_max_length == len(log_probs)\n",
    "\n",
    "        tl, nc, bs = log_probs.shape\n",
    "        charset = get_charset(self.lang)\n",
    "        assert nc == len(charset)\n",
    "\n",
    "        # V x bs, or c_s x c_t -> bs x V\n",
    "        valid_log_probs = self._eff_weight.matmul(log_probs.view(-1, bs)).t()\n",
    "\n",
    "        sl = almt_distr.shape[-1]\n",
    "        pos = get_tensor(torch.arange(sl).float(), requires_grad=False)\n",
    "\n",
    "        mean_pos = (pos * almt_distr).sum(dim=-1)  # bs x tl\n",
    "        mean_pos = torch.cat([get_zeros(bs, 2, requires_grad=False).fill_(-1.0), mean_pos],\n",
    "                             dim=-1)\n",
    "\n",
    "        reg_weight = lost_lengths.float().view(-1, 1) - 1.0 - mean_pos[:, :-2]\n",
    "        reg_weight.clamp_(0.0, 1.0)\n",
    "        # here we should go through alignment between characters at distance 2 instead of 1 for the tgt length\n",
    "        # assuming that 1 LinB char -> 2 AncGreek chars\n",
    "        rel_pos = mean_pos[:, 2:] - mean_pos[:, :-2]  # bs x tl\n",
    "        rel_pos_diff = rel_pos - 1\n",
    "        margin = rel_pos_diff != 0\n",
    "        reg_loss = margin.float() * (rel_pos_diff ** 2)  # bs x tl\n",
    "        reg_loss = (reg_loss * reg_weight).sum()\n",
    "\n",
    "        out = Map(reg_loss=reg_loss, valid_log_probs=valid_log_probs)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orwm78mS7Q9c"
   },
   "source": [
    "#### Test Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FJRSWmmgtZ9",
    "outputId": "660162a7-0cd0-4c51-bc9b-472e41069cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 6])\n",
      "torch.Size([4, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5]),\n",
       " tensor([[ 0.7402,  0.1287, -0.4363,  0.9131,  0.2074],\n",
       "         [ 1.4584, -2.4849, -4.6585,  1.6417, -6.7441],\n",
       "         [-1.1625, -0.5616,  0.2273,  0.1895, -0.2578],\n",
       "         [ 0.8348, -2.7073,  1.1064,  0.7540, -0.8689]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate the shapes for Wh_s and ctx_t\n",
    "batch_size = 4  # Example batch size\n",
    "source_sequence_length = 5  # Source sequence length\n",
    "target_hidden_size = 6  # Hidden size of the target\n",
    "target_sequence_length = 3  # Target sequence length\n",
    "\n",
    "# Random initialization of Wh_s and ctx_t\n",
    "Wh_s = torch.randn(batch_size, source_sequence_length, target_hidden_size)  # [batch_size, source_sequence_length, target_hidden_size]\n",
    "print(Wh_s.shape)\n",
    "ctx_t = torch.randn(batch_size, target_hidden_size)  # [batch_size, target_hidden_size]\n",
    "print(ctx_t.shape)\n",
    "# Perform the unsqueeze operation\n",
    "ctx_t_unsqueezed = ctx_t.unsqueeze(dim=-1)  # [batch_size, target_hidden_size, 1]\n",
    "\n",
    "# Perform the matrix multiplication\n",
    "Wh_s_drop = Wh_s  # No dropout for simplicity in this case\n",
    "scores = Wh_s_drop.matmul(ctx_t_unsqueezed).squeeze(dim=-1)  # [batch_size, source_sequence_length]\n",
    "\n",
    "# Show the result\n",
    "scores.shape, scores  # Shape and values of the attention scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sunw-P2bJpW3",
    "outputId": "3f8af9b3-a1c4-4148-d182-def4c7998d3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_911723/78306522.py:7: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
      "  weight = torch.sparse.FloatTensor(\n"
     ]
    }
   ],
   "source": [
    "rows = rows = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "cols = [32*0 + 8, 32*1+13, 32*2+30, 32 * 3 + 8, 32 * 0 + 9, 32* 1 + 10, 32*2 + 15, 32 * 3 + 8]\n",
    "data = np.ones([len(rows)])\n",
    "max_len = 4\n",
    "len_char_set = 32\n",
    "len_words = 2\n",
    "weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len_words, max_len * len_char_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMoiD_0kKlP9",
    "outputId": "bb599ed4-2929-4ab3-b149-d2560badcf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0]) tensor([  8,  45,  94, 104])\n"
     ]
    }
   ],
   "source": [
    "ors, c = weight._indices()[:, :4]\n",
    "print(ors, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7elfqidiFx9z"
   },
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ikI8xzL1F2Wq"
   },
   "outputs": [],
   "source": [
    "def compute_expected_edits(known_charset, log_probs, wordlist, valid_log_probs, num_samples=10, alpha=1e1, edit=False):\n",
    "    logging.debug('Computing expected edits')\n",
    "    device = log_probs.device\n",
    "    log_probs = log_probs.transpose(0, 2).transpose(1, 2)  # size: bs x tl x C\n",
    "    log_probs = torch.log_softmax(log_probs * alpha, dim=-1) # alpha is just a scaling factor\n",
    "    probs = log_probs.exp()\n",
    "    bs, tl, nc = probs.shape\n",
    "    # get samples\n",
    "    if num_samples > 0:\n",
    "        samples = torch.multinomial(probs.cpu().reshape(bs * tl, nc), num_samples, replacement=True).to(device) #Added .cpu() and to() device to avoid cuda determinism error\n",
    "        samples = samples.view(bs, tl, num_samples)\n",
    "        # get tokens\n",
    "        tokens = known_charset.get_tokens(samples.transpose(1, 2))  # size: bs x num_samples\n",
    "        # get probs\n",
    "        sample_log_probs = log_probs[torch.arange(bs, device=device).long().view(-1, 1, 1),\n",
    "                                     torch.arange(tl, device=device).long().view(1, -1, 1), samples]  # bs x tl x ns\n",
    "        lengths = get_tensor(np.vectorize(len)(tokens) + 1, dtype='f').to(device)  # bs x num_samples\n",
    "        mask = get_tensor(torch.arange(tl, device=device)).float().view(\n",
    "            1, -1, 1).expand(bs, tl, num_samples).to(device) < lengths.unsqueeze(dim=1)\n",
    "        sample_log_probs = (mask.float() * sample_log_probs).sum(dim=1)  # bs x num_samples\n",
    "    else:  # This means we are taking the argmax according to token-level probs, not character-level probs.\n",
    "        # Take argmax\n",
    "        _, idx = valid_log_probs.max(dim=-1)\n",
    "        tokens = wordlist[idx.cpu().numpy()].reshape(bs, 1)\n",
    "        num_samples = 1\n",
    "        sample_log_probs = get_tensor(np.ones([bs, 1])).to(device)\n",
    "    # use chunks to get all edits\n",
    "    chunk_size = 1000\n",
    "    num_chunks = len(wordlist) // chunk_size + (len(wordlist) % chunk_size > 0)\n",
    "    expected_edits = list()\n",
    "    for i in range(num_chunks):\n",
    "        logging.debug('Computing chunk %d/%d' % (i + 1, num_chunks))\n",
    "        start = i * chunk_size\n",
    "        end = min(start + chunk_size, len(wordlist))\n",
    "\n",
    "        valid_log_prob_chunk = valid_log_probs[:, start: end]\n",
    "        if edit:\n",
    "            # get dists\n",
    "            dists = compute_dists(tokens, wordlist[start: end])  # bs x c_s x (1 + ns)\n",
    "            dists = get_tensor(dists, 'f').to(device)\n",
    "            # remove accidental hits\n",
    "            duplicates = compute_duplicates(tokens, wordlist[start: end])  # bs x c_s x (1 + ns)\n",
    "            duplicates = get_tensor(duplicates, 'f').to(device)\n",
    "            edit_chunk = dists * duplicates\n",
    "            # compute expected edits\n",
    "            ex_sample_log_probs = sample_log_probs.view(\n",
    "                bs, 1, num_samples).expand(-1, valid_log_prob_chunk.shape[-1], -1)\n",
    "            all_sample_log_probs = torch.cat([valid_log_prob_chunk.unsqueeze(dim=-1), ex_sample_log_probs], dim=-1)\n",
    "            # make it less sharp\n",
    "            all_sample_log_probs = all_sample_log_probs + (1.0 - duplicates) * (-999.)\n",
    "            logits = all_sample_log_probs  # * alpha\n",
    "            sm_log_probs = torch.log_softmax(logits, dim=-1)  # NOTE sm stands for softmax\n",
    "            sm_probs = sm_log_probs.exp()\n",
    "            expected_edits.append((edit_chunk * sm_probs).sum(dim=-1))\n",
    "        else:\n",
    "            expected_edits.append(-valid_log_prob_chunk.tensor.to(device))\n",
    "    return torch.cat(expected_edits, dim=1)\n",
    "\n",
    "\n",
    "def compute_dists(sample_forms, wordlist):\n",
    "    # global _DISTS_CACHE\n",
    "    bs, ns = sample_forms.shape\n",
    "    sample_forms = sample_forms.flatten()\n",
    "    edits = cdist(wordlist, sample_forms, scorer=Levenshtein.distance, workers=-1)\n",
    "    edits = edits.reshape(len(wordlist), bs, ns)\n",
    "    dists = np.transpose(edits, [1, 0, 2])\n",
    "    dists = np.concatenate([np.zeros([bs, len(wordlist), 1], dtype='int64'), dists], axis=-1)\n",
    "    dists = dists.astype('float32')\n",
    "    lengths = np.asarray(list(map(len, wordlist)))\n",
    "    sample_lengths = np.asarray(list(map(len, sample_forms))).reshape(bs, ns)\n",
    "    min_lengths = np.minimum(lengths.reshape(1, -1, 1), sample_lengths.reshape(bs, 1, ns))\n",
    "    min_lengths = np.concatenate([np.repeat(lengths.reshape(1, -1), bs, axis=0).reshape(bs, -1, 1),\n",
    "                                  min_lengths], axis=-1) + 1\n",
    "    dists = dists / min_lengths\n",
    "    return dists\n",
    "\n",
    "\n",
    "def compute_duplicates(sample_forms, wordlist):\n",
    "    bs, ns = sample_forms.shape\n",
    "    dups = np.ones([bs, len(wordlist), 1 + ns])\n",
    "    for i, b_samples in enumerate(sample_forms):\n",
    "        sampled = {}\n",
    "        for k, b_sample in enumerate(b_samples, 1):\n",
    "            if b_sample in sampled:\n",
    "                dups[i, :, k] = 0.0\n",
    "                continue\n",
    "            sampled[b_sample] = k\n",
    "        for j, orig in enumerate(wordlist):\n",
    "            if orig in sampled:\n",
    "                k = sampled[orig]\n",
    "                dups[i, j, k] = 0.0\n",
    "    return dups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "T15faXi7wmDK"
   },
   "outputs": [],
   "source": [
    "#This class manages a matrix of similarity scores between words in the lost language and words in the known language. It updates iteratively the scores by calling a model and uses a (static?) momentum\n",
    "#to slow down the updates as the iterations pile up\n",
    "@has_properties('lost_lang', 'known_lang', 'num_cognates')\n",
    "class Flow:\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, momentum, num_cognates, evaluation_mode=False, split=None):\n",
    "        super().__init__()\n",
    "        lost_words = get_words(lost_lang)\n",
    "        known_words = get_words(known_lang)\n",
    "\n",
    "        if evaluation_mode:\n",
    "            words = []\n",
    "            train_idxs = set(split['train'])\n",
    "            for w in lost_words:\n",
    "                if w.idx in train_idxs:\n",
    "                    words.append(w)\n",
    "            lost_words = words\n",
    "        flow = get_tensor(np.zeros([len(lost_words), len(known_words)]))\n",
    "        self.flow = MagicTensor(flow, lost_words, known_words)\n",
    "        self._warmed_up = False\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def state_dict(self):\n",
    "        \"\"\"Use words as the indices.\"\"\"\n",
    "\n",
    "        return {'lost_forms': get_forms(self.lost_lang),\n",
    "                'known_forms': get_forms(self.known_lang),\n",
    "                'flow': self.flow}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        lost_forms = get_forms(self.lost_lang)\n",
    "        known_forms = get_forms(self.known_lang)\n",
    "        saved_lost_forms = state_dict['lost_forms']\n",
    "        saved_known_forms = state_dict['known_forms']\n",
    "        saved_flow = state_dict['flow']\n",
    "        assert (lost_forms == saved_lost_forms).all()\n",
    "        assert (known_forms == saved_known_forms).all()\n",
    "        self.flow.data.copy_(state_dict['flow'].tensor)\n",
    "        logging.critical(state_dict['flow'])\n",
    "    @log_this('IMP')\n",
    "    def warm_up(self):\n",
    "        value = self.num_cognates / self.flow.numel()\n",
    "        self.flow.tensor[:] = value\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def update(self, model, data_loader, num_cognates, edit, capacity):\n",
    "        model.eval()\n",
    "        entire_batch = data_loader.entire_batch\n",
    "        \n",
    "        model_ret = model(entire_batch, mode='flow', capacity=capacity, num_cognates=num_cognates, edit=edit)\n",
    "        new_flow = model_ret.flow\n",
    "        logging.critical(f\"FLOW TENSOR: {self.flow.tensor.shape}, FLOW MODEL:{new_flow.tensor.shape}\")\n",
    "        logging.critical(f\"new_flow: {new_flow}, {new_flow.tensor.sum()}, {data_loader}, {num_cognates}, {edit}, {capacity}\")\n",
    "        self._check_acc(new_flow)\n",
    "        self.flow = self.momentum * self.flow + (1.0 - self.momentum) * new_flow\n",
    "\n",
    "    def _check_acc(self, flow):\n",
    "        preds = flow.get_best(nonzero=True)\n",
    "        # Checking lost.\n",
    "        #logging.critical(f\"preds: {preds}\")\n",
    "        acc = sum([has_cognate(w, self.known_lang) for w in preds.keys()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the lost side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking known.\n",
    "        acc = sum([has_cognate(w, self.lost_lang) for w in preds.values()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the known side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking lost and known.\n",
    "        acc = sum([is_cognate(w1, w2) for w1, w2 in preds.items()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy for lost-known {acc} / {len(preds)} = {rate:.3f} ')\n",
    "\n",
    "    def select(self, lost_words, known_words):\n",
    "        \"\"\"Take the subtensor, specified by the words.\"\"\"\n",
    "        flow = self.flow.select_rows(lost_words).select_cols(known_words)\n",
    "        flow_k = flow.tensor.sum(dim=0)\n",
    "        flow_l = flow.tensor.sum(dim=1)\n",
    "\n",
    "        return {'flow': flow,\n",
    "                'flow_k': flow_k,\n",
    "                'flow_l': flow_l,\n",
    "                'total_flow_k': flow_k.sum(),\n",
    "                'total_flow_l': flow_l.sum()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "79SKd4HyF9xS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def min_cost_flow(dists, demand, n_similar=None, capacity=1):\n",
    "    '''\n",
    "    Modified from https://developers.google.com/optimization/flow/mincostflow.\n",
    "\n",
    "    ``capacity`` controls how many lost tokens can be mapped to the same known token.\n",
    "    If it is set to -1, then there is no constraint at all, otherwise use its value.\n",
    "    '''\n",
    "    #logging.critical('Solving flow')\n",
    "    dists = (dists * 100.0).astype('int64')\n",
    "    max_demand = min(dists.shape[0], dists.shape[1])\n",
    "    if demand > max_demand:\n",
    "        logging.warning('demand too big, set to %d instead' % (max_demand))\n",
    "        demand = max_demand\n",
    "    # between each pair. For instance, the arc from node 0 to node 1 has a\n",
    "    # capacity of 15 and a unit cost of 4.\n",
    "    nt, ns = dists.shape\n",
    "    start_nodes = list()\n",
    "    end_nodes = list()\n",
    "    unit_costs = list()\n",
    "    capacities = list()\n",
    "    # source to c_t\n",
    "    for t in range(nt):\n",
    "        start_nodes.append(0)\n",
    "        end_nodes.append(t + 2)  # NOTE 0 is reserved for source, and 1 for sink\n",
    "        unit_costs.append(0)\n",
    "        capacities.append(1)\n",
    "    # c_s to sink\n",
    "    for s in range(ns):\n",
    "        start_nodes.append(s + 2 + nt)\n",
    "        end_nodes.append(1)\n",
    "        unit_costs.append(0)\n",
    "        if capacity == -1:\n",
    "            capacities.append(nt + ns)  # NOTE Ignore capacity constraint.\n",
    "        else:\n",
    "            capacities.append(capacity)\n",
    "    #logging.critical(f\"{nt}, {ns}\")\n",
    "    # c_t to c_s\n",
    "    if n_similar:  # and False:\n",
    "        idx = dists.argpartition(n_similar - 1, axis=1)[:, :n_similar]\n",
    "        all_words = set()\n",
    "        for t in range(nt):\n",
    "            all_s = idx[t]\n",
    "            all_words.update(all_s)\n",
    "        #    for s in all_s:\n",
    "        #        start_nodes.append(t + 2)\n",
    "        #        end_nodes.append(s + 2 + nt)\n",
    "        #        unit_costs.append(dists[t, s])\n",
    "        if len(all_words) < demand:\n",
    "            logging.warning('pruned too many words, adding some more')\n",
    "            # raised an exception, as random.sample needs a collection (e.g. list) as first input param\n",
    "            # added = random.sample(set(range(ns)) - all_words, demand - len(all_words))\n",
    "            added = random.sample(list(set(range(ns)) - all_words), demand - len(all_words))\n",
    "            all_words.update(added)\n",
    "        #    for s in added:\n",
    "        #        for t in range(nt):\n",
    "        #            start_nodes.append(t + 2)\n",
    "        #            end_nodes.append(s + 2 + nt)\n",
    "        #            unit_costs.append(dists[t, s])\n",
    "        for t, s in itertools.product(range(nt), all_words):\n",
    "            start_nodes.append(t + 2)\n",
    "            end_nodes.append(s + 2 + nt)\n",
    "            unit_costs.append(dists[t, s])\n",
    "            capacities.append(1)\n",
    "    else:\n",
    "        for t, s in itertools.product(range(nt), range(ns)):\n",
    "            start_nodes.append(t + 2)\n",
    "            end_nodes.append(s + 2 + nt)\n",
    "            unit_costs.append(dists[t, s])\n",
    "            capacities.append(1)\n",
    "    #logging.critical(f\"{sys.getsizeof(start_nodes)}, {len(start_nodes)}\")\n",
    "    # Define an array of supplies at each node.\n",
    "    supplies = [demand, -demand]  # + [0] * (nt + ns)\n",
    "\n",
    "    # Instantiate a SimpleMinCostFlow solver.\n",
    "    min_cost_flow = mcf.SimpleMinCostFlow()\n",
    "\n",
    "    # Add each arc.\n",
    "    #for i in range(0, len(start_nodes)):\n",
    "    #    min_cost_flow.AddArcWithCapacityAndUnitCost(\n",
    "    #        int(start_nodes[i]),\n",
    "    #        int(end_nodes[i]),\n",
    "    #        int(capacities[i]),\n",
    "    #        int(unit_costs[i])\n",
    "    #    )\n",
    "\n",
    "    # now library supports list initialization\n",
    "    min_cost_flow.add_arcs_with_capacity_and_unit_cost(\n",
    "            start_nodes,\n",
    "            end_nodes,\n",
    "            capacities,\n",
    "            unit_costs\n",
    "        )\n",
    "\n",
    "    # Add node supplies.\n",
    "    #for i in range(0, len(supplies)):\n",
    "    #    min_cost_flow.SetNodeSupply(i, supplies[i])\n",
    "    # also here list initialization\n",
    "    min_cost_flow.set_nodes_supplies(np.arange(0, len(supplies)), supplies)\n",
    "    #logging.critical(\"Starting solver\")\n",
    "    # Find the minimum cost flow between node 0 and node 4.\n",
    "    if min_cost_flow.solve() == min_cost_flow.OPTIMAL:\n",
    "        cost = min_cost_flow.optimal_cost()\n",
    "        flow = np.zeros([nt, ns])\n",
    "        for i in range(min_cost_flow.num_arcs()):\n",
    "            t = min_cost_flow.tail(i)\n",
    "            s = min_cost_flow.head(i)\n",
    "            if t > 1 and s > 1 + nt:\n",
    "                flow[t - 2, s - 2 - nt] = min_cost_flow.flow(i)\n",
    "        return flow, cost\n",
    "    else:\n",
    "        logging.error('There was an issue with the min cost flow input.')\n",
    "        raise RuntimeError('Min cost flow solver error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NNR6ByCaHhOt"
   },
   "outputs": [],
   "source": [
    "\n",
    "_SAFE_METHODS = {'numel', '__str__', 'data', 'shape', 'unsqueeze'}\n",
    "_SAFE_METHODS_WITH_WRAPPER = {'log'}\n",
    "\n",
    "#Tensor to handle complex operations\n",
    "@has_properties('tensor', 'row_words', 'col_words')\n",
    "class MagicTensor:\n",
    "\n",
    "    def __init__(self, tensor, row_words, col_words):\n",
    "        assert tensor.ndimension() == 2\n",
    "        #logging.debug(f\"{row_words}\\n {isinstance(row_words[0], Word)}\\n {type(row_words[0])}\")\n",
    "        assert isinstance(row_words[0], Word)\n",
    "        assert isinstance(col_words[0], Word)\n",
    "        assert len(row_words) == tensor.shape[0]\n",
    "        assert len(col_words) == tensor.shape[1]\n",
    "\n",
    "        def freeze(words):\n",
    "            if isinstance(words, tuple):\n",
    "                return words\n",
    "            else:\n",
    "                return tuple(words)\n",
    "\n",
    "        # NOTE Freeze these to speed up checking procedure.\n",
    "        self._row_words = freeze(row_words)\n",
    "        self._col_words = freeze(col_words)\n",
    "        # Fast indexing.\n",
    "        self._word2row = {w: i for i, w in enumerate(self._row_words)}\n",
    "        self._word2col = {w: i for i, w in enumerate(self._col_words)}\n",
    "\n",
    "    def _check_value(self, other):\n",
    "        if isinstance(other, MagicTensor):\n",
    "            try:\n",
    "                assert self.row_words == other.row_words\n",
    "                assert self.col_words == other.col_words\n",
    "                return other.tensor\n",
    "            except AssertionError:\n",
    "                self._permute(other)\n",
    "                return other.tensor\n",
    "        elif isinstance(other, (float, int)):\n",
    "            return other\n",
    "        else:\n",
    "            raise NotImplementedError(f'Type {type(other)} not supported.')\n",
    "\n",
    "    @log_this()\n",
    "    def _permute(self, other):\n",
    "        # Have to re-index the my own tensor. But make sure that the set of words are identical first.\n",
    "        assert set(self.row_words) == set(other.row_words)\n",
    "        assert set(self.col_words) == set(other.col_words)\n",
    "        my_rows = [self._word2row[w] for w in other.row_words]\n",
    "        my_cols = [self._word2col[w] for w in other.col_words]\n",
    "        self._row_words = other.row_words\n",
    "        self._col_words = other.col_words\n",
    "        self._word2row = other._word2row\n",
    "        self._word2col = other._word2col\n",
    "        self._tensor = self._tensor[my_rows][:, my_cols]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MagicTensor({self.tensor!r})'\n",
    "\n",
    "    def __getattribute__(self, attr):\n",
    "        try:\n",
    "            return super().__getattribute__(attr)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                tensor = super().__getattribute__('_tensor')\n",
    "            except AttributeError:\n",
    "                raise\n",
    "\n",
    "            orig = getattr(tensor, attr)\n",
    "            if attr in _SAFE_METHODS:\n",
    "                setattr(self, attr, orig)\n",
    "            elif attr in _SAFE_METHODS_WITH_WRAPPER:\n",
    "\n",
    "                @wraps(orig)\n",
    "                def wrapper(self, *args, **kwargs):\n",
    "                    ret = orig(*args, **kwargs)\n",
    "                    return MagicTensor(ret, self.row_words, self.col_words)\n",
    "\n",
    "                setattr(self, attr, types.MethodType(wrapper, self))\n",
    "            else:\n",
    "                raise AttributeError\n",
    "            return getattr(self, attr)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        value = self._check_value(other)\n",
    "        return MagicTensor(self.tensor + value, self.row_words, self.col_words)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        value = self._check_value(other)\n",
    "        return MagicTensor(self.tensor * value, self.row_words, self.col_words)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        new_tensor = self.tensor[key]\n",
    "        if isinstance(key, tuple):\n",
    "            assert len(key) == 2\n",
    "            s0, s1 = key\n",
    "            return MagicTensor(new_tensor, self.row_words[s0], self.col_words[s1])\n",
    "        else:\n",
    "            return MagicTensor(new_tensor, self.row_words[key], self.col_words)\n",
    "\n",
    "    def select_rows(self, words):\n",
    "        ids = [self._word2row[w] for w in words]\n",
    "        return MagicTensor(self.tensor[ids], words, self.col_words)\n",
    "\n",
    "    def select_cols(self, words):\n",
    "        ids = [self._word2col[w] for w in words]\n",
    "        return MagicTensor(self.tensor[:, ids], self.row_words, words)\n",
    "    '''\n",
    "    def get_best(self, topk=1, nonzero=False, return_scores=False, exp_probs=True):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          - If return_scores=False:\n",
    "              dict {lost_word: known_word or [known_word1, ...]}\n",
    "          - If return_scores=True:\n",
    "              tuple of two dicts:\n",
    "                * ret_words:  {lost_word: known_word or [known_word1, ...]}\n",
    "                * ret_scores: {lost_word: prob or [prob1, ...]} (exp(log_prob) if exp_probs=True)\n",
    "        \n",
    "        Args:\n",
    "          - topk: number of predictions per lost word\n",
    "          - nonzero: if True, filters out results with score <= 0\n",
    "          - return_scores: if True, returns a separate dictionary of scores\n",
    "          - exp_probs: if True, applies exp(log_prob) to return real probabilities\n",
    "        \"\"\"\n",
    "        ret_words = dict()\n",
    "        ret_scores = dict() if return_scores else None\n",
    "    \n",
    "        topk_vals, topk_idxs = self.tensor.topk(topk, dim=-1)\n",
    "    \n",
    "        for lost_idx in range(len(self.row_words)):\n",
    "            lost = self.row_words[lost_idx]\n",
    "            known_list = []\n",
    "            score_list = []\n",
    "    \n",
    "            for rank in range(topk):\n",
    "                log_score = topk_vals[lost_idx, rank].item()\n",
    "                if nonzero and log_score <= 0:\n",
    "                    continue\n",
    "                known_idx = topk_idxs[lost_idx, rank].item()\n",
    "                known = self.col_words[known_idx]\n",
    "    \n",
    "                known_list.append(known)\n",
    "                if return_scores:\n",
    "                    prob = math.exp(log_score) if exp_probs else log_score\n",
    "                    score_list.append(prob)\n",
    "    \n",
    "            if known_list:\n",
    "                ret_words[lost] = known_list[0] if topk == 1 else known_list\n",
    "                if return_scores:\n",
    "                    ret_scores[lost] = score_list[0] if topk == 1 else score_list\n",
    "    \n",
    "        return (ret_words, ret_scores) if return_scores else ret_words\n",
    "    '''\n",
    "    #takes best scores for each word of the lost language\n",
    "    def get_best(self, nonzero=False):\n",
    "        ret = dict()\n",
    "        # this is an argmax\n",
    "        best_idx = self.tensor.max(dim=-1)[1].cpu().numpy()\n",
    "        for lost_idx, known_idx in enumerate(best_idx):\n",
    "            if not nonzero or self.tensor[lost_idx, known_idx].item() > 0:\n",
    "                lost = self.row_words[lost_idx]\n",
    "                known = self.col_words[known_idx]\n",
    "                assert lost not in ret\n",
    "                ret[lost] = known\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "RZR6rGyOQfS7",
    "outputId": "38325fb9-0111-4737-d352-f14c883e0529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#This class manages a matrix of similarity scores between words in the lost language and words in the known language. It updates iteratively the scores by calling a model and uses a (static?) momentum\\n#to slow down the updates as the iterations pile up\\n@has_properties(\\'lost_lang\\', \\'known_lang\\', \\'momentum\\', \\'num_cognates\\')\\nclass Flow:\\n\\n    def __init__(self, lost_lang, known_lang, momentum, num_cognates):\\n        super().__init__()\\n        lost_words = get_words(lost_lang)\\n        known_words = get_words(known_lang)\\n        flow = get_tensor(np.zeros([len(lost_words), len(known_words)]))\\n        flow = MagicTensor(flow, lost_words, known_words)\\n\\n        self._warmed_up = False\\n        self.flow_file = \"flow_backup.pt\"\\n\\n        self.store_flow_tensor(flow)\\n\\n    def store_flow_tensor(self, flow):\\n        logging.critical(f\"Saving Flow: {flow.shape}\")\\n        torch.save(flow.tensor, self.flow_file)\\n\\n    def load_flow_tensor(self):\\n        return torch.load(self.flow_file, weights_only=True)\\n\\n    def get_magic_tensor(self, flow):\\n        lost_words = get_words(self.lost_lang)\\n        known_words = get_words(self.known_lang)\\n        return MagicTensor(flow, lost_words, known_words)\\n\\n    def state_dict(self):\\n        \"\"\"Use words as the indices.\"\"\"\\n        flow = self.get_magic_tensor(self.load_flow_tensor())\\n        return {\\'lost_forms\\': get_forms(self.lost_lang),\\n                \\'known_forms\\': get_forms(self.known_lang),\\n                \\'flow\\': flow}\\n\\n    def load_state_dict(self, state_dict):\\n        lost_forms = get_forms(self.lost_lang)\\n        known_forms = get_forms(self.known_lang)\\n        saved_lost_forms = state_dict[\\'lost_forms\\']\\n        saved_known_forms = state_dict[\\'known_forms\\']\\n        assert (lost_forms == saved_lost_forms).all()\\n        assert (known_forms == saved_known_forms).all()\\n        self.store_flow_tensor(state_dict[\\'flow\\'].tensor)\\n\\n    @log_this(\\'IMP\\')\\n    def warm_up(self):\\n        flow = self.get_magic_tensor(self.load_flow_tensor())\\n        value = self.num_cognates / flow.numel()\\n        flow.tensor[:] = value\\n        self.store_flow_tensor(flow)\\n\\n\\n    @log_this(\\'IMP\\')\\n    def update(self, model, data_loader, num_cognates, edit, capacity):\\n        flow = self.get_magic_tensor(self.load_flow_tensor())\\n        model.eval()\\n        entire_batch = data_loader.entire_batch\\n        model_ret = model(entire_batch, mode=\\'flow\\', capacity=capacity, num_cognates=num_cognates, edit=edit)\\n        new_flow = model_ret.flow\\n        self._check_acc(new_flow)\\n        flow = self.momentum * flow + (1.0 - self.momentum) * new_flow\\n        self.store_flow_tensor(flow)\\n\\n    def _check_acc(self, flow):\\n        preds = flow.get_best(nonzero=True)\\n        # Checking lost.\\n        acc = sum([has_cognate(w, self.known_lang) for w in preds.keys()])\\n        rate = acc / len(preds)\\n        logging.imp(f\\'Accuracy on the lost side {acc} / {len(preds)} = {rate:.3f} \\')\\n        # Checking known.\\n        acc = sum([has_cognate(w, self.lost_lang) for w in preds.values()])\\n        rate = acc / len(preds)\\n        logging.imp(f\\'Accuracy on the known side {acc} / {len(preds)} = {rate:.3f} \\')\\n        # Checking lost and known.\\n        acc = sum([is_cognate(w1, w2) for w1, w2 in preds.items()])\\n        rate = acc / len(preds)\\n        logging.imp(f\\'Accuracy for lost-known {acc} / {len(preds)} = {rate:.3f} \\')\\n\\n    def select(self, lost_words, known_words):\\n        \"\"\"Take the subtensor, specified by the words.\"\"\"\\n        flow = self.get_magic_tensor(self.load_flow_tensor())\\n        flow = flow.select_rows(lost_words).select_cols(known_words)\\n        flow_k = flow.tensor.sum(dim=0)\\n        flow_l = flow.tensor.sum(dim=1)\\n\\n        return {\\'flow\\': flow,\\n                \\'flow_k\\': flow_k,\\n                \\'flow_l\\': flow_l,\\n                \\'total_flow_k\\': flow_k.sum(),\\n                \\'total_flow_l\\': flow_l.sum()}\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This class manages a matrix of similarity scores between words in the lost language and words in the known language. It updates iteratively the scores by calling a model and uses a (static?) momentum\n",
    "#to slow down the updates as the iterations pile up\n",
    "@has_properties('lost_lang', 'known_lang', 'momentum', 'num_cognates')\n",
    "class Flow:\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, momentum, num_cognates):\n",
    "        super().__init__()\n",
    "        lost_words = get_words(lost_lang)\n",
    "        known_words = get_words(known_lang)\n",
    "        flow = get_tensor(np.zeros([len(lost_words), len(known_words)]))\n",
    "        flow = MagicTensor(flow, lost_words, known_words)\n",
    "\n",
    "        self._warmed_up = False\n",
    "        self.flow_file = \"flow_backup.pt\"\n",
    "\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "    def store_flow_tensor(self, flow):\n",
    "        logging.critical(f\"Saving Flow: {flow.shape}\")\n",
    "        torch.save(flow.tensor, self.flow_file)\n",
    "\n",
    "    def load_flow_tensor(self):\n",
    "        return torch.load(self.flow_file, weights_only=True)\n",
    "\n",
    "    def get_magic_tensor(self, flow):\n",
    "        lost_words = get_words(self.lost_lang)\n",
    "        known_words = get_words(self.known_lang)\n",
    "        return MagicTensor(flow, lost_words, known_words)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Use words as the indices.\"\"\"\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        return {'lost_forms': get_forms(self.lost_lang),\n",
    "                'known_forms': get_forms(self.known_lang),\n",
    "                'flow': flow}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        lost_forms = get_forms(self.lost_lang)\n",
    "        known_forms = get_forms(self.known_lang)\n",
    "        saved_lost_forms = state_dict['lost_forms']\n",
    "        saved_known_forms = state_dict['known_forms']\n",
    "        assert (lost_forms == saved_lost_forms).all()\n",
    "        assert (known_forms == saved_known_forms).all()\n",
    "        self.store_flow_tensor(state_dict['flow'].tensor)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def warm_up(self):\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        value = self.num_cognates / flow.numel()\n",
    "        flow.tensor[:] = value\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def update(self, model, data_loader, num_cognates, edit, capacity):\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        model.eval()\n",
    "        entire_batch = data_loader.entire_batch\n",
    "        model_ret = model(entire_batch, mode='flow', capacity=capacity, num_cognates=num_cognates, edit=edit)\n",
    "        new_flow = model_ret.flow\n",
    "        self._check_acc(new_flow)\n",
    "        flow = self.momentum * flow + (1.0 - self.momentum) * new_flow\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "    def _check_acc(self, flow):\n",
    "        preds = flow.get_best(nonzero=True)\n",
    "        # Checking lost.\n",
    "        acc = sum([has_cognate(w, self.known_lang) for w in preds.keys()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the lost side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking known.\n",
    "        acc = sum([has_cognate(w, self.lost_lang) for w in preds.values()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the known side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking lost and known.\n",
    "        acc = sum([is_cognate(w1, w2) for w1, w2 in preds.items()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy for lost-known {acc} / {len(preds)} = {rate:.3f} ')\n",
    "\n",
    "    def select(self, lost_words, known_words):\n",
    "        \"\"\"Take the subtensor, specified by the words.\"\"\"\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        flow = flow.select_rows(lost_words).select_cols(known_words)\n",
    "        flow_k = flow.tensor.sum(dim=0)\n",
    "        flow_l = flow.tensor.sum(dim=1)\n",
    "\n",
    "        return {'flow': flow,\n",
    "                'flow_k': flow_k,\n",
    "                'flow_l': flow_l,\n",
    "                'total_flow_k': flow_k.sum(),\n",
    "                'total_flow_l': flow_l.sum()}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYrpaOIu2_lQ"
   },
   "source": [
    "### FastText Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZYF5ECl3Fq-",
    "outputId": "bf86cb40-4f27-4dba-b473-8cd11080f78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in ./.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from fasttext) (1.23.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.local/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.local/lib/python3.10/site-packages (from fasttext) (80.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "class LinearBWordEmbedding:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext\"), \"linearb_fasttext_model.bin\"),\n",
    "                 train_data_path=\"linearb_train.txt\",\n",
    "                 model='skipgram',\n",
    "                 dim=400,\n",
    "                 ws=5,\n",
    "                 min_count=1,\n",
    "                 epoch=50,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=3,\n",
    "                 neg=5,\n",
    "                 loss='softmax',\n",
    "                 bucket=2000000,\n",
    "                 thread=4,\n",
    "                 t=0.0001,\n",
    "                 word_ngrams=1,\n",
    "                 verbose=2\n",
    "                ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'model': model,\n",
    "            'dim': dim,\n",
    "            'ws': ws,\n",
    "            'minCount': min_count,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'neg': neg,\n",
    "            'loss': loss,\n",
    "            'bucket': bucket,\n",
    "            'thread': thread,\n",
    "            't': t,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, sentences):\n",
    "        processed = []\n",
    "        uppercase_pattern = re.compile(r'[A-Z]')\n",
    "        digits_only_pattern = re.compile(r'^\\d+$')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            filtered_tokens = []\n",
    "            for token in tokens:\n",
    "                token_no_hyphen = token.replace('-', '')\n",
    "                if uppercase_pattern.search(token_no_hyphen):\n",
    "                    continue\n",
    "                if digits_only_pattern.match(token_no_hyphen):\n",
    "                    continue\n",
    "                filtered_tokens.append(token_no_hyphen)\n",
    "            processed.append(filtered_tokens)\n",
    "        return processed\n",
    "\n",
    "    def _write_train_data(self, sentences):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for tokens in sentences:\n",
    "                f.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "    def train(self, sentences):\n",
    "        tokenized = self.preprocess_text(sentences)\n",
    "        self._write_train_data(tokenized)\n",
    "        self.model = fasttext.train_unsupervised(\n",
    "            input=self.train_data_path,\n",
    "            model=self.params['model'],\n",
    "            dim=self.params['dim'],\n",
    "            ws=self.params['ws'],\n",
    "            minCount=self.params['minCount'],\n",
    "            epoch=self.params['epoch'],\n",
    "            lr=self.params['lr'],\n",
    "            minn=self.params['minn'],\n",
    "            maxn=self.params['maxn'],\n",
    "            neg=self.params['neg'],\n",
    "            loss=self.params['loss'],\n",
    "            bucket=self.params['bucket'],\n",
    "            thread=self.params['thread'],\n",
    "            t=self.params['t'],\n",
    "            wordNgrams=self.params['wordNgrams'],\n",
    "            verbose=self.params['verbose']\n",
    "        )\n",
    "        # Optional: remove training data file after training\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if self.model:\n",
    "            return self.model.get_word_vector(word)\n",
    "        return None\n",
    "\n",
    "    def find_nearest_neighbors(self, word, k=5):\n",
    "        if self.model:\n",
    "            return self.model.get_nearest_neighbors(word, k)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "\n",
    "# train the model\n",
    "#embedding_model.train(sequences)\n",
    "#embedding_model.save_model()  # saves to default path 'linearb_fasttext_model.bin'\n",
    "\n",
    "embedding_model.load_model()  # loads from default path 'linearb_fasttext_model.bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:{'aminiso': [(0.8581138849258423, 'aminisode'), (0.8470271825790405, 'miniso'), (0.8402059674263, 'aminisi'), (0.8115618228912354, 'aminisijo'), (0.7711604237556458, 'aminisija')], 'konoso': [(0.8202811479568481, 'kono'), (0.784777820110321, 'konosode'), (0.7607357501983643, 'ekosono'), (0.7548788785934448, 'kumonoso'), (0.6943257451057434, 'koso')], 'dorikajo': [(0.8442605137825012, 'dorikao'), (0.7919374108314514, 'dorikano'), (0.721727192401886, 'doriwo'), (0.6376739144325256, 'dorijo'), (0.6342973709106445, 'dikatajo')], 'a-mi-ni-so': [(0.8006194233894348, 'aminiso'), (0.7711402773857117, 'miniso'), (0.7276173830032349, 'daimiso'), (0.6552466750144958, 'aminisi'), (0.6410072445869446, 'zamiso')], 'ko-no-so': [(0.8620367050170898, 'konoso'), (0.8470064997673035, 'koso'), (0.6635887622833252, 'ekoso'), (0.6566115021705627, 'kono'), (0.6430438756942749, 'kosojo')], 'do-ri-ka-jo': [(0.713040292263031, 'dorikao'), (0.7096354365348816, 'dorijo'), (0.6676896214485168, 'doriwo'), (0.6477639675140381, 'akatajojo'), (0.6168322563171387, 'dikatajo')]}\n"
     ]
    }
   ],
   "source": [
    "words = [\"aminiso\", \"konoso\", \"dorikajo\", \"a-mi-ni-so\", \"ko-no-so\", \"do-ri-ka-jo\", ]\n",
    "dic = {word: embedding_model.find_nearest_neighbors(word) for word in words}\n",
    "logging.critical(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5yEV0dkIah7"
   },
   "source": [
    "### DecipherModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "HKmoyHvegoWr"
   },
   "outputs": [],
   "source": [
    "@has_properties('char_emb_dim', 'hidden_size', 'num_layers', 'dropout', 'universal_charset_size', 'lost_lang', 'known_lang', 'norms_or_ratios', 'control_mode', 'residual')\n",
    "class DecipherModel(nn.Module):\n",
    "\n",
    "    def __init__(self, trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(self.char_emb_dim, self.hidden_size, num_layers=self.num_layers,\n",
    "                               dropout=dropout, bidirectional=True, batch_first=True) # dropout set to 0 cause this shit manages to not be deterministic even with seeds and all kinds shit\n",
    "        self.decoder = MultiLayerLSTMCell(2*self.char_emb_dim,\n",
    "                                          self.hidden_size, self.num_layers, self.dropout)\n",
    "        self.attention = GlobalAttention(2 * self.hidden_size, self.hidden_size, dropout=self.dropout)\n",
    "        langs = (self.lost_lang, self.known_lang)\n",
    "        self.char_emb = UniversalCharEmbedding(langs, self.char_emb_dim, self.universal_charset_size)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 3, self.char_emb_dim), # MODIFIED 3 into 5\n",
    "            nn.LeakyReLU())\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        if self.residual:\n",
    "            self.controlled_residual = NormControlledResidual(\n",
    "                norms_or_ratios=self.norms_or_ratios, control_mode=self.control_mode)\n",
    "        self.trie = trie\n",
    "        #self.otherGPU = torch.device(\"cuda:1\")\n",
    "\n",
    "        #ft_dim is currently 256\n",
    "        self.ft_proj = nn.Linear(256, 2 * self.num_layers * self.hidden_size)\n",
    "        #self.ft_residual = nn.Linear(256, self.hidden_size)\n",
    "    def encode(self, batch, fasttext_embs=None):\n",
    "        inp_enc = self.char_emb(batch.lost.id_seqs, self.lost_lang)  # bs x L x d\n",
    "        # inp_enc = self.drop(inp_enc)\n",
    "        bs = inp_enc.shape[0]\n",
    "        inp_packed = nn.utils.rnn.pack_padded_sequence(self.drop(inp_enc), batch.lost.lengths.cpu(), batch_first=True)\n",
    "        # initial hidden and cell states are zeros\n",
    "        c = get_zeros(2 * self.num_layers, bs, self.hidden_size)  # NOTE bidirectional, therefore 2\n",
    "        h = get_zeros(2 * self.num_layers, bs, self.hidden_size)\n",
    "        \n",
    "        if fasttext_embs is None:\n",
    "            forms = batch.lost.forms\n",
    "            fasttext_embs = [get_tensor(embedding_model.get_vector(form.replace(\"-\", \"\")), dtype=\"f\") for form in forms]\n",
    "            #random.shuffle(fasttext_embs)\n",
    "            fasttext_embs = torch.stack(fasttext_embs)\n",
    "\n",
    "        #fasttext_embs = fasttext_embs.to(self.otherGPU) if EVAL[0] else fasttext_embs\n",
    "        #ft_proj = self.ft_proj(fasttext_embs) # [bs, 2 * num_layers * hidden_size]\n",
    "        #ft_proj = ft_proj.view(2 * self.num_layers, bs, self.hidden_size)\n",
    "        #c = ft_proj\n",
    "        \n",
    "        # encoding is a tuple (h, c)\n",
    "        #inp_packed = inp_packed.to(self.otherGPU) if EVAL[0] else inp_packed\n",
    "        #h = h.to(self.otherGPU) if EVAL[0] else h\n",
    "        h_s_packed, encoding = self.encoder(inp_packed, (h, c))\n",
    "        #encoding is a tuple of long and short term memory of 2*num_layers, bs, hs\n",
    "        h_s = nn.utils.rnn.pad_packed_sequence(h_s_packed, batch_first=True)[0]\n",
    "        #encoding = LSTMState.from_pytorch(encoding)\n",
    "        encoding = self.decoder.init_state(h_s.shape[0], encoding)\n",
    "        #h_s: bs, l, 2*hidden_size\n",
    "        #encoding: bs, hidden_size\n",
    "\n",
    "        return inp_enc, h_s, encoding#, fasttext_embs\n",
    "\n",
    "    def forward(self, batch, fasttext_embs=None):\n",
    "        # Remember to clear cache.\n",
    "        #logging.critical(f\"batch idseq {batch.lost.id_seqs[0]}\")\n",
    "        clear_cache()\n",
    "\n",
    "        lost = batch.lost.lang\n",
    "        known = batch.known.lang\n",
    "        # Encode.\n",
    "        #emb_s, h_s, encoding, fasttext_embs = self.encode(batch)\n",
    "        emb_s, h_s, encoding = self.encode(batch, fasttext_embs)\n",
    "\n",
    "        #logging.critical(f\"emb_s: {emb_s}\")\n",
    "        #logging.critical(f\"h_s: {h_s}\")\n",
    "        #logging.critical(f\"encoding: {encoding}\")\n",
    "\n",
    "        # mask out padding\n",
    "        mask_lost = (batch.lost.id_seqs != PAD_ID).float() # bs x sl\n",
    "        # Start decoding.\n",
    "        bs, sl, _ = h_s.shape\n",
    "        # recover the embedding of SOW\n",
    "        input_emb = self.char_emb.get_start_emb(known).expand(bs, -1)  # bs x d\n",
    "        #logging.critical(f\"input_emb: {input_emb}\")\n",
    "\n",
    "        # initialize decoder state using encoder states (combined for bidirectionality)\n",
    "        state = encoding\n",
    "        h_tilde = get_zeros(bs, self.char_emb_dim)\n",
    "        empty_ctx_s = get_zeros(bs, self.hidden_size * 2)\n",
    "        max_len = max(batch.known.lengths)\n",
    "        all_log_probs = list()\n",
    "        all_almt_distrs = list()\n",
    "\n",
    "            \n",
    "        for dec_step in range(max_len):\n",
    "            input_ = torch.cat([h_tilde, input_emb], dim=-1) #input emb is the embedding of start of word\n",
    "            input_ = self.drop(input_)\n",
    "            # updates LSTM states\n",
    "            state = self.decoder(input_, state)\n",
    "            # get hidden state of last LSTM state\n",
    "            ctx_t = state.get_output()  # bs x d\n",
    "            # get ctx_s\n",
    "            # it combines h_s (encoder's output) with ctx_t (decoder's output) and some learnable params\n",
    "            # we argue that this is some kind of encoder-decoder attention\n",
    "            almt_distr = self.attention(ctx_t, h_s, mask_lost) # bs x sl\n",
    "\n",
    "            #almt_distr, ft_contribute = self.attention(ctx_t, h_s, mask_lost, fasttext_embs) # bs x sl # MODIFIED\n",
    "            # uses attention on encoder's output and aggregates on sequence length\n",
    "            ctx_s = (almt_distr.view(bs, sl, 1) * h_s).sum(dim=1)  # bs x 2d\n",
    "            #ft_s = (ft_contribute.view(bs, sl, 1) * h_s).sum(dim=1) # MODIFIED\n",
    "            # get h_tilde\n",
    "            # concatenates attentioned encoder output with decoder output\n",
    "            cat = torch.cat([ctx_s, ctx_t], dim=-1)\n",
    "\n",
    "            #cat = torch.cat([ctx_s, ft_s, ctx_t], dim=-1) # MODIFIED\n",
    "            h_tilde_rnn = self.hidden(self.drop(cat))\n",
    "            if self.residual:\n",
    "                # uses attention weights also on encoder input!!!\n",
    "                ctx_s_emb = (almt_distr.view(bs, sl, 1) * emb_s).sum(dim=1)  # bs x d\n",
    "                h_tilde = self.controlled_residual(ctx_s_emb, h_tilde_rnn)#, self.ft_residual(fasttext_embs))\n",
    "            else:\n",
    "                h_tilde = h_tilde_rnn\n",
    "            # get probs\n",
    "            # projection lost->universal->known\n",
    "            logits = self.char_emb.project(self.drop(h_tilde), known)\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # bs x num_char\n",
    "            probs = log_probs.exp()\n",
    "\n",
    "            # back projection known->universal (to continue decoding)\n",
    "            input_emb = self.char_emb.soft_emb(probs, known)\n",
    "            # Collect stuff.\n",
    "            all_log_probs.append(log_probs.t())\n",
    "            all_almt_distrs.append(almt_distr)\n",
    "\n",
    "        log_probs = torch.stack(all_log_probs, dim=0)  # tl x nc x bs\n",
    "        almt_distr = torch.stack(all_almt_distrs, dim=1)  # bs x tl x sl\n",
    "\n",
    "        ret = self.trie.analyze(log_probs, almt_distr,\n",
    "                                batch.known.words, batch.lost.lengths)\n",
    "        ret.log_probs = log_probs\n",
    "        ret.valid_log_probs = MagicTensor(ret.valid_log_probs, batch.lost.words, batch.known.words)\n",
    "        ret.update(almt_distr=almt_distr)\n",
    "        return ret\n",
    "\n",
    "@has_properties('n_similar')\n",
    "class DecipherModelWithFlow(DecipherModel):\n",
    "    def __init__(self, trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual, n_similar):\n",
    "        super().__init__(trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            batch,\n",
    "            num_cognates=None,\n",
    "            mode='mle',\n",
    "            edit=True,\n",
    "            capacity=1,\n",
    "            fasttext_embs=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        if mode == 'mle':\n",
    "            ret = super().forward(batch, fasttext_embs)\n",
    "            #logging.critical(f\"forza valid_log_probs {ret.valid_log_probs.tensor[0]}\")\n",
    "            #logging.critical(f\"forza log_probs {ret.log_probs[0]}\")\n",
    "            #logging.critical(f\"known: {batch.known.words[0]}\")\n",
    "            #logging.critical(f\"lost: {batch.lost.words[0]}\")\n",
    "\n",
    "        else:\n",
    "            assert not self.training\n",
    "            with torch.no_grad():\n",
    "                ret = super().forward(batch)\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "                ret.update(flow=flow, cost=cost, expected_edits=expected_edits)\n",
    "\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6labFmFVLx0k"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4xOAdHJ7OqSs"
   },
   "outputs": [],
   "source": [
    "#import gc\n",
    "\n",
    "@has_properties('num_rounds', 'num_epochs_per_M_step', 'saved_path', 'learning_rate', 'log_dir', 'num_cognates', 'inc', 'warm_up_steps', 'capacity', 'save_all', 'eval_interval', 'reg_hyper', 'lost_lang', 'known_lang', 'momentum', 'check_interval')\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_data_loader, flow_data_loader, num_rounds, num_epochs_per_M_step, saved_path, learning_rate, log_dir, num_cognates, inc, warm_up_steps, capacity, save_all, eval_interval, reg_hyper, lost_lang, known_lang, momentum, check_interval, finetune=False, eval_mode=False, split=None):\n",
    "\n",
    "        self.tracker = Tracker('decipher')\n",
    "        stage = self.tracker.add_stage('round', self.num_rounds)\n",
    "        stage.add_stage('E step')\n",
    "        stage.add_stage('M step', self.num_epochs_per_M_step)\n",
    "        self.tracker.fix_schedule()\n",
    "        \n",
    "        self.eval_mode = eval_mode\n",
    "        self.split = split\n",
    "\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.flow_data_loader = flow_data_loader\n",
    "        self._init_optimizer()\n",
    "        self.flow = Flow(self.lost_lang, self.known_lang, self.momentum, self.num_cognates, self.eval_mode, self.split)\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "        self.finetune = finetune\n",
    "\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_params(self):\n",
    "        for name, param in self._get_trainable_params(names=True):\n",
    "            if param.ndimension() == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                logging.debug('initialized %s' % name)\n",
    "            else:\n",
    "                if 'bias_ih' in name or 'bias_hh' in name:\n",
    "                    size = param.size(0)\n",
    "                    ind = torch.arange(size // 4, size // 2).long()\n",
    "                    param.data[ind] = 1.0\n",
    "                    logging.debug(f'Forget gate bias initialized to 1.0 for {name}')\n",
    "\n",
    "    def _get_trainable_params(self, names=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if names:\n",
    "                    yield name, param\n",
    "                else:\n",
    "                    yield param\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\" or name == \"flow\": logging.critical(src)\n",
    "\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        if not self.finetune:\n",
    "            try_load('flow')\n",
    "        else:\n",
    "            self.flow.warm_up()\n",
    "\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "        #logging.critical(self.round_num)\n",
    "        #logging.critical(self.model)\n",
    "        logging.critical(self.flow.flow)\n",
    "        #stage = self.tracker.add_stage('round', self.num_rounds)\n",
    "        #stage.add_stage('E step')\n",
    "        #stage.add_stage('M step', self.num_epochs_per_M_step)\n",
    "        logging.critical(f\"{self.tracker}, {self.tracker.current_stage}, {self.tracker.get('round')}\")\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict(),\n",
    "                'flow': self.flow.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "            logging.critical(self.round_num)\n",
    "            logging.critical(self.stage)\n",
    "            logging.critical(self.flow.flow)\n",
    "            logging.critical(self.flow.flow.shape)\n",
    "            logging.critical(self.tracker.schedule.substages)\n",
    "            logging.critical(f\"{self.tracker}, {self.tracker.get('round')}, {self.tracker.current_stage}, {self.tracker.metrics.num_samples}\")\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "            #return\n",
    "        else:\n",
    "            self._init_params()\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'E step':\n",
    "            self._do_E_step()\n",
    "        elif self.stage.name == 'M step':\n",
    "            self._do_M_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    def _do_E_step(self):\n",
    "        logging.critical(f\"round {self.round_num}\")\n",
    "        num_cognates = min((self.round_num - 1) * self.inc, self.num_cognates)\n",
    "        logging.critical(f\"{self.round_num}, {self.warm_up_steps}, {self.stage.step}\")\n",
    "        edit = self.round_num > self.warm_up_steps\n",
    "        warm_up = self.stage.step == 0 and self.round_num == 1\n",
    "        self._E_step_kernel(num_cognates, warm_up, edit)\n",
    "\n",
    "        #gc.collect()                  # Python garbage collection\n",
    "        #torch.cuda.empty_cache()      # Releases unused memory held by caching allocator\n",
    "        #torch.cuda.ipc_collect()      # Reclaims memory from inter-process communication\n",
    "\n",
    "\n",
    "    @log_this('IMP', arg_list=['num_cognates', 'warm_up', 'edit'])\n",
    "    def _E_step_kernel(self, num_cognates, warm_up, edit):\n",
    "        if warm_up:\n",
    "            logging.debug(\"WARMUP FLOW\")\n",
    "            self.flow.warm_up()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.flow.update(self.model, self.flow_data_loader, num_cognates, edit, self.capacity[0])\n",
    "                self._init_params()\n",
    "                self._init_optimizer()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return (self.round_num - 1) * self.num_epochs_per_M_step + self.stage.step + 1\n",
    "\n",
    "    def _prepare_flow(self, batch):\n",
    "        \"\"\"Add flow-related info to the batch.\"\"\"\n",
    "        flow_info = self.flow.select(batch.lost.words, batch.known.words)\n",
    "        batch.update(flow_info)\n",
    "\n",
    "    def _do_M_step(self, evaluator):\n",
    "        self._M_step_kernel()\n",
    "        self._do_post_M_step(evaluator)\n",
    "\n",
    "    def _M_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._M_step_kernel_loop(batch)\n",
    "\n",
    "    def _M_step_kernel_loop(self, batch, update=True):\n",
    "        self._prepare_flow(batch)\n",
    "        return self._do_M_step_batch(batch, update=update)\n",
    "\n",
    "    def _do_post_M_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        num_cognates = min(self.round_num * self.inc, self.num_cognates)\n",
    "        eval_scores = evaluator.evaluate(self.epoch, num_cognates)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            logging.critical(f\"{setting}, {score}\")\n",
    "            for split, value in score.items():\n",
    "                self.tb_writer.add_scalar(setting + '_' + split, value, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_M_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        #logging.critical(f\"{batch.lost.id_seqs.shape, batch.known.id_seqs.shape}\")\n",
    "        model_ret = self.model(batch)\n",
    "        #logging.critical(f\"{model_ret.log_probs.shape}, {model_ret.valid_log_probs.shape}\")\n",
    "        if update:\n",
    "            self._do_M_step_batch_update(model_ret, batch)\n",
    "\n",
    "        return model_ret\n",
    "\n",
    "    def _do_M_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        num_samples = Metric('num_samples', batch.num_samples, 0, report_mean=False)\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        metrics.loss.mean.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(self._get_trainable_params(), 5.0)\n",
    "        grad_norm = Metric('grad_norm', grad_norm * num_samples.total, num_samples.total)\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        metrics += Metrics(num_samples, grad_norm)\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "\n",
    "        # free my ram plz\n",
    "        #del model_ret\n",
    "        #del batch\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        reg_loss = Metric('reg_loss', model_ret.reg_loss, batch.total_flow_k)\n",
    "        nll_losses = torch.logsumexp((model_ret.valid_log_probs + (batch.flow + 1e-8).log()).tensor, dim=0)\n",
    "        nll_losses = nll_losses * batch.flow_k\n",
    "        nll_loss = Metric('nll_loss', -nll_losses.sum(), batch.total_flow_k)\n",
    "        loss = Metric('loss', self.reg_hyper * reg_loss.mean + nll_loss.mean, 1)\n",
    "\n",
    "        #logging.critical(f\"{model_ret.reg_loss.shape}, {model_ret.valid_log_probs.shape}, {batch.flow.shape}, {self.flow.flow.tensor.shape}\")        \n",
    "        return Metrics(loss, nll_loss, reg_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtjQWbLwLviQ"
   },
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "VjjnNUAaL1v0"
   },
   "outputs": [],
   "source": [
    "def move_batch_to_device(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, (list, tuple, set)):\n",
    "        return type(obj)(move_batch_to_device(x, device) for x in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: move_batch_to_device(v, device) for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        for attr, value in obj.__dict__.items():\n",
    "            moved = move_batch_to_device(value, device)\n",
    "            setattr(obj, attr, moved)\n",
    "        return obj\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'capacity', 'num_cognates')\n",
    "class Evaluator:\n",
    "    def __init__(self, model, data_loader, lost_lang, known_lang, capacity, num_cognates, split=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self._settings = list()\n",
    "        self.split = split\n",
    "\n",
    "    def add_setting(self, mode=None, edit=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        assert edit in [True, False]\n",
    "\n",
    "        lost_size = self.data_loader.size(self.lost_lang)\n",
    "        known_size = self.data_loader.size(self.known_lang)\n",
    "        if mode == 'mle':\n",
    "            self._settings.append(\n",
    "                EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, None, None))\n",
    "        else:\n",
    "            for c in self.capacity:\n",
    "                self._settings.append(\n",
    "                    EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, edit, c))\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'lost_size', 'known_size', 'mode', 'edit', 'capacity'\n",
    "        for s in self._settings:\n",
    "            table.add_row([getattr(s, field) for field in table.field_names])\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    #def evaluate(self, epoch, num_cognates):\n",
    "\n",
    "        #olddevice = torch.device(\"cuda:0\")\n",
    "        #newdevice = torch.device(\"cuda:1\")\n",
    "        #self.model.to(newdevice)\n",
    "                \n",
    "        #self.model.eval()\n",
    "        #table = pt()\n",
    "        #table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'score'\n",
    "        #EVAL[0] = True\n",
    "\n",
    "        #eval_scores = dict()\n",
    "        #for s in self._settings:\n",
    "          #  batch = self.data_loader.entire_batch\n",
    "            #move_batch_to_device(batch, newdevice)\n",
    "            #logging.critical(batch.lost.forms[0])\n",
    "         #   model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "            # Magic tensor to the rescue!\n",
    "         #   almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "         #   preds = almt.get_best()\n",
    "         #   acc = self._evaluate_one_setting(preds)\n",
    "         #   score = acc / len(preds)\n",
    "         #   fmt_score = f'{acc}/{len(preds)}={score:.3f}'\n",
    "         #   table.add_row([getattr(s, field) for field in table.field_names[:-1]] + [fmt_score])\n",
    "         #   eval_scores[str(s)] = score\n",
    "\n",
    "        #EVAL[0] = False\n",
    "\n",
    "        #table.align = 'l'\n",
    "        #table.title = f'Epoch: {epoch}'\n",
    "        #log_pp(table)\n",
    "        #self.model.to(olddevice)\n",
    "        #return eval_scores\n",
    "\n",
    "\n",
    "    def evaluate(self, epoch, num_cognates):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'score'\n",
    "    \n",
    "        eval_scores = dict()\n",
    "        \n",
    "        for s in self._settings:\n",
    "            total_acc = 0\n",
    "            total_preds = 0\n",
    "    \n",
    "            for batch in self.data_loader:\n",
    "                # Optionally move batch to appropriate device here\n",
    "                # move_batch_to_device(batch, device)\n",
    "    \n",
    "                model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "                almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "                preds = almt.get_best()\n",
    "    \n",
    "                acc = self._evaluate_one_setting(preds)\n",
    "                total_acc += acc\n",
    "                total_preds += len(preds)\n",
    "    \n",
    "            score = total_acc / total_preds\n",
    "            fmt_score = f'{total_acc}/{total_preds}={score:.3f}'\n",
    "            table.add_row([getattr(s, field) for field in table.field_names[:-1]] + [fmt_score])\n",
    "            eval_scores[str(s)] = score\n",
    "    \n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "    \n",
    "        return eval_scores\n",
    "\n",
    "\n",
    "    def _evaluate_one_setting(self, preds):\n",
    "        acc = 0\n",
    "        for lost, known in preds.items():\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_batch_to_device(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, (list, tuple, set)):\n",
    "        return type(obj)(move_batch_to_device(x, device) for x in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: move_batch_to_device(v, device) for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        for attr, value in obj.__dict__.items():\n",
    "            moved = move_batch_to_device(value, device)\n",
    "            setattr(obj, attr, moved)\n",
    "        return obj\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'capacity', 'num_cognates')\n",
    "class Evaluator:\n",
    "    def __init__(self, model, data_loader, lost_lang, known_lang, capacity, num_cognates, eval_mode=False, split=None):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self._settings = list()\n",
    "        self.eval_mode = eval_mode\n",
    "        self.split = {key: set(v) for key, v in split.items()} if eval_mode and split else None\n",
    "\n",
    "    def add_setting(self, mode=None, edit=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        assert edit in [True, False]\n",
    "\n",
    "        lost_size = self.data_loader.size(self.lost_lang)\n",
    "        known_size = self.data_loader.size(self.known_lang)\n",
    "        if mode == 'mle':\n",
    "            self._settings.append(\n",
    "                EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, None, None))\n",
    "        else:\n",
    "            for c in self.capacity:\n",
    "                self._settings.append(\n",
    "                    EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, edit, c))\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'lost_size', 'known_size', 'mode', 'edit', 'capacity'\n",
    "        for s in self._settings:\n",
    "            table.add_row([getattr(s, field) for field in table.field_names])\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch, num_cognates):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'split', 'score'\n",
    "\n",
    "        eval_scores = dict()\n",
    "        for s in self._settings:\n",
    "            batch = self.data_loader.entire_batch\n",
    "            model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "            almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "            preds = almt.get_best()\n",
    "\n",
    "            if self.eval_mode and self.split:\n",
    "                split_scores = {}\n",
    "                for split_name, indices in self.split.items():\n",
    "                    acc = self._evaluate_one_setting(preds, indices)\n",
    "                    score = acc / len(indices) if len(indices) > 0 else 0\n",
    "                    fmt_score = f'{acc}/{len(indices)}={score:.3f}'\n",
    "                    table.add_row([getattr(s, field) for field in table.field_names[:-2]] + [split_name, fmt_score])\n",
    "                    split_scores[split_name] = score\n",
    "                eval_scores[str(s)] = split_scores\n",
    "            else:\n",
    "                acc = self._evaluate_one_setting(preds)\n",
    "                score = acc / len(preds)\n",
    "                fmt_score = f'{acc}/{len(preds)}={score:.3f}'\n",
    "                table.add_row([getattr(s, field) for field in table.field_names[:-2]] + ['all', fmt_score])\n",
    "                eval_scores[str(s)] = {'all': score}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def _evaluate_one_setting(self, preds, indices=None):\n",
    "        acc = 0\n",
    "        for idx, (lost, known) in enumerate(preds.items()):\n",
    "            if indices is not None and idx not in indices:\n",
    "                continue\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwkouvbR5sUg"
   },
   "source": [
    "### Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_splits_cache = {}\n",
    "\n",
    "def get_indices(subset, split=(0.6, 0.2, 0.2)):\n",
    "    if f'splits_{split}' not in _splits_cache:\n",
    "        assert(len(split)==3 and sum(split)==1.0), f\"what the hell is that split? {split}\"\n",
    "        indices = list(range(len(get_vocab('transliterated_linear_b'))))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        train_split = int(split[0] * len(indices))\n",
    "        val_split = int((split[0] + split[1]) * len(indices))\n",
    "\n",
    "        _splits_cache[f'splits_{split}'] = {\n",
    "            'train': indices[:train_split],\n",
    "            'validation': indices[train_split:val_split],\n",
    "            'test': indices[val_split:]\n",
    "        }\n",
    "\n",
    "    if subset not in _splits_cache[f'splits_{split}']:\n",
    "        raise ValueError(f\"Invalid split: {subset}\")\n",
    "    return _splits_cache[f'splits_{split}'][subset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "5ExNZGTk8GW1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@has_properties('cog_path', 'lost_lang', 'known_lang', 'batch_size')\n",
    "class Manager:\n",
    "\n",
    "    model_cls = DecipherModelWithFlow\n",
    "    trainer_cls = Trainer\n",
    "    \n",
    "    def __init__(self, cog_path, lost_lang, known_lang, batch_size, args):\n",
    "        self.loader_cls = LostKnownDataLoader\n",
    "        if args['finetune']: \n",
    "            self.loader_cls = BatchedLinearBDataLoader\n",
    "        self.split = None\n",
    "        build_vocabs(self.cog_path, self.lost_lang, self.known_lang)\n",
    "        \n",
    "        self.eval_mode = args.get('evaluation', False)\n",
    "        if self.eval_mode:\n",
    "            split = {\n",
    "                'train': get_indices('train', args['split']),\n",
    "                'validation': get_indices('validation', args['split']),\n",
    "                'test': get_indices('test', args['split'])\n",
    "            }\n",
    "            self.split = split\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.flow_data_loader, args[\"num_rounds\"], args[\"num_epochs_per_M_step\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"num_cognates\"], args[\"inc\"], args[\"warm_up_steps\"], args[\"capacity\"], args[\"save_all\"], args[\"eval_interval\"], args[\"reg_hyper\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"momentum\"], args[\"check_interval\"], args[\"finetune\"], self.eval_mode, self.split)\n",
    "        self.evaluator = Evaluator(self.model, self.eval_data_loader, args[\"lost_lang\"], args[\"known_lang\"], args[\"capacity\"], args[\"num_cognates\"], self.eval_mode, self.split)\n",
    "\n",
    "        self.evaluator.add_setting(mode='mle', edit=False)\n",
    "        self.evaluator.add_setting(mode='flow', edit=False)\n",
    "        self.evaluator.add_setting(mode='flow', edit=True)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "        self._show_data()\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        if self.eval_mode:\n",
    "            self.train_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=False, indices=self.split['train'])\n",
    "            logging.critical(\"TRAIN LOADER DONE\")\n",
    "            self.eval_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=True, indices=sum(self.split.values(), []))\n",
    "            logging.critical(\"EVAL LOADER DONE\")\n",
    "\n",
    "        else:\n",
    "            self.train_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=False)\n",
    "            self.eval_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=True)\n",
    "\n",
    "        self.flow_data_loader = self.train_data_loader # NOTE The flow instance shares its entire_batch property with train_data_loader.\n",
    "\n",
    "    def _show_data(self):\n",
    "        log_pp(self.train_data_loader.stats('train'))\n",
    "        log_pp(self.eval_data_loader.stats('eval'))\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        trie = Trie(self.known_lang)\n",
    "        self.model = type(self).model_cls(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "        trie_weight_device = self.model.trie._weight.device\n",
    "        logging.critical(f\"model.trie.weight is on device: {trie_weight_device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNKkqNBKZERD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4euG7B2UCzu"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"final_results_translation_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final_translation.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 4964,\n",
    "    \"inc\" : 100, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.8, 0.1, 0.1) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_evaluation\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\":False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.6, 0.2, 0.2) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"finetune_40_rounds\"\n",
    "#EVAL = [False]\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 120, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "psLVyzjuSNA2",
    "outputId": "109692b3-8987-4774-e22a-b9cbf1f4892e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_80_rounds\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\":False\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RELOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:37:41 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1911,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_with_invalid.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'finetune': True,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 200,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/finetune',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 13308,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 90,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/finetune/saved.latest',\n",
      "           'seed': 17,\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"finetune\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 90, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TnUUMUF6DEu",
    "outputId": "a5396595-a69d-4057-e8b5-7fc882c75f32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:37:41 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1024,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_final.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'finetune': False,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 75,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/final_results_zero_long_term',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 1911,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 80,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/final_results_zero_long_term/saved.latest',\n",
      "           'seed': 17,\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"final_results_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : COG_PATH,\n",
    "    \"char_emb_dim\" : 400, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:38:10 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1024,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_final_translation.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'evaluation': False,\n",
      "           'finetune': False,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 100,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/final_results_translation_zero_long_term',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 4964,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 80,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/final_results_translation_zero_long_term/saved.latest',\n",
      "           'seed': 17,\n",
      "           'split': (0.8, 0.1, 0.1),\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"final_results_translation_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final_translation.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 4964,\n",
    "    \"inc\" : 100, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.8, 0.1, 0.1) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CYWpKUn050IH",
    "outputId": "b9e9b91a-6578-47f0-98c9-c8b4ea38aacc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 1000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 2000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 3000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 4000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:62 - Finished enumeration of size 4964\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +--------------------------------+\n",
      "          |             train              |\n",
      "          +-------------------------+------+\n",
      "          | lang                    | size |\n",
      "          +-------------------------+------+\n",
      "          | transliterated_linear_b | 4964 |\n",
      "          | greek                   | 7818 |\n",
      "          +-------------------------+------+\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +--------------------------------+\n",
      "          |              eval              |\n",
      "          +-------------------------+------+\n",
      "          | lang                    | size |\n",
      "          +-------------------------+------+\n",
      "          | transliterated_linear_b | 4964 |\n",
      "          | greek                   | 7818 |\n",
      "          +-------------------------+------+\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          DecipherModelWithFlow(\n",
      "            (encoder): LSTM(400, 200, num_layers=16, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "            (decoder): MultiLayerLSTMCell(\n",
      "              800, 200, num_layers=16\n",
      "              (drop): Dropout(p=0.2, inplace=False)\n",
      "              (cells): ModuleList(\n",
      "                (0): LSTMCell(800, 200)\n",
      "                (1-15): 15 x LSTMCell(200, 200)\n",
      "              )\n",
      "            )\n",
      "            (attention): GlobalAttention(\n",
      "              src=400, tgt=200\n",
      "              (drop): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (char_emb): UniversalCharEmbedding(\n",
      "              (char_emb): Embedding(400, 400)\n",
      "              (char_weights): ModuleDict(\n",
      "                (transliterated_linear_b): Embedding(93, 400)\n",
      "                (greek): Embedding(33, 400)\n",
      "              )\n",
      "            )\n",
      "            (hidden): Sequential(\n",
      "              (0): Linear(in_features=600, out_features=400, bias=True)\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "            (controlled_residual): NormControlledResidual()\n",
      "            (ft_proj): Linear(in_features=256, out_features=6400, bias=True)\n",
      "          )\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>round 100%|| 80/80 [00:01&lt;00:00, 69.22 samples/s]</pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>M step 100%|| 100/100 [00:00&lt;00:00, 19763.01 samples/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mIMP - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:143 - *STARTING* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:143 - *FINISHED* _init_optimizer\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          Stage(name=decipher, num_steps=1)\n",
      "           Stage(name=round, num_steps=80)\n",
      "               Stage(name=E step, num_steps=1)\n",
      "               Stage(name=M step, num_steps=100)\n",
      "          \n",
      "          \u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\n",
      "          | lost                    | known | lost_size | known_size | mode | edit  | capacity |\n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | mle  | None  | None     |\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | flow | False | 3        |\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | flow | True  | 3        |\n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:56 - {'_schedule': {'_pbars': {}, '_stages': [{'_pbars': {'round': {'count': 79}}, '_stages': [{'_pbars': {}, '_stages': []}, {'_pbars': {'M step': {'count': 99}}, '_stages': []}]}]}, 'best_score': None, 'best_stage': None, '_metrics': Metrics(loss, grad_norm, nll_loss, reg_loss, num_samples)}\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:56 - {'lost_forms': array(['*18-jo', '*18-to-no', '*22-ja-ro', ..., 'zo-wa', 'zo-wi-jo',\n",
      "                                                                    'zo-wo'], dtype='<U29'), 'known_forms': array(['ff', 'ff', 'ff', ..., '', '', ''],\n",
      "                                                                   dtype='<U18'), 'flow': MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))}\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3930245403.py:39 - MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))\u001b[0m\n",
      "\u001b[36mIMP - 09/18/25 00:38:20 - 0:00:10 at 1009358421.py:50 - Loaded saved states from ./DMPROJECT/repo_cinese/final_results_translation_zero_long_term/saved.latest\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:75 - MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:79 - <__main__.Tracker object at 0x7ffdd6777070>, \"M step\": 99, 79\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:24 - 0:00:15 at 2413757980.py:2 - Computing expected edits\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 1/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 2/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 3/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 4/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 5/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 6/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 7/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 8/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:40 - 0:01:31 at 2413757980.py:2 - Computing expected edits\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:41 - 0:01:32 at 2413757980.py:32 - Computing chunk 1/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:43 - 0:01:34 at 2413757980.py:32 - Computing chunk 2/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:45 - 0:01:35 at 2413757980.py:32 - Computing chunk 3/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:47 - 0:01:37 at 2413757980.py:32 - Computing chunk 4/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:48 - 0:01:39 at 2413757980.py:32 - Computing chunk 5/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:50 - 0:01:40 at 2413757980.py:32 - Computing chunk 6/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:52 - 0:01:42 at 2413757980.py:32 - Computing chunk 7/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:54 - 0:01:44 at 2413757980.py:32 - Computing chunk 8/8\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:41:14 - 0:03:04 at 1009358421.py:172 - \n",
      "          +-------------------------------------------------------------------------------------+\n",
      "          |                                      Epoch: -1                                      |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\n",
      "          | lost                    | known | mode | edit  | capacity | split | score           |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\n",
      "          | transliterated_linear_b | greek | mle  | None  | None     | all   | 2652/4964=0.534 |\n",
      "          | transliterated_linear_b | greek | flow | False | 3        | all   | 2705/4964=0.545 |\n",
      "          | transliterated_linear_b | greek | flow | True  | 3        | all   | 2752/4964=0.554 |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lost_transliterated_linear_b__known_greek__mode_mle__edit_None__capacity_None': {'all': 0.5342465753424658},\n",
       " 'lost_transliterated_linear_b__known_greek__mode_flow__edit_False__capacity_3': {'all': 0.5449234488315874},\n",
       " 'lost_transliterated_linear_b__known_greek__mode_flow__edit_True__capacity_3': {'all': 0.5543916196615633}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. (Re)build the Manager exactly as before.\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "man = Manager(\n",
    "    args[\"cog_path\"],\n",
    "    args[\"lost_lang\"],\n",
    "    args[\"known_lang\"],\n",
    "    args[\"batch_size\"],\n",
    "    args,\n",
    ")\n",
    "# 2. Load *everything* from disk: model + flow + tracker.\n",
    "man.trainer.load()\n",
    "#man._get_trained_model(args[\"saved_path\"])\n",
    "# 3. Rebuild a fresh Evaluator on the *loaded* man.model.\n",
    "\n",
    "# 4. Finally, evaluate with exactly the same num_cognates the model saw when it was saved.\n",
    "#    (Optionally, you can recompute it from `man.trainer.tracker.get('round') + 1` and `args[\"inc\"]`.)\n",
    "man.evaluator.evaluate(-1, args[\"num_cognates\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k25s_lh08Awx"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"old_data\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "old_args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if old_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(old_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = old_args[\"gpu\"]\n",
    "if not old_args[\"random\"]:\n",
    "    random.seed(old_args[\"seed\"])\n",
    "    np.random.seed(old_args[\"seed\"])\n",
    "    torch.manual_seed(old_args[\"seed\"])\n",
    "\n",
    "\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "man = Manager(old_args[\"cog_path\"], old_args[\"lost_lang\"], old_args[\"known_lang\"], old_args[\"batch_size\"], old_args)\n",
    "\n",
    "model = man._get_trained_model(old_args['saved_path'])\n",
    "#man.trainer.load()\n",
    "del man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wR0OhepUCJ6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"for_pipeline_try\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 5, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 3882,#919,\n",
    "    \"inc\" : 500,#50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args, transfer=True, model=model)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbNgpKHjKhQI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"for_pipeline_try\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 15, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 3882,#919,\n",
    "    \"inc\" : 500,#50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSBs7tLqADUI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"old_data_2\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"transliterated_linear_b-greek.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 225, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTN0seM0ZeMS"
   },
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKGKQINpiOjf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Old Model New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO0zgTaV3uiY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Import Linear B sequences from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JB-WfWkDdGKr"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\" and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    indexes = [defaultdict(list) for seq in sequences]\n",
    "    for j, seq in enumerate(sequences):\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "                indexes[j][int(pos)].append(int(pos2))\n",
    "\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res, indexes\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis, idxs):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "            idxs.pop(j)\n",
    "    return seq, mis, idxs\n",
    "\n",
    "\n",
    "def split_dataset(base, *others, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a base list and any number of other aligned lists into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        base (list): The primary list to shuffle and split.\n",
    "        *others (list): Any number of other lists aligned with the base list.\n",
    "        train_ratio (float): Proportion of data to use for training (default 0.9).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_base, test_base, train_others..., test_others...)\n",
    "    \"\"\"\n",
    "    length = len(base)\n",
    "    assert all(len(lst) == length for lst in others), \"All input lists must have the same length\"\n",
    "\n",
    "    indices = np.random.permutation(length)\n",
    "\n",
    "    # Shuffle the base and others\n",
    "    base_shuffled = [base[i] for i in indices]\n",
    "    others_shuffled = [[lst[i] for i in indices] for lst in others]\n",
    "\n",
    "    train_size = int(train_ratio * length)\n",
    "\n",
    "    # Split the base list\n",
    "    train_base = base_shuffled[:train_size]\n",
    "    test_base = base_shuffled[train_size:]\n",
    "\n",
    "    # Split all other lists\n",
    "    train_others = [lst[:train_size] for lst in others_shuffled]\n",
    "    test_others = [lst[train_size:] for lst in others_shuffled]\n",
    "\n",
    "    return (train_base, test_base, *train_others, *test_others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5Jii4wGF3blM"
   },
   "outputs": [],
   "source": [
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing, idxs = create_missing_dataset(sequences)\n",
    "sequences, missing, unknown= clean_datasets(sequences, missing, idxs)\n",
    "idxs = [idx for idx in range(len(sequences))]\n",
    "logging.info(f\"{sequences[1]}, {missing[1]}, {unknown[1]}, {idxs[1]}\")\n",
    "\n",
    "train_seqs, test_seqs, train_missings, train_unknown, train_idxs, test_missings, test_unknown, test_idxs = split_dataset(sequences, missing, unknown, idxs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQtEmYC3pEcV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Phonetic Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obuLB4rasOdi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "E5reuit9sNmy"
   },
   "outputs": [],
   "source": [
    "class SyllableAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size=None, embedding_dim=None, hidden_dim=None, latent_dim=None, charsIndexes=None, verbose=False):\n",
    "        super(SyllableAutoencoder, self).__init__()\n",
    "        if not vocab_size or not embedding_dim or not hidden_dim or not latent_dim:\n",
    "            if verbose:\n",
    "                print(\"Missing parameters for autoencoder\")\n",
    "            return\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Map hidden state to latent vector\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "        # Output projection for each time step to vocabulary size (for reconstruction)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.charsIndexes = charsIndexes #vocabulary of the SyllableAutoencoder\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.encoder_rnn(packed)\n",
    "        if isinstance(hidden, tuple): #we don't need wider context\n",
    "            hidden = hidden[0]\n",
    "        last_hidden = hidden[-1]\n",
    "        latent = self.fc_mu(last_hidden)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent, target_seq, lengths):\n",
    "        # Prepare the initial hidden state for decoder from latent vector\n",
    "        hidden_init = self.fc_latent(latent).unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        # Embed the target sequence (teacher forcing during training)\n",
    "        embedded = self.embedding(target_seq)\n",
    "\n",
    "        # Initialize the cell state as zeros (or you could use hidden_init too)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        # Combine into a tuple for LSTM senn non funzia fareshi\n",
    "        hidden_init = (hidden_init, cell_init)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.decoder_rnn(packed, hidden_init)\n",
    "        decoder_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        logits = self.output_projection(decoder_out)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        latent = self.encode(x, lengths)\n",
    "        #For training, use the input sequence as target (teacher forcing)\n",
    "        logits = self.decode(latent, x, lengths)\n",
    "        return logits, latent\n",
    "\n",
    "    def get_probabilities(self, logits):\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    #Given syllable, returns its tensor one-hot-encoded representation\n",
    "    def syl2tensor(self, syl):\n",
    "        return torch.tensor([self.charsIndexes[c] for c in syl], dtype=torch.long).unsqueeze(0), torch.tensor([len(syl)])\n",
    "\n",
    "    #Given list of syllables, produces dictionary <syllable> : <embedding>\n",
    "    def get_embeddings(self, syllables, vocab=None): #original vocabulary of transformer\n",
    "\n",
    "      #Get mapping <syllable> : <one-hot-encoded-syllable> from original vocabulary, positions in the list are the encodings\n",
    "      if vocab != None:\n",
    "        mapping = {}\n",
    "        count = 0\n",
    "        for tok in vocab:\n",
    "          mapping[tok] = count\n",
    "          count += 1\n",
    "\n",
    "      embeddings_per_syllable = {}\n",
    "      for syl in syllables:\n",
    "        syl_, length = self.syl2tensor(syl)\n",
    "        out = self.encode(syl_, length).squeeze(0).detach().numpy()\n",
    "        syl = syl.replace(\"0\", \"\") #remove padding when saving\n",
    "\n",
    "        #Returns representation <one-hot-encoded-syllable> : <embedding>\n",
    "        if vocab != None:\n",
    "          syl = mapping[syl]\n",
    "\n",
    "        embeddings_per_syllable[syl] = out\n",
    "\n",
    "      return embeddings_per_syllable\n",
    "\n",
    "    def save_embeddings(self, filename, syllables, prefix_path=prefix_path, tokenList=None):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      phonetic_embeddings = self.get_embeddings(syllables, tokenList)\n",
    "      np.save(embeddings_path, phonetic_embeddings)\n",
    "\n",
    "    def load_embeddings(self, filename, prefix_path=prefix_path):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      return np.load(embeddings_path, allow_pickle=True).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI3Ul8KCs1hv"
   },
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AIV7pBLDs4IY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SyllableDataset(Dataset):\n",
    "    def __init__(self, syllables, char2idx):\n",
    "        self.syllables = syllables\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.syllables)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        syll = self.syllables[idx]\n",
    "        #Convert each character to its corresponding index\n",
    "        token_indices = [self.char2idx[c] for c in syll]\n",
    "        length = len(token_indices)\n",
    "        return torch.tensor(token_indices, dtype=torch.long), length\n",
    "\n",
    "def collate_fn_syl(batch):\n",
    "    sequences, lengths = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return sequences_padded, torch.tensor(lengths)\n",
    "\n",
    "charsIndexes = {'0': 0,'a': 1,'b': 2,'c': 3,'d': 4,'e': 5,'f': 6,'g': 7,'h': 8,'i': 9,'j': 10,'k': 11,'l': 12,'m': 13,'n': 14,'o': 15,'p': 16,'q': 17,'r': 18,'s': 19,'t': 20,'u': 21,'v': 22,'w': 23,'x': 24,'y': 25,'z': 26}\n",
    "indexesChars = {v: k for k, v in charsIndexes.items()} #reverse mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "k9sCD1bRtVx-"
   },
   "outputs": [],
   "source": [
    "syllables = [] #dataset for unsupervised learning, '0' is padding\n",
    "charset = get_charset(\"transliterated_linear_b\")._CHARS[:-15]\n",
    "\n",
    "#Get syllable tokens\n",
    "for tok in charset:\n",
    "    syllables.append(tok)\n",
    "\n",
    "max_len_syl = max(len(syl) for syl in syllables)\n",
    "#Pad tokens to max length (3)\n",
    "for i in range(len(syllables)):\n",
    "  if len(syllables[i]) < max_len_syl:\n",
    "    syllables[i] += '0'*(max_len_syl-len(syllables[i]))\n",
    "\n",
    "#get dataset\n",
    "dataset = SyllableDataset(syllables, charsIndexes)\n",
    "data_loader_syl = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_syl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYmgonqwtAOv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yQxieV1RtC_y"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "##define autoencoder\n",
    "#vocab_size = len(charsIndexes)\n",
    "#embedding_dim =  16\n",
    "#hidden_dim = 32\n",
    "#latent_dim = EMBEDDINGS_DIM #THIS IS THE SIZE THAT MUST BE EQUAL TO TRANSFORMER VECTOR, Dimension of encoder result\n",
    "## COME HERE IF YOU NET TO DEBUG DEBUG DEBUG (look prev line !!!!)\n",
    "#\n",
    "#syllable_autoencoder = SyllableAutoencoder(vocab_size, embedding_dim, hidden_dim, latent_dim, charsIndexes) ##IMPORTANT, this is the autoencoder model used later to create and save the embeddings\n",
    "#optimizer = optim.Adam(syllable_autoencoder.parameters(), lr=0.001)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=charsIndexes['0'])\n",
    "#\n",
    "##Train autoencoder\n",
    "#num_epochs = 300\n",
    "#for epoch in range(num_epochs):\n",
    "#    total_loss = 0.0\n",
    "#    for batch, lengths in data_loader_syl:\n",
    "#        optimizer.zero_grad()\n",
    "#\n",
    "#        logits, latent = syllable_autoencoder(batch, lengths)\n",
    "#        logits_flat = logits.view(-1, vocab_size)  # shape: (batch_size*seq_length, vocab_size)\n",
    "#        targets_flat = batch.view(-1)              # shape: (batch_size*seq_length,)\n",
    "#\n",
    "#        loss = criterion(logits_flat, targets_flat)\n",
    "#        loss.backward()\n",
    "#        optimizer.step()\n",
    "#        total_loss += loss.item()\n",
    "#\n",
    "#    avg_loss = total_loss / len(data_loader_syl)\n",
    "#    if epoch % 50 == 0:\n",
    "#      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "#\n",
    "#print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu9ifIcOvJ34",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "UjO8QC3IvLrk"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#Test just to see if it works as autoencoder, ignores padding, input == output for good autoencoding\n",
    "#with torch.no_grad():\n",
    "#  for batch, lengths in data_loader_syl:\n",
    "#    logits, latent = syllable_autoencoder(batch, lengths)\n",
    "#    g_truth = list(batch.detach().numpy())\n",
    "#    for el in range(len(g_truth)):\n",
    "#      g_truth[el] = [indexesChars[i] for i in g_truth[el]]\n",
    "#    probs = syllable_autoencoder.get_probabilities(logits) #(batch, syllable, char)\n",
    "#    predicted_indices = list(torch.argmax(probs, dim=-1).detach().numpy())\n",
    "#    for el in range(len(predicted_indices)):\n",
    "#      predicted_indices[el] = [indexesChars[i] for i in predicted_indices[el]]\n",
    "#    print(\"GroundTruth & Predicted: \")\n",
    "#    print(g_truth)\n",
    "#    print(predicted_indices)\n",
    "#    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "zuC4mBr5vhir"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "##Assess quality of embeddings through clustering\n",
    "#from sklearn.cluster import KMeans\n",
    "#lengths = [3] #each syllable is always of len 3\n",
    "#embeddings_per_syllable = syllable_autoencoder.get_embeddings(syllables)\n",
    "#\n",
    "#syllable_keys = list(embeddings_per_syllable.keys())\n",
    "#embedding_list = [embeddings_per_syllable[s] for s in syllable_keys]\n",
    "#\n",
    "#\n",
    "#num_clusters = 14\n",
    "#kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "#clusters = kmeans.fit_predict(embedding_list)\n",
    "#\n",
    "#for cluster_id in range(num_clusters): #print clusters\n",
    "#    cluster_syllables = [syllable_keys[i] for i in range(len(syllable_keys)) if clusters[i] == cluster_id]\n",
    "#    print(f\"Cluster {cluster_id}: {cluster_syllables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "nrqDGDpTvp85"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "#\n",
    "##PLOT CLUSTERS PRINTED IN PREVIOUS CELL\n",
    "#embedding_array = np.array(embedding_list)  #shape: (num_samples, embedding_dim)\n",
    "#\n",
    "#tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#embedding_2d = tsne.fit_transform(embedding_array)\n",
    "#\n",
    "#colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "#\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#for cluster_id in range(num_clusters):\n",
    "#    indices = [i for i, c in enumerate(clusters) if c == cluster_id]\n",
    "#    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],\n",
    "#                color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "#\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#plt.title(\"Clustering of Syllable Embeddings (t-SNE Projection)\")\n",
    "#plt.xlabel(\"t-SNE Dimension 1\")\n",
    "#plt.ylabel(\"t-SNE Dimension 2\")\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NNyKMajwQ0z",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Save and Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "rQxTCmw5wTEJ"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#embeddings = syllable_autoencoder.get_embeddings(syllables)\n",
    "#embeddings\n",
    "##Save embeddings on file with one-hot-encoded mapping\n",
    "#syllable_autoencoder.save_embeddings(\"syllable_embeddings_one-hot-encoded.npy\", syllables, tokenList=charset)\n",
    "#syllable_autoencoder.save_embeddings(\"syllable_embeddings.npy\", syllables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "upXWuTS8xwoM"
   },
   "outputs": [],
   "source": [
    "#Load embeddings for further use\n",
    "#loaded_embeddings = syllable_autoencoder.load_embeddings(\"syllable_embeddings.npy\")\n",
    "#loaded_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SDXTbmly8wm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Reimport embeddings for usage in Text Infiller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8969Qyu0zAw-"
   },
   "outputs": [],
   "source": [
    "syllable_autoencoder = SyllableAutoencoder()\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(\"syllable_embeddings.npy\")\n",
    "\n",
    "def recover_phonetic_embeddings(embedding_dict, dataset):\n",
    "    all_embeddings = []  # To store all sequence embeddings\n",
    "    embed_size = len(list(embedding_dict.values())[0])  # Assuming all embeddings have the same size\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_embeddings = []  # To store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            for syllable in word.split(\"-\"):\n",
    "                if syllable in embedding_dict:\n",
    "                    emb = embedding_dict[syllable] # Get the word embedding\n",
    "                else:\n",
    "                    emb = np.zeros(embed_size)\n",
    "                seq_embeddings.append(emb)  # Add to the sequence's embeddings\n",
    "            if i != len(seq) - 1: # this adds spaces' embeddings !\n",
    "                space_embed = np.zeros(embed_size)\n",
    "                seq_embeddings.append(space_embed)  # Add to the sequence's embeddings\n",
    "\n",
    "        # adding zeros for SOS at the beginning of the sequence\n",
    "        sos = np.zeros(embed_size)\n",
    "        seq_embeddings = [sos] + seq_embeddings\n",
    "\n",
    "        # Concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = np.stack(seq_embeddings)\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return all_embeddings  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "phonetic_embeddings_train_x = recover_phonetic_embeddings(loaded_embeddings, train_missings)\n",
    "phonetic_embeddings_test_x = recover_phonetic_embeddings(loaded_embeddings, test_missings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SWlgPua1QQY",
    "outputId": "16d1b206-1d16-45b9-b2c2-9c1710902fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko-wa NUM ?-wo NUM\n",
      "ko-wa NUM ko-wo NUM\n",
      "(10, 512)\n",
      "[0. 0. 0.]\n",
      "[-0.96574688 -0.06027425  0.32187825]\n",
      "[-0.56332594  1.07461643 -0.75313926]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[-0.57493556  1.25258875 -0.68535912]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "print(train_missings[n])\n",
    "print(train_seqs[n])\n",
    "print(phonetic_embeddings_train_x[n].shape)\n",
    "for i in range(len(phonetic_embeddings_train_x[n])):\n",
    "  print(phonetic_embeddings_train_x[n][i][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb1XkEqWk5uG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sequence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "jqWURIG4VLFc"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, order=True)\n",
    "class Word:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "t9v_w__6X1Gy"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sequence:\n",
    "    def __init__(self, form, missing_form, unknown, phonetic_embedding, idx, vocab):\n",
    "        self.form = form\n",
    "        self.missing_form = missing_form\n",
    "        self.word_list = []\n",
    "        self.unknown = unknown\n",
    "        self.vocab = vocab\n",
    "        self.phonetic_embedding = phonetic_embedding\n",
    "        self.idx = idx\n",
    "        for form_w in form.split(\" \"):\n",
    "            word_idx = vocab.get_form_idx(form_w)\n",
    "            self.word_list.append(vocab.get_word(word_idx))\n",
    "\n",
    "    def num_words(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(s) for s in self.word_list])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Sequence idx={self.idx} form='{self.form}' missing_form='{self.missing_form}'\"\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        seq = []\n",
    "        missing = []\n",
    "        base_logo_idx_vocab = self.vocab.log_start\n",
    "        base_logo_idx_tokens = len(get_charset(self.vocab.lang))\n",
    "\n",
    "        for i, word in enumerate(self.word_list):\n",
    "            # tokenize each word and extend the list\n",
    "            tokenized_word = word.id_seq.copy()\n",
    "\n",
    "            if tokenized_word[0] == UNK_ID:\n",
    "                tokenized_word[0] = base_logo_idx_tokens + (word.idx - base_logo_idx_vocab)\n",
    "            seq.extend(tokenized_word)\n",
    "\n",
    "            # replace unknown tokens with unknown index in tokenized sequence\n",
    "            for u in self.unknown[i]:\n",
    "                tokenized_word[u] = UNK_ID\n",
    "\n",
    "            missing.extend(tokenized_word)\n",
    "\n",
    "        #adjustments to tokenized sequence:\n",
    "        # y always ends with EOS\n",
    "        seq[-1] = EOS_ID\n",
    "        # x always starts with SOW\n",
    "        missing = [SOW_ID] + missing[:-1]\n",
    "\n",
    "        return missing, seq\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, sequence_list, lang):\n",
    "        self.word_list = set()\n",
    "        self.log_start = 0\n",
    "        logo_list = set()\n",
    "        self.lang = lang\n",
    "\n",
    "        for seq in sequence_list:\n",
    "            for word in seq.split(\" \"):\n",
    "                if (word.startswith(\"*\") and not \"-\" in word) or word[0].isupper() or word == \"1\" or word == \"2\":\n",
    "                    logo_list.add(word)\n",
    "                else:\n",
    "                    self.word_list.add(word)\n",
    "\n",
    "        self.word_list = list(sorted(self.word_list))\n",
    "        self.log_start = len(self.word_list)\n",
    "        logo_list = list(sorted(logo_list))\n",
    "        self.word_list.extend(logo_list)\n",
    "\n",
    "        self.build_logo_vocab(sequence_list)\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.form2idx = {w: i for i, w in enumerate(self.word_list)}\n",
    "        word_list = [Word(self.lang, w, i) for i, w in enumerate(self.word_list)]\n",
    "        self.word_list = word_list\n",
    "\n",
    "    @property\n",
    "    def get_words(self):\n",
    "        return self.word_list[:self.log_start]\n",
    "\n",
    "    @property\n",
    "    def get_logograms(self):\n",
    "        return self.word_list[self.log_start:]\n",
    "\n",
    "    @property\n",
    "    def get_vocabulary(self):\n",
    "        return self.word_list\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.word_list[idx]\n",
    "\n",
    "    def get_form_idx(self, form, status=False):\n",
    "        #status: True means that it is a word, False means it is a Logogram/Numeral\n",
    "        if status:\n",
    "            return self.form2idx[form], self.form2idx[form] < self.log_start\n",
    "        return self.form2idx[form]\n",
    "\n",
    "    def build_logo_vocab(self, sequences):\n",
    "        self.logogram_vocab = {}\n",
    "        mappings = {\n",
    "            \"AES\": \"\",\n",
    "            \"AUR\": \"\",\n",
    "            \"BOS\": \"\",\n",
    "            \"BOSf\": \"\",\n",
    "            \"BOSm\": \"\",\n",
    "            \"CAP\": \"\",\n",
    "            \"CAPf\": \"\",\n",
    "            \"CAPm\": \"\",\n",
    "            \"CORN\": \"\",\n",
    "            \"CROC\": \"\",\n",
    "            \"CYP\": \"\",\n",
    "            \"EQU\": \"\",\n",
    "            \"EQUf\": \"\",\n",
    "            \"EQUm\": \"\",\n",
    "            \"FAR\": \"\",\n",
    "            \"GAL\": \"\",\n",
    "            \"GRA\": \"\",\n",
    "            \"HAS\": \"\",\n",
    "            \"HORD\": \"\",\n",
    "            \"LANA\": \"\",\n",
    "            \"LUNA\": \"\",\n",
    "            \"OLIV\": \"\",\n",
    "            \"OVIS\": \"\",\n",
    "            \"OVISf\": \"\",\n",
    "            \"OVISm\": \"\",\n",
    "            \"SUS\": \"\",\n",
    "            \"SUSf\": \"\",\n",
    "            \"SUSm\": \"\",\n",
    "            \"TELA\": \"\",\n",
    "            \"VIN\": \"\",\n",
    "            \"JAC\": \"\",\n",
    "            \"BIG\": \"\",\n",
    "            \"AROM\": \"\",\n",
    "            \"ARB\": \"\",\n",
    "            \"ALV\": \"\",\n",
    "            \"ARM\": \"\",\n",
    "            \"CUR\": \"\",\n",
    "            \"MUL\": \"\",\n",
    "            \"OLE\": \"\",\n",
    "            \"PUG\": \"\",\n",
    "            \"ROTA\": \"\",\n",
    "            \"TUN\": \"\",\n",
    "            \"VIR\": \"\",\n",
    "            \"TELAI\": \"\",\n",
    "            \"TELHA\": \"\",\n",
    "            \"CAPS\": \"\",\n",
    "            \"VAS\": \"\"\n",
    "        }\n",
    "        for seq in sequences:\n",
    "          for word in seq.split(\" \"):\n",
    "                if word in mappings: #correctly translate logograms from latin\n",
    "                    self.logogram_vocab[word] = mappings[word]\n",
    "                #handle variants\n",
    "                elif \"VAS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VAS\"]\n",
    "                elif \"AROM+CYP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"CYP\"]\n",
    "                elif \"BOS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"BOS\"]\n",
    "                elif \"CAP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"CAP\"]\n",
    "                elif \"CYP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"CYP\"]\n",
    "                elif \"EQU\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"EQU\"]\n",
    "                elif  \"GRA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"GRA\"]\n",
    "                elif \"OLE\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OLE\"]\n",
    "                elif \"OLIV\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OLIV\"]\n",
    "                elif \"OVIS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OVIS\"]\n",
    "                elif \"SUS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"SUS\"]\n",
    "                elif \"ROTA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"ROTA\"]\n",
    "                elif \"TUN\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TUN\"]\n",
    "                elif \"VIR\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VIR\"]\n",
    "                elif \"TELAI\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELAI\"]\n",
    "                elif \"TELHA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELHA\"]\n",
    "                elif \"TELA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELA\"]\n",
    "                elif \"VIN\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VIN\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "f2cL-CBZWoEC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize Vocabulary with ALL known LinB words\n",
    "lang = \"transliterated_linear_b\"\n",
    "our_vocabulary = Vocabulary(sequences, lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAkFxhoQ4x8M",
    "outputId": "c5152d8d-8049-4120-eb53-aac2991d8d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4215"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_vocabulary.get_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "vVvD5H1NL-1g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_to_dense(a, dtype='f'):\n",
    "    '''\n",
    "    Pads a list of arrays to create a dense 2D or 3D array, depending on input shape.\n",
    "    - 2D case: list of 1D arrays  shape (batch_size, max_seq_len)\n",
    "    - 3D case: list of 2D arrays  shape (batch_size, max_seq_len, feature_dim)\n",
    "\n",
    "    Args:\n",
    "        a: list of 1D or 2D numpy arrays\n",
    "        dtype: 'f' (float32) or 'l' (int64)\n",
    "\n",
    "    Returns:\n",
    "        A padded dense numpy array.\n",
    "    '''\n",
    "    assert dtype in ['f', 'l']\n",
    "    dtype = np.float32 if dtype == 'f' else np.int64\n",
    "\n",
    "    if len(a) == 0:\n",
    "        raise ValueError(\"Input list is empty\")\n",
    "\n",
    "    first_elem = a[0]\n",
    "    if isinstance(first_elem, list):\n",
    "        first_elem = np.array(first_elem)\n",
    "\n",
    "    if first_elem.ndim == 1:\n",
    "        # Case: list of 1D arrays  pad to 2D\n",
    "        maxlen = max(len(row) for row in a)\n",
    "        result = np.zeros((len(a), maxlen), dtype=dtype)\n",
    "        for i, row in enumerate(a):\n",
    "            row = np.array(row, dtype=dtype)\n",
    "            result[i, :len(row)] = row\n",
    "        return result\n",
    "\n",
    "    elif first_elem.ndim == 2:\n",
    "        # Case: list of 2D arrays  pad to 3D\n",
    "        maxlen = max(arr.shape[0] for arr in a)\n",
    "        feature_dim = first_elem.shape[1]\n",
    "        result = np.zeros((len(a), maxlen, feature_dim), dtype=dtype)\n",
    "        for i, arr in enumerate(a):\n",
    "            arr = np.array(arr, dtype=dtype)\n",
    "            result[i, :arr.shape[0], :] = arr\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Only lists of 1D or 2D arrays are supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "DSLQJ0PzJGJt"
   },
   "outputs": [],
   "source": [
    "def collate_seq(batch):\n",
    "    sequences = _get_item('sequence', batch)\n",
    "    xs = _get_item('x', batch)\n",
    "    ys = _get_item('y', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "    missings = _get_item('missing', batch)\n",
    "    phonetics = _get_item('phonetic', batch)\n",
    "\n",
    "    lengths, sequences, xs, ys, forms, missings, phonetics = sort_all(sequences, xs, ys, forms, missings, phonetics)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    xs = pad_to_dense(xs, dtype='l')\n",
    "    ys = pad_to_dense(ys, dtype='l')\n",
    "    phonetics = pad_to_dense(phonetics, dtype='f')\n",
    "\n",
    "    xs = get_tensor(xs[:, :max_len])\n",
    "    ys = get_tensor(ys[:, :max_len])\n",
    "    phonetics = get_tensor(phonetics[:, :max_len, :])\n",
    "\n",
    "    lang = batch[0].lang\n",
    "    return Map(sequences=sequences, x=xs, y=ys, forms=forms, missings=missings, phonetic=phonetics, lengths=lengths, lang=lang, num_samples=len(sequences))\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, missing, unknown, phonetic_embeddings, idxs, vocab):\n",
    "        self.sequences = [Sequence(s, m, u, phon, idx, vocab) for (s, m, u, phon, idx) in zip(sequences, missing, unknown, phonetic_embeddings, idxs)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        x, y = seq.tokenized_sequence\n",
    "        return Map(sequence=seq, x=x, y=y, lang=self.sequences[idx].word_list[0].lang, form=seq.form, missing=seq.missing_form, phonetic=seq.phonetic_embedding)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        return collate_seq([self[i] for i in range(len(self))])\n",
    "\n",
    "class SequenceDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.dataset)\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_seq)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return self.dataset.entire_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    #def stats(self, name):\n",
    "    #    row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "    #    row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "    #    table = _prepare_stats(name, row1, row2)\n",
    "    #    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "pW-gixlt9lkk"
   },
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "\n",
    "#train_dataset = SequenceDataset(train_seqs, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs, our_vocabulary)\n",
    "#train_data_loader = SequenceDataLoader(train_dataset, 8)\n",
    "#n = 0\n",
    "#train_data_loader.entire_batch.phonetic[n], train_data_loader.entire_batch.lengths[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VzJkbCQlBDu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Text Infiller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ttl8pohjA7AM"
   },
   "outputs": [],
   "source": [
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "\n",
    "        # they are sorted\n",
    "        packed_embedded = pack_padded_sequence(embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        pred = self.softmax(out)\n",
    "        return Map(predictions=pred, embeddings=embedded, encoding=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "MOGJEH0KELab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SqueezeExcitationFusion(nn.Module):\n",
    "    def __init__(self, d_model, norms_or_ratios, control_mode='relative', reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)  # Global avg pooling (batch, d_model, 1)\n",
    "\n",
    "        # Fully connected layers to model interactions\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // reduction_ratio),  # Reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // reduction_ratio, d_model),  # Restore dimensionality\n",
    "            nn.Sigmoid()  # Normalize importance scores between 0 and 1\n",
    "        )\n",
    "        self.fuse_embeddings = NormControlledResidual(norms_or_ratios=norms_or_ratios, control_mode=control_mode)\n",
    "\n",
    "    def forward(self, token_emb, phon_emb):\n",
    "        # Concatenate embeddings along the feature dimension\n",
    "        fusion_emb = self.fuse_embeddings(token_emb, phon_emb) # (batch, seq, d_model)\n",
    "\n",
    "        # Squeeze: Compute global context (reduce across sequence dimension)\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1))  # (batch, d_model, 1)\n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  # Flatten to (batch, d_model)\n",
    "\n",
    "        # Excitation: Generate importance weights\n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "\n",
    "        # Scale embeddings by learned importance scores\n",
    "        fused_emb = fusion_emb * attention_weights  # Adaptive weighting\n",
    "\n",
    "        return fused_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "o9TtTC7gF8cn"
   },
   "outputs": [],
   "source": [
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, norms_or_ratios):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.se_fusion = SqueezeExcitationFusion(d_model=embed_size, norms_or_ratios=norms_or_ratios)\n",
    "    def forward(self, batch):\n",
    "\n",
    "        embedded = self.embedding(batch.x)\n",
    "        embedded = self.se_fusion(embedded, batch.phonetic)\n",
    "\n",
    "        # they are sorted\n",
    "        packed_embedded = pack_padded_sequence(embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        pred = self.softmax(out)\n",
    "        return Map(predictions=pred, embeddings=embedded, encoding=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r45qGiVOlEQJ"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5h3JTztH5z_b"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 97) (3728850356.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[52], line 97\u001b[0;36m\u001b[0m\n\u001b[0;31m    logging.critical(f\"{setting}, )\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 97)\n"
     ]
    }
   ],
   "source": [
    "@has_properties('num_epochs', 'saved_path', 'learning_rate', 'log_dir', 'save_all', 'eval_interval', 'check_interval')\n",
    "class TextInfillerTrainer:\n",
    "    def __init__(self, model, train_data_loader, test_data_loader, num_epochs, saved_path, learning_rate, log_dir, save_all, eval_interval, check_interval):\n",
    "\n",
    "        self.tracker = Tracker('text-infilling')\n",
    "        stage = self.tracker.add_stage('round', self.num_epochs)\n",
    "        stage.add_stage('train step')\n",
    "        self.tracker.fix_schedule()\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self._init_optimizer()\n",
    "        self._init_loss()\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_loss(self):\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"none\")\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'train step':\n",
    "            self._do_train_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return self.round_num\n",
    "\n",
    "    def _do_train_step(self, evaluator):\n",
    "        self._train_step_kernel()\n",
    "        self._do_post_train_step(evaluator)\n",
    "\n",
    "    def _train_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._do_train_step_batch(batch)\n",
    "\n",
    "    def _do_post_train_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        eval_scores = evaluator.evaluate(self.epoch)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            logging.critical(f\"{setting}, )\n",
    "            self.tb_writer.add_scalar(setting, score, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_train_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        model_ret = self.model(batch)\n",
    "        if update:\n",
    "            self._do_train_step_batch_update(model_ret, batch)\n",
    "        return model_ret\n",
    "\n",
    "    def _do_train_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        metrics.loss.mean.backward()\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        # NOTE This means we are conditioning on one specific flow.\n",
    "        flattened_pred = model_ret.predictions.view(-1, model_ret.predictions.shape[-1])\n",
    "\n",
    "        flattened_gt = batch.y.view(-1)\n",
    "        nll_loss = self.nll_loss(flattened_pred, flattened_gt)\n",
    "\n",
    "        nll_loss = Metric('nll_loss', nll_loss.sum(dim=-1), batch.num_samples)\n",
    "        loss = Metric('loss', nll_loss.mean, 1)\n",
    "\n",
    "        return Metrics(loss, nll_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvAvEDpclHQR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Wkcp6d41jgvD"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "class TextInfillerEvaluator:\n",
    "    def __init__(self, model, data_loader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'sequence_accuracy', 'mrr', 'top_1', 'top_5', 'top_10', 'top_15', 'top_20'\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'sequence_accuracy', 'mrr', 'top_1', 'top_5', 'top_10', 'top_15', 'top_20'\n",
    "\n",
    "        metrics = Metrics()\n",
    "\n",
    "        for batch in self.data_loader:\n",
    "            model_ret = self.model(batch)\n",
    "            # Magic tensor to the rescue!\n",
    "            gt = batch.y\n",
    "            preds = model_ret.predictions\n",
    "\n",
    "            # Mask to ignore PAD_IDs in the ground truth\n",
    "            self.mask = (gt != PAD_ID)\n",
    "\n",
    "            seq_acc = self.compute_sequence_accuracy(preds, gt)\n",
    "            mrr, top_1, top_5, top_10, top_15, top_20 = self.compute_mrr(preds, batch)\n",
    "\n",
    "            new_metrics = Metrics(seq_acc, mrr, top_1, top_5, top_10, top_15, top_20)\n",
    "            metrics += new_metrics\n",
    "\n",
    "        values = [epoch] + [getattr(metrics, field).mean for field in table.field_names[1:]]\n",
    "        table.add_row(values)\n",
    "\n",
    "        eval_scores = {fn: val for (fn, val) in zip(table.field_names, values)}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def compute_sequence_accuracy(self, preds, gt):\n",
    "        pred_indices = preds.argmax(dim=-1)\n",
    "\n",
    "        # Compare with ground truth and check where predictions are correct\n",
    "        correct = (pred_indices == gt) & self.mask\n",
    "        # Mean over the batch\n",
    "        accuracy = correct.sum(dim=-1)\n",
    "        return Metric('sequence_accuracy', (accuracy.sum(dim=0)).item(), accuracy.shape[0])\n",
    "\n",
    "    def compute_mrr(self, preds, batch):\n",
    "        mrrs = []\n",
    "        top_1 = []\n",
    "        top_5 = []\n",
    "        top_10 = []\n",
    "        top_15 = []\n",
    "        top_20 = []\n",
    "\n",
    "        for b in range(batch.x.size(0)):\n",
    "            unk_positions = (batch.x[b] == UNK_ID).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            for pos in unk_positions:\n",
    "                if pos.item() == 0:\n",
    "                    continue  # skip if index - 1 would be invalid\n",
    "\n",
    "                gt_token = batch.y[b, pos - 1].item()\n",
    "                pred_probs = preds[b, pos - 1]\n",
    "\n",
    "                # Get descending ranks (most likely first)\n",
    "                sorted_indices = torch.argsort(pred_probs, descending=True)\n",
    "\n",
    "                # Find the rank (1-based) of the ground truth token\n",
    "                rank = (sorted_indices == gt_token).nonzero(as_tuple=True)[0].item() + 1\n",
    "\n",
    "                mrrs.append(1.0 / rank)\n",
    "                top_1.append(1 if rank <= 1 else 0)\n",
    "                top_5.append(1 if rank <= 5 else 0)\n",
    "                top_10.append(1 if rank <= 10 else 0)\n",
    "                top_15.append(1 if rank <= 15 else 0)\n",
    "                top_20.append(1 if rank <= 20 else 0)\n",
    "\n",
    "        return Metric('mrr', sum(mrrs), len(mrrs)), Metric('top_1', sum(top_1), len(top_1)), Metric('top_5', sum(top_5), len(top_5)), Metric('top_10', sum(top_10), len(top_10)), Metric('top_15', sum(top_15), len(top_15)), Metric('top_20', sum(top_20), len(top_20))\n",
    "\n",
    "\n",
    "    def _evaluate_one_setting(self, preds):\n",
    "        acc = 0\n",
    "        for lost, known in preds.items():\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS9G66palJnO"
   },
   "source": [
    "#### Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "GLz-jm9Ez9CE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BRNNTextInfiller' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@has_properties\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextInfillerManager\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     model_cls \u001b[38;5;241m=\u001b[39m BRNNTextInfiller\n\u001b[1;32m      5\u001b[0m     trainer_cls \u001b[38;5;241m=\u001b[39m TextInfillerTrainer\n",
      "Cell \u001b[0;32mIn[252], line 4\u001b[0m, in \u001b[0;36mTextInfillerManager\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@has_properties\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextInfillerManager\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     model_cls \u001b[38;5;241m=\u001b[39m \u001b[43mBRNNTextInfiller\u001b[49m\n\u001b[1;32m      5\u001b[0m     trainer_cls \u001b[38;5;241m=\u001b[39m TextInfillerTrainer\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, lang, batch_size, args, vocabulary):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BRNNTextInfiller' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "@has_properties('data', 'lang', 'batch_size', 'vocabulary')\n",
    "class TextInfillerManager:\n",
    "\n",
    "    model_cls = BRNNTextInfiller\n",
    "    trainer_cls = TextInfillerTrainer\n",
    "\n",
    "    def __init__(self, data, lang, batch_size, args, vocabulary):\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.test_data_loader, args[\"num_epochs\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"save_all\"], args[\"eval_interval\"], args[\"check_interval\"])\n",
    "        self.evaluator = TextInfillerEvaluator(self.model, self.test_data_loader)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        train_seqs, train_missings, train_unknown, train_phon, train_idxs, test_seqs, test_missings, test_unknown, test_phon, test_idxs = self.data\n",
    "        self.vocab_size = len(self.vocabulary.get_vocabulary) - self.vocabulary.log_start + len(get_charset(self.lang))\n",
    "        train_dataset = SequenceDataset(train_seqs, train_missings, train_unknown, train_phon, train_idxs, self.vocabulary)\n",
    "        self.train_data_loader = SequenceDataLoader(train_dataset, self.batch_size)\n",
    "        test_dataset = SequenceDataset(test_seqs, test_missings, test_unknown, test_phon, test_idxs, self.vocabulary)\n",
    "        self.test_data_loader = SequenceDataLoader(test_dataset, self.batch_size)\n",
    "\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        self.model = type(self).model_cls(self.vocab_size, args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"norms_or_ratios\"])\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xaS0wzUlMKN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "g_l8xbsTqY3S",
    "outputId": "1dcb2eed-5da4-4a57-cc9f-4dedd0972783",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[43mtrain_seqs\u001b[49m, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs,\n\u001b[1;32m      2\u001b[0m         test_seqs, test_missings, test_unknown, phonetic_embeddings_test_x, test_idxs]\n\u001b[1;32m      3\u001b[0m lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransliterated_linear_b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_seqs' is not defined"
     ]
    }
   ],
   "source": [
    "data = [train_seqs, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs,\n",
    "        test_seqs, test_missings, test_unknown, phonetic_embeddings_test_x, test_idxs]\n",
    "lang = \"transliterated_linear_b\"\n",
    "batch_size = 32\n",
    "log_dir = os.path.join(prefix_path, \"BRNN\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "brnn_args = {\n",
    "    \"embed_size\": EMBEDDINGS_DIM,\n",
    "    \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "    \"num_layers\": 8,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 200,\n",
    "    \"saved_path\": os.path.join(log_dir, \"saved.latest\"),#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": log_dir,\n",
    "    \"save_all\": True,\n",
    "    \"eval_interval\": 1,\n",
    "    \"check_interval\": 1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"norms_or_ratios\": [1.0, 0.8],\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "if brnn_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(brnn_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = brnn_args[\"gpu\"]\n",
    "\n",
    "#create_logger(filepath=brnn_args[\"log_dir\"] + '/log', log_level=brnn_args[\"log_level\"])\n",
    "#log_pp(pformat(brnn_args))\n",
    "\n",
    "clear_stages()\n",
    "\n",
    "\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args, our_vocabulary)\n",
    "#tim.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "7numMNdUEAjp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextInfillerManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n\u001b[1;32m      2\u001b[0m clear_stages()\n\u001b[0;32m----> 3\u001b[0m tim \u001b[38;5;241m=\u001b[39m \u001b[43mTextInfillerManager\u001b[49m(data, lang, batch_size, brnn_args, our_vocabulary)\n\u001b[1;32m      4\u001b[0m tim\u001b[38;5;241m.\u001b[39m_get_trained_model(brnn_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m tim\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextInfillerManager' is not defined"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "clear_stages()\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args, our_vocabulary)\n",
    "tim._get_trained_model(brnn_args[\"saved_path\"])\n",
    "tim.evaluator.evaluate(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Translation Supporting Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Linear B documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {3: [[[['e-ri-sa-ta', True], ['M', True], ['1', True]], True]],\n",
       "             4: [[[['me-ri', False]], False]],\n",
       "             5: [[[['da-so', False]], False],\n",
       "              [[['SA', True], ['M', True], ['1', True]], True]],\n",
       "             6: [[[['a-na-ka', True], ['M', False]], False]],\n",
       "             9: [[[['to-so', True],\n",
       "                ['a-pu-do-so-mo', True],\n",
       "                ['a-mi-ni-so', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             10: [[[['di-mi-zo', False]], False]],\n",
       "             11: [[[['qa-ra-su-ti-jo', False]], False], [[['M', True]], True]],\n",
       "             12: [[[['si-da-jo', False]], False],\n",
       "              [[['M', True], ['1', True]], True]],\n",
       "             13: [[[['M', True],\n",
       "                ['RI', True],\n",
       "                ['NUM', True],\n",
       "                ['O', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['2', False]],\n",
       "               False],\n",
       "              [[['KE', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['*146', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             14: [[[['wi-pi-o', True], ['N', True], ['NUM', False]], False]],\n",
       "             15: [[[['e', False]], False], [[['M', False]], False]],\n",
       "             16: [[[['ku', False]], False]],\n",
       "             17: [[[['ta', False]], False],\n",
       "              [[['M', True], ['1', True]], True]],\n",
       "             18: [[[['M', True]], True]],\n",
       "             19: [[[['wa-ko', False]], False], [[['SA', False]], False]],\n",
       "             20: [[[['M', True], ['1', True]], True]],\n",
       "             21: [[[['M', True], ['1', False]], False]],\n",
       "             22: [[[['M', True]], True]],\n",
       "             23: [[[['M', True], ['1', True]], True]],\n",
       "             24: [[[['jo', False]], False],\n",
       "              [[['M', True], ['1', True]], True]],\n",
       "             25: [[[['ta-jo', False]], False],\n",
       "              [[['M', True], ['1', True]], True]],\n",
       "             26: [[[['sa-na-so', False]], False]],\n",
       "             27: [[[['jo', False]], False],\n",
       "              [[['M', True], ['1', False]], False]],\n",
       "             28: [[[['SA', False]], False], [[['M', True]], True]],\n",
       "             29: [[[['du', False]], False]],\n",
       "             30: [[[['ko-ri-jo', False]], False]],\n",
       "             31: [[[['M', True], ['NUM', True]], True]],\n",
       "             32: [[[['zo', False]], False],\n",
       "              [[['M', True], ['NUM', False]], False]],\n",
       "             33: [[[['e-si', False]], False], [[['o-pe', False]], False]],\n",
       "             34: [[[['jo', False]], False], [[['i-we', True]], True]],\n",
       "             35: [[[['M', True], ['1', False]], False]],\n",
       "             36: [[[['ro', False]], False]],\n",
       "             37: [[[['e', False]], False]],\n",
       "             38: [[[['e-ri', False]], False]],\n",
       "             40: [[[['no-ro', False]], False]],\n",
       "             41: [[[['ra-ma', False]], False]],\n",
       "             42: [[[['te-we', False]], False]],\n",
       "             43: [[[['tu', False]], False]],\n",
       "             44: [[[['M', True]], True]],\n",
       "             45: [[[['M', False]], False], [[['1', True]], True]],\n",
       "             46: [[[['M', True], ['1', False]], False]],\n",
       "             47: [[[['M', False]], False]],\n",
       "             48: [[[['ja', False]], False]],\n",
       "             49: [[[['1', False]], False]],\n",
       "             50: [[[['ka', False]], False]],\n",
       "             51: [[[['ku', False]], False]],\n",
       "             52: [[[['ra-ro', False]], False]],\n",
       "             53: [[[['M', True]], True]],\n",
       "             54: [[[['ja', False]], False]],\n",
       "             55: [[[['de', False]], False]],\n",
       "             56: [[[['M', False]], False]],\n",
       "             57: [[[['M', False]], False]],\n",
       "             58: [[[['qa', False]], False]],\n",
       "             59: [[[['ka-ta-ra', True],\n",
       "                ['CROC', True],\n",
       "                ['QI', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             60: [[[['sa-ma-da', True], ['CROC', True], ['RO', False]],\n",
       "               False]],\n",
       "             61: [[[['to', False]], False],\n",
       "              [[['CROC', True], ['RO', True], ['1', True]], True]],\n",
       "             62: [[[['ni-ja', False]], False], [[['CROC', True]], True]],\n",
       "             63: [[[['ja', False]], False],\n",
       "              [[['CROC', True],\n",
       "                ['P', True],\n",
       "                ['2', True],\n",
       "                ['QI', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             64: [[[['re', False]], False],\n",
       "              [[['CROC', False]], False],\n",
       "              [[['RO', True], ['1', True], ['QI', True], ['1', True]], True]],\n",
       "             65: [[[['da-wo', True], ['CROC', True], ['Q', True], ['1', True]],\n",
       "               True]],\n",
       "             66: [[[['ti-ja-no', False]], False],\n",
       "              [[['CROC', True], ['P', True], ['NUM', False]], False]],\n",
       "             67: [[[['CROC', False]], False],\n",
       "              [[['N', True], ['1', True], ['P', True], ['1', True]], True]],\n",
       "             68: [[[['so', False]], False], [[['CROC', False]], False]],\n",
       "             69: [[[['CROC', False]], False],\n",
       "              [[['QI', True], ['NUM', True]], True]],\n",
       "             71: [[[['CROC', True], ['P', True], ['2', True]], True]],\n",
       "             72: [[[['a-u-ta-na', True],\n",
       "                ['da-na-pi', True],\n",
       "                ['CROC', True],\n",
       "                ['N', False]],\n",
       "               False]],\n",
       "             73: [[[['jo-zo', False]], False],\n",
       "              [[['CROC', True],\n",
       "                ['P', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['P', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             74: [[[['nu-po', False]], False],\n",
       "              [[['a-nu-ko', True], ['CROC', True], ['N', True], ['1', True]],\n",
       "               True]],\n",
       "             75: [[[['jo', False]], False],\n",
       "              [[['sa-ma-ri-jo', True], ['CROC', True]], True]],\n",
       "             76: [[[['ma-ki-nu-wo', False]], False], [[['CROC', True]], True]],\n",
       "             77: [[[['wo', False]], False],\n",
       "              [[['CROC', True], ['P', True], ['NUM', True], ['o', True]],\n",
       "               True]],\n",
       "             78: [[[['CROC', True],\n",
       "                ['P', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['P', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             79: [[[['ro', False]], False],\n",
       "              [[['CROC', True], ['N', True], ['1', True]], True]],\n",
       "             80: [[[['si-du-wo', False]], False],\n",
       "              [[['CROC', True], ['N', False]], False]],\n",
       "             81: [[[['CROC', False]], False],\n",
       "              [[['N', True], ['1', True]], True]],\n",
       "             82: [[[['o', False]], False],\n",
       "              [[['P', True], ['NUM', False]], False]],\n",
       "             83: [[[['qa-sa-ro-we', True], ['CROC', False]], False],\n",
       "              [[['P', True], ['1', False]], False]],\n",
       "             84: [[[['jo', False]], False],\n",
       "              [[['tu-qa-ni-ja-so', True],\n",
       "                ['CROC', True],\n",
       "                ['N', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             85: [[[['e-u-na-wo', True],\n",
       "                ['CROC', True],\n",
       "                ['N', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             86: [[[['P', False]], False], [[['NUM', True]], True]],\n",
       "             87: [[[['CROC', False]], False],\n",
       "              [[['N', True], ['1', True]], True]],\n",
       "             88: [[[['CROC', False]], False]],\n",
       "             89: [[[['CROC', False]], False],\n",
       "              [[['N', True], ['1', True]], True]],\n",
       "             90: [[[['CROC', True], ['P', False]], False]],\n",
       "             91: [[[['CROC', False]], False]],\n",
       "             92: [[[['ra', False]], False], [[['CROC', False]], False]],\n",
       "             93: [[[['wa-to', True], ['CROC', False]], False],\n",
       "              [[['N', False]], False],\n",
       "              [[['2', True]], True],\n",
       "              [[['ri-no', True], ['M', True], ['P', True], ['2', False]],\n",
       "               False]],\n",
       "             94: [[[['CROC', False]], False]],\n",
       "             95: [[[['CROC', False]], False],\n",
       "              [[['N', True], ['1', True]], True]],\n",
       "             96: [[[['2', False]], False], [[['P', True], ['1', True]], True]],\n",
       "             97: [[[['CROC', False]], False], [[['P', True]], True]],\n",
       "             98: [[[['o', False]], False],\n",
       "              [[['P', True], ['1', False]], False]],\n",
       "             99: [[[['RO', False]], False],\n",
       "              [[['1', True], ['Q', True], ['1', True]], True]],\n",
       "             101: [[[['ra-to', False]], False]],\n",
       "             102: [[[['N', False]], False], [[['1', True]], True]],\n",
       "             103: [[[['N', False]], False], [[['2', True]], True]],\n",
       "             104: [[[['P', False]], False], [[['2', True]], True]],\n",
       "             105: [[[['2', False]], False],\n",
       "              [[['P', True], ['1', True]], True]],\n",
       "             106: [[[['CROC', False]], False], [[['N', False]], False]],\n",
       "             107: [[[['CROC', False]], False]],\n",
       "             108: [[[['P', True], ['NUM', False]], False]],\n",
       "             109: [[[['jo', False]], False], [[['CROC', False]], False]],\n",
       "             110: [[[['CROC', False]], False], [[['P', False]], False]],\n",
       "             111: [[[['P', False]], False]],\n",
       "             113: [[[['P', False]], False], [[['2', True]], True]],\n",
       "             114: [[[['CROC', False]], False]],\n",
       "             115: [[[['P', True], ['1', True]], True]],\n",
       "             117: [[[['CROC', False]], False]],\n",
       "             118: [[[['o', False]], False],\n",
       "              [[['P', True], ['NUM', False]], False]],\n",
       "             119: [[[['*167', False]], False],\n",
       "              [[['NUM', True],\n",
       "                ['L', True],\n",
       "                ['NUM', True],\n",
       "                ['M', True],\n",
       "                ['2', True]],\n",
       "               True]],\n",
       "             120: [[[['L', False]], False]],\n",
       "             121: [[[['re-o', False]], False], [[['L', False]], False]],\n",
       "             122: [[[['*167+PE', False]], False],\n",
       "              [[['NUM', True], ['L', True], ['NUM', False]], False]],\n",
       "             123: [[[['jo', False]], False],\n",
       "              [[['AES', True], ['*167+PE', False]], False]],\n",
       "             124: [[[['a-ka', False]], False],\n",
       "              [[['jo-jo', False]], False],\n",
       "              [[['me-no', False]], False],\n",
       "              [[['da-phu-ri-to-jo', True], ['po-ti-ni-ja', False]], False],\n",
       "              [[['ri', True], ['*166+WE', True], ['NUM', False]], False]],\n",
       "             125: [[[['*166+WE', True],\n",
       "                ['NUM', True],\n",
       "                ['e-to-ro-qa-ta', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             126: [[[['*166+WE', False]], False], [[['1', True]], True]],\n",
       "             127: [[[['ko-ro-to', True], ['o-nu-ka', False]], False],\n",
       "              [[['LANA', True]], True]],\n",
       "             128: [[[['ko-ro-to', True], ['ta-ma', False]], False]],\n",
       "             129: [[[['ko-ro-to', True],\n",
       "                ['LANA', True],\n",
       "                ['M', True],\n",
       "                ['1', True],\n",
       "                ['ka', False]],\n",
       "               False],\n",
       "              [[['LANA', True], ['2', True]], True]],\n",
       "             130: [[[['pa-i-to', True],\n",
       "                ['we-we-si-jo-jo', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             131: [[[['o-pi', True],\n",
       "                ['ti-mu-nu-we', True],\n",
       "                ['LANA', True],\n",
       "                ['2', False]],\n",
       "               False]],\n",
       "             132: [[[['o-pi', False]], False],\n",
       "              [[['no-nu-we', True],\n",
       "                ['a-ti-pa-mo', True],\n",
       "                ['pe-re', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True],\n",
       "                ['si-da-jo', False]],\n",
       "               False],\n",
       "              [[['pe-re', True],\n",
       "                ['po-ro-to', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True],\n",
       "                ['a-po-te', True],\n",
       "                ['pe-re', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             133: [[[['ri-jo-ni-jo', True],\n",
       "                ['e-ze-to', True],\n",
       "                ['to-ro-qo', True],\n",
       "                ['a-to-mo-na', True],\n",
       "                ['su-mo-no-qe', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             134: [[[['o', False]], False],\n",
       "              [[['o-pi', True], ['ki-si', False]], False]],\n",
       "             135: [[[['to-so', True],\n",
       "                ['o', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True],\n",
       "                ['ke-me-no', False]],\n",
       "               False],\n",
       "              [[['au-u-te', True], ['a-pe-i-si', True]], True],\n",
       "              [[['ke-re-wa', False]], False],\n",
       "              [[['LANA', True],\n",
       "                ['NUM', True],\n",
       "                ['M', True],\n",
       "                ['1', True],\n",
       "                ['P', True],\n",
       "                ['NUM', True],\n",
       "                ['o-mu-ka-ra', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True],\n",
       "                ['N', True],\n",
       "                ['2', True],\n",
       "                ['P', True],\n",
       "                ['2', True]],\n",
       "               True]],\n",
       "             136: [[[['ku-pi-ri-ja', True],\n",
       "                ['LANA', True],\n",
       "                ['1', True],\n",
       "                ['M', True],\n",
       "                ['2', True],\n",
       "                ['P', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['ke', False]], False],\n",
       "              [[['ja', False]], False],\n",
       "              [[['M', True], ['2', True], ['P', True], ['1', False]], False],\n",
       "              [[['sa-mu', False]], False],\n",
       "              [[['1', True], ['qo-ja-te', True], ['P', True], ['1', False]],\n",
       "               False]],\n",
       "             137: [[[['e-na-po-na', True],\n",
       "                ['o-nu', True],\n",
       "                ['pa-i-ti-jo', True],\n",
       "                ['e-ti-wa-ja-qe', True],\n",
       "                ['LANA', False]],\n",
       "               False],\n",
       "              [[['qo-ja-te', True], ['a-pu-do-ke', True], ['ti-ra', True]],\n",
       "               True]],\n",
       "             138: [[[['o-nu-ke', True], ['LANA', True], ['NUM', True]], True]],\n",
       "             139: [[[['ti-ra', True],\n",
       "                ['a-mi-ke-te-to', True],\n",
       "                ['ne-ki-ri-si', True],\n",
       "                ['LANA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             140: [[[['e-ki', True],\n",
       "                ['ri-jo-ni-ja', True],\n",
       "                ['to-sa', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             141: [[[['ne-wo', True],\n",
       "                ['o-pi', True],\n",
       "                ['po-po', True],\n",
       "                ['LANA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             142: [[[['o-pi', True],\n",
       "                ['a-to-mo-na', True],\n",
       "                ['LANA', True],\n",
       "                ['2', True],\n",
       "                ['o-pi', True],\n",
       "                ['po-ro-i-ra', True],\n",
       "                ['LANA', True],\n",
       "                ['2', True]],\n",
       "               True]],\n",
       "             143: [[[['ne-we', False]], False],\n",
       "              [[['pe-ko-to', True], ['*1641', False]], False],\n",
       "              [[['e-pi-ro-pa-ja', False]], False],\n",
       "              [[['o-du-we', True],\n",
       "                ['te-o-po-ri-ja', True],\n",
       "                ['M', True],\n",
       "                ['2', False]],\n",
       "               False],\n",
       "              [[['i-jo-te', False]], False],\n",
       "              [[['ku-su-a-ta-o', True], ['NUM', False]], False],\n",
       "              [[['LANA', True],\n",
       "                ['2', True],\n",
       "                ['M', True],\n",
       "                ['1', True],\n",
       "                ['jo-du-mi', False]],\n",
       "               False],\n",
       "              [[['wo-ke', True], ['P', True], ['1', False]], False]],\n",
       "             144: [[[['a-*65-na', True],\n",
       "                ['e-re-u-ti-ja', True],\n",
       "                ['LANA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             145: [[[['e-re-u-ti-ja', True],\n",
       "                ['ta-wa-ko-to', True],\n",
       "                ['LANA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             146: [[[['e-re-u-ti-ja', False]], False],\n",
       "              [[['LANA', True], ['NUM', True]], True]],\n",
       "             147: [[[['a-mi-ni-so', False]], False]],\n",
       "             148: [[[['pa-i-to', True], ['ka-ra', False]], False],\n",
       "              [[['a-me-a', True], ['LANA', True], ['M', True]], True]],\n",
       "             149: [[[['LANA', True], ['PE', True], ['NUM', True]], True]],\n",
       "             150: [[[['LANA', False]], False],\n",
       "              [[['PE', True], ['NUM', True]], True]],\n",
       "             151: [[[['wi-ri-za', False]], False], [[['LANA', True]], True]],\n",
       "             152: [[[['ki-ri-ta-i', True], ['LANA', True], ['2', False]],\n",
       "               False]],\n",
       "             153: [[[['ni-ki-jo', False]], False], [[['LANA', False]], False]],\n",
       "             155: [[[['nu-we', False]], False],\n",
       "              [[['o-na', True], ['LANA', False]], False]],\n",
       "             156: [[[['LANA', False]], False], [[['1', False]], False]],\n",
       "             157: [[[['LANA', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['LANA', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             158: [[[['1', False]], False],\n",
       "              [[['o', True], ['LANA', True], ['NUM', True]], True]],\n",
       "             159: [[[['NUM', True], ['LANA', False]], False]],\n",
       "             160: [[[['LANA', True], ['M', True], ['2', False]], False]],\n",
       "             161: [[[['LANA', False]], False], [[['NUM', True]], True]],\n",
       "             162: [[[['ri', False]], False],\n",
       "              [[['LANA', True], ['NUM', True]], True]],\n",
       "             163: [[[['2', False]], False],\n",
       "              [[['LANA', True], ['NUM', False]], False]],\n",
       "             164: [[[['1', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['o', True], ['LANA', True], ['NUM', True]], True]],\n",
       "             165: [[[['2', False]], False],\n",
       "              [[['LANA', True], ['NUM', False]], False],\n",
       "              [[['NUM', False]], False]],\n",
       "             166: [[[['ra', False]], False],\n",
       "              [[['LANA', True], ['NUM', False]], False]],\n",
       "             167: [[[['ne', False]], False],\n",
       "              [[['LANA', True], ['M', True], ['NUM', False]], False]],\n",
       "             168: [[[['NUM', False]], False],\n",
       "              [[['LANA', True], ['NUM', True]], True]],\n",
       "             169: [[[['LANA', True], ['NUM', False]], False]],\n",
       "             170: [[[['we', False]], False],\n",
       "              [[['LANA', True], ['M', False]], False]],\n",
       "             171: [[[['ta', False]], False],\n",
       "              [[['LANA', True], ['1', True], ['da-i-ra', True]], True]],\n",
       "             172: [[[['LANA', True], ['M', True], ['2', True], ['N', False]],\n",
       "               False]],\n",
       "             173: [[[['NUM', False]], False], [[['LANA', False]], False]],\n",
       "             174: [[[['LANA', False]], False], [[['2', True]], True]],\n",
       "             175: [[[['we-we-si-jo-jo', True], ['o', True], ['LANA', False]],\n",
       "               False]],\n",
       "             176: [[[['wi-ri-za', True],\n",
       "                ['su-ri-mo', True],\n",
       "                ['LANA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             177: [[[['LANA', False]], False],\n",
       "              [[['LANA', False]], False],\n",
       "              [[['NUM', True], ['o', True], ['LANA', False]], False]],\n",
       "             178: [[[['ne-a', False]], False], [[['LANA', True]], True]],\n",
       "             179: [[[['LANA', True], ['M', True], ['1', False]], False]],\n",
       "             180: [[[['LANA', False]], False], [[['NUM', True]], True]],\n",
       "             181: [[[['LANA', False]], False], [[['NUM', False]], False]],\n",
       "             182: [[[['o', True], ['LANA', False]], False]],\n",
       "             183: [[[['LANA', False]], False],\n",
       "              [[['NUM', True], ['M', True], ['1', False]], False]],\n",
       "             184: [[[['NUM', False]], False], [[['LANA', False]], False]],\n",
       "             185: [[[['LANA', False]], False], [[['NUM', False]], False]],\n",
       "             186: [[[['2', False]], False], [[['LANA', False]], False]],\n",
       "             187: [[[['NUM', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['LANA', False]], False]],\n",
       "             188: [[[['LANA', False]], False], [[['NUM', True]], True]],\n",
       "             189: [[[['LANA', True], ['NUM', False]], False],\n",
       "              [[['LANA', True], ['NUM', True]], True]],\n",
       "             190: [[[['2', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['M', True]], True]],\n",
       "             191: [[[['NUM', False]], False],\n",
       "              [[['M', False]], False],\n",
       "              [[['LANA', False]], False],\n",
       "              [[['NUM', False]], False]],\n",
       "             192: [[[['NUM', False]], False], [[['LANA', False]], False]],\n",
       "             193: [[[['OVISm', True], ['LANA', False]], False],\n",
       "              [[['NUM', False]], False]],\n",
       "             194: [[[['LANA', False]], False],\n",
       "              [[['NUM', True], ['o', False]], False]],\n",
       "             195: [[[['LANA', True], ['M', True], ['1', False]], False]],\n",
       "             196: [[[['NUM', False]], False],\n",
       "              [[['o', True], ['LANA', True], ['NUM', True]], True]],\n",
       "             197: [[[['LANA', False]], False], [[['NUM', False]], False]],\n",
       "             198: [[[['1', False]], False],\n",
       "              [[['o', True], ['LANA', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             199: [[[['NUM', False]], False], [[['LANA', False]], False]],\n",
       "             201: [[[['2', False]], False],\n",
       "              [[['o', True], ['LANA', False]], False]],\n",
       "             202: [[[['o', False]], False], [[['LANA', False]], False]],\n",
       "             203: [[[['LANA', False]], False]],\n",
       "             204: [[[['LANA', False]], False]],\n",
       "             205: [[[['LANA', False]], False]],\n",
       "             206: [[[['LANA', False]], False]],\n",
       "             208: [[[['o', False]], False], [[['LANA', True]], True]],\n",
       "             209: [[[['LANA', False]], False]],\n",
       "             210: [[[['pa-i-ti-ja', True], ['M', True], ['NUM', False]],\n",
       "               False],\n",
       "              [[['da-wi-ja', True], ['M', True], ['NUM', False]], False],\n",
       "              [[['to', False]], False],\n",
       "              [[['M', True], ['NUM', False]], False],\n",
       "              [[['M', True], ['NUM', False]], False]],\n",
       "             211: [[[['su-ri-mi-jo', False]], False],\n",
       "              [[['u-ta-ni-jo', False]], False],\n",
       "              [[['ti-ri-ti-jo', False]], False],\n",
       "              [[['M', True],\n",
       "                ['NUM', True],\n",
       "                ['qa-mi-jo', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['pu-si-jo', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['ru-ki-ti-jo', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['tu-ri-si-jo', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['qa-ra-jo', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['to-so', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             212: [[[['mo-ri-wo-do', False]], False],\n",
       "              [[['M', True], ['NUM', True], ['2', False]], False],\n",
       "              [[['mo-ri-wo-do', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['N', False]],\n",
       "               False],\n",
       "              [[['2', True],\n",
       "                ['mo-ri-wo-do', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['mo-ri-wo-do', True],\n",
       "                ['M', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             213: [[[['RO', True], ['2', True], ['ra-jo', False]], False],\n",
       "              [[['qa-ra-o', True], ['P', True], ['1', True]], True]],\n",
       "             214: [[[['jo-a-mi-ni-so-de', True], ['di-do', False]], False],\n",
       "              [[['ku-pe-se-ro', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['me-to-re', True],\n",
       "                ['M', True],\n",
       "                ['ne-ri-wa-to', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['pi-ri', False]],\n",
       "               False]],\n",
       "             215: [[[['P', True], ['NUM', True], ['no', False]], False],\n",
       "              [[['P', True], ['NUM', True]], True]],\n",
       "             216: [[[['P', False]], False],\n",
       "              [[['1', True], ['Q', True], ['1', True]], True]],\n",
       "             217: [[[['ka-te-ro', False]], False],\n",
       "              [[['L', True], ['NUM', True], ['M', False]], False]],\n",
       "             218: [[[['M', True], ['NUM', True]], True]],\n",
       "             219: [[[['ri-no', True], ['M', True], ['1', False]], False]],\n",
       "             220: [[[['M', True], ['NUM', True]], True]],\n",
       "             221: [[[['P', True], ['NUM', True], ['P', True], ['NUM', True]],\n",
       "               True]],\n",
       "             222: [[[['ne-a', False]], False],\n",
       "              [[['M', True], ['1', False]], False]],\n",
       "             223: [[[['to', False]], False],\n",
       "              [[['M', True], ['ke', False]], False]],\n",
       "             224: [[[['P', False]], False], [[['NUM', False]], False]],\n",
       "             225: [[[['e-re-pa-ta', True],\n",
       "                ['L', True],\n",
       "                ['M', True],\n",
       "                ['NUM', True],\n",
       "                ['ka', False]],\n",
       "               False]],\n",
       "             227: [[[['o-no', False]], False],\n",
       "              [[['M', True], ['NUM', True], ['u', False]], False],\n",
       "              [[['M', True], ['NUM', True]], True]],\n",
       "             228: [[[['P', True], ['NUM', True]], True]],\n",
       "             229: [[[['N', False]], False],\n",
       "              [[['2', True], ['P', True]], True]],\n",
       "             230: [[[['NUM', False]], False],\n",
       "              [[['M', True], ['NUM', True], ['Q', True], ['1', False]],\n",
       "               False]],\n",
       "             231: [[[['N', False]], False], [[['1', True]], True]],\n",
       "             232: [[[['M', True]], True]],\n",
       "             233: [[[['MU', False]], False],\n",
       "              [[['NUM', True]], True],\n",
       "              [[['M', True], ['NUM', True]], True]],\n",
       "             234: [[[['ti-ri-to', False]], False],\n",
       "              [[['di-za-so', True], ['*168+SE', False]], False]],\n",
       "             235: [[[['su-ri-mo', True],\n",
       "                ['a-na-re-u', True],\n",
       "                ['*168+SE', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             236: [[[['qa-ra', False]], False],\n",
       "              [[['re-me-to', True], ['*168+SE', True], ['NUM', True]], True]],\n",
       "             237: [[[['u-ta-no', False]], False],\n",
       "              [[['pa-da', False]], False],\n",
       "              [[['*168+SE', True], ['NUM', True]], True]],\n",
       "             238: [[[['qa-mo', True], ['*168+SE', False]], False],\n",
       "              [[['NUM', True]], True]],\n",
       "             239: [[[['e-ra', True],\n",
       "                ['ta-ta-ro', True],\n",
       "                ['*168', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['mu-ka-ra', True], ['pa-i-to', True], ['*168', False]],\n",
       "               False]],\n",
       "             240: [[[['to-so', False]], False],\n",
       "              [[['*168+SE', True], ['NUM', True]], True]],\n",
       "             241: [[[['qa', False]], False]],\n",
       "             243: [[[['NUM', False]], False]],\n",
       "             244: [[[['je-ne', False]], False],\n",
       "              [[['ZE', True], ['NUM', True]], True]],\n",
       "             245: [[[['e-ke-a', True],\n",
       "                ['ka-ka', True],\n",
       "                ['re-a', True],\n",
       "                ['HAS', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             246: [[[['SAG', True],\n",
       "                ['NUM', True],\n",
       "                ['SAG', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             248: [[[['e-re-pa-te', True],\n",
       "                ['de-de-me-na', True],\n",
       "                ['PUG', False]],\n",
       "               False],\n",
       "              [[['zo-wa', False]], False],\n",
       "              [[['e-pi-zo-ta', True],\n",
       "                ['ke-ra', True],\n",
       "                ['de-de-me-na', True],\n",
       "                ['GUP', False]],\n",
       "               False]],\n",
       "             249: [[[['e-re-pa-te', False]], False],\n",
       "              [[['qo-jo', False]], False],\n",
       "              [[['zo-wa', True], ['e-pi-zo-ta', False]], False],\n",
       "              [[['GUP', False]], False],\n",
       "              [[['NUM', False]], False]],\n",
       "             250: [[[['to-sa', True],\n",
       "                ['pa-ka-na', True],\n",
       "                ['PUG', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             251: [[[['ka-si-ko-no', True],\n",
       "                ['pa-ka-na', True],\n",
       "                ['a-ra-ru-wo-a', True],\n",
       "                ['PUG', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             252: [[[['a-ra-ru-wo-a', True], ['PUG', True], ['1', False]],\n",
       "               False]],\n",
       "             253: [[[['de-so-mo', True],\n",
       "                ['a-mi-to-no', True],\n",
       "                ['pi-ri-te', True],\n",
       "                ['a-ra-ru-wo-a', True],\n",
       "                ['PUG', False]],\n",
       "               False]],\n",
       "             254: [[[['PUG', True], ['2', False]], False]],\n",
       "             255: [[[['pa-ka-na', True], ['a-ra-ru-wo-a', True]], True]],\n",
       "             256: [[[['ka-si-ko-no', True], ['PUG', True], ['2', True]],\n",
       "               True]],\n",
       "             257: [[[['da-zo', True], ['pi-ri-je-te', False]], False]],\n",
       "             258: [[[['de-so-mo', True],\n",
       "                ['ku-ka-ro', True],\n",
       "                ['pi-ri-je-te', True],\n",
       "                ['pa-ka-na', True],\n",
       "                ['a-ra-ru-wo-a', True],\n",
       "                ['PUG', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             259: [[[['no', False]], False],\n",
       "              [[['pi-ri-je-te', True], ['pa-ka-na', True]], True]],\n",
       "             260: [[[['te', False]], False],\n",
       "              [[['pa-ka-na', True], ['a-ra-ru-wo-a', True]], True]],\n",
       "             262: [[[['a-ra-ru-wo-a', True],\n",
       "                ['ka-si-ko-no', True],\n",
       "                ['pa-ka-na', True]],\n",
       "               True]],\n",
       "             263: [[[['pa-ka-na', True], ['a-ra-ru-wo-a', True]], True]],\n",
       "             264: [[[['a-ra-ru-wo-a', True], ['pa-ka-na', False]], False],\n",
       "              [[['PUG', False]], False]],\n",
       "             265: [[[['pa-ka-na', False]], False],\n",
       "              [[['a-ra-ru-wo-a', True]], True]],\n",
       "             266: [[[['wi-jo', False]], False],\n",
       "              [[['ka-si-ko-no', True]], True]],\n",
       "             267: [[[['o-wa-si-jo', True],\n",
       "                ['ka-si-ko-no', True],\n",
       "                ['PUG', False]],\n",
       "               False]],\n",
       "             268: [[[['ka-si-ko-no', True]], True]],\n",
       "             269: [[[['u-ta-jo', False]], False],\n",
       "              [[['ka-si-ko-no', True]], True]],\n",
       "             270: [[[['PUG', True], ['NUM', True]], True]],\n",
       "             271: [[[['ri', False]], False],\n",
       "              [[['PUG', True],\n",
       "                ['NUM', True],\n",
       "                ['e-pi-zo-ta', True],\n",
       "                ['ke-ra', True],\n",
       "                ['de-de-me-na', True],\n",
       "                ['GUP', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             272: [[[['ke-ra', True], ['de-de-me-na', True]], True]],\n",
       "             273: [[[['PUG', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['GUP', False]], False],\n",
       "              [[['2', False]], False]],\n",
       "             274: [[[['ke-ra', False]], False]],\n",
       "             275: [[[['ki-rya-i-jo', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True]],\n",
       "               True]],\n",
       "             276: [[[['ru-o-wo', False]], False],\n",
       "              [[['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             277: [[[['qa-mi-si-jo', True], ['TUN', True], ['BIG', False]],\n",
       "               False]],\n",
       "             279: [[[['ta', False]], False],\n",
       "              [[['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True]],\n",
       "               True],\n",
       "              [[['a-mi-ni-si', True]], True]],\n",
       "             280: [[[['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             281: [[[['TUN', False]], False],\n",
       "              [[['BIG', True], ['1', True]], True]],\n",
       "             282: [[[['1', True], ['EQU', True], ['MO', True], ['1', True]],\n",
       "               True]],\n",
       "             283: [[[['TUN', False]], False],\n",
       "              [[['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['MO', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             284: [[[['me-za-wo', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             285: [[[['DA', False]], False],\n",
       "              [[['1', True], ['BIG', True], ['AES', True]], True]],\n",
       "             286: [[[['pa-di-jo', True],\n",
       "                ['TUN+QE', True],\n",
       "                ['2', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['2', True]],\n",
       "               True]],\n",
       "             287: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['MO', True]], True],\n",
       "              [[['*166', True], ['1', True]], True]],\n",
       "             288: [[[['ti-ri-jo-qa', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['TUN', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['1', True],\n",
       "                ['e-ko', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             289: [[[['TUN+QE', True],\n",
       "                ['2', True],\n",
       "                ['EQU', True],\n",
       "                ['MO', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             290: [[[['ri-jo', False]], False],\n",
       "              [[['TUN+QE', True], ['2', True], ['MO', False]], False]],\n",
       "             291: [[[['o-pi-ri-mi-ni-jo', True],\n",
       "                ['TUN', True],\n",
       "                ['1', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', False]],\n",
       "               False]],\n",
       "             292: [[[['2', False]], False],\n",
       "              [[['BIG', True], ['1', True], ['EQU', True], ['ZE', False]],\n",
       "               False]],\n",
       "             293: [[[['a-qa-ro', False]], False], [[['TUN', True]], True]],\n",
       "             294: [[[['TUN', True], ['2', True], ['BIG', False]], False]],\n",
       "             295: [[[['po-*34-wi-do', True], ['TUN', True], ['BIG', False]],\n",
       "               False]],\n",
       "             296: [[[['ku-ru-me-no', True], ['TUN', True], ['BIG', True]],\n",
       "               True]],\n",
       "             297: [[[['a-e-da-do-ro', True], ['TUN', True], ['BIG', False]],\n",
       "               False],\n",
       "              [[['a-mi-ni-si', False]], False]],\n",
       "             298: [[[['me-nu-wa', True], ['TUN', False]], False],\n",
       "              [[['BIG', True]], True]],\n",
       "             299: [[[['a-ko-to', True], ['TUN', True]], True]],\n",
       "             300: [[[['ta-pa-no', True], ['BIG', False]], False]],\n",
       "             301: [[[['EQU', False]], False],\n",
       "              [[['MO', True], ['1', True]], True]],\n",
       "             302: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', True], ['1', True]], True],\n",
       "              [[['a-*47', False]], False]],\n",
       "             303: [[[['jo', False]], False],\n",
       "              [[['phu-re-wa', True],\n",
       "                ['TUN', True],\n",
       "                ['1', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', False]],\n",
       "               False]],\n",
       "             304: [[[['no', False]], False],\n",
       "              [[['e-ro-e', True], ['BIG', False]], False]],\n",
       "             305: [[[['re-o', False]], False],\n",
       "              [[['BIG', True], ['1', True]], True]],\n",
       "             306: [[[['qe-ra-di-ri-jo', True], ['*165', False]], False]],\n",
       "             307: [[[['pa-re', True], ['*165', True], ['1', False]], False]],\n",
       "             308: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['*165', True], ['1', True]], True]],\n",
       "             309: [[[['pa-re', False]], False],\n",
       "              [[['BIG', True], ['1', True], ['*165', False]], False]],\n",
       "             310: [[[['a-ko-to', True], ['TUN', False]], False]],\n",
       "             311: [[[['je-u', False]], False],\n",
       "              [[['TUN', True], ['BIG', False]], False]],\n",
       "             312: [[[['we-wa-do-ro', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['a-mi-ni-si-jo', True]], True]],\n",
       "             313: [[[['i-sa-wo', True], ['TUN', True], ['BIG', False]],\n",
       "               False]],\n",
       "             314: [[[['pa-wa-wo', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', False]],\n",
       "               False]],\n",
       "             315: [[[['di-so', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', False]],\n",
       "               False],\n",
       "              [[['po-*34', False]], False]],\n",
       "             316: [[[['a-ka-to', True],\n",
       "                ['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', False]],\n",
       "               False],\n",
       "              [[['1', False]], False],\n",
       "              [[['EQU', True], ['ZE', True], ['1', True]], True],\n",
       "              [[['a-re-ka-tu-ru-wo', True], ['T', True]], True]],\n",
       "             317: [[[['ka-ro-qo', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['qa-*83-to', True],\n",
       "                ['DA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             318: [[[['pa-ra-ko', False]], False],\n",
       "              [[['1', True],\n",
       "                ['pa-wa-so', True],\n",
       "                ['1', True],\n",
       "                ['TUN', True],\n",
       "                ['1', True],\n",
       "                ['TUN', False]],\n",
       "               False],\n",
       "              [[['1', True], ['i-ka-se', True], ['1', True], ['di-ka', False]],\n",
       "               False],\n",
       "              [[['ku-ne', False]], False]],\n",
       "             319: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True], ['TUN', True], ['2', True]], True]],\n",
       "             320: [[[['jo', False]], False],\n",
       "              [[['TUN', True], ['1', True], ['BIG', False]], False]],\n",
       "             321: [[[['a-*64-jo', True], ['TUN', False]], False]],\n",
       "             322: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['1', True]], True]],\n",
       "             323: [[[['si-mo', True], ['TUN', False]], False]],\n",
       "             324: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             325: [[[['TUN+QE', False]], False],\n",
       "              [[['1', True], ['QE', True], ['ZE', True], ['1', True]], True]],\n",
       "             326: [[[['TUN', True], ['2', True], ['BIG', False]], False]],\n",
       "             327: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             328: [[[['si-jo', False]], False],\n",
       "              [[['TUN', True], ['BIG', False]], False]],\n",
       "             329: [[[['BIG', False]], False], [[['1', True]], True]],\n",
       "             330: [[[['da-nwa-re', False]], False],\n",
       "              [[['BIG', True], ['1', True], ['EQU', False]], False]],\n",
       "             331: [[[['1', False]], False], [[['BIG', False]], False]],\n",
       "             332: [[[['TUN', True],\n",
       "                ['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             333: [[[['BIG', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['EQU', False]], False]],\n",
       "             334: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['MO', True], ['1', True]], True]],\n",
       "             335: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True]], True]],\n",
       "             336: [[[['EQU', False]], False]],\n",
       "             337: [[[['jo', False]], False],\n",
       "              [[['TUN', True], ['BIG', True], ['1', False]], False]],\n",
       "             338: [[[['TUN', False]], False]],\n",
       "             339: [[[['EQU', True], ['ZE', False]], False]],\n",
       "             340: [[[['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             341: [[[['wa', False]], False], [[['BIG', False]], False]],\n",
       "             342: [[[['BIG', False]], False]],\n",
       "             343: [[[['TUN', False]], False]],\n",
       "             344: [[[['TUN', False]], False],\n",
       "              [[['2', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             345: [[[['TUN', False]], False],\n",
       "              [[['1', True], ['BIG', True], ['1', True], ['EQU', True]],\n",
       "               True]],\n",
       "             346: [[[['EQU', False]], False], [[['ZE', True]], True]],\n",
       "             347: [[[['EQU', False]], False], [[['1', True]], True]],\n",
       "             348: [[[['TUN', False]], False],\n",
       "              [[['BIG', True], ['1', True]], True]],\n",
       "             349: [[[['TUN', False]], False],\n",
       "              [[['BIG', True], ['1', True]], True]],\n",
       "             350: [[[['TUN', False]], False],\n",
       "              [[['BIG', True], ['1', True], ['EQU', True]], True]],\n",
       "             351: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True]], True],\n",
       "              [[['*166', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             352: [[[['1', False]], False], [[['BIG', True]], True]],\n",
       "             353: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', False]], False]],\n",
       "             354: [[[['BIG', False]], False],\n",
       "              [[['EQU', False]], False],\n",
       "              [[['pa', False]], False]],\n",
       "             355: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             356: [[[['BIG', False]], False]],\n",
       "             357: [[[['EQU', False]], False]],\n",
       "             358: [[[['BIG', False]], False]],\n",
       "             359: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             360: [[[['TUN', False]], False],\n",
       "              [[['BIG', True], ['1', True]], True]],\n",
       "             361: [[[['zo', False]], False],\n",
       "              [[['TUN', True], ['2', True]], True]],\n",
       "             362: [[[['BIG', False]], False]],\n",
       "             363: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             364: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             365: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             366: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             367: [[[['TUN', False]], False], [[['BIG', False]], False]],\n",
       "             368: [[[['jo', False]], False],\n",
       "              [[['TUN', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['ZE', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             369: [[[['BIG', True]], True]],\n",
       "             370: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['1', True]], True]],\n",
       "             371: [[[['EQU', True], ['1', True]], True]],\n",
       "             372: [[[['EQU', False]], False],\n",
       "              [[['1', True], ['ZE', True]], True]],\n",
       "             373: [[[['TUN', False]], False],\n",
       "              [[['BIG', True],\n",
       "                ['1', True],\n",
       "                ['EQU', True],\n",
       "                ['MO', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             374: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             375: [[[['TUN', False]], False], [[['2', True]], True]],\n",
       "             376: [[[['ta-o', False]], False],\n",
       "              [[['EQU', True], ['ZE', True], ['1', False]], False]],\n",
       "             377: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             378: [[[['qo', False]], False],\n",
       "              [[['TUN', True], ['2', True]], True]],\n",
       "             379: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False],\n",
       "              [[['to', True]], True]],\n",
       "             380: [[[['BIG', False]], False]],\n",
       "             381: [[[['BIG', False]], False], [[['1', True]], True]],\n",
       "             382: [[[['*165', False]], False]],\n",
       "             383: [[[['BIG', False]], False], [[['*166', False]], False]],\n",
       "             384: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             385: [[[['EQU', False]], False]],\n",
       "             386: [[[['jo', False]], False], [[['TUN', False]], False]],\n",
       "             387: [[[['BIG', False]], False], [[['1', True]], True]],\n",
       "             388: [[[['2', False]], False], [[['BIG', False]], False]],\n",
       "             389: [[[['TUN', True], ['BIG', False]], False],\n",
       "              [[['a-*47-wi', False]], False]],\n",
       "             390: [[[['BIG', False]], False]],\n",
       "             391: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['ZE', True]], True],\n",
       "              [[['ja', False]], False],\n",
       "              [[['ka-wo', True]], True]],\n",
       "             392: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             393: [[[['BIG', False]], False]],\n",
       "             394: [[[['wa', False]], False],\n",
       "              [[['TUN', True], ['1', False]], False]],\n",
       "             395: [[[['BIG', False]], False],\n",
       "              [[['a-mi-ni-si-jo', False]], False]],\n",
       "             396: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', False]], False]],\n",
       "             397: [[[['EQU+QE', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             398: [[[['jo', False]], False],\n",
       "              [[['to-wo', True], ['TUN', False]], False]],\n",
       "             399: [[[['u', False]], False],\n",
       "              [[['TUN', True], ['2', True]], True]],\n",
       "             400: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             401: [[[['BIG', False]], False], [[['EQU', False]], False]],\n",
       "             402: [[[['TUN', True], ['BIG', False]], False],\n",
       "              [[['a-mi-ni-si-jo', False]], False]],\n",
       "             403: [[[['ta', False]], False],\n",
       "              [[['TUN', True], ['BIG', False]], False],\n",
       "              [[['a-mi-ni-si-jo', False]], False]],\n",
       "             404: [[[['EQU', True], ['ZE', False]], False]],\n",
       "             405: [[[['TUN', False]], False]],\n",
       "             406: [[[['ja', False]], False], [[['BIG', False]], False]],\n",
       "             407: [[[['ni-ko', False]], False],\n",
       "              [[['TUN', True], ['na', False]], False]],\n",
       "             408: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             409: [[[['TUN', False]], False], [[['2', True]], True]],\n",
       "             410: [[[['EQU', False]], False], [[['1', True]], True]],\n",
       "             411: [[[['wo', False]], False], [[['TUN', False]], False]],\n",
       "             412: [[[['wo', False]], False], [[['TUN+QE', False]], False]],\n",
       "             413: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False],\n",
       "              [[['jo', False]], False]],\n",
       "             414: [[[['e-wo', False]], False], [[['TUN+QE', False]], False]],\n",
       "             415: [[[['TUN', False]], False]],\n",
       "             416: [[[['TUN', False]], False]],\n",
       "             417: [[[['TUN', False]], False]],\n",
       "             418: [[[['TUN', False]], False]],\n",
       "             419: [[[['TUN', False]], False],\n",
       "              [[['1', True], ['BIG', False]], False],\n",
       "              [[['si-jo', False]], False]],\n",
       "             420: [[[['BIG', False]], False]],\n",
       "             421: [[[['1', False]], False], [[['BIG', False]], False]],\n",
       "             422: [[[['BIG', False]], False]],\n",
       "             423: [[[['BIG', False]], False]],\n",
       "             424: [[[['BIG', False]], False]],\n",
       "             425: [[[['BIG', False]], False], [[['EQU', False]], False]],\n",
       "             426: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', True], ['ZE', True], ['1', True]], True]],\n",
       "             427: [[[['EQU', False]], False], [[['1', True]], True]],\n",
       "             428: [[[['EQU', False]], False], [[['ZE', False]], False]],\n",
       "             429: [[[['ZE', True], ['1', True]], True]],\n",
       "             430: [[[['TUN', False]], False], [[['1', True]], True]],\n",
       "             431: [[[['ri', False]], False], [[['TUN', False]], False]],\n",
       "             432: [[[['EQU', False]], False]],\n",
       "             433: [[[['TUN', False]], False]],\n",
       "             434: [[[['TUN', False]], False], [[['1', True]], True]],\n",
       "             435: [[[['1', False]], False], [[['BIG', False]], False]],\n",
       "             436: [[[['BIG', False]], False]],\n",
       "             437: [[[['BIG', False]], False]],\n",
       "             438: [[[['BIG', False]], False], [[['1', False]], False]],\n",
       "             439: [[[['BIG', False]], False],\n",
       "              [[['1', True], ['EQU', False]], False]],\n",
       "             440: [[[['EQU', False]], False], [[['ZE', False]], False]],\n",
       "             441: [[[['EQU', False]], False],\n",
       "              [[['ZE', True], ['1', True]], True]],\n",
       "             442: [[[['EQU', False]], False]],\n",
       "             443: [[[['EQU', False]], False], [[['1', True]], True]],\n",
       "             444: [[[['EQU', False]], False]],\n",
       "             445: [[[['BIG', False]], False]],\n",
       "             446: [[[['BIG', False]], False]],\n",
       "             447: [[[['BIG', False]], False]],\n",
       "             448: [[[['EQU', False]], False], [[['ZE', False]], False]],\n",
       "             449: [[[['BIG', False]], False]],\n",
       "             450: [[[['EQU', False]], False], [[['ZE', False]], False]],\n",
       "             451: [[[['a-ra-ru-ja', False]], False],\n",
       "              [[['a-ni-ja-pi', True],\n",
       "                ['wi-ri-ni-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['i-qi-jo', True],\n",
       "                ['a-ja-me-no', True],\n",
       "                ['e-re-pa-te', True],\n",
       "                ['a-ra-ro-mo-te-me-no', True],\n",
       "                ['po-ni-ki-jo', True]],\n",
       "               True]],\n",
       "             452: [[[['a-u-qe', True],\n",
       "                ['a-re-ta-to', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['pte-no', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['au-ro', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['pe-qa-to', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['i-qi-ja', False]], False],\n",
       "              [[['a-ra-ro-mo-te-me-na', True],\n",
       "                ['po-ni-ki-ja', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['a-ni-ja', True],\n",
       "                ['po-si', True]],\n",
       "               True]],\n",
       "             453: [[[['e-re-pa-te-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['ko-ki-da', True],\n",
       "                ['o-pa', True],\n",
       "                ['CUR', True],\n",
       "                ['NUM', True],\n",
       "                ['i-qi-ja', False]],\n",
       "               False],\n",
       "              [[['a-ja-me-na', True],\n",
       "                ['e-re-pa-te', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['a-ra-ru-ja', True]],\n",
       "               True]],\n",
       "             454: [[[['jo', False]], False],\n",
       "              [[['i-qo-e-qe', True],\n",
       "                ['wi-ri-ni-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['i-qi-ja', False]], False],\n",
       "              [[['ku-do-ni-ja', True],\n",
       "                ['mi-to-we-sa-e', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['po-ni-ki-ja', True],\n",
       "                ['BIG', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             455: [[[['wi-ri-ni-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['pte-no', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['1', False]], False],\n",
       "              [[['i-qi-ja', False]], False],\n",
       "              [[['po-ni-ki-ja', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['a-ra-ru-ja', True],\n",
       "                ['a-ni-ja-pi', True]],\n",
       "               True]],\n",
       "             456: [[[['wi-ri-ni-jo', False]], False],\n",
       "              [[['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['to', False]], False],\n",
       "              [[['a-ra-ro-mo-te-me-na', True]], True]],\n",
       "             458: [[[['a-ra-ru-wo-ja', False]], False],\n",
       "              [[['a-ni-ja-pi', True],\n",
       "                ['wi-ri-ne-o', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['i-qi-ja', True],\n",
       "                ['a-ja-me-na', True],\n",
       "                ['e-re-pa-te', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['po-ni-ja-ja', True],\n",
       "                ['CUR', False]],\n",
       "               False]],\n",
       "             459: [[[['wi-ri-ne-o', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ka-ke-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', True],\n",
       "                ['1', True],\n",
       "                ['i-qi-ja', False]],\n",
       "               False],\n",
       "              [[['po-ni-ki-ja', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['a-ja-me-na', True]],\n",
       "               True]],\n",
       "             460: [[[['ka-ke-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['po-si', True],\n",
       "                ['e-re-pa', True],\n",
       "                ['me-na', False]],\n",
       "               False],\n",
       "              [[['a-ja-me-na', True], ['CUR', True], ['1', True]], True]],\n",
       "             461: [[[['a-ra-ru-ja', True],\n",
       "                ['a-ni-ja-pi', True],\n",
       "                ['wi-ri-ni-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['i-qi-ja', True],\n",
       "                ['pa-i-to', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['do-we-jo', True],\n",
       "                ['i-qo-e-qe', True],\n",
       "                ['po-ni-ki-ja', True]],\n",
       "               True]],\n",
       "             462: [[[['wi-ri-ne-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', True],\n",
       "                ['2', True],\n",
       "                ['i-qi-jo', True],\n",
       "                ['mi-to-we-sa', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['a-ja-me', True]],\n",
       "               True]],\n",
       "             463: [[[['o-u-qe', True],\n",
       "                ['a-ni-ja', True],\n",
       "                ['po-si', True],\n",
       "                ['CUR', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['i-qi-ja', False]], False],\n",
       "              [[['mi-to-we-sa', True], ['a-ra-ro-mo-to-me-na', False]], False],\n",
       "              [[['a-ja-me-na', True]], True]],\n",
       "             464: [[[['o-pa', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['pe-qa-to', True],\n",
       "                ['u-po', True],\n",
       "                ['CUR', False]],\n",
       "               False],\n",
       "              [[['i-qi-ja', False]], False],\n",
       "              [[['a-ro-mo-te-me-na', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['a-ni-ja', True],\n",
       "                ['po-si', True],\n",
       "                ['e-e-si', False]],\n",
       "               False]],\n",
       "             465: [[[['ke-ra-i-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['o-u-qe', True],\n",
       "                ['pte-no', True],\n",
       "                ['po-si', True],\n",
       "                ['a-ra-ru-ja', True],\n",
       "                ['a-ni-ja-pi', True],\n",
       "                ['wi-ri-ne-o', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['i-qi-ja', True],\n",
       "                ['po-ni-ki-ja', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True],\n",
       "                ['e-re-pa-te', True],\n",
       "                ['do-we-jo', True],\n",
       "                ['i-qo-e-qe', True]],\n",
       "               True]],\n",
       "             466: [[[['wi-ri-ne-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['i-qi-ja', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True]],\n",
       "               True]],\n",
       "             467: [[[['jo', False]], False],\n",
       "              [[['o-po-qo', True],\n",
       "                ['ka-ke-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['o', False]],\n",
       "               False],\n",
       "              [[['a-ra-ro-mo-te-me-na', True], ['a-ra-ru-ja', False]], False]],\n",
       "             468: [[[['i-qi-ja', True]], True]],\n",
       "             469: [[[['po-ni-ki-ja', False]], False]],\n",
       "             470: [[[['CUR', False]], False], [[['1', False]], False]],\n",
       "             471: [[[['i-qi-ja', False]], False]],\n",
       "             472: [[[['i-qi-ja', True]], True]],\n",
       "             473: [[[['i-qi-ja', True]], True]],\n",
       "             474: [[[['i-qi-ja', True]], True]],\n",
       "             475: [[[['ai-ki-no-o', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['pa-ra-ja', True],\n",
       "                ['e-te-re-ta', True],\n",
       "                ['po-ro-ti-ri', True],\n",
       "                ['CUR', False]],\n",
       "               False]],\n",
       "             476: [[[['po-ni-ke-a', True],\n",
       "                ['wo-ra-we-sa', True],\n",
       "                ['CUR', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             477: [[[['CUR', True], ['1', True], ['o', False]], False]],\n",
       "             478: [[[['po-ni-ki-ja', True], ['CUR', False]], False]],\n",
       "             479: [[[['CUR', False]], False],\n",
       "              [[['1', True], ['pi', False]], False]],\n",
       "             480: [[[['pte-re-wa', True]], True]],\n",
       "             481: [[[['e-re-pa-te-jo-pi', True],\n",
       "                ['o-mo-pi', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['e-ka-te-re-ta', True],\n",
       "                ['ai-ki-no-o', True],\n",
       "                ['2', True],\n",
       "                ['e-re-pa-te-jo-pi', True]],\n",
       "               True]],\n",
       "             482: [[[['pte-re-wa', False]], False]],\n",
       "             483: [[[['we-ja', False]], False],\n",
       "              [[['pte-re-wa', True], ['ka-ke', False]], False]],\n",
       "             484: [[[['e-wi-su-zo-ko', True], ['ka', False]], False],\n",
       "              [[['po-ni-ki-ja', True]], True]],\n",
       "             485: [[[['e-wi-su-zo-ko', True],\n",
       "                ['e-re-pa-te-o', True],\n",
       "                ['o-mo', False]],\n",
       "               False]],\n",
       "             486: [[[['po-ni-ki-ja', True]], True]],\n",
       "             487: [[[['pte-re-wa', False]], False], [[['wi', False]], False]],\n",
       "             488: [[[['CUR', False]], False], [[['1', True]], True]],\n",
       "             489: [[[['pte-re', False]], False]],\n",
       "             490: [[[['CUR', False]], False], [[['1', True]], True]],\n",
       "             491: [[[['pte-re-wa', False]], False]],\n",
       "             492: [[[['CUR', False]], False]],\n",
       "             493: [[[['po-ni-ki-ja', True], ['CUR', False]], False]],\n",
       "             494: [[[['CUR', False]], False], [[['1', False]], False]],\n",
       "             495: [[[['CUR', False]], False], [[['2', False]], False]],\n",
       "             496: [[[['i-qi-ja', True], ['e-ka-te-jo', True], ['CAPS', False]],\n",
       "               False]],\n",
       "             497: [[[['i-qi-ja', False]], False],\n",
       "              [[['a-na-i-ta', True], ['CAPS', True], ['NUM', True]], True]],\n",
       "             498: [[[['a-re-ki-si-to-jo', True],\n",
       "                ['o-pa', True],\n",
       "                ['i-qi-ja', True],\n",
       "                ['a-na-ta', True],\n",
       "                ['a-na-mo-to', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', True]],\n",
       "               True],\n",
       "              [[['a-mo-ta', False]], False],\n",
       "              [[['a', False]], False]],\n",
       "             499: [[[['i-qi-ja', True],\n",
       "                ['a-na-mo-to', True],\n",
       "                ['a-ja-me-na', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             500: [[[['i-qi-ja', False]], False],\n",
       "              [[['a-na-mo-to', True],\n",
       "                ['a-na-to', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             501: [[[['i-qi-ja', True],\n",
       "                ['a-ja-me-na', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             502: [[[['i-qi-ja', True], ['a-na-to', True], ['CAPS', False]],\n",
       "               False]],\n",
       "             503: [[[['i-qi-ja', False]], False],\n",
       "              [[['a-ja-me-na', True], ['CAPS', True], ['NUM', True]], True]],\n",
       "             504: [[[['i-qi-ja', False]], False],\n",
       "              [[['a-na-mo-to', True],\n",
       "                ['a-ja-me-na', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             505: [[[['wi-ri-ne-o', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['i-qi-ja', True],\n",
       "                ['po-ni-ki-ja', True],\n",
       "                ['me-ta-ke-ku-me-na', True],\n",
       "                ['CAPS', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             506: [[[['a-na-mo-ta', True], ['*245', True], ['NUM', True]],\n",
       "               True]],\n",
       "             507: [[[['i-qi-ja', False]], False]],\n",
       "             508: [[[['i-qi-ja', True], ['a-ja-me', False]], False]],\n",
       "             509: [[[['a-na-mo-to', False]], False]],\n",
       "             510: [[[['a-na-i-ta', False]], False]],\n",
       "             511: [[[['a-ja-me-na', True], ['a-na-mo-ta', False]], False]],\n",
       "             512: [[[['i-qi-ja', True]], True]],\n",
       "             513: [[[['e-na-ri-po-to', False]], False],\n",
       "              [[['CAPS', False]], False]],\n",
       "             514: [[[['CAPS', True], ['1', True]], True]],\n",
       "             515: [[[['CAPS', False]], False], [[['1', True]], True]],\n",
       "             516: [[[['na', False]], False], [[['CAPS', False]], False]],\n",
       "             517: [[[['po-ro-su-re', True],\n",
       "                ['a-na-to', True],\n",
       "                ['o', True],\n",
       "                ['CAPS', False]],\n",
       "               False]],\n",
       "             518: [[[['CAPS', True], ['1', True]], True]],\n",
       "             519: [[[['CAPS', True],\n",
       "                ['NUM', True],\n",
       "                ['CAPS', True],\n",
       "                ['NUM', True],\n",
       "                ['te-mi-we-te', False]],\n",
       "               False],\n",
       "              [[['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['o-da-ke-we-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['a-mo', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['o-da-ke-we-ta', False]], False],\n",
       "              [[['ROTA', True], ['ZE', True], ['NUM', True]], True],\n",
       "              [[['ROTA', True], ['NUM', False]], False],\n",
       "              [[['ROTA', True], ['NUM', False]], False],\n",
       "              [[['ROTA', True], ['NUM', False]], False]],\n",
       "             520: [[[['a-na-to', True], ['CAPS', False]], False]],\n",
       "             521: [[[['ja', False]], False], [[['CAPS', False]], False]],\n",
       "             522: [[[['CAPS', False]], False]],\n",
       "             524: [[[['do-we-jo', True],\n",
       "                ['i-qo-e-qe', True],\n",
       "                ['wi-ri-ni-jo', True],\n",
       "                ['o-po-qo', True],\n",
       "                ['ke-ra-ja-pi', True],\n",
       "                ['o-pi-i-ja-pi', True],\n",
       "                ['CUR', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['se-to-i-ja', True],\n",
       "                ['mi-to-we-sa', True],\n",
       "                ['a-ra-ro-mo-te-me-na', True]],\n",
       "               True]],\n",
       "             546: [[[['qe-ryo', False]], False],\n",
       "              [[['2', True],\n",
       "                ['ko-ru', True],\n",
       "                ['GAL', True],\n",
       "                ['1', True],\n",
       "                ['o-pa-wo-ta', False]],\n",
       "               False]],\n",
       "             549: [[[['qe-ryo', True], ['2', False]], False],\n",
       "              [[['e-po-mi-jo', True], ['2', False]], False],\n",
       "              [[['o-pa-wo-ta', True],\n",
       "                ['2', True],\n",
       "                ['ko-ru', True],\n",
       "                ['GAL', True],\n",
       "                ['1', True],\n",
       "                ['o-pi-ko-ru-si-ja', False]],\n",
       "               False]],\n",
       "             550: [[[['ko-ru', True], ['GAL', True], ['1', True]], True]],\n",
       "             551: [[[['qe-ryo', True],\n",
       "                ['2', True],\n",
       "                ['e-po-mi-jo', True],\n",
       "                ['to-ra', True],\n",
       "                ['e-pi-ko-ru-si-jo', True],\n",
       "                ['2', True],\n",
       "                ['pa-ra-wa-jo', False]],\n",
       "               False]],\n",
       "             553: [[[['qe-ryo', True],\n",
       "                ['2', True],\n",
       "                ['e-po-mi-jo', True],\n",
       "                ['2', True],\n",
       "                ['o-pa-wo-ta', True],\n",
       "                ['o-pa-wo-ta', True],\n",
       "                ['2', True],\n",
       "                ['to-ra', True],\n",
       "                ['ko-ru', True],\n",
       "                ['GAL', True],\n",
       "                ['1', True],\n",
       "                ['o-pi-ko-ru-si-ja', True],\n",
       "                ['2', True],\n",
       "                ['pa-ra-wa-jo', True],\n",
       "                ['2', True]],\n",
       "               True]],\n",
       "             554: [[[['qe-ryo', True], ['1', True], ['2', False]], False],\n",
       "              [[['o-pa-wo-ta', True]], True]],\n",
       "             555: [[[['a-te-re-te-a', True],\n",
       "                ['pe-te-re-wa', True],\n",
       "                ['te-mi-dwe', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['ka-ki-jo', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True],\n",
       "                ['ka-ko-de-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['ki-da-pa', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['MO', False]],\n",
       "               False],\n",
       "              [[['o-da-tu-we-ta', True],\n",
       "                ['e-ri-ka', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             556: [[[['a', False]], False],\n",
       "              [[['a-re-ki-si-to-jo', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', False]],\n",
       "               False]],\n",
       "             557: [[[['pte-re-e', False]], False],\n",
       "              [[['a-mo-ta', True],\n",
       "                ['o-da-ku-we-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['e-ri-ko', False]],\n",
       "               False]],\n",
       "             558: [[[['e-ri-ka', True],\n",
       "                ['o-da-twe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['MO', True],\n",
       "                ['ROTA', True]],\n",
       "               True]],\n",
       "             559: [[[['a-mo-ta', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['a-ryo-jo', True],\n",
       "                ['te-mi-dwe-te', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             560: [[[['de-do-me-na', True], ['a-mo-ta', False]], False],\n",
       "              [[['pte-re-wa', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['ROTA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             561: [[[['LANA', False]], False],\n",
       "              [[['NUM', True], ['LANA', False]], False],\n",
       "              [[['2', False]], False]],\n",
       "             562: [[[['ko-ki-da', True],\n",
       "                ['o-pa', True],\n",
       "                ['ne-wa', True],\n",
       "                ['e-ri-ka', True],\n",
       "                ['o-da-twe-ta', True],\n",
       "                ['a-ryo-a', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['MO', True],\n",
       "                ['ROTA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             563: [[[['a-mo-ta', False]], False],\n",
       "              [[['te-mi-dwe-ta', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['ROTA', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             564: [[[['e-ri-ka', False]], False],\n",
       "              [[['o-da-twe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['MO', True],\n",
       "                ['ROTA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             565: [[[['te-mi-dwe-te', True],\n",
       "                ['a-re-ki-si-to', True],\n",
       "                ['wo-zo-me-no', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             566: [[[['e-ri-ka', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['2', True],\n",
       "                ['MO', True],\n",
       "                ['ROTA', False]],\n",
       "               False],\n",
       "              [[['1', True]], True]],\n",
       "             567: [[[['e-ri-ka', False]], False],\n",
       "              [[['wo-zo-me-na', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             568: [[[['a-mo-ta', True],\n",
       "                ['e-ri-ka', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True],\n",
       "                ['MO', True],\n",
       "                ['ROTA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             569: [[[['de-do-me-na', True],\n",
       "                ['a-mo-ta', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['o-da-twe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             570: [[[['e-ri-ka', True],\n",
       "                ['o-da-twe-ta', True],\n",
       "                ['de-do-me-na', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['ka', False]], False]],\n",
       "             571: [[[['o-pe-ro', True],\n",
       "                ['se-to-i-ja', True],\n",
       "                ['a-mo-te', True],\n",
       "                ['pe-ru-si-nwa', True],\n",
       "                ['ta-ra-si-ja', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             572: [[[['wa-ra-wi-ta', True], ['ta', False]], False],\n",
       "              [[['ROTA', True], ['ZE', False]], False]],\n",
       "             573: [[[['pte-re-wa', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             574: [[[['a-mo-ta', False]], False],\n",
       "              [[['e-ri-ka', True],\n",
       "                ['o-da-ke-we-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', False]],\n",
       "               False],\n",
       "              [[['MO', True], ['ROTA', False]], False],\n",
       "              [[['1', True], ['to', False]], False],\n",
       "              [[['o-pe-ro', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True],\n",
       "              [[['e-ko', True], ['ROTA', True], ['ROTA', True]], True]],\n",
       "             575: [[[['o-pe-te-we', True],\n",
       "                ['e-ri-ka', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             576: [[[['pa-i-to', True],\n",
       "                ['a-mo-ta', True],\n",
       "                ['pte-re-wa', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             577: [[[['pte-re-wa', True],\n",
       "                ['te-mi-dwe-ta', True],\n",
       "                ['ne-wa', True],\n",
       "                ['ROTA', True],\n",
       "                ['ZE', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             578: [[[['ROTA', False]], False],\n",
       "              [[['ZE', True], ['NUM', True]], True]],\n",
       "             579: [[[['e-ri', False]], False]],\n",
       "             580: [[[['twe-te', False]], False],\n",
       "              [[['ri-ko', False]], False],\n",
       "              [[['ROTA', True]], True]],\n",
       "             581: [[[['o-da-twe-ta', False]], False]],\n",
       "             582: [[[['wo-ra-e', True]], False],\n",
       "              [[['*253', True], ['2', True]], True]],\n",
       "             583: [[[['wo-ra', False]], False],\n",
       "              [[['ka-za', True], ['*253', True], ['1', True]], True]],\n",
       "             584: [[[['ri-*65-no', True],\n",
       "                ['a-pe-re', True],\n",
       "                ['QO', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             585: [[[['*178', True], ['NUM', True]], True]],\n",
       "             586: [[[['ro', False]], False],\n",
       "              [[['*179', True], ['2', False]], False]],\n",
       "             587: [[[['a-ku-wo', True], ['pa-ra-ja', True], ['*256', False]],\n",
       "               False]],\n",
       "             588: [[[['pa-ra-ja', True], ['*256', True], ['1', True]], True]],\n",
       "             589: [[[['ke-ti-ro', True], ['*180', True], ['1', False]],\n",
       "               False]],\n",
       "             590: [[[['NUM', False]], False], [[['*172+KE', False]], False]],\n",
       "             591: [[[['ko-so-ni-ja', False]], False],\n",
       "              [[['*246', True], ['NUM', True]], True]],\n",
       "             592: [[[['na-u-do-mo', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['e-to-ro-qa-ta', True], ['*181', True], ['NUM', True]], True],\n",
       "              [[['NUM', False]], False]],\n",
       "             593: [[[['NUM', False]], False],\n",
       "              [[['*172+KE+RO2', True], ['NUM', True]], True]],\n",
       "             594: [[[['so', False]], False],\n",
       "              [[['*258', True], ['1', True]], True]],\n",
       "             595: [[[['*182', True], ['NUM', True], ['a-pi-te', False]],\n",
       "               False]],\n",
       "             596: [[[['wi-ja', False]], False],\n",
       "              [[['*183', True], ['NUM', True]], True]],\n",
       "             598: [[[['si-ja-ma-to', True],\n",
       "                ['e', True],\n",
       "                ['*177', True],\n",
       "                ['2', True],\n",
       "                ['me-wo-ni-jo', True],\n",
       "                ['e', True],\n",
       "                ['*177', True],\n",
       "                ['2', True],\n",
       "                ['ma', True],\n",
       "                ['*177', True],\n",
       "                ['2', True],\n",
       "                ['ai-ki-si-jo', True],\n",
       "                ['e', True],\n",
       "                ['*177', True],\n",
       "                ['2', True],\n",
       "                ['ta-u-po-no', True],\n",
       "                ['ma', True],\n",
       "                ['*177', True],\n",
       "                ['1', True],\n",
       "                ['pu-re-wa', True],\n",
       "                ['ma', False]],\n",
       "               False],\n",
       "              [[['*177', True],\n",
       "                ['1', True],\n",
       "                ['pa-na-re-jo', True],\n",
       "                ['ka-u-ja', False]],\n",
       "               False],\n",
       "              [[['de-u-ke-ro', True],\n",
       "                ['do-ri-ka-no', True],\n",
       "                ['ai-ki-po', True],\n",
       "                ['ke', False]],\n",
       "               False],\n",
       "              [[['a-nu', False]], False],\n",
       "              [[['e-ke', False]], False],\n",
       "              [[['e-da', False]], False],\n",
       "              [[['nu-wo', False]], False],\n",
       "              [[['ni-ja', False]], False],\n",
       "              [[['e', True], ['*177', False]], False],\n",
       "              [[['ka-pa-ri-jo', False]], False],\n",
       "              [[['e', True], ['*177', False]], False],\n",
       "              [[['we-ka-di-jo', True], ['ma', True], ['*177', False]], False],\n",
       "              [[['pi-ri-sa-ta', True], ['ma', False]], False],\n",
       "              [[['ku-ryo', True], ['ma', False]], False],\n",
       "              [[['e-ke-me-de', True], ['ma', True], ['*177', False]], False],\n",
       "              [[['pa-na-re-jo', True], ['e', True], ['*177', False]], False],\n",
       "              [[['se-ri-na-ta', True], ['e', True], ['*177', False]], False],\n",
       "              [[['pa-ke-ta', True], ['ma', True], ['*177', False]], False],\n",
       "              [[['a-wi-to-do-to', True], ['ma', True], ['*177', False]],\n",
       "               False],\n",
       "              [[['ma', True], ['no', False]], False],\n",
       "              [[['ma', False]], False],\n",
       "              [[['*177', True], ['wo', False]], False],\n",
       "              [[['ma', True], ['*177', False]], False]],\n",
       "             599: [[[['*246', True], ['1', True], ['1', False]], False]],\n",
       "             600: [[[['NUM', False]], False],\n",
       "              [[['*190', True], ['NUM', False]], False]],\n",
       "             601: [[[['1', False]], False],\n",
       "              [[['KI', True], ['NUM', False]], False]],\n",
       "             602: [[[['f', False]], False],\n",
       "              [[['NUM', True], ['E', True], ['NUM', True]], True]],\n",
       "             603: [[[['sa', False]], False],\n",
       "              [[['OVISm', True],\n",
       "                ['2', True],\n",
       "                ['OVISf', True],\n",
       "                ['1', True],\n",
       "                ['*190', True],\n",
       "                ['NUM', True],\n",
       "                ['VIN', True],\n",
       "                ['OVISf', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['pa', True],\n",
       "                ['OVISf', True],\n",
       "                ['1', True],\n",
       "                ['*190', True],\n",
       "                ['NUM', True],\n",
       "                ['OVISx', False]],\n",
       "               False],\n",
       "              [[['2', False]], False],\n",
       "              [[['OVISx', False]], False]],\n",
       "             604: [[[['*209VAS', False]], False], [[['NUM', True]], True]],\n",
       "             605: [[[['me-ri-te-o', False]], False],\n",
       "              [[['*168', True], ['NUM', False]], False],\n",
       "              [[['a', False]], False]],\n",
       "             606: [[[['de-do-me-na', False]], False],\n",
       "              [[['*256', False]], False]],\n",
       "             607: [[[['e-re-ta', False]], False],\n",
       "              [[['*259', True], ['1', False]], False]],\n",
       "             608: [[[['re-u-ka', False]], False],\n",
       "              [[['si-ki-ro', True], ['DI+PTE', True], ['NUM', True]], True]],\n",
       "             609: [[[['WA', False]], False], [[['NUM', True]], True]],\n",
       "             610: [[[['me-ri', False]], False],\n",
       "              [[['Z', True], ['NUM', True], ['Z', True], ['qi', False]],\n",
       "               False],\n",
       "              [[['V', False]], False],\n",
       "              [[['1', True],\n",
       "                ['Z', True],\n",
       "                ['1', True],\n",
       "                ['VINb', True],\n",
       "                ['S', True],\n",
       "                ['1', True],\n",
       "                ['V', True],\n",
       "                ['NUM', True],\n",
       "                ['Z', True],\n",
       "                ['2', True],\n",
       "                ['V', True],\n",
       "                ['NUM', True],\n",
       "                ['Z', True],\n",
       "                ['NUM', True],\n",
       "                ['de-re-u-ko', True],\n",
       "                ['VIN', True],\n",
       "                ['S', True],\n",
       "                ['NUM', True],\n",
       "                ['V', False]],\n",
       "               False],\n",
       "              [[['NUM', True],\n",
       "                ['Z', True],\n",
       "                ['2', True],\n",
       "                ['CYP', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['a-pi-po-re-we', True], ['*209VAS', True], ['NUM', False]],\n",
       "               False],\n",
       "              [[['i-po-no', True],\n",
       "                ['*213VAS', True],\n",
       "                ['NUM', True],\n",
       "                ['ro', False]],\n",
       "               False],\n",
       "              [[['*212VAS', True], ['NUM', True]], True]],\n",
       "             611: [[[['OVIS', False]], False],\n",
       "              [[['NUM', True],\n",
       "                ['SUS', True],\n",
       "                ['1', True],\n",
       "                ['NUM', True],\n",
       "                ['OLIV', True],\n",
       "                ['NUM', True],\n",
       "                ['NI', True],\n",
       "                ['NUM', True],\n",
       "                ['VIN', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             613: [[[['ka-wi-ja', False]], False],\n",
       "              [[['DA', True], ['NUM', True]], True]],\n",
       "             616: [[[['ti-ri-to', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['ri', False]],\n",
       "               False],\n",
       "              [[['i-wa-ka', True], ['ra-mo', True]], True]],\n",
       "             617: [[[['ja', False]], False],\n",
       "              [[['qe-da-do-ro', True],\n",
       "                ['pa-na-so', True],\n",
       "                ['DA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             618: [[[['pa-to-ro', True], ['DA', True], ['1', True]], True]],\n",
       "             619: [[[['ta-na-ti', False]], False],\n",
       "              [[['DA', True], ['1', False]], False]],\n",
       "             620: [[[['DA', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['DA', True], ['1', True], ['PA', False]], False],\n",
       "              [[['phu-ru-da-ro', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['PA', True],\n",
       "                ['1', True],\n",
       "                ['di-ra', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['PA', True],\n",
       "                ['2', True],\n",
       "                ['e-te-do-mo', True],\n",
       "                ['ki-te', False]],\n",
       "               False],\n",
       "              [[['da-ru-*56', True], ['DA', False]], False],\n",
       "              [[['qa-ra-jo', False]], False]],\n",
       "             621: [[[['ke-ke-me-na', True], ['do', False]], False],\n",
       "              [[['e-ke', True],\n",
       "                ['ti-ri-to', True],\n",
       "                ['pu-te', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['PA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             622: [[[['wo-we-u', True],\n",
       "                ['ku-ka-da-ro', True],\n",
       "                ['qa-ra', True],\n",
       "                ['pi-di-jo', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['PA', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             623: [[[['te-wa-te-u', False]], False],\n",
       "              [[['ta-ko-ro', False]], False],\n",
       "              [[['DA', True], ['1', True], ['PA', True], ['1', True]], True]],\n",
       "             624: [[[['ta', False]], False],\n",
       "              [[['a-ke-re-mo', True], ['DA', True], ['1', True]], True]],\n",
       "             625: [[[['te-re-ta', True],\n",
       "                ['ke-ma-qe-me', True],\n",
       "                ['me-ra', True],\n",
       "                ['ko-do', True],\n",
       "                ['da-*22-to', True],\n",
       "                ['ke-nu-wa-so', True],\n",
       "                ['1', False]],\n",
       "               False]],\n",
       "             626: [[[['te-re-ta', True], ['ra', False]], False],\n",
       "              [[['ti-ri-to', True], ['wo-ne', False]], False]],\n",
       "             627: [[[['da-jo', False]], False],\n",
       "              [[['qa-ra', True], ['te-re-ta', False]], False]],\n",
       "             628: [[[['ko-to-i-na', True],\n",
       "                ['e-ri-ke-re-we', True],\n",
       "                ['e-ke-pu-te-ri-ja', True]],\n",
       "               True]],\n",
       "             629: [[[['o-pi', True],\n",
       "                ['po-to-ri-ka-ta', True],\n",
       "                ['do-wo', False]],\n",
       "               False],\n",
       "              [[['e-ko-so', True], ['ke-ke-me-na', True]], True]],\n",
       "             630: [[[['ai-ki-wa-to', True],\n",
       "                ['ti-ri-to', True],\n",
       "                ['pu-te', False]],\n",
       "               False]],\n",
       "             631: [[[['a-ri-ja-wo', True],\n",
       "                ['qa-ra', True],\n",
       "                ['te-re-ta', False]],\n",
       "               False]],\n",
       "             632: [[[['si-jo', False]], False],\n",
       "              [[['ti-ri-to', True], ['pu', False]], False]],\n",
       "             633: [[[['i-ra-ta', True], ['ti-ri-to', True], ['pu', False]],\n",
       "               False]],\n",
       "             634: [[[['ko-to-i-na', True],\n",
       "                ['pe-ri-qo-ta', True],\n",
       "                ['e-ke-pu-te-ri-ja', True]],\n",
       "               True]],\n",
       "             635: [[[['wo', False]], False],\n",
       "              [[['nwa-jo', False]], False],\n",
       "              [[['qa-ra', True]], True]],\n",
       "             636: [[[['ko-to-i-na', True],\n",
       "                ['pe-ri-je-ja', True],\n",
       "                ['e-ke', True],\n",
       "                ['pu-te-ri-ja', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['ti-ri-to', True]],\n",
       "               True]],\n",
       "             637: [[[['te-wa-jo', False]], False]],\n",
       "             638: [[[['te-ro-ri-jo', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['phu-to', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['me-ta-no-re', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['mi-ni-so', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['e-ke-da-mo', True],\n",
       "                ['DA', True],\n",
       "                ['1', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['DA', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             639: [[[['ka-da-i-to', True], ['pu-te', True]], True]],\n",
       "             640: [[[['te-wa-te-u', False]], False],\n",
       "              [[['qa-ra', True], ['*83-re-te', True], ['DA', False]], False]],\n",
       "             641: [[[['2', False]], False],\n",
       "              [[['PA', True], ['1', False]], False]],\n",
       "             642: [[[['me-na', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             643: [[[['DA', True], ['1', True]], True]],\n",
       "             644: [[[['*65-no', False]], False],\n",
       "              [[['DA', True], ['NUM', True]], True]],\n",
       "             645: [[[['te-nu', True], ['DA', True], ['1', True]], True]],\n",
       "             646: [[[['DA', True], ['1', True]], True]],\n",
       "             647: [[[['2', False]], False],\n",
       "              [[['ku-no', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             648: [[[['no', False]], False],\n",
       "              [[['DA', True], ['NUM', True]], True]],\n",
       "             649: [[[['ri', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             650: [[[['pa-na-so', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             651: [[[['ra', False]], False],\n",
       "              [[['DA', True], ['1', True], ['ru-po', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             652: [[[['so', False]], False],\n",
       "              [[['DA', True], ['1', True]], True]],\n",
       "             653: [[[['pu-te', False]], False]],\n",
       "             654: [[[['DA', False]], False], [[['1', True]], True]],\n",
       "             655: [[[['DA', False]], False],\n",
       "              [[['DA', True], ['NUM', False]], False],\n",
       "              [[['DA', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             656: [[[['DA', False]], False],\n",
       "              [[['NUM', True], ['PA', True], ['2', True]], True]],\n",
       "             657: [[[['DA', False]], False],\n",
       "              [[['1', True], ['PA', False]], False]],\n",
       "             658: [[[['a-ta-na-po-ti-ni-ja', True], ['1', True], ['u', False]],\n",
       "               False],\n",
       "              [[['e-nu-wa-ri-jo', True],\n",
       "                ['1', True],\n",
       "                ['pa-ja-wo-ne', True],\n",
       "                ['1', True],\n",
       "                ['po-se-da-o-ne', True],\n",
       "                ['e-ri-nu-we', True],\n",
       "                ['pe-ro', True]],\n",
       "               True]],\n",
       "             659: [[[['ko-no-si-jo', True],\n",
       "                ['e-qe-a-o', True],\n",
       "                ['a-to-mo', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             660: [[[['e-u-da-mo', True], ['e-we-wa-ta', True]], True]],\n",
       "             661: [[[['wo-di-jo', True], ['1', True], ['qo-ta', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['a-ni-o-ko', True],\n",
       "                ['i-wa-ka', True],\n",
       "                ['1', True],\n",
       "                ['ka-pa-ri-jo', False]],\n",
       "               False],\n",
       "              [[['ti-jo', False]], False],\n",
       "              [[['1', True], ['ki-si-wi-jo', False]], False],\n",
       "              [[['me-nu-wa', True],\n",
       "                ['1', True],\n",
       "                ['wi-da-jo', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['1', False]], False],\n",
       "              [[['di-we-so', True],\n",
       "                ['1', True],\n",
       "                ['o-ku-na-wo', True],\n",
       "                ['1', True],\n",
       "                ['i-to-ma', False]],\n",
       "               False],\n",
       "              [[['ta', False]], False],\n",
       "              [[['1', True],\n",
       "                ['a-ke-u', True],\n",
       "                ['1', True],\n",
       "                ['pe-ri-ta', True],\n",
       "                ['1', True],\n",
       "                ['a-ka', False]],\n",
       "               False]],\n",
       "             662: [[[['te-wa', False]], False],\n",
       "              [[['1', True], ['ka-pa-ri-jo', True], ['1', True]], True]],\n",
       "             663: [[[['a-mi-ni-so', True],\n",
       "                ['pa-ze', True],\n",
       "                ['pe-da', True],\n",
       "                ['wa-tu', True]],\n",
       "               True],\n",
       "              [[['pa-ze', True],\n",
       "                ['a-mi-ni-so', True],\n",
       "                ['pe-da', True],\n",
       "                ['wa-tu', True]],\n",
       "               True]],\n",
       "             664: [[[['ne-jo', False]], False],\n",
       "              [[['te', False]], False],\n",
       "              [[['NUM', True], ['u-o', False]], False],\n",
       "              [[['wo-ne', False]], False],\n",
       "              [[['a-pe-o', False]], False]],\n",
       "             665: [[[['po-ru-da-si-jo', True],\n",
       "                ['ha-ke-te-re', True],\n",
       "                ['2', False]],\n",
       "               False]],\n",
       "             666: [[[['ta-mo', False]], False],\n",
       "              [[['u-wo-qe-ne', True],\n",
       "                ['u-du-ru-wo', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['NUM', True],\n",
       "                ['we-re-we', True],\n",
       "                ['ku-pa-sa', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['NUM', True],\n",
       "                ['we-re-we', True],\n",
       "                ['ka-ta-ra-pi', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['NUM', True],\n",
       "                ['a-ke-to-ro', True],\n",
       "                ['to-ni', True],\n",
       "                ['2', True],\n",
       "                ['o', True],\n",
       "                ['NUM', True],\n",
       "                ['NUM', True],\n",
       "                ['o', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             667: [[[['we-we-ro', True],\n",
       "                ['1', True],\n",
       "                ['e-u-ru-qo-ta', True],\n",
       "                ['1', True],\n",
       "                ['na-e-si-jo', True],\n",
       "                ['1', True],\n",
       "                ['da-te-wa', True],\n",
       "                ['1', True],\n",
       "                ['te-ra-pe-te', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             668: [[[['to-ko-so-ta', True], ['a-te-u-ke', True], ['1', True]],\n",
       "               True]],\n",
       "             669: [[[['NUM', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['ru-ki-to', True], ['NUM', True], ['NUM', False]], False],\n",
       "              [[['ti-ri-to', True], ['NUM', True], ['so', False]], False],\n",
       "              [[['NUM', True],\n",
       "                ['phu-te-re', True],\n",
       "                ['wa-si-ro', True],\n",
       "                ['NUM', True],\n",
       "                ['NUM', True],\n",
       "                ['ra-pte-re', True],\n",
       "                ['NUM', False]],\n",
       "               False]],\n",
       "             670: [[[['su-pa-ta', False]], False],\n",
       "              [[['1', True]], True],\n",
       "              [[['no', False]], False]],\n",
       "             671: [[[['wo-de-wi-jo', True],\n",
       "                ['to-pe-za', True],\n",
       "                ['o-u-ki-te-mi', True],\n",
       "                ['a-pe-ti-rya', True],\n",
       "                ['o-u-te-mi', True],\n",
       "                ['o-u-te-mi', True],\n",
       "                ['o-u-te-mi', True],\n",
       "                ['o-u-te-mi', True],\n",
       "                ['e-pi', True],\n",
       "                ['i-ku-wo-i-pi', True]],\n",
       "               True]],\n",
       "             672: [[[['a-pi-re-we', True], ['NUM', False]], False],\n",
       "              [[['da-mi-ni-jo', False]], False],\n",
       "              [[['po-mi-jo', False]], False],\n",
       "              [[['to-so', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['jo', False]], False],\n",
       "              [[['mo-ni-ko', True],\n",
       "                ['NUM', True],\n",
       "                ['NUM', True],\n",
       "                ['wi-to', False]],\n",
       "               False],\n",
       "              [[['NUM', True], ['o', False]], False]],\n",
       "             673: [[[['qa-ra-jo', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['ku-ka-so', True], ['1', False]], False],\n",
       "              [[['ja-pa-ra-ro', False]], False]],\n",
       "             674: [[[['e-ra-jo', False]], False],\n",
       "              [[['pu-da-so', False]], False]],\n",
       "             675: [[[['1', False]], False],\n",
       "              [[['qa-ra-i-so', True],\n",
       "                ['1', True],\n",
       "                ['si-ra-no', True],\n",
       "                ['1', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['i-ra-ko-to', True],\n",
       "                ['1', True],\n",
       "                ['wa-na-ta-jo', True],\n",
       "                ['1', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             676: [[[['du-phu-ra-zo', True],\n",
       "                ['1', True],\n",
       "                ['qe-ro', True],\n",
       "                ['1', True],\n",
       "                ['su-ko', True],\n",
       "                ['1', True],\n",
       "                ['di-zo', True],\n",
       "                ['1', True],\n",
       "                ['pu-ri', True],\n",
       "                ['1', True],\n",
       "                ['wi-da-ma-ro', True],\n",
       "                ['1', True],\n",
       "                ['o-ro-qa', True],\n",
       "                ['1', True],\n",
       "                ['wa-je', True],\n",
       "                ['1', True],\n",
       "                ['da-i-ra', True],\n",
       "                ['1', True]],\n",
       "               False],\n",
       "              [[['1', True]], True],\n",
       "              [[['pe-ri-ro-qo', True],\n",
       "                ['da-*83-jo', True],\n",
       "                ['1', True],\n",
       "                ['e-ri', False]],\n",
       "               False],\n",
       "              [[['1', False]], False],\n",
       "              [[['da', False]], False],\n",
       "              [[['o-du', True], ['1', True], ['te-ja-ro', True], ['1', True]],\n",
       "               True]],\n",
       "             677: [[[['e-u-ko-ro', True],\n",
       "                ['1', True],\n",
       "                ['qe-ra-jo', True],\n",
       "                ['1', True],\n",
       "                ['a-mi-nwa', False]],\n",
       "               False],\n",
       "              [[['nu-to', True],\n",
       "                ['1', True],\n",
       "                ['mi-ru-ro', True],\n",
       "                ['1', True],\n",
       "                ['me-to', False]],\n",
       "               False],\n",
       "              [[['ko-wo', True], ['2', True], ['wo', False]], False]],\n",
       "             678: [[[['na-ko-to', False]], False],\n",
       "              [[['1', True], ['de-so', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             679: [[[['dwo-jo', False]], False],\n",
       "              [[['1', True], ['pa-ki', False]], False],\n",
       "              [[['na-po', False]], False],\n",
       "              [[['1', True], ['ru-ki', False]], False]],\n",
       "             680: [[[['a-nu-mo', True],\n",
       "                ['1', True],\n",
       "                ['ta-za-ro', True],\n",
       "                ['1', True],\n",
       "                ['wa', False]],\n",
       "               False],\n",
       "              [[['*47-ti-jo', True],\n",
       "                ['1', True],\n",
       "                ['ja-ma-ra', True],\n",
       "                ['1', True],\n",
       "                ['pa-ja', False]],\n",
       "               False],\n",
       "              [[['po-mi-ni-jo', True],\n",
       "                ['1', True],\n",
       "                ['wa-du-na', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             681: [[[['no-mo', False]], False],\n",
       "              [[['1', True], ['tu-ti', True], ['1', False]], False],\n",
       "              [[['ki-ti-jo', False]], False],\n",
       "              [[['1', True], ['ja-pa-ra-ro', True], ['1', True]], True]],\n",
       "             682: [[[['do-ti-jo', True], ['a', False]], False],\n",
       "              [[['*49-sa-ro', True], ['1', True], ['po-ti', False]], False],\n",
       "              [[['ra-ku', True], ['1', True], ['ku-ka-ro', False]], False],\n",
       "              [[['ra-te-me', True], ['1', True], ['*56', False]], False]],\n",
       "             683: [[[['e-re-dwo-e', False]], False],\n",
       "              [[['i', False]], False],\n",
       "              [[['sa-ma-ru', False]], False],\n",
       "              [[['pe-ri-to-wo', True],\n",
       "                ['da-wo', True],\n",
       "                ['1', True],\n",
       "                ['ne-o', False]],\n",
       "               False],\n",
       "              [[['1', True], ['ja-ma-ta-ro', False]], False],\n",
       "              [[['ta-de-so', True],\n",
       "                ['ja-po', True],\n",
       "                ['1', True],\n",
       "                ['po', False]],\n",
       "               False],\n",
       "              [[['1', True], ['pe-to-me', False]], False],\n",
       "              [[['ko-to', False]], False],\n",
       "              [[['pa-ro', True], ['a', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['to-so', False]], False]],\n",
       "             684: [[[['e-re-pa-to', True],\n",
       "                ['ka-ra-ma-to', True],\n",
       "                ['NUM', True],\n",
       "                ['ka-so', True],\n",
       "                ['ke-ma-ta', True],\n",
       "                ['NUM', True]],\n",
       "               True]],\n",
       "             685: [[[['po-ti-ro', False]], False],\n",
       "              [[['wa-wi', True], ['1', True], ['a-mu-ta-wo-qe', False]],\n",
       "               False],\n",
       "              [[['da-*22-ti-ja', True]], True]],\n",
       "             686: [[[['ra-to', False]], False],\n",
       "              [[['1', True],\n",
       "                ['e-ke-a', True],\n",
       "                ['1', True],\n",
       "                ['e-mi-ja-ta', True],\n",
       "                ['1', True],\n",
       "                ['ka-mu-ko-to', False]],\n",
       "               False],\n",
       "              [[['1', True], ['ke-ra-ja', True], ['1', True], ['pu', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['we-ka-di-jo', True],\n",
       "                ['1', True],\n",
       "                ['ma-ke-ra', True],\n",
       "                ['1', True],\n",
       "                ['de-ro', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['ma-ti-ko', True],\n",
       "                ['1', True],\n",
       "                ['a-ti-ka', True],\n",
       "                ['1', True],\n",
       "                ['we-u', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['qa-da-ro', True],\n",
       "                ['1', True],\n",
       "                ['ka-sa-no', True],\n",
       "                ['1', True],\n",
       "                ['wo', False]],\n",
       "               False],\n",
       "              [[['1', True],\n",
       "                ['to-ro-ki-no', True],\n",
       "                ['1', True],\n",
       "                ['to', False]],\n",
       "               False],\n",
       "              [[['no-re', False]], False],\n",
       "              [[['1', True],\n",
       "                ['wi-ri-ki-no', True],\n",
       "                ['1', True],\n",
       "                ['ko-no-si-jo', False]],\n",
       "               False],\n",
       "              [[['1', True], ['do-ti-jo-no', False]], False]],\n",
       "             687: [[[['we', False]], False],\n",
       "              [[['si-ra-pe-te-so', True],\n",
       "                ['1', True],\n",
       "                ['ka-pu-ro', True],\n",
       "                ['1', True],\n",
       "                ['ka-na-po-to', True],\n",
       "                ['1', True],\n",
       "                ['pi-ma', False]],\n",
       "               False],\n",
       "              [[['ru-ro', True], ['1', True], ['a', False]], False],\n",
       "              [[['ta-u-ro', True],\n",
       "                ['1', True],\n",
       "                ['u-ta-jo', True],\n",
       "                ['1', True],\n",
       "                ['ja-sa-ro', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             688: [[[['1', True],\n",
       "                ['ru-ki-to', True],\n",
       "                ['a-ke-re-mo-no', True],\n",
       "                ['1', True],\n",
       "                ['qa-ra', True],\n",
       "                ['ko-re-te', True],\n",
       "                ['*258', True],\n",
       "                ['1', True],\n",
       "                ['qa-ra', True],\n",
       "                ['a-ke-re-mo-no', True],\n",
       "                ['1', True],\n",
       "                ['qa-ra', True],\n",
       "                ['po-ro-ko-re-te', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             689: [[[['ni-ja', False]], False],\n",
       "              [[['wa', False]], False],\n",
       "              [[['ma-na-je-u', False]], False],\n",
       "              [[['te-ra', False]], False],\n",
       "              [[['e-da-e', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['te-te-u', True],\n",
       "                ['1', True],\n",
       "                ['do-ri-ka-o', True],\n",
       "                ['a-ka', False]],\n",
       "               False],\n",
       "              [[['ka-ke-u', True], ['ka-ta', False]], False],\n",
       "              [[['1', True], ['wi-pi-no-o', True], ['1', True], ['se', False]],\n",
       "               False],\n",
       "              [[['pu-ra', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             690: [[[['wo', False]], False],\n",
       "              [[['wo-ka-re', True], ['NUM', False]], False]],\n",
       "             691: [[[['po-ti-ro', True],\n",
       "                ['pi-ra-ki-jo', True],\n",
       "                ['1', True],\n",
       "                ['pe-ri-jo-ta-qe', True],\n",
       "                ['1', True],\n",
       "                ['da-*22-ti-ja', True]],\n",
       "               True]],\n",
       "             692: [[[['po-ti-ro', True], ['ka-di-ti-ja', False]], False]],\n",
       "             693: [[[['po-ti-ro', False]], False],\n",
       "              [[['pa-na-re-jo', True],\n",
       "                ['1', True],\n",
       "                ['ku-da-jo-qe', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             694: [[[['po-ti-ro', True],\n",
       "                ['e-wa-ko-ro', True],\n",
       "                ['1', True],\n",
       "                ['pi-ra-ka-wo-qe', True],\n",
       "                ['1', True],\n",
       "                ['ki-ra-di-ja', True]],\n",
       "               True]],\n",
       "             695: [[[['po-ti-ro', True],\n",
       "                ['da-i-wo-wo', True],\n",
       "                ['1', True],\n",
       "                ['to-no-qe', True],\n",
       "                ['1', True],\n",
       "                ['pa-si-ja', False]],\n",
       "               False]],\n",
       "             696: [[[['ta-ra-nu', True], ['a-nu', False]], False],\n",
       "              [[['ta-ra-nu', False]], False],\n",
       "              [[['a-nu', False]], False],\n",
       "              [[['ta-ra-nu', True], ['qe', False]], False],\n",
       "              [[['ta-ra-nu', True]], True]],\n",
       "             697: [[[['i-jo', True], ['wo', False]], False],\n",
       "              [[['a', False]], False],\n",
       "              [[['wa-du-na', True]], False],\n",
       "              [[['to', False]], False],\n",
       "              [[['e', False]], False],\n",
       "              [[['o-pi', True], ['sa-ka-ri-jo', False]], False],\n",
       "              [[['i-jo', True], ['i-jo', True], ['o-pi', False]], False],\n",
       "              [[['di-zo', True],\n",
       "                ['pi-ma-na-ro', True],\n",
       "                ['zo-wi-jo', True],\n",
       "                ['1', True],\n",
       "                ['a-tu-qo-te-ra-to', True],\n",
       "                ['1', False]],\n",
       "               False],\n",
       "              [[['i-jo', True],\n",
       "                ['o-pi', True],\n",
       "                ['ri-zo', True],\n",
       "                ['pi-ma-na-ro', True],\n",
       "                ['pi-ro-i-ta', True],\n",
       "                ['1', True],\n",
       "                ['o-pi', True],\n",
       "                ['pa-ka', True],\n",
       "                ['di-wa-jo', True],\n",
       "                ['1', True],\n",
       "                ['pi', False]],\n",
       "               False],\n",
       "              [[['o-na-se-u', True], ['1', True], ['ri', False]], False],\n",
       "              [[['wi-du', False]], False],\n",
       "              [[['1', True], ['ke', False]], False],\n",
       "              [[['za', False]], False]],\n",
       "             698: [[[['qe', False]], False],\n",
       "              [[['ma-no-ne', False]], False],\n",
       "              [[['1', True], ['po-da', True], ['1', True], ['se', False]],\n",
       "               False],\n",
       "              [[['po-da', True], ['1', True], ['do-ma', False]], False],\n",
       "              [[['ki-si-wo', False]], False],\n",
       "              [[['po-da', False]], False]],\n",
       "             700: [[[['po-ti-ro', False]], False],\n",
       "              [[['si-mi-te-u', True],\n",
       "                ['1', True],\n",
       "                ['a-ra-ko-qe', True],\n",
       "                ['1', True],\n",
       "                ['ti-ja', False]],\n",
       "               False]],\n",
       "             701: [[[['to-i-je', False]], False],\n",
       "              [[['ru-ko', False]], False],\n",
       "              [[['1', True], ['no-du', False]], False],\n",
       "              [[['1', True], ['da-na-jo', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             702: [[[['me', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['zo', False]], False],\n",
       "              [[['do-wa', True], ['NUM', False]], False]],\n",
       "             703: [[[['ki-si', False]], False],\n",
       "              [[['ka-na-po', False]], False],\n",
       "              [[['1', True], ['jo', False]], False],\n",
       "              [[['so', False]], False],\n",
       "              [[['1', True], ['te', False]], False]],\n",
       "             704: [[[['qa-na', False]], False], [[['ka-sa', False]], False]],\n",
       "             705: [[[['we-to-ro', False]], False],\n",
       "              [[['NUM', False]], False],\n",
       "              [[['NUM', False]], False]],\n",
       "             706: [[[['ja-ro', False]], False],\n",
       "              [[['2', False]], False],\n",
       "              [[['ro-we', False]], False],\n",
       "              [[['2', False]], False],\n",
       "              [[['se-me-ni', False]], False]],\n",
       "             707: [[[['mi-ru-ro', True], ['1', False]], False],\n",
       "              [[['o-da-ra-o', True], ['1', True], ['wa', False]], False],\n",
       "              [[['nu-to', True], ['1', True]], True]],\n",
       "             708: [[[['ri-jo', False]], False],\n",
       "              [[['1', True], ['qe-ro', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['ku-ke-mo', False]], False]],\n",
       "             709: [[[['ke-ti-ra-wo', False]], False],\n",
       "              [[['wo-do', True], ['1', True]], True]],\n",
       "             710: [[[['NUM', True],\n",
       "                ['u-su', True],\n",
       "                ['NUM', True],\n",
       "                ['a-mi-ni-si-jo', True],\n",
       "                ['ta-qa-ra-ti', True],\n",
       "                ['1', True],\n",
       "                ['ku-ma-to', False]],\n",
       "               False]],\n",
       "             711: [[[['ma-ro', False]], False],\n",
       "              [[['1', True], ['o-ru', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['qe-re-ma-o', True], ['1', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             712: [[[['1', True], ['a', False]], False],\n",
       "              [[['jo', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             713: [[[['me-ja', False]], False],\n",
       "              [[['1', True], ['ja', False]], False],\n",
       "              [[['1', True]], True]],\n",
       "             714: [[[['ro', False]], False],\n",
       "              [[['1', True], ['1', False]], False],\n",
       "              [[['ri-zo', True], ['1', True]], True]],\n",
       "             715: [[[['ko', False]], False], [[['NUM', True]], True]],\n",
       "             716: [[[['to', False]], False], [[['1', True]], True]],\n",
       "             717: [[[['po-ti-ro', True], ['di-pi-ja', True]], True]],\n",
       "             718: [[[['a-qi-ta', False]], False],\n",
       "              [[['ki-ma-to', False]], False],\n",
       "              [[['1', False]], False],\n",
       "              [[['e-u-po-ro-wo', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             719: [[[['a-pa-ta-wa-ja', True]], True]],\n",
       "             720: [[[['ra-tu', False]], False]],\n",
       "             721: [[[['te-we', False]], False], [[['1', True]], True]],\n",
       "             722: [[[['po-ti-ro', False]], False]],\n",
       "             723: [[[['ko', False]], False],\n",
       "              [[['2', True], ['2', False]], False]],\n",
       "             724: [[[['pe-re-ta', False]], False],\n",
       "              [[['1', True], ['ka', False]], False]],\n",
       "             725: [[[['je-u-qe', False]], False]],\n",
       "             726: [[[['ka-di-ti-ja', False]], False]],\n",
       "             727: [[[['ti', False]], False],\n",
       "              [[['1', True], ['a', False]], False]],\n",
       "             728: [[[['a-mu-ta-wo', False]], False]],\n",
       "             729: [[[['a-phu-ka', True], ['pa-ja', False]], False],\n",
       "              [[['za-mi-so', True],\n",
       "                ['ku-ta-to', True],\n",
       "                ['DA', True],\n",
       "                ['1', True]],\n",
       "               True]],\n",
       "             730: [[[['o-pe-re-ra', False]], False],\n",
       "              [[['me-na-qe', False]], False]],\n",
       "             731: [[[['pa-i-to', False]], False]],\n",
       "             732: [[[['wa', False]], False], [[['me-no', False]], False]],\n",
       "             733: [[[['e-ke-pi', False]], False], [[['a-ra', False]], False]],\n",
       "             734: [[[['da-wo', False]], False]],\n",
       "             735: [[[['ti', False]], False], [[['OVISm', False]], False]],\n",
       "             736: [[[['OVISf', False]], False], [[['LANA', False]], False]],\n",
       "             737: [[[['ku-ta-ti-jo', False]], False]],\n",
       "             739: [[[['da-*22-ti', False]], False]],\n",
       "             740: [[[['OVISm', True]], True]],\n",
       "             741: [[[['da-mi', False]], False]],\n",
       "             742: [[[['OVISm', True]], True]],\n",
       "             743: [[[['qa', False]], False]],\n",
       "             744: [[[['ja', False]], False]],\n",
       "             745: [[[['qa', False]], False]],\n",
       "             746: [[[['jo-jo', False]], False]],\n",
       "             747: [[[['ra-to', False]], False]],\n",
       "             748: [[[['si-we', False]], False],\n",
       "              [[['OVISm', True], ['ra-ze', False]], False]],\n",
       "             749: [[[['ka-so', False]], False], [[['OVISm', True]], True]],\n",
       "             750: [[[['ku-do-ni-ja', True], ['ta', False]], False]],\n",
       "             751: [[[['a-mo-ta', False]], False]],\n",
       "             752: [[[['ra', False]], False]],\n",
       "             753: [[[['ke', False]], False]],\n",
       "             754: [[[['OVISm', False]], False]],\n",
       "             756: [[[['o-a-pu', False]], False], [[['tu-na-no', True]], True]],\n",
       "             757: [[[['ki-ri-jo-te', False]], False],\n",
       "              [[['e-ko-so', False]], False]],\n",
       "             758: [[[['da-wo', False]], False]],\n",
       "             759: [[[['jo', False]], False]],\n",
       "             760: [[[['*56', False]], False]],\n",
       "             762: [[[['e', False]], False]],\n",
       "             763: [[[['jo', False]], False]],\n",
       "             764: [[[['ki', False]], False]],\n",
       "             765: [[[['ai-wo-re-u', False]], False],\n",
       "              [[['si', False]], False],\n",
       "              [[['do-ke', True]], False]],\n",
       "             766: [[[['me-sa-to', True], ['*146', False]], False],\n",
       "              [[['NUM', True]], True]],\n",
       "             767: [[[['me-sa-to', True], ['*146', False]], False]],\n",
       "             768: [[[['*146', True], ['NUM', True]], True]],\n",
       "             769: [[[['me-sa-to', True]], True]],\n",
       "             770: [[[['me-sa-to', False]], False]],\n",
       "             771: [[[['*146', False]], False]],\n",
       "             772: [[[['sa-ma-ja-so', True]], True]],\n",
       "             773: [[[['*146', False]], False]],\n",
       "             774: [[[['se-to-i-ja', True],\n",
       "                ['ki-ri-ta-de', True],\n",
       "                ['te', True],\n",
       "                ['LANA', True],\n",
       "                ['do-ke', True]],\n",
       "               True]],\n",
       "             775: [[[['pi-mo-no', True], ['na-ki-zo', True], ['pa-wo', True]],\n",
       "               True]],\n",
       "             776: [[[['VIR', True]], True]],\n",
       "             777: [[[['MUL', True]], True]],\n",
       "             779: [[[['P', True], ['ta-to-mo', True], ['o-nu-ke', True]],\n",
       "               True]],\n",
       "             782: [[[['VIR', True]], True]],\n",
       "             783: [[[['LANA', True], ['ne-ki', False]], False],\n",
       "              [[['ri-de', False]], False]],\n",
       "             784: [[[['JAC', True], ['o-pa', True], ['pa-ta-ja', True]],\n",
       "               True]],\n",
       "             785: [[[['TELAI+TE', True], ['te-pa', True]], True]],\n",
       "             787: [[[['JAC', True], ['pa-ta-ja', True], ['o-pa', True]],\n",
       "               True]],\n",
       "             788: [[[['*190', True]], True]],\n",
       "             790: [[[['o-pa', True], ['ko-we', True]], True]],\n",
       "             791: [[[['GRA', True]], True]],\n",
       "             792: [[[['e-po', True]], True]],\n",
       "             795: [[[['ku-wa-ta', True], ['o-pi', True], ['a-nu-wi-ko', True]],\n",
       "               True]],\n",
       "             796: [[[['ko-ro-sa-no', True], ['1', True]], True]],\n",
       "             797: [[[['po-ro-tu-qo-no', True]], True]],\n",
       "             798: [[[['i-mo-ro-ne-u', True], ['1', True]], True]],\n",
       "             800: [[[['mi-ka-ta', True], ['1', True]], True]],\n",
       "             802: [[[['pe-ka-wo', True]], True]],\n",
       "             803: [[[['mi-ka-to', True]], True]],\n",
       "             805: [[[['ka-pa-ri-jo', True]], True]],\n",
       "             806: [[[['wa-na-ka', True]], True]],\n",
       "             807: [[[['po-ru-ka-to', True]], True]],\n",
       "             808: [[[['a-ka-to-wa', True]], True]],\n",
       "             809: [[[['ka-no', False]], False]],\n",
       "             810: [[[['e-wi-ta-jo', True]], True]],\n",
       "             811: [[[['a-ki-re-u', True]], True]],\n",
       "             812: [[[['za-ki-ri-jo', True]], True]],\n",
       "             813: [[[['qe-wa', False]], False]],\n",
       "             814: [[[['da-o-ta', True], ['1', True]], True]],\n",
       "             815: [[[['o-ko-te', True]], True]],\n",
       "             816: [[[['e-u-o-mo', False]], False]],\n",
       "             817: [[[['e-pi-da-o', True], ['1', True]], True]],\n",
       "             819: [[[['ri-to-wo', False]], False], [[['1', True]], True]],\n",
       "             820: [[[['a-ka-i-jo', True], ['1', True]], True]],\n",
       "             821: [[[['wo-ra-ke-re', False]], False]],\n",
       "             822: [[[['a-no-qo-ta', True], ['ai', False]], False],\n",
       "              [[['je', False]], False]],\n",
       "             823: [[[['pe-ri-to', True]], True]],\n",
       "             824: [[[['a-pi-da-ta', True], ['1', True]], True]],\n",
       "             825: [[[['po-ru-te-we', True], ['1', True]], True]],\n",
       "             826: [[[['wa-ke-i-jo', False]], False], [[['di', False]], False]],\n",
       "             827: [[[['sa-u-ko', True]], True]],\n",
       "             828: [[[['pi-ra-qo', True]], True]],\n",
       "             829: [[[['pe-re-wa-ta', True]], True]],\n",
       "             830: [[[['a-ro-wo-ta', True], ['1', True]], True]],\n",
       "             831: [[[['a-ne-te-wa', True], ['1', True]], True]],\n",
       "             832: [[[['te-re-ja-wo', True], ['1', True]], True]],\n",
       "             833: [[[['wa-ke-i-jo', True]], True]],\n",
       "             834: [[[['pe-ri-to-wo', True]], True]],\n",
       "             835: [[[['a-pi-re-jo', True], ['1', True]], True]],\n",
       "             836: [[[['pu-mo-ne', False]], False]],\n",
       "             837: [[[['ta-ra-sa-ta', True], ['1', True]], True],\n",
       "              [[['du', False]], False]],\n",
       "             839: [[[['a-ke-ra-no', True], ['1', True]], True]],\n",
       "             840: [[[['pa-pa-ro', True]], True]],\n",
       "             841: [[[['a-re-jo', False]], False], [[['1', True]], True]],\n",
       "             842: [[[['pa-sa-ko-me-no', True]], True]],\n",
       "             843: [[[['e-re-pa-i-ro', True]], True]],\n",
       "             844: [[[['a-ne-u-da', True], ['1', True]], True]],\n",
       "             845: [[[['1', True], ['di-wo-a-ne', False]], False]],\n",
       "             846: [[[['re-ka-se-ra-wo', False]], False]],\n",
       "             847: [[[['jo', True], ['du-pi-jo', True]], True]],\n",
       "             848: [[[['a-mi-ni-si-jo', True]], True]],\n",
       "             849: [[[['wo-ro-to-qo', True], ['1', False]], False]],\n",
       "             850: [[[['me-ta-ri-ko-wo', True]], True]],\n",
       "             851: [[[['di-wi-ja-wo', True], ['1', True]], True]],\n",
       "             853: [[[['ti-ri-jo-qa', False]], False],\n",
       "              [[['wi', False]], False]],\n",
       "             854: [[[['e-we-de-u', True]], True]],\n",
       "             855: [[[['a-ke-ra-wo', True], ['1', False]], False]],\n",
       "             857: [[[['a-ta-no', True], ['1', True]], True]],\n",
       "             858: [[[['ku-ru-ka', True], ['1', True]], True]],\n",
       "             860: [[[['da-*83-jo', True], ['1', True]], True]],\n",
       "             861: [[[['ka-na-a-po', True], ['1', True]], True]],\n",
       "             862: [[[['ne-to', False]], False], [[['1', True]], True]],\n",
       "             863: [[[['wa-si-jo', False]], False], [[['1', True]], True]],\n",
       "             864: [[[['di-so', False]], False], [[['1', True]], True]],\n",
       "             865: [[[['ra-wo', False]], False], [[['1', True]], True]],\n",
       "             867: [[[['ma-ra', False]], False], [[['1', True]], True]],\n",
       "             868: [[[['ka-sa-to', True], ['1', True]], True]],\n",
       "             870: [[[['ai-wa-ta', True]], True]],\n",
       "             871: [[[['a-re-i', False]], False]],\n",
       "             872: [[[['pa-ra-ne', True], ['1', True]], True]],\n",
       "             873: [[[['a-pi', False]], False]],\n",
       "             874: [[[['ra-ko', False]], False], [[['1', True]], True]],\n",
       "             875: [[[['te-ta', False]], False]],\n",
       "             876: [[[['nwa-re', False]], False]],\n",
       "             877: [[[['ne-u', False]], False]],\n",
       "             878: [[[['we-wa-ta', False]], False]],\n",
       "             879: [[[['re-wa', False]], False]],\n",
       "             880: [[[['i-ta', False]], False]],\n",
       "             881: [[[['no', False]], False], [[['1', True]], True]],\n",
       "             882: [[[['ro', False]], False]],\n",
       "             883: [[[['i', False]], False]],\n",
       "             884: [[[['qe-na', False]], False]],\n",
       "             885: [[[['ka-di-ja', False]], False]],\n",
       "             886: [[[['qe-re', False]], False]],\n",
       "             887: [[[['we-ro', False]], False], [[['a-ka-i-jo', True]], True]],\n",
       "             888: [[[['wa-na-ka', False]], False]],\n",
       "             889: [[[['me-no', False]], False], [[['da-te', False]], False]],\n",
       "             890: [[[['o', False]], False],\n",
       "              [[['ko-to', True], ['pe-qe-u', True], ['o-ko-te', True]], True]],\n",
       "             891: [[[['ke-phu-je-u', True], ['1', True]], True]],\n",
       "             892: [[[['a-ta-ti-nu', True], ['si-wa', False]], False]],\n",
       "             894: [[[['ti', False]], False], [[['1', True]], True]],\n",
       "             895: [[[['ka-ra-na-ta', True], ['1', True]], True]],\n",
       "             896: [[[['ra-wo-ti-jo', True], ['1', False]], False]],\n",
       "             897: [[[['ma-ti-ko', False]], False], [[['1', True]], True]],\n",
       "             898: [[[['ti-ma', True], ['1', True]], True],\n",
       "              [[['a', False]], False]],\n",
       "             899: [[[['qe-ra-jo', True]], True]],\n",
       "             900: [[[['to', False]], False], [[['1', True]], True]],\n",
       "             901: [[[['pa-ta', False]], False], [[['1', True]], True]],\n",
       "             902: [[[['OVISm', True]], True]],\n",
       "             903: [[[['*185', True], ['ma-se', True]], False]],\n",
       "             904: [[[['pa-ta-ja', True]], True]],\n",
       "             905: [[[['*257', True]], True]],\n",
       "             906: [[[['AES', True],\n",
       "                ['*246', True],\n",
       "                ['ke-ni-qa', True],\n",
       "                ['a-sa-mi-to', True]],\n",
       "               True]],\n",
       "             908: [[[['e-sa-re-wi-ja', False]], False],\n",
       "              [[['wa', True], ['*260', True], ['1', False]], False]],\n",
       "             909: [[[['zo-wa', True], ['tu-ri-jo', True]], True]],\n",
       "             911: [[[['*22-ja-ro', True]], True]],\n",
       "             912: [[[['e-ri-ko', False]], False],\n",
       "              [[['qa-ka-na-pi', False]], False]],\n",
       "             913: [[[['si-ja-phu', False]], False]],\n",
       "             914: [[[['pa-pu', False]], False]],\n",
       "             915: [[[['ko', False]], False], [[['pa-ra-ku', False]], False]],\n",
       "             916: [[[['pa-i-to', False]], False], [[['e', False]], False]],\n",
       "             918: [[[['o-pi', False]], False],\n",
       "              [[['wa-ta', False]], False],\n",
       "              [[['pi', False]], False]],\n",
       "             919: [[[['o-pe-re', False]], False]],\n",
       "             920: [[[['ra-i-ja', False]], False]],\n",
       "             921: [[[['na-pu', False]], False]],\n",
       "             922: [[[['su-ra', False]], False],\n",
       "              [[['ka-ta-ro', True], ['ku-ri-sa-to', True], ['ra-wi', False]],\n",
       "               False],\n",
       "              [[['ro', False]], False]],\n",
       "             923: [[[['e-re-pa-te-o', False]], False]],\n",
       "             924: [[[['qe', False]], False]],\n",
       "             925: [[[['NUM', False]], False]],\n",
       "             926: [[[['ka', False]], False]],\n",
       "             927: [[[['ki', False]], False]],\n",
       "             928: [[[['i', False]], False]],\n",
       "             929: [[[['1', False]], False]],\n",
       "             930: [[[['*22-je-mi', False]], False]],\n",
       "             932: [[[['1', False]], False]],\n",
       "             934: [[[['NUM', False]], False]],\n",
       "             935: [[[['ra-to', True], ['pa-ta-re', False]], False]],\n",
       "             936: [[[['o-ka', False]], False]],\n",
       "             937: [[[['di-ka', False]], False]],\n",
       "             938: [[[['ku-ka-ra-re', False]], False]],\n",
       "             939: [[[['pe-ka-wo', False]], False],\n",
       "              [[['ku-ta-to', False]], False]],\n",
       "             940: [[[['a-ka-me-ne', False]], False]],\n",
       "             941: [[[['a-ka-to', False]], False]],\n",
       "             942: [[[['ka-wi', False]], False]],\n",
       "             943: [[[['ru-po-to', False]], False]],\n",
       "             944: [[[['ki-je-u', True], ['a-pi-ja-re', True]], True],\n",
       "              [[['tu', True]], True]],\n",
       "             945: [[[['di-wi-je-ja', True], ['di-wi-ja', True]], True]],\n",
       "             946: [[[['ki-si-wi-je-ja', True], ['pe-rya-wo', False]], False]],\n",
       "             947: [[[['wi-jo', False]], False]],\n",
       "             948: [[[['*47-da', False]], False], [[['mi-ko', False]], False]],\n",
       "             949: [[[['no', False]], False],\n",
       "              [[['i-ja', True]], False],\n",
       "              [[['di', True], ['i-*65-ke-o', True]], True]],\n",
       "             950: [[[['o', False]], False],\n",
       "              [[['ma-ki', True], ['pa', False]], False]],\n",
       "             951: [[[['a-ku-di-ri-jo', True]], True]],\n",
       "             952: [[[['a-ku-di-ri', False]], False]],\n",
       "             953: [[[['o-ke-te', False]], False],\n",
       "              [[['a-e', False]], False],\n",
       "              [[['e-ro-e-o', True]], True]],\n",
       "             954: [[[['ne-ri-jo', False]], False], [[['e', False]], False]],\n",
       "             955: [[[['ja', False]], False], [[['qe-re-wa', True]], True]],\n",
       "             956: [[[['da-*22-to', True], ['wo-no-da', False]], False]],\n",
       "             957: [[[['da-pu-ri-to', False]], False],\n",
       "              [[['pa-ze-qe', True], ['ke-wo', False]], False],\n",
       "              [[['*47-ta-qo', False]], False],\n",
       "              [[['*47', False]], False]],\n",
       "             958: [[[['pu-ra-ko', False]], False]],\n",
       "             959: [[[['pu-ko-ro', False]], False]],\n",
       "             960: [[[['i-ke-se', True]], True]],\n",
       "             961: [[[['ku-ta-i-to', True], ['a-pi-do-ro', False]], False],\n",
       "              [[['ma-u-do', False]], False],\n",
       "              [[['i-ja-wo-ne', False]], False]],\n",
       "             962: [[[['ro-a', True], ['ku', False]], False],\n",
       "              [[['OLE', False]], False],\n",
       "              [[['NUM', True], ['S', True], ['2', True], ['qa-mo', False]],\n",
       "               False],\n",
       "              [[['OLE', False]], False],\n",
       "              [[['NUM', True]], True]],\n",
       "             963: [[[['e-ra', True],\n",
       "                ['tu-ni-ja', True],\n",
       "                ['ri-u-no', True],\n",
       "                ['to', False]],\n",
       "               False],\n",
       "              [[['o', False]], False],\n",
       "              [[['NUM', True]], True]],\n",
       "             964: [[[['ra-wa-ke-si', False]], False],\n",
       "              [[['ma-ke-ra-mo', False]], False],\n",
       "              [[['qa-ra-su-ti-jo', False]], False],\n",
       "              [[['i-da-ra-ta', True]], False],\n",
       "              [[['1', False]], False]],\n",
       "             965: [[[['a-qi-ra', True],\n",
       "                ['e-u-ru-da-mo', True],\n",
       "                ['so-tu-wo-no', False]],\n",
       "               False]],\n",
       "             966: [[[['do-ri-wo', True]], True]],\n",
       "             967: [[[['ko-no-si-jo', True], ['ru-ki-ti-jo', True]], True]],\n",
       "             968: [[[['ku-do-ni-jo', False]], False]],\n",
       "             969: [[[['a-re-u-ke', False]], False]],\n",
       "             970: [[[['ko-ma-ra', False]], False]],\n",
       "             971: [[[['a-re-se', False]], False]],\n",
       "             972: [[[['wo-no-qi', False]], False]],\n",
       "             973: [[[['ja', False]], False], [[['u-ta-no', True]], True]],\n",
       "             974: [[[['du-ni-jo', False]], False]],\n",
       "             975: [[[['pa-pa-ro', True], ['ra', False]], False]],\n",
       "             976: [[[['ma-ti-ri', False]], False]],\n",
       "             977: [[[['ja', False]], False],\n",
       "              [[['na-u-si-ke-re', False]], False]],\n",
       "             978: [[[['ke-re-wa', True], ['wo-do', True]], True]],\n",
       "             979: [[[['te-u-to-ri-*65', False]], False]],\n",
       "             980: [[[['ti-ri-jo-qa', False]], False]],\n",
       "             981: [[[['qe-re-wa', False]], False]],\n",
       "             982: [[[['e-ru-ti-ri-jo', False]], False]],\n",
       "             983: [[[['ta-ra-i', True], ['BIG', False]], False]],\n",
       "             984: [[[['e-ko-so-wo-ko', False]], False]],\n",
       "             985: [[[['a-qi-ra', False]], False]],\n",
       "             986: [[[['ma-se', False]], False]],\n",
       "             987: [[[['e-na-i-jo', False]], False]],\n",
       "             988: [[[['e-ri-ta-ri-jo', False]], False]],\n",
       "             989: [[[['ra-ke-re-we', False]], False]],\n",
       "             990: [[[['ta', False]], False], [[['a-ke', False]], False]],\n",
       "             991: [[[['do-qi', False]], False]],\n",
       "             992: [[[['po-ki', False]], False]],\n",
       "             993: [[[['pa-no', False]], False], [[['qa', False]], False]],\n",
       "             994: [[[['se-ke-ru-pa-ko', False]], False]],\n",
       "             995: [[[['e-we-wa', False]], False], [[['a', False]], False]],\n",
       "             996: [[[['ru-ki-ti-ja', False]], False]],\n",
       "             997: [[[['e-pi-ta', False]], False]],\n",
       "             998: [[[['te-me-u', False]], False]],\n",
       "             999: [[[['a-pa-te', False]], False]],\n",
       "             1000: [[[['wo-ni-jo', False]], False]],\n",
       "             1001: [[[['a-ri-we-we', False]], False]],\n",
       "             1002: [[[['a-ta-wo-ne', False]], False]],\n",
       "             1003: [[[['we-wa', False]], False]],\n",
       "             1004: [[[['no-da-ma', False]], False]],\n",
       "             1005: [[[['e-ri-ra-i', False]], False]],\n",
       "             1006: [[[['pa-*34-so', False]], False]],\n",
       "             1007: [[[['re-mi-si-jo', False]], False]],\n",
       "             1008: [[[['ra-wo-te', False]], False]],\n",
       "             1009: [[[['a-phu-ka', False]], False]],\n",
       "             1010: [[[['ja', False]], False], [[['i-da-wo', False]], False]],\n",
       "             1011: [[[['ze-pu', False]], False]],\n",
       "             1012: [[[['mo', False]], False], [[['ke-to-ri', False]], False]],\n",
       "             1013: [[[['ke-si-jo', False]], False], [[['ri', False]], False]],\n",
       "             1014: [[[['pe-ra-te-ro', False]], False]],\n",
       "             1015: [[[['a-ka-mu', False]], False]],\n",
       "             1016: [[[['di', False]], False]],\n",
       "             1017: [[[['je-wa', False]], False]],\n",
       "             1018: [[[['OLE', False]], False], [[['NUM', True]], True]],\n",
       "             1019: [[[['qe', False]], False]],\n",
       "             1020: [[[['o-wo-ta', False]], False],\n",
       "              [[['ki-ni', False]], False],\n",
       "              [[['o-ta-ke', False]], False]],\n",
       "             1021: [[[['i-wi-ro', True]], True]],\n",
       "             1022: [[[['o-ko-te', False]], False]],\n",
       "             1023: [[[['a-na-pe-we', False]], False]],\n",
       "             1024: [[[['mo-ro-qa', False]], False]],\n",
       "             1025: [[[['pi-ri-ja', False]], False]],\n",
       "             1026: [[[['ti-ri-to', False]], False]],\n",
       "             1027: [[[['ru-ko-si', False]], False]],\n",
       "             1028: [[[['e-na-i', False]], False]],\n",
       "             1029: [[[['ka-ta', False]], False]],\n",
       "             1030: [[[['ku-ru', False]], False]],\n",
       "             1031: [[[['qa', False]], False]],\n",
       "             1032: [[[['e', False]], False]],\n",
       "             1033: [[[['pi-ri', False]], False]],\n",
       "             1034: [[[['ra-wi', False]], False]],\n",
       "             1035: [[[['a-ko-to', True]], True]],\n",
       "             1036: [[[['ru-wa', False]], False]],\n",
       "             1037: [[[['ka-ro-qo', True]], True]],\n",
       "             1038: [[[['e-ko', False]], False]],\n",
       "             1039: [[[['a-re', False]], False], [[['a', False]], False]],\n",
       "             1041: [[[['di-ki', False]], False]],\n",
       "             1042: [[[['e-u', False]], False]],\n",
       "             1043: [[[['ke-ku-ro', False]], False]],\n",
       "             1044: [[[['i-to-ma', False]], False]],\n",
       "             1045: [[[['ma-ra', True]], True]],\n",
       "             1046: [[[['re-wo', True]], True]],\n",
       "             1047: [[[['wo-tu', False]], False]],\n",
       "             1048: [[[['ta', False]], False]],\n",
       "             1050: [[[['po-ta', False]], False]],\n",
       "             1051: [[[['a-ti', False]], False]],\n",
       "             1052: [[[['i-se-re', False]], False]],\n",
       "             1053: [[[['a-wa', False]], False]],\n",
       "             1054: [[[['ki-ri', False]], False]],\n",
       "             1055: [[[['a', False]], False]],\n",
       "             1056: [[[['me-nu-wa', False]], False]],\n",
       "             1057: [[[['me', False]], False],\n",
       "              [[['pa-ro', False]], False],\n",
       "              [[['se', False]], False]],\n",
       "             1058: [[[['to-so-pa', False]], False]],\n",
       "             1059: [[[['ri', False]], False], [[['ja', False]], False]],\n",
       "             1060: [[[['i-ta', False]], False],\n",
       "              [[['EQU', False]], False],\n",
       "              [[['ri-so-wa', False]], False]],\n",
       "             1061: [[[['o-pi', False]], False], [[['a-ne', False]], False]],\n",
       "             1062: [[[['wa', False]], False], [[['na-ni', False]], False]],\n",
       "             1063: [[[['wi-pe-wa', False]], False], [[['1', True]], True]],\n",
       "             1064: [[[['du-wo-pe', False]], False]],\n",
       "             1065: [[[['e-ri-ko-wo', False]], False]],\n",
       "             1066: [[[['ki-se', False]], False]],\n",
       "             1067: [[[['NUM', False]], False],\n",
       "              [[['qe', False]], False],\n",
       "              [[['wa', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             1068: [[[['BIG', False]], False],\n",
       "              [[['EQU', False]], False],\n",
       "              [[['nu-a', False]], False]],\n",
       "             1069: [[[['*22-jo', False]], False]],\n",
       "             1070: [[[['i-jo', False]], False]],\n",
       "             1071: [[[['me-re-ja', False]], False]],\n",
       "             1072: [[[['ko', False]], False],\n",
       "              [[['ke-ti-ro', True]], True],\n",
       "              [[['qi', True]], True]],\n",
       "             1073: [[[['qi-ja', False]], False]],\n",
       "             1074: [[[['u-te', False]], False],\n",
       "              [[['EQU', False]], False],\n",
       "              [[['1', False]], False]],\n",
       "             1075: [[[['ka-ra-na-ta', False]], False]],\n",
       "             1076: [[[['ke-ke-me', False]], False]],\n",
       "             ...})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALEXNEWCODE\n",
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "    reconstruct_nums = defaultdict(list)\n",
    "    \n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "                reconstruct_nums[document_name].append(w)\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents, reconstruct_nums\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\" and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    indexes = [defaultdict(list) for seq in sequences]\n",
    "    for j, seq in enumerate(sequences):\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "                indexes[j][int(pos)].append(int(pos2))\n",
    "\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res, indexes\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis, idxs):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "            idxs.pop(j)\n",
    "    return seq, mis, idxs\n",
    "\n",
    "\n",
    "def split_dataset(base, *others, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a base list and any number of other aligned lists into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        base (list): The primary list to shuffle and split.\n",
    "        *others (list): Any number of other lists aligned with the base list.\n",
    "        train_ratio (float): Proportion of data to use for training (default 0.9).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_base, test_base, train_others..., test_others...)\n",
    "    \"\"\"\n",
    "    length = len(base)\n",
    "    assert all(len(lst) == length for lst in others), \"All input lists must have the same length\"\n",
    "\n",
    "    indices = np.random.permutation(length)\n",
    "\n",
    "    # Shuffle the base and others\n",
    "    base_shuffled = [base[i] for i in indices]\n",
    "    others_shuffled = [[lst[i] for i in indices] for lst in others]\n",
    "\n",
    "    train_size = int(train_ratio * length)\n",
    "\n",
    "    # Split the base list\n",
    "    train_base = base_shuffled[:train_size]\n",
    "    test_base = base_shuffled[train_size:]\n",
    "\n",
    "    # Split all other lists\n",
    "    train_others = [lst[:train_size] for lst in others_shuffled]\n",
    "    test_others = [lst[train_size:] for lst in others_shuffled]\n",
    "\n",
    "    return (train_base, test_base, *train_others, *test_others)\n",
    "\n",
    "\n",
    "'''\n",
    "def create_documents_dataset(documents):\n",
    "    processed_docs = defaultdict(list)\n",
    "    for doc_id, doc in documents.items():\n",
    "        fully_complete = True\n",
    "        curr_seq = []\n",
    "        for seq, complete in doc:\n",
    "            fully_complete = fully_complete and complete and (not \"?\" in seq)\n",
    "            if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            if (not complete or \"?\" in seq or seq == \"separatum\" or seq == \"qs\") and len(curr_seq) > 0:\n",
    "                processed_docs[doc_id].append([\" \".join(curr_seq), fully_complete])\n",
    "                curr_seq = []\n",
    "                fully_complete = True\n",
    "        if len(curr_seq) > 0:\n",
    "            processed_docs[doc_id].append([\" \".join(curr_seq), fully_complete])\n",
    "    return processed_docs\n",
    "'''\n",
    "\n",
    "def create_documents_dataset(documents):\n",
    "    processed_docs = defaultdict(list)\n",
    "    seq_to_doc = defaultdict(list)\n",
    "    for doc_id, doc in documents.items():\n",
    "        fully_complete = True\n",
    "        curr_seq = []\n",
    "        for seq, complete in doc:\n",
    "            is_break = (not complete) or (\"?\" in seq) or (seq in {\"separatum\", \"qs\", \"mut\"})\n",
    "\n",
    "            # Contribute to the current chunk only if it's a normal token   \n",
    "            if seq not in {\"separatum\", \"qs\", \"mut\"} and (\"?\" not in seq):\n",
    "                curr_seq.append([seq, complete])\n",
    "                seq_to_doc[seq].append([doc_id, complete])\n",
    "            \n",
    "            # Update completeness (note: separatum/qs don't affect completeness)\n",
    "            if not complete or (\"?\" in seq):\n",
    "                fully_complete = False\n",
    "                \n",
    "            # Break and flush when needed\n",
    "            if is_break and len(curr_seq) > 0:\n",
    "                processed_docs[doc_id].append([curr_seq, fully_complete])\n",
    "                curr_seq = []\n",
    "                fully_complete = True\n",
    "                \n",
    "        if len(curr_seq) > 0:\n",
    "            processed_docs[doc_id].append([curr_seq, fully_complete])\n",
    "    return processed_docs, seq_to_doc\n",
    "\n",
    "corpus_LB, nums = reconstruct_LB_documents(replace_numerals=True)\n",
    "docs_data, sequence_data = create_documents_dataset(corpus_LB)\n",
    "docs_data # Dictionary{doc_id: List[Tuple[List[Tuple[seq, complete]], fully_complete]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e-ri-sa-ta': [{3: True}, 1.0],\n",
       " 'M': [{3: True,\n",
       "   5: True,\n",
       "   6: False,\n",
       "   9: True,\n",
       "   11: True,\n",
       "   12: True,\n",
       "   13: True,\n",
       "   15: False,\n",
       "   17: True,\n",
       "   18: True,\n",
       "   20: True,\n",
       "   21: True,\n",
       "   22: True,\n",
       "   23: True,\n",
       "   24: True,\n",
       "   25: True,\n",
       "   27: True,\n",
       "   28: True,\n",
       "   31: True,\n",
       "   32: True,\n",
       "   35: True,\n",
       "   44: True,\n",
       "   45: False,\n",
       "   46: True,\n",
       "   47: False,\n",
       "   53: True,\n",
       "   56: False,\n",
       "   57: False,\n",
       "   93: True,\n",
       "   119: True,\n",
       "   129: True,\n",
       "   135: True,\n",
       "   136: True,\n",
       "   143: True,\n",
       "   148: True,\n",
       "   160: True,\n",
       "   167: True,\n",
       "   170: False,\n",
       "   172: True,\n",
       "   179: True,\n",
       "   183: True,\n",
       "   190: True,\n",
       "   191: False,\n",
       "   195: True,\n",
       "   210: True,\n",
       "   211: True,\n",
       "   212: True,\n",
       "   214: True,\n",
       "   217: False,\n",
       "   218: True,\n",
       "   219: True,\n",
       "   220: True,\n",
       "   222: True,\n",
       "   223: True,\n",
       "   225: True,\n",
       "   227: True,\n",
       "   230: True,\n",
       "   232: True,\n",
       "   233: True,\n",
       "   1273: False,\n",
       "   2047: False,\n",
       "   2330: False,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2335: True,\n",
       "   2431: True,\n",
       "   2758: True,\n",
       "   2759: True,\n",
       "   2764: False,\n",
       "   2765: True,\n",
       "   2768: True,\n",
       "   2771: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3318: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3334: True,\n",
       "   3339: True,\n",
       "   3373: True,\n",
       "   3381: True,\n",
       "   3385: False,\n",
       "   3388: True,\n",
       "   3394: True,\n",
       "   3397: True,\n",
       "   3414: True,\n",
       "   3425: True,\n",
       "   3426: False,\n",
       "   3427: False,\n",
       "   3428: True,\n",
       "   3438: True,\n",
       "   3439: True,\n",
       "   3447: True,\n",
       "   3448: True,\n",
       "   4132: True,\n",
       "   4133: True,\n",
       "   4139: True,\n",
       "   4140: True,\n",
       "   4141: True,\n",
       "   4142: True,\n",
       "   4176: True,\n",
       "   4182: True,\n",
       "   4183: True,\n",
       "   4184: False,\n",
       "   4185: True,\n",
       "   4186: True,\n",
       "   4192: True,\n",
       "   4216: True,\n",
       "   4278: True,\n",
       "   4285: True,\n",
       "   4376: True,\n",
       "   4378: True,\n",
       "   4386: True,\n",
       "   4411: True,\n",
       "   4416: True,\n",
       "   4421: True,\n",
       "   4467: True,\n",
       "   4476: True,\n",
       "   4477: True,\n",
       "   4478: True,\n",
       "   4479: True,\n",
       "   4480: True,\n",
       "   4481: True,\n",
       "   4482: True,\n",
       "   4483: True,\n",
       "   4484: True,\n",
       "   4486: True,\n",
       "   4487: True,\n",
       "   4488: True,\n",
       "   4492: True,\n",
       "   4493: True,\n",
       "   5055: True,\n",
       "   5056: True,\n",
       "   5057: True,\n",
       "   5058: True,\n",
       "   5059: True,\n",
       "   5060: False,\n",
       "   5061: True,\n",
       "   5062: True,\n",
       "   5063: True,\n",
       "   5064: True,\n",
       "   5065: True,\n",
       "   5066: True,\n",
       "   5067: True,\n",
       "   5068: True,\n",
       "   5069: True,\n",
       "   5070: True,\n",
       "   5071: True,\n",
       "   5072: True,\n",
       "   5073: True,\n",
       "   5074: True,\n",
       "   5075: True,\n",
       "   5076: False,\n",
       "   5077: True,\n",
       "   5078: True,\n",
       "   5085: False,\n",
       "   5093: True,\n",
       "   5094: True,\n",
       "   5095: True,\n",
       "   5096: True,\n",
       "   5097: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5100: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5103: True,\n",
       "   5104: True,\n",
       "   5105: True,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5142: True,\n",
       "   5382: True,\n",
       "   5386: True},\n",
       "  0.9465811965811965],\n",
       " '1': [{3: True,\n",
       "   5: True,\n",
       "   12: True,\n",
       "   17: True,\n",
       "   20: True,\n",
       "   21: False,\n",
       "   23: True,\n",
       "   24: True,\n",
       "   25: True,\n",
       "   27: False,\n",
       "   35: False,\n",
       "   45: True,\n",
       "   46: False,\n",
       "   49: False,\n",
       "   61: True,\n",
       "   64: True,\n",
       "   65: True,\n",
       "   67: True,\n",
       "   74: True,\n",
       "   79: True,\n",
       "   81: True,\n",
       "   83: False,\n",
       "   84: False,\n",
       "   85: True,\n",
       "   87: True,\n",
       "   89: True,\n",
       "   95: True,\n",
       "   96: True,\n",
       "   98: False,\n",
       "   99: True,\n",
       "   102: True,\n",
       "   105: True,\n",
       "   115: True,\n",
       "   126: True,\n",
       "   129: True,\n",
       "   135: True,\n",
       "   136: False,\n",
       "   139: True,\n",
       "   143: False,\n",
       "   144: True,\n",
       "   145: True,\n",
       "   156: False,\n",
       "   157: False,\n",
       "   158: False,\n",
       "   164: False,\n",
       "   171: True,\n",
       "   176: True,\n",
       "   179: False,\n",
       "   183: False,\n",
       "   195: False,\n",
       "   198: False,\n",
       "   212: False,\n",
       "   213: True,\n",
       "   216: True,\n",
       "   219: False,\n",
       "   222: False,\n",
       "   230: False,\n",
       "   231: True,\n",
       "   251: False,\n",
       "   252: False,\n",
       "   275: True,\n",
       "   276: True,\n",
       "   279: True,\n",
       "   280: True,\n",
       "   281: True,\n",
       "   282: True,\n",
       "   283: True,\n",
       "   284: True,\n",
       "   285: True,\n",
       "   287: True,\n",
       "   288: True,\n",
       "   289: True,\n",
       "   291: True,\n",
       "   292: True,\n",
       "   301: True,\n",
       "   302: True,\n",
       "   303: True,\n",
       "   305: True,\n",
       "   307: False,\n",
       "   308: True,\n",
       "   309: True,\n",
       "   312: False,\n",
       "   316: True,\n",
       "   317: True,\n",
       "   318: True,\n",
       "   319: True,\n",
       "   320: True,\n",
       "   322: True,\n",
       "   324: True,\n",
       "   325: True,\n",
       "   327: True,\n",
       "   329: True,\n",
       "   330: True,\n",
       "   331: False,\n",
       "   332: False,\n",
       "   333: False,\n",
       "   334: True,\n",
       "   335: True,\n",
       "   337: False,\n",
       "   340: True,\n",
       "   344: False,\n",
       "   345: True,\n",
       "   347: True,\n",
       "   348: True,\n",
       "   349: True,\n",
       "   350: True,\n",
       "   351: True,\n",
       "   352: False,\n",
       "   353: True,\n",
       "   355: True,\n",
       "   359: True,\n",
       "   360: True,\n",
       "   363: True,\n",
       "   364: True,\n",
       "   365: True,\n",
       "   366: True,\n",
       "   368: False,\n",
       "   370: True,\n",
       "   371: True,\n",
       "   372: True,\n",
       "   373: True,\n",
       "   374: True,\n",
       "   376: False,\n",
       "   377: True,\n",
       "   379: True,\n",
       "   381: True,\n",
       "   384: True,\n",
       "   387: True,\n",
       "   391: True,\n",
       "   392: True,\n",
       "   394: False,\n",
       "   396: False,\n",
       "   397: True,\n",
       "   400: True,\n",
       "   408: True,\n",
       "   410: True,\n",
       "   413: True,\n",
       "   419: True,\n",
       "   421: False,\n",
       "   426: True,\n",
       "   427: True,\n",
       "   429: True,\n",
       "   430: True,\n",
       "   434: True,\n",
       "   435: False,\n",
       "   438: False,\n",
       "   439: True,\n",
       "   441: True,\n",
       "   443: True,\n",
       "   454: True,\n",
       "   455: False,\n",
       "   459: True,\n",
       "   460: True,\n",
       "   470: False,\n",
       "   476: True,\n",
       "   477: True,\n",
       "   479: True,\n",
       "   488: True,\n",
       "   490: True,\n",
       "   494: False,\n",
       "   505: True,\n",
       "   514: True,\n",
       "   515: True,\n",
       "   518: True,\n",
       "   519: False,\n",
       "   546: True,\n",
       "   549: True,\n",
       "   550: True,\n",
       "   553: True,\n",
       "   554: True,\n",
       "   555: True,\n",
       "   560: True,\n",
       "   562: True,\n",
       "   563: False,\n",
       "   564: True,\n",
       "   565: True,\n",
       "   566: True,\n",
       "   568: True,\n",
       "   571: True,\n",
       "   574: True,\n",
       "   583: True,\n",
       "   584: True,\n",
       "   588: True,\n",
       "   589: False,\n",
       "   594: True,\n",
       "   598: True,\n",
       "   599: False,\n",
       "   601: False,\n",
       "   603: True,\n",
       "   607: False,\n",
       "   610: False,\n",
       "   611: True,\n",
       "   616: True,\n",
       "   617: True,\n",
       "   618: True,\n",
       "   619: False,\n",
       "   620: True,\n",
       "   621: True,\n",
       "   622: True,\n",
       "   623: True,\n",
       "   624: True,\n",
       "   625: False,\n",
       "   636: True,\n",
       "   638: True,\n",
       "   641: False,\n",
       "   642: True,\n",
       "   643: True,\n",
       "   645: True,\n",
       "   646: True,\n",
       "   647: True,\n",
       "   649: True,\n",
       "   650: True,\n",
       "   651: True,\n",
       "   652: True,\n",
       "   654: True,\n",
       "   655: False,\n",
       "   657: True,\n",
       "   658: True,\n",
       "   661: True,\n",
       "   662: True,\n",
       "   667: True,\n",
       "   668: True,\n",
       "   670: True,\n",
       "   673: False,\n",
       "   675: True,\n",
       "   676: True,\n",
       "   677: True,\n",
       "   678: True,\n",
       "   679: True,\n",
       "   680: True,\n",
       "   681: True,\n",
       "   682: True,\n",
       "   683: False,\n",
       "   685: True,\n",
       "   686: True,\n",
       "   687: True,\n",
       "   688: True,\n",
       "   689: True,\n",
       "   691: True,\n",
       "   693: True,\n",
       "   694: True,\n",
       "   695: True,\n",
       "   697: True,\n",
       "   698: True,\n",
       "   700: True,\n",
       "   701: True,\n",
       "   703: True,\n",
       "   707: True,\n",
       "   708: False,\n",
       "   709: True,\n",
       "   710: True,\n",
       "   711: False,\n",
       "   712: True,\n",
       "   713: True,\n",
       "   714: True,\n",
       "   716: True,\n",
       "   718: False,\n",
       "   721: True,\n",
       "   724: True,\n",
       "   727: True,\n",
       "   729: True,\n",
       "   796: True,\n",
       "   798: True,\n",
       "   800: True,\n",
       "   814: True,\n",
       "   817: True,\n",
       "   819: True,\n",
       "   820: True,\n",
       "   824: True,\n",
       "   825: True,\n",
       "   830: True,\n",
       "   831: True,\n",
       "   832: True,\n",
       "   835: True,\n",
       "   837: True,\n",
       "   839: True,\n",
       "   841: True,\n",
       "   844: True,\n",
       "   845: True,\n",
       "   849: False,\n",
       "   851: True,\n",
       "   855: False,\n",
       "   857: True,\n",
       "   858: True,\n",
       "   860: True,\n",
       "   861: True,\n",
       "   862: True,\n",
       "   863: True,\n",
       "   864: True,\n",
       "   865: True,\n",
       "   867: True,\n",
       "   868: True,\n",
       "   872: True,\n",
       "   874: True,\n",
       "   881: True,\n",
       "   891: True,\n",
       "   894: True,\n",
       "   895: True,\n",
       "   896: False,\n",
       "   897: True,\n",
       "   898: True,\n",
       "   900: True,\n",
       "   901: True,\n",
       "   908: False,\n",
       "   929: False,\n",
       "   932: False,\n",
       "   964: False,\n",
       "   1063: True,\n",
       "   1067: False,\n",
       "   1074: False,\n",
       "   1090: False,\n",
       "   1093: True,\n",
       "   1094: True,\n",
       "   1096: True,\n",
       "   1099: True,\n",
       "   1109: False,\n",
       "   1113: True,\n",
       "   1123: True,\n",
       "   1125: True,\n",
       "   1130: False,\n",
       "   1134: False,\n",
       "   1135: False,\n",
       "   1149: True,\n",
       "   1150: True,\n",
       "   1157: True,\n",
       "   1162: False,\n",
       "   1166: True,\n",
       "   1168: False,\n",
       "   1208: False,\n",
       "   1226: False,\n",
       "   1286: False,\n",
       "   1287: True,\n",
       "   1412: False,\n",
       "   1415: False,\n",
       "   1468: False,\n",
       "   1499: True,\n",
       "   1508: True,\n",
       "   1520: True,\n",
       "   1538: True,\n",
       "   1616: False,\n",
       "   1617: False,\n",
       "   1620: False,\n",
       "   1645: True,\n",
       "   1657: False,\n",
       "   1658: False,\n",
       "   1674: False,\n",
       "   1675: False,\n",
       "   1676: False,\n",
       "   1677: False,\n",
       "   1678: False,\n",
       "   1689: True,\n",
       "   1690: False,\n",
       "   1702: False,\n",
       "   1714: True,\n",
       "   1726: True,\n",
       "   1768: True,\n",
       "   1769: False,\n",
       "   1782: False,\n",
       "   1790: False,\n",
       "   1807: True,\n",
       "   1809: False,\n",
       "   1884: False,\n",
       "   1887: False,\n",
       "   1888: True,\n",
       "   1897: False,\n",
       "   1898: False,\n",
       "   1932: False,\n",
       "   1965: False,\n",
       "   1976: False,\n",
       "   1977: False,\n",
       "   2047: False,\n",
       "   2053: False,\n",
       "   2075: False,\n",
       "   2083: True,\n",
       "   2128: False,\n",
       "   2133: False,\n",
       "   2148: False,\n",
       "   2149: False,\n",
       "   2153: False,\n",
       "   2159: False,\n",
       "   2187: True,\n",
       "   2188: False,\n",
       "   2189: False,\n",
       "   2192: True,\n",
       "   2202: False,\n",
       "   2220: True,\n",
       "   2307: True,\n",
       "   2316: True,\n",
       "   2317: True,\n",
       "   2320: False,\n",
       "   2321: False,\n",
       "   2322: True,\n",
       "   2326: True,\n",
       "   2328: True,\n",
       "   2329: True,\n",
       "   2330: True,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2335: True,\n",
       "   2336: True,\n",
       "   2340: True,\n",
       "   2346: True,\n",
       "   2347: False,\n",
       "   2360: True,\n",
       "   2365: False,\n",
       "   2372: False,\n",
       "   2374: True,\n",
       "   2380: False,\n",
       "   2382: False,\n",
       "   2392: False,\n",
       "   2419: True,\n",
       "   2422: True,\n",
       "   2424: False,\n",
       "   2425: True,\n",
       "   2426: False,\n",
       "   2432: True,\n",
       "   2433: False,\n",
       "   2441: False,\n",
       "   2489: True,\n",
       "   2490: True,\n",
       "   2491: True,\n",
       "   2492: True,\n",
       "   2493: False,\n",
       "   2494: True,\n",
       "   2495: True,\n",
       "   2496: True,\n",
       "   2498: True,\n",
       "   2499: True,\n",
       "   2500: False,\n",
       "   2502: True,\n",
       "   2503: True,\n",
       "   2504: True,\n",
       "   2505: True,\n",
       "   2506: True,\n",
       "   2512: True,\n",
       "   2519: True,\n",
       "   2520: False,\n",
       "   2522: True,\n",
       "   2523: True,\n",
       "   2534: True,\n",
       "   2541: False,\n",
       "   2544: False,\n",
       "   2545: False,\n",
       "   2547: False,\n",
       "   2549: True,\n",
       "   2550: True,\n",
       "   2551: False,\n",
       "   2552: True,\n",
       "   2553: True,\n",
       "   2554: True,\n",
       "   2557: False,\n",
       "   2558: False,\n",
       "   2559: True,\n",
       "   2562: False,\n",
       "   2563: False,\n",
       "   2564: True,\n",
       "   2567: False,\n",
       "   2568: True,\n",
       "   2570: True,\n",
       "   2572: False,\n",
       "   2573: True,\n",
       "   2577: True,\n",
       "   2580: True,\n",
       "   2582: False,\n",
       "   2583: True,\n",
       "   2585: True,\n",
       "   2587: True,\n",
       "   2588: False,\n",
       "   2591: True,\n",
       "   2593: True,\n",
       "   2594: True,\n",
       "   2595: True,\n",
       "   2597: False,\n",
       "   2605: False,\n",
       "   2608: False,\n",
       "   2615: True,\n",
       "   2621: False,\n",
       "   2636: True,\n",
       "   2638: False,\n",
       "   2640: False,\n",
       "   2643: True,\n",
       "   2644: True,\n",
       "   2646: True,\n",
       "   2650: True,\n",
       "   2651: True,\n",
       "   2652: True,\n",
       "   2653: True,\n",
       "   2654: False,\n",
       "   2655: False,\n",
       "   2656: True,\n",
       "   2657: True,\n",
       "   2658: True,\n",
       "   2659: True,\n",
       "   2662: True,\n",
       "   2663: False,\n",
       "   2667: True,\n",
       "   2669: False,\n",
       "   2670: True,\n",
       "   2671: True,\n",
       "   2672: True,\n",
       "   2673: False,\n",
       "   2674: True,\n",
       "   2675: True,\n",
       "   2677: True,\n",
       "   2678: True,\n",
       "   2679: True,\n",
       "   2680: True,\n",
       "   2681: True,\n",
       "   2682: True,\n",
       "   2683: False,\n",
       "   2684: False,\n",
       "   2685: False,\n",
       "   2686: False,\n",
       "   2687: False,\n",
       "   2688: False,\n",
       "   2689: True,\n",
       "   2692: True,\n",
       "   2693: True,\n",
       "   2694: True,\n",
       "   2697: True,\n",
       "   2698: True,\n",
       "   2699: True,\n",
       "   2704: False,\n",
       "   2706: True,\n",
       "   2718: True,\n",
       "   2721: False,\n",
       "   2723: True,\n",
       "   2726: False,\n",
       "   2728: True,\n",
       "   2735: True,\n",
       "   2742: True,\n",
       "   2772: True,\n",
       "   2773: True,\n",
       "   2775: True,\n",
       "   2776: True,\n",
       "   2777: True,\n",
       "   2778: True,\n",
       "   2781: True,\n",
       "   2785: True,\n",
       "   2787: True,\n",
       "   2789: True,\n",
       "   2790: True,\n",
       "   2792: True,\n",
       "   2794: True,\n",
       "   2806: False,\n",
       "   2809: False,\n",
       "   2810: True,\n",
       "   2812: True,\n",
       "   2813: True,\n",
       "   2817: True,\n",
       "   2819: False,\n",
       "   2820: False,\n",
       "   2821: False,\n",
       "   2823: True,\n",
       "   2830: True,\n",
       "   2835: True,\n",
       "   2836: False,\n",
       "   2840: True,\n",
       "   2842: False,\n",
       "   2845: True,\n",
       "   2846: True,\n",
       "   2847: True,\n",
       "   2848: True,\n",
       "   2849: False,\n",
       "   2850: True,\n",
       "   2853: False,\n",
       "   2855: True,\n",
       "   2861: False,\n",
       "   2862: True,\n",
       "   2865: True,\n",
       "   2895: True,\n",
       "   2905: True,\n",
       "   2938: True,\n",
       "   3137: True,\n",
       "   3145: True,\n",
       "   3154: False,\n",
       "   3172: True,\n",
       "   3175: False,\n",
       "   3176: True,\n",
       "   3181: True,\n",
       "   3183: True,\n",
       "   3184: True,\n",
       "   3190: True,\n",
       "   3194: True,\n",
       "   3199: False,\n",
       "   3207: True,\n",
       "   3210: True,\n",
       "   3212: True,\n",
       "   3215: True,\n",
       "   3245: True,\n",
       "   3252: True,\n",
       "   3255: True,\n",
       "   3261: False,\n",
       "   3284: False,\n",
       "   3286: True,\n",
       "   3287: True,\n",
       "   3297: False,\n",
       "   3304: False,\n",
       "   3309: False,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3334: False,\n",
       "   3336: False,\n",
       "   3342: False,\n",
       "   3360: False,\n",
       "   3366: False,\n",
       "   3373: True,\n",
       "   3381: True,\n",
       "   3394: True,\n",
       "   3405: True,\n",
       "   3406: False,\n",
       "   3425: True,\n",
       "   3426: True,\n",
       "   3439: True,\n",
       "   3447: False,\n",
       "   3448: False,\n",
       "   3545: False,\n",
       "   3593: False,\n",
       "   3700: False,\n",
       "   3759: False,\n",
       "   3767: True,\n",
       "   3782: False,\n",
       "   3792: False,\n",
       "   3793: False,\n",
       "   3851: False,\n",
       "   3870: False,\n",
       "   3871: True,\n",
       "   3875: True,\n",
       "   3889: False,\n",
       "   3894: True,\n",
       "   3900: True,\n",
       "   3902: False,\n",
       "   3905: True,\n",
       "   3908: False,\n",
       "   3915: True,\n",
       "   3916: True,\n",
       "   3917: True,\n",
       "   3924: True,\n",
       "   3925: False,\n",
       "   3926: True,\n",
       "   3927: True,\n",
       "   3930: False,\n",
       "   3936: True,\n",
       "   3938: True,\n",
       "   3939: True,\n",
       "   3940: True,\n",
       "   3942: True,\n",
       "   3943: True,\n",
       "   3947: True,\n",
       "   3948: True,\n",
       "   3950: False,\n",
       "   3951: True,\n",
       "   3958: True,\n",
       "   3961: True,\n",
       "   3964: True,\n",
       "   3969: False,\n",
       "   3973: True,\n",
       "   3979: True,\n",
       "   3987: True,\n",
       "   3989: True,\n",
       "   3990: True,\n",
       "   3992: True,\n",
       "   3994: True,\n",
       "   3998: True,\n",
       "   4001: True,\n",
       "   4012: False,\n",
       "   4016: True,\n",
       "   4019: False,\n",
       "   4024: True,\n",
       "   4025: True,\n",
       "   4032: True,\n",
       "   4033: True,\n",
       "   4036: True,\n",
       "   4042: False,\n",
       "   4046: False,\n",
       "   4052: False,\n",
       "   4059: False,\n",
       "   4064: True,\n",
       "   4067: False,\n",
       "   4083: False,\n",
       "   4088: True,\n",
       "   4089: True,\n",
       "   4090: True,\n",
       "   4091: False,\n",
       "   4092: True,\n",
       "   4093: True,\n",
       "   4094: True,\n",
       "   4095: True,\n",
       "   4096: True,\n",
       "   4098: True,\n",
       "   4099: True,\n",
       "   4101: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4107: False,\n",
       "   4108: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4113: False,\n",
       "   4114: False,\n",
       "   4115: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4119: True,\n",
       "   4120: False,\n",
       "   4122: True,\n",
       "   4123: True,\n",
       "   4130: True,\n",
       "   4132: False,\n",
       "   4134: True,\n",
       "   4137: True,\n",
       "   4140: True,\n",
       "   4141: True,\n",
       "   4142: True,\n",
       "   4148: True,\n",
       "   4149: True,\n",
       "   4159: True,\n",
       "   4162: False,\n",
       "   4163: True,\n",
       "   4165: False,\n",
       "   4178: False,\n",
       "   4185: True,\n",
       "   4187: True,\n",
       "   4193: False,\n",
       "   4196: True,\n",
       "   4197: False,\n",
       "   4198: True,\n",
       "   4216: True,\n",
       "   4217: False,\n",
       "   4219: False,\n",
       "   4221: False,\n",
       "   4223: False,\n",
       "   4228: True,\n",
       "   4230: True,\n",
       "   4231: True,\n",
       "   4232: True,\n",
       "   4235: True,\n",
       "   4237: True,\n",
       "   4241: True,\n",
       "   4251: False,\n",
       "   4278: True,\n",
       "   4281: True,\n",
       "   4282: True,\n",
       "   4285: False,\n",
       "   4296: True,\n",
       "   4306: False,\n",
       "   4307: False,\n",
       "   4324: False,\n",
       "   4328: True,\n",
       "   4332: True,\n",
       "   4346: True,\n",
       "   4348: True,\n",
       "   4360: False,\n",
       "   4364: True,\n",
       "   4376: True,\n",
       "   4378: True,\n",
       "   4382: False,\n",
       "   4385: True,\n",
       "   4386: True,\n",
       "   4388: False,\n",
       "   4390: True,\n",
       "   4395: True,\n",
       "   4400: True,\n",
       "   4404: True,\n",
       "   4406: True,\n",
       "   4409: False,\n",
       "   4411: False,\n",
       "   4414: True,\n",
       "   4416: True,\n",
       "   4417: False,\n",
       "   4421: True,\n",
       "   4425: True,\n",
       "   4426: True,\n",
       "   4427: True,\n",
       "   4429: True,\n",
       "   4430: True,\n",
       "   4431: True,\n",
       "   4436: True,\n",
       "   4438: False,\n",
       "   4439: True,\n",
       "   4445: True,\n",
       "   4448: False,\n",
       "   4456: True,\n",
       "   4458: False,\n",
       "   4460: True,\n",
       "   4461: True,\n",
       "   4464: True,\n",
       "   4468: False,\n",
       "   4471: False,\n",
       "   4473: True,\n",
       "   4475: True,\n",
       "   4479: True,\n",
       "   4486: True,\n",
       "   4501: False,\n",
       "   4509: True,\n",
       "   4510: True,\n",
       "   4514: True,\n",
       "   4515: True,\n",
       "   4517: False,\n",
       "   4536: True,\n",
       "   4538: True,\n",
       "   4540: True,\n",
       "   4541: True,\n",
       "   4542: True,\n",
       "   4543: True,\n",
       "   4544: True,\n",
       "   4545: True,\n",
       "   4546: True,\n",
       "   4547: True,\n",
       "   4549: False,\n",
       "   4550: True,\n",
       "   4551: True,\n",
       "   4554: True,\n",
       "   4555: True,\n",
       "   4556: True,\n",
       "   4557: True,\n",
       "   4558: True,\n",
       "   4559: True,\n",
       "   4560: True,\n",
       "   4561: True,\n",
       "   4562: True,\n",
       "   4563: True,\n",
       "   4564: True,\n",
       "   4565: True,\n",
       "   4566: True,\n",
       "   4567: True,\n",
       "   4568: True,\n",
       "   4569: True,\n",
       "   4570: True,\n",
       "   4571: True,\n",
       "   4572: True,\n",
       "   4573: True,\n",
       "   4574: True,\n",
       "   4575: True,\n",
       "   4576: True,\n",
       "   4578: True,\n",
       "   4582: True,\n",
       "   4586: True,\n",
       "   4588: True,\n",
       "   4591: True,\n",
       "   4594: False,\n",
       "   4598: False,\n",
       "   4600: True,\n",
       "   4602: True,\n",
       "   4605: True,\n",
       "   4606: False,\n",
       "   4609: False,\n",
       "   4611: True,\n",
       "   4612: True,\n",
       "   4616: False,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: False,\n",
       "   4627: False,\n",
       "   4632: True,\n",
       "   4636: False,\n",
       "   4660: True,\n",
       "   4674: True,\n",
       "   4675: True,\n",
       "   4677: True,\n",
       "   4678: True,\n",
       "   4679: True,\n",
       "   4681: True,\n",
       "   4682: True,\n",
       "   4683: True,\n",
       "   4685: True,\n",
       "   4686: True,\n",
       "   4692: True,\n",
       "   4693: True,\n",
       "   4695: True,\n",
       "   4696: True,\n",
       "   4697: True,\n",
       "   4699: False,\n",
       "   4701: True,\n",
       "   4703: True,\n",
       "   4704: True,\n",
       "   4705: True,\n",
       "   4706: False,\n",
       "   4707: False,\n",
       "   4708: True,\n",
       "   4710: True,\n",
       "   4712: True,\n",
       "   4714: True,\n",
       "   4715: False,\n",
       "   4717: False,\n",
       "   4719: True,\n",
       "   4720: True,\n",
       "   4722: False,\n",
       "   4723: True,\n",
       "   4728: True,\n",
       "   4729: True,\n",
       "   4732: True,\n",
       "   4737: True,\n",
       "   4738: True,\n",
       "   4743: True,\n",
       "   4746: True,\n",
       "   4753: False,\n",
       "   4759: True,\n",
       "   4769: False,\n",
       "   4781: True,\n",
       "   4782: True,\n",
       "   4783: True,\n",
       "   4784: False,\n",
       "   4785: True,\n",
       "   4787: True,\n",
       "   4788: True,\n",
       "   4789: True,\n",
       "   4790: True,\n",
       "   4791: True,\n",
       "   4792: True,\n",
       "   4795: True,\n",
       "   4801: True,\n",
       "   4805: True,\n",
       "   4806: False,\n",
       "   4809: True,\n",
       "   4819: True,\n",
       "   4822: True,\n",
       "   4826: True,\n",
       "   4827: True,\n",
       "   4831: True,\n",
       "   4832: True,\n",
       "   4833: True,\n",
       "   4835: True,\n",
       "   4836: True,\n",
       "   4841: True,\n",
       "   4844: True,\n",
       "   4850: True,\n",
       "   4851: True,\n",
       "   4853: True,\n",
       "   4854: True,\n",
       "   4855: True,\n",
       "   4858: True,\n",
       "   4861: True,\n",
       "   4866: True,\n",
       "   4867: True,\n",
       "   4869: True,\n",
       "   4870: True,\n",
       "   4873: True,\n",
       "   4874: False,\n",
       "   4881: True,\n",
       "   4884: True,\n",
       "   4885: True,\n",
       "   4886: True,\n",
       "   4899: True,\n",
       "   4904: True,\n",
       "   4905: True,\n",
       "   4906: True,\n",
       "   4919: True,\n",
       "   4929: True,\n",
       "   4931: True,\n",
       "   4932: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4935: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4941: True,\n",
       "   4943: True,\n",
       "   4944: True,\n",
       "   4945: True,\n",
       "   4946: True,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   4955: True,\n",
       "   4956: True,\n",
       "   4961: True,\n",
       "   4963: True,\n",
       "   4964: True,\n",
       "   4965: True,\n",
       "   4967: True,\n",
       "   4971: True,\n",
       "   4972: True,\n",
       "   4976: True,\n",
       "   4980: True,\n",
       "   4982: True,\n",
       "   4983: True,\n",
       "   4984: True,\n",
       "   4985: True,\n",
       "   4986: True,\n",
       "   4987: True,\n",
       "   4988: True,\n",
       "   4990: False,\n",
       "   4991: False,\n",
       "   4992: False,\n",
       "   4996: False,\n",
       "   4997: False,\n",
       "   ...},\n",
       "  0.8366412213740458],\n",
       " 'me-ri': [{4: False,\n",
       "   610: False,\n",
       "   2707: False,\n",
       "   4196: True,\n",
       "   4198: True,\n",
       "   4216: True,\n",
       "   5385: False},\n",
       "  0.5],\n",
       " 'da-so': [{5: False, 2779: True}, 0.5],\n",
       " 'SA': [{5: True,\n",
       "   19: False,\n",
       "   28: False,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   5156: True,\n",
       "   5157: True,\n",
       "   5158: True,\n",
       "   5159: True,\n",
       "   5160: True,\n",
       "   5161: True,\n",
       "   5162: True,\n",
       "   5163: True,\n",
       "   5164: False,\n",
       "   5165: True,\n",
       "   5166: False,\n",
       "   5167: True,\n",
       "   5168: True,\n",
       "   5169: True,\n",
       "   5170: True,\n",
       "   5171: True,\n",
       "   5172: True,\n",
       "   5173: True,\n",
       "   5174: True,\n",
       "   5175: True,\n",
       "   5176: True,\n",
       "   5177: True,\n",
       "   5178: False,\n",
       "   5179: True,\n",
       "   5180: True,\n",
       "   5181: True,\n",
       "   5182: False,\n",
       "   5183: True,\n",
       "   5184: True,\n",
       "   5185: False,\n",
       "   5186: True,\n",
       "   5187: True,\n",
       "   5188: True,\n",
       "   5189: True,\n",
       "   5190: True,\n",
       "   5191: True,\n",
       "   5193: True,\n",
       "   5194: True,\n",
       "   5195: True,\n",
       "   5196: True,\n",
       "   5197: True,\n",
       "   5198: True,\n",
       "   5199: False,\n",
       "   5200: True,\n",
       "   5201: True,\n",
       "   5202: True,\n",
       "   5203: True,\n",
       "   5204: True,\n",
       "   5205: True,\n",
       "   5206: True,\n",
       "   5207: False,\n",
       "   5208: True,\n",
       "   5209: True,\n",
       "   5210: True,\n",
       "   5211: True,\n",
       "   5212: True,\n",
       "   5213: True,\n",
       "   5214: True,\n",
       "   5215: True,\n",
       "   5216: True,\n",
       "   5218: True,\n",
       "   5219: False,\n",
       "   5220: True,\n",
       "   5221: True,\n",
       "   5222: True,\n",
       "   5223: True,\n",
       "   5224: True,\n",
       "   5225: True,\n",
       "   5226: True,\n",
       "   5227: True,\n",
       "   5228: True,\n",
       "   5229: True,\n",
       "   5230: True,\n",
       "   5231: True,\n",
       "   5232: True,\n",
       "   5233: True,\n",
       "   5234: True,\n",
       "   5235: True,\n",
       "   5236: True,\n",
       "   5237: True,\n",
       "   5238: True,\n",
       "   5239: True,\n",
       "   5240: True,\n",
       "   5241: True,\n",
       "   5242: True,\n",
       "   5243: True,\n",
       "   5244: True,\n",
       "   5245: True,\n",
       "   5246: True,\n",
       "   5247: False,\n",
       "   5248: True,\n",
       "   5249: False,\n",
       "   5250: True,\n",
       "   5251: True,\n",
       "   5252: True,\n",
       "   5253: False,\n",
       "   5254: True,\n",
       "   5255: True,\n",
       "   5256: False,\n",
       "   5257: True,\n",
       "   5258: False,\n",
       "   5259: True,\n",
       "   5260: True,\n",
       "   5261: True},\n",
       "  0.8986486486486487],\n",
       " 'a-na-ka': [{6: True}, 1.0],\n",
       " 'to-so': [{9: True,\n",
       "   135: True,\n",
       "   211: True,\n",
       "   240: False,\n",
       "   672: False,\n",
       "   683: False,\n",
       "   1212: True,\n",
       "   1223: True,\n",
       "   1228: True,\n",
       "   1324: False,\n",
       "   1339: True,\n",
       "   1346: True,\n",
       "   1386: True,\n",
       "   1602: False,\n",
       "   2044: False,\n",
       "   2317: False,\n",
       "   2321: True,\n",
       "   2327: True,\n",
       "   2328: True,\n",
       "   2377: False,\n",
       "   2639: True,\n",
       "   2641: True,\n",
       "   2642: True,\n",
       "   2667: False,\n",
       "   2669: False,\n",
       "   2675: False,\n",
       "   2677: False,\n",
       "   2678: True,\n",
       "   2680: True,\n",
       "   2681: True,\n",
       "   2687: True,\n",
       "   2728: True,\n",
       "   2740: True,\n",
       "   2757: True,\n",
       "   2780: True,\n",
       "   3872: False,\n",
       "   3874: True,\n",
       "   3877: True,\n",
       "   3902: True,\n",
       "   3959: True,\n",
       "   3999: True,\n",
       "   4088: True,\n",
       "   4097: False,\n",
       "   4194: True,\n",
       "   4696: True,\n",
       "   4926: True,\n",
       "   4930: True,\n",
       "   4934: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: False,\n",
       "   4951: False,\n",
       "   4953: False,\n",
       "   4954: True,\n",
       "   4957: True,\n",
       "   4959: True,\n",
       "   4960: True,\n",
       "   5001: True,\n",
       "   5005: True,\n",
       "   5011: True,\n",
       "   5385: True,\n",
       "   5565: True,\n",
       "   5719: True},\n",
       "  0.8442622950819673],\n",
       " 'a-pu-do-so-mo': [{9: True}, 1.0],\n",
       " 'a-mi-ni-so': [{9: True,\n",
       "   147: False,\n",
       "   663: True,\n",
       "   2514: True,\n",
       "   2569: True,\n",
       "   2681: True,\n",
       "   4088: True,\n",
       "   4398: True,\n",
       "   4410: False,\n",
       "   4468: True,\n",
       "   4514: True,\n",
       "   4709: True},\n",
       "  0.8461538461538461],\n",
       " 'NUM': [{9: True,\n",
       "   13: True,\n",
       "   14: False,\n",
       "   31: True,\n",
       "   32: False,\n",
       "   59: True,\n",
       "   63: True,\n",
       "   66: False,\n",
       "   69: True,\n",
       "   73: False,\n",
       "   77: True,\n",
       "   78: True,\n",
       "   82: False,\n",
       "   86: True,\n",
       "   108: False,\n",
       "   118: False,\n",
       "   119: True,\n",
       "   122: False,\n",
       "   124: False,\n",
       "   125: True,\n",
       "   130: False,\n",
       "   132: True,\n",
       "   133: True,\n",
       "   135: True,\n",
       "   136: False,\n",
       "   138: True,\n",
       "   140: True,\n",
       "   141: True,\n",
       "   143: False,\n",
       "   146: True,\n",
       "   149: True,\n",
       "   150: True,\n",
       "   158: True,\n",
       "   159: True,\n",
       "   161: True,\n",
       "   162: True,\n",
       "   163: False,\n",
       "   164: True,\n",
       "   165: False,\n",
       "   166: False,\n",
       "   167: False,\n",
       "   168: True,\n",
       "   169: False,\n",
       "   173: False,\n",
       "   177: True,\n",
       "   180: True,\n",
       "   181: False,\n",
       "   183: True,\n",
       "   184: False,\n",
       "   185: False,\n",
       "   187: False,\n",
       "   188: True,\n",
       "   189: True,\n",
       "   190: False,\n",
       "   191: False,\n",
       "   192: False,\n",
       "   193: False,\n",
       "   194: True,\n",
       "   196: True,\n",
       "   197: False,\n",
       "   199: False,\n",
       "   210: False,\n",
       "   211: True,\n",
       "   212: True,\n",
       "   214: True,\n",
       "   215: True,\n",
       "   217: True,\n",
       "   218: True,\n",
       "   220: True,\n",
       "   221: True,\n",
       "   224: False,\n",
       "   225: True,\n",
       "   227: True,\n",
       "   228: True,\n",
       "   230: True,\n",
       "   233: True,\n",
       "   235: False,\n",
       "   236: True,\n",
       "   237: True,\n",
       "   238: True,\n",
       "   239: False,\n",
       "   240: True,\n",
       "   243: False,\n",
       "   244: True,\n",
       "   245: True,\n",
       "   246: True,\n",
       "   249: False,\n",
       "   250: False,\n",
       "   258: True,\n",
       "   270: True,\n",
       "   271: True,\n",
       "   273: False,\n",
       "   317: True,\n",
       "   453: True,\n",
       "   463: False,\n",
       "   497: True,\n",
       "   498: True,\n",
       "   499: True,\n",
       "   500: False,\n",
       "   501: True,\n",
       "   503: True,\n",
       "   504: False,\n",
       "   506: True,\n",
       "   519: False,\n",
       "   524: False,\n",
       "   555: False,\n",
       "   557: True,\n",
       "   558: True,\n",
       "   559: True,\n",
       "   560: True,\n",
       "   561: True,\n",
       "   562: True,\n",
       "   563: True,\n",
       "   564: True,\n",
       "   567: True,\n",
       "   568: True,\n",
       "   569: True,\n",
       "   570: False,\n",
       "   573: True,\n",
       "   574: True,\n",
       "   575: True,\n",
       "   576: True,\n",
       "   577: True,\n",
       "   578: True,\n",
       "   585: True,\n",
       "   590: False,\n",
       "   591: True,\n",
       "   592: False,\n",
       "   593: True,\n",
       "   595: True,\n",
       "   596: True,\n",
       "   600: False,\n",
       "   601: False,\n",
       "   602: True,\n",
       "   603: True,\n",
       "   604: True,\n",
       "   605: False,\n",
       "   608: True,\n",
       "   609: True,\n",
       "   610: True,\n",
       "   611: False,\n",
       "   613: True,\n",
       "   622: True,\n",
       "   644: True,\n",
       "   648: True,\n",
       "   655: False,\n",
       "   656: True,\n",
       "   659: True,\n",
       "   664: True,\n",
       "   666: True,\n",
       "   669: False,\n",
       "   672: True,\n",
       "   684: True,\n",
       "   690: False,\n",
       "   702: False,\n",
       "   705: False,\n",
       "   710: True,\n",
       "   715: True,\n",
       "   766: True,\n",
       "   768: True,\n",
       "   925: False,\n",
       "   934: False,\n",
       "   962: True,\n",
       "   963: True,\n",
       "   1018: True,\n",
       "   1067: False,\n",
       "   1090: False,\n",
       "   1093: True,\n",
       "   1097: False,\n",
       "   1124: True,\n",
       "   1127: False,\n",
       "   1128: False,\n",
       "   1133: False,\n",
       "   1155: False,\n",
       "   1156: False,\n",
       "   1158: False,\n",
       "   1255: False,\n",
       "   1256: False,\n",
       "   1263: False,\n",
       "   1265: False,\n",
       "   1277: False,\n",
       "   1297: False,\n",
       "   1301: False,\n",
       "   1303: False,\n",
       "   1331: True,\n",
       "   1334: False,\n",
       "   1342: False,\n",
       "   1345: False,\n",
       "   1348: False,\n",
       "   1349: False,\n",
       "   1357: False,\n",
       "   1411: False,\n",
       "   1412: False,\n",
       "   1413: False,\n",
       "   1458: False,\n",
       "   1461: False,\n",
       "   1470: True,\n",
       "   1483: False,\n",
       "   1496: True,\n",
       "   1497: False,\n",
       "   1498: False,\n",
       "   1500: False,\n",
       "   1502: False,\n",
       "   1503: True,\n",
       "   1504: True,\n",
       "   1505: True,\n",
       "   1506: True,\n",
       "   1507: False,\n",
       "   1509: False,\n",
       "   1510: False,\n",
       "   1511: False,\n",
       "   1515: False,\n",
       "   1518: False,\n",
       "   1522: True,\n",
       "   1527: False,\n",
       "   1541: False,\n",
       "   1557: False,\n",
       "   1558: True,\n",
       "   1559: False,\n",
       "   1560: False,\n",
       "   1561: True,\n",
       "   1562: True,\n",
       "   1563: False,\n",
       "   1564: False,\n",
       "   1567: False,\n",
       "   1577: False,\n",
       "   1614: False,\n",
       "   1618: False,\n",
       "   1619: False,\n",
       "   1621: False,\n",
       "   1622: False,\n",
       "   1623: False,\n",
       "   1624: False,\n",
       "   1625: False,\n",
       "   1628: False,\n",
       "   1629: False,\n",
       "   1631: False,\n",
       "   1632: False,\n",
       "   1633: False,\n",
       "   1634: False,\n",
       "   1635: False,\n",
       "   1636: False,\n",
       "   1637: False,\n",
       "   1638: False,\n",
       "   1639: False,\n",
       "   1640: False,\n",
       "   1641: False,\n",
       "   1643: False,\n",
       "   1659: False,\n",
       "   1673: False,\n",
       "   1679: False,\n",
       "   1686: False,\n",
       "   1699: False,\n",
       "   1706: False,\n",
       "   1713: False,\n",
       "   1728: False,\n",
       "   1762: False,\n",
       "   1763: False,\n",
       "   1764: False,\n",
       "   1765: False,\n",
       "   1767: False,\n",
       "   1813: False,\n",
       "   1849: False,\n",
       "   1851: False,\n",
       "   1852: False,\n",
       "   1853: False,\n",
       "   1855: False,\n",
       "   1876: False,\n",
       "   1877: False,\n",
       "   1878: True,\n",
       "   1879: False,\n",
       "   1885: False,\n",
       "   1902: False,\n",
       "   1930: False,\n",
       "   1935: False,\n",
       "   1936: False,\n",
       "   1937: False,\n",
       "   1938: False,\n",
       "   1939: False,\n",
       "   1940: False,\n",
       "   1942: False,\n",
       "   1943: False,\n",
       "   1947: False,\n",
       "   1948: False,\n",
       "   1978: False,\n",
       "   1980: False,\n",
       "   1981: False,\n",
       "   1991: False,\n",
       "   1994: False,\n",
       "   2049: False,\n",
       "   2050: False,\n",
       "   2051: False,\n",
       "   2052: False,\n",
       "   2054: False,\n",
       "   2055: False,\n",
       "   2056: False,\n",
       "   2058: False,\n",
       "   2099: False,\n",
       "   2107: False,\n",
       "   2116: False,\n",
       "   2118: False,\n",
       "   2122: True,\n",
       "   2147: False,\n",
       "   2151: False,\n",
       "   2164: False,\n",
       "   2181: False,\n",
       "   2183: False,\n",
       "   2184: False,\n",
       "   2198: False,\n",
       "   2199: False,\n",
       "   2270: True,\n",
       "   2310: False,\n",
       "   2316: True,\n",
       "   2321: True,\n",
       "   2323: True,\n",
       "   2324: False,\n",
       "   2325: False,\n",
       "   2328: True,\n",
       "   2329: True,\n",
       "   2330: True,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2335: True,\n",
       "   2339: True,\n",
       "   2341: True,\n",
       "   2342: True,\n",
       "   2344: True,\n",
       "   2345: False,\n",
       "   2346: True,\n",
       "   2350: True,\n",
       "   2353: True,\n",
       "   2359: True,\n",
       "   2361: True,\n",
       "   2362: True,\n",
       "   2366: False,\n",
       "   2367: True,\n",
       "   2368: True,\n",
       "   2369: True,\n",
       "   2370: True,\n",
       "   2374: True,\n",
       "   2375: True,\n",
       "   2376: True,\n",
       "   2377: True,\n",
       "   2378: True,\n",
       "   2379: False,\n",
       "   2380: False,\n",
       "   2381: False,\n",
       "   2383: False,\n",
       "   2395: True,\n",
       "   2396: True,\n",
       "   2400: False,\n",
       "   2417: False,\n",
       "   2422: True,\n",
       "   2426: False,\n",
       "   2431: True,\n",
       "   2433: True,\n",
       "   2499: True,\n",
       "   2501: True,\n",
       "   2505: True,\n",
       "   2506: True,\n",
       "   2507: True,\n",
       "   2509: True,\n",
       "   2516: False,\n",
       "   2517: False,\n",
       "   2518: True,\n",
       "   2520: False,\n",
       "   2521: True,\n",
       "   2523: False,\n",
       "   2524: True,\n",
       "   2528: True,\n",
       "   2529: False,\n",
       "   2530: False,\n",
       "   2534: False,\n",
       "   2536: False,\n",
       "   2537: True,\n",
       "   2542: False,\n",
       "   2543: False,\n",
       "   2545: False,\n",
       "   2550: True,\n",
       "   2551: True,\n",
       "   2552: True,\n",
       "   2553: False,\n",
       "   2554: True,\n",
       "   2555: False,\n",
       "   2556: True,\n",
       "   2557: True,\n",
       "   2558: True,\n",
       "   2559: False,\n",
       "   2560: True,\n",
       "   2562: True,\n",
       "   2563: True,\n",
       "   2564: True,\n",
       "   2566: True,\n",
       "   2567: False,\n",
       "   2570: True,\n",
       "   2572: True,\n",
       "   2573: False,\n",
       "   2574: True,\n",
       "   2575: True,\n",
       "   2576: False,\n",
       "   2578: False,\n",
       "   2587: False,\n",
       "   2588: False,\n",
       "   2589: True,\n",
       "   2591: True,\n",
       "   2593: True,\n",
       "   2594: False,\n",
       "   2597: True,\n",
       "   2598: True,\n",
       "   2599: True,\n",
       "   2600: True,\n",
       "   2601: False,\n",
       "   2604: False,\n",
       "   2605: False,\n",
       "   2606: True,\n",
       "   2609: True,\n",
       "   2610: False,\n",
       "   2612: False,\n",
       "   2613: True,\n",
       "   2614: False,\n",
       "   2616: False,\n",
       "   2617: True,\n",
       "   2619: True,\n",
       "   2627: False,\n",
       "   2630: False,\n",
       "   2633: False,\n",
       "   2634: False,\n",
       "   2636: False,\n",
       "   2640: False,\n",
       "   2641: True,\n",
       "   2642: True,\n",
       "   2643: True,\n",
       "   2645: True,\n",
       "   2648: True,\n",
       "   2649: False,\n",
       "   2650: False,\n",
       "   2651: True,\n",
       "   2652: True,\n",
       "   2654: True,\n",
       "   2658: False,\n",
       "   2659: True,\n",
       "   2661: True,\n",
       "   2662: False,\n",
       "   2667: True,\n",
       "   2669: True,\n",
       "   2674: True,\n",
       "   2675: True,\n",
       "   2677: True,\n",
       "   2678: True,\n",
       "   2679: True,\n",
       "   2680: True,\n",
       "   2681: True,\n",
       "   2684: False,\n",
       "   2687: False,\n",
       "   2691: False,\n",
       "   2692: True,\n",
       "   2700: False,\n",
       "   2701: True,\n",
       "   2703: True,\n",
       "   2705: False,\n",
       "   2706: True,\n",
       "   2708: True,\n",
       "   2714: False,\n",
       "   2715: True,\n",
       "   2719: False,\n",
       "   2720: False,\n",
       "   2724: True,\n",
       "   2727: True,\n",
       "   2728: False,\n",
       "   2736: True,\n",
       "   2739: True,\n",
       "   2740: False,\n",
       "   2741: True,\n",
       "   2744: True,\n",
       "   2745: True,\n",
       "   2747: False,\n",
       "   2755: True,\n",
       "   2756: False,\n",
       "   2757: True,\n",
       "   2758: True,\n",
       "   2759: True,\n",
       "   2764: False,\n",
       "   2765: True,\n",
       "   2772: False,\n",
       "   2775: True,\n",
       "   2776: True,\n",
       "   2779: False,\n",
       "   2780: True,\n",
       "   2782: True,\n",
       "   2783: True,\n",
       "   2784: False,\n",
       "   2786: True,\n",
       "   2791: False,\n",
       "   2797: True,\n",
       "   2799: False,\n",
       "   2800: True,\n",
       "   2802: True,\n",
       "   2803: True,\n",
       "   2808: True,\n",
       "   2811: False,\n",
       "   2812: False,\n",
       "   2814: False,\n",
       "   2816: True,\n",
       "   2826: True,\n",
       "   2828: True,\n",
       "   2829: True,\n",
       "   2832: True,\n",
       "   2833: False,\n",
       "   2835: True,\n",
       "   2836: True,\n",
       "   2837: True,\n",
       "   2838: False,\n",
       "   2839: True,\n",
       "   2841: False,\n",
       "   2842: False,\n",
       "   2843: True,\n",
       "   2844: False,\n",
       "   2866: True,\n",
       "   2867: True,\n",
       "   2868: True,\n",
       "   2869: True,\n",
       "   2870: True,\n",
       "   2871: True,\n",
       "   2872: True,\n",
       "   2874: False,\n",
       "   2875: True,\n",
       "   2876: True,\n",
       "   2877: True,\n",
       "   2878: True,\n",
       "   2879: True,\n",
       "   2880: True,\n",
       "   2881: True,\n",
       "   2882: False,\n",
       "   2883: True,\n",
       "   2884: False,\n",
       "   2886: True,\n",
       "   2887: False,\n",
       "   2889: True,\n",
       "   2890: False,\n",
       "   2891: True,\n",
       "   2892: False,\n",
       "   2893: False,\n",
       "   2894: False,\n",
       "   2896: True,\n",
       "   2897: True,\n",
       "   2900: False,\n",
       "   2902: False,\n",
       "   2904: False,\n",
       "   2906: False,\n",
       "   2908: True,\n",
       "   2909: True,\n",
       "   2910: False,\n",
       "   2912: False,\n",
       "   2913: False,\n",
       "   2915: True,\n",
       "   2918: False,\n",
       "   2919: True,\n",
       "   2920: False,\n",
       "   2921: False,\n",
       "   2922: False,\n",
       "   2925: False,\n",
       "   2926: False,\n",
       "   2929: False,\n",
       "   2930: True,\n",
       "   2931: True,\n",
       "   2932: True,\n",
       "   2933: True,\n",
       "   2934: True,\n",
       "   2935: True,\n",
       "   2937: True,\n",
       "   2938: True,\n",
       "   2939: True,\n",
       "   2940: False,\n",
       "   2941: False,\n",
       "   2942: True,\n",
       "   2943: True,\n",
       "   2944: True,\n",
       "   2945: False,\n",
       "   2946: True,\n",
       "   2947: False,\n",
       "   2948: True,\n",
       "   2949: True,\n",
       "   2950: True,\n",
       "   2951: False,\n",
       "   2952: True,\n",
       "   2953: True,\n",
       "   2954: True,\n",
       "   2955: True,\n",
       "   2956: True,\n",
       "   2957: True,\n",
       "   2958: True,\n",
       "   2959: True,\n",
       "   2960: True,\n",
       "   2961: True,\n",
       "   2962: True,\n",
       "   2963: True,\n",
       "   2964: False,\n",
       "   2965: True,\n",
       "   2966: True,\n",
       "   2967: False,\n",
       "   2968: True,\n",
       "   2969: True,\n",
       "   2970: True,\n",
       "   2971: True,\n",
       "   2972: True,\n",
       "   2973: True,\n",
       "   2974: False,\n",
       "   2975: True,\n",
       "   2976: True,\n",
       "   2977: True,\n",
       "   2978: True,\n",
       "   2979: True,\n",
       "   2980: False,\n",
       "   2981: False,\n",
       "   2982: True,\n",
       "   2983: True,\n",
       "   2984: True,\n",
       "   2985: True,\n",
       "   2986: True,\n",
       "   2987: True,\n",
       "   2988: False,\n",
       "   2989: True,\n",
       "   2990: True,\n",
       "   2991: True,\n",
       "   2992: True,\n",
       "   2993: True,\n",
       "   2994: True,\n",
       "   2995: True,\n",
       "   2996: False,\n",
       "   2997: True,\n",
       "   2998: True,\n",
       "   2999: True,\n",
       "   3000: True,\n",
       "   3001: True,\n",
       "   3002: True,\n",
       "   3003: True,\n",
       "   3004: True,\n",
       "   3005: True,\n",
       "   3006: True,\n",
       "   3007: True,\n",
       "   3008: True,\n",
       "   3009: True,\n",
       "   3010: True,\n",
       "   3011: False,\n",
       "   3012: True,\n",
       "   3013: True,\n",
       "   3014: False,\n",
       "   3015: True,\n",
       "   3016: True,\n",
       "   3017: False,\n",
       "   3018: True,\n",
       "   3019: True,\n",
       "   3020: True,\n",
       "   3021: True,\n",
       "   3022: True,\n",
       "   3023: True,\n",
       "   3024: True,\n",
       "   3025: True,\n",
       "   3026: True,\n",
       "   3027: True,\n",
       "   3028: True,\n",
       "   3029: True,\n",
       "   3030: True,\n",
       "   3031: True,\n",
       "   3032: True,\n",
       "   3033: True,\n",
       "   3034: False,\n",
       "   3035: True,\n",
       "   3036: False,\n",
       "   3037: True,\n",
       "   3038: True,\n",
       "   3039: False,\n",
       "   3040: False,\n",
       "   3041: False,\n",
       "   3042: True,\n",
       "   3043: True,\n",
       "   3044: True,\n",
       "   3045: True,\n",
       "   3046: True,\n",
       "   3047: True,\n",
       "   3048: False,\n",
       "   3049: True,\n",
       "   3050: False,\n",
       "   3051: True,\n",
       "   3052: True,\n",
       "   3053: False,\n",
       "   3054: True,\n",
       "   3055: True,\n",
       "   3056: True,\n",
       "   3057: True,\n",
       "   3058: True,\n",
       "   3059: True,\n",
       "   3060: True,\n",
       "   3061: True,\n",
       "   3062: False,\n",
       "   3063: False,\n",
       "   3064: True,\n",
       "   3065: True,\n",
       "   3066: True,\n",
       "   3067: True,\n",
       "   3068: True,\n",
       "   3069: True,\n",
       "   3070: True,\n",
       "   3071: True,\n",
       "   3072: True,\n",
       "   3073: True,\n",
       "   3074: False,\n",
       "   3075: True,\n",
       "   3076: True,\n",
       "   3077: True,\n",
       "   3078: True,\n",
       "   3079: True,\n",
       "   3080: True,\n",
       "   3081: True,\n",
       "   3082: True,\n",
       "   3083: True,\n",
       "   3084: True,\n",
       "   3085: True,\n",
       "   3086: True,\n",
       "   3087: True,\n",
       "   3088: True,\n",
       "   3089: True,\n",
       "   3090: True,\n",
       "   3091: True,\n",
       "   3092: True,\n",
       "   3093: True,\n",
       "   3094: True,\n",
       "   3095: True,\n",
       "   3096: True,\n",
       "   3097: True,\n",
       "   3098: True,\n",
       "   3099: True,\n",
       "   3100: True,\n",
       "   3101: True,\n",
       "   3102: True,\n",
       "   3103: True,\n",
       "   3104: True,\n",
       "   3105: True,\n",
       "   3106: False,\n",
       "   3107: True,\n",
       "   3108: False,\n",
       "   3109: True,\n",
       "   3110: True,\n",
       "   3111: True,\n",
       "   3112: True,\n",
       "   3113: True,\n",
       "   3114: True,\n",
       "   3115: False,\n",
       "   3116: True,\n",
       "   3117: True,\n",
       "   3118: True,\n",
       "   3119: True,\n",
       "   3120: True,\n",
       "   3121: True,\n",
       "   3122: True,\n",
       "   3123: True,\n",
       "   3124: True,\n",
       "   3125: True,\n",
       "   3126: True,\n",
       "   3127: True,\n",
       "   3128: True,\n",
       "   3129: True,\n",
       "   3130: True,\n",
       "   3131: True,\n",
       "   3132: True,\n",
       "   3133: True,\n",
       "   3134: True,\n",
       "   3135: True,\n",
       "   3136: True,\n",
       "   3137: True,\n",
       "   3138: True,\n",
       "   3139: True,\n",
       "   3140: True,\n",
       "   3141: True,\n",
       "   3142: True,\n",
       "   3143: True,\n",
       "   3144: True,\n",
       "   3145: True,\n",
       "   3146: True,\n",
       "   3147: True,\n",
       "   3148: True,\n",
       "   3149: True,\n",
       "   3150: True,\n",
       "   3151: True,\n",
       "   3152: True,\n",
       "   3153: True,\n",
       "   3154: True,\n",
       "   3155: True,\n",
       "   3156: True,\n",
       "   3157: True,\n",
       "   3158: True,\n",
       "   3159: False,\n",
       "   3160: True,\n",
       "   3161: True,\n",
       "   3162: True,\n",
       "   3163: False,\n",
       "   3164: True,\n",
       "   3165: True,\n",
       "   3166: True,\n",
       "   3167: False,\n",
       "   3168: True,\n",
       "   3169: True,\n",
       "   3170: True,\n",
       "   3171: True,\n",
       "   3172: True,\n",
       "   3173: True,\n",
       "   3175: True,\n",
       "   3176: True,\n",
       "   3177: True,\n",
       "   3178: True,\n",
       "   3179: True,\n",
       "   3180: True,\n",
       "   3181: False,\n",
       "   3182: True,\n",
       "   3183: True,\n",
       "   3184: True,\n",
       "   3185: True,\n",
       "   3186: True,\n",
       "   3187: True,\n",
       "   3188: True,\n",
       "   3189: True,\n",
       "   3190: True,\n",
       "   3191: True,\n",
       "   3192: True,\n",
       "   3193: True,\n",
       "   3194: True,\n",
       "   3195: True,\n",
       "   3196: False,\n",
       "   3197: True,\n",
       "   3198: False,\n",
       "   3199: False,\n",
       "   3200: True,\n",
       "   3201: True,\n",
       "   3202: False,\n",
       "   3203: True,\n",
       "   3204: True,\n",
       "   3205: True,\n",
       "   3206: True,\n",
       "   3207: True,\n",
       "   3208: True,\n",
       "   3209: True,\n",
       "   3210: True,\n",
       "   3211: True,\n",
       "   3212: False,\n",
       "   3213: True,\n",
       "   3214: True,\n",
       "   3215: False,\n",
       "   3216: True,\n",
       "   3217: True,\n",
       "   3218: True,\n",
       "   3219: True,\n",
       "   3220: True,\n",
       "   3221: True,\n",
       "   3222: True,\n",
       "   3223: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3226: True,\n",
       "   3227: True,\n",
       "   3228: True,\n",
       "   3229: True,\n",
       "   3230: True,\n",
       "   3231: True,\n",
       "   3232: False,\n",
       "   3233: True,\n",
       "   3234: True,\n",
       "   3235: True,\n",
       "   3236: False,\n",
       "   3237: True,\n",
       "   3238: True,\n",
       "   3239: True,\n",
       "   3240: True,\n",
       "   3241: True,\n",
       "   3242: True,\n",
       "   3243: True,\n",
       "   3244: True,\n",
       "   3245: True,\n",
       "   3246: True,\n",
       "   3247: True,\n",
       "   3248: True,\n",
       "   3249: True,\n",
       "   3250: True,\n",
       "   3251: True,\n",
       "   3252: True,\n",
       "   3253: False,\n",
       "   3254: True,\n",
       "   3255: True,\n",
       "   3256: True,\n",
       "   3257: True,\n",
       "   3258: True,\n",
       "   3259: True,\n",
       "   3260: True,\n",
       "   3261: False,\n",
       "   3262: True,\n",
       "   3263: True,\n",
       "   3264: True,\n",
       "   3265: False,\n",
       "   3266: False,\n",
       "   3267: True,\n",
       "   3268: True,\n",
       "   3269: True,\n",
       "   3270: False,\n",
       "   3271: True,\n",
       "   3272: True,\n",
       "   3273: True,\n",
       "   3275: True,\n",
       "   3276: True,\n",
       "   3277: False,\n",
       "   3278: True,\n",
       "   3279: True,\n",
       "   3280: True,\n",
       "   3281: True,\n",
       "   3282: False,\n",
       "   3283: False,\n",
       "   3284: False,\n",
       "   3285: True,\n",
       "   3286: False,\n",
       "   3287: True,\n",
       "   3288: True,\n",
       "   3289: True,\n",
       "   3290: True,\n",
       "   3291: True,\n",
       "   3292: True,\n",
       "   3293: True,\n",
       "   3294: True,\n",
       "   3295: True,\n",
       "   3296: True,\n",
       "   3297: True,\n",
       "   3298: True,\n",
       "   3299: True,\n",
       "   3300: False,\n",
       "   3301: False,\n",
       "   3303: True,\n",
       "   3304: True,\n",
       "   3305: True,\n",
       "   3306: True,\n",
       "   3307: False,\n",
       "   3308: True,\n",
       "   3309: True,\n",
       "   3310: True,\n",
       "   3311: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3317: True,\n",
       "   3318: True,\n",
       "   3319: True,\n",
       "   3320: True,\n",
       "   3321: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3325: True,\n",
       "   3326: False,\n",
       "   3327: True,\n",
       "   3328: False,\n",
       "   3329: True,\n",
       "   3332: True,\n",
       "   3333: False,\n",
       "   3334: True,\n",
       "   3335: True,\n",
       "   3336: False,\n",
       "   3337: True,\n",
       "   3339: False,\n",
       "   3340: False,\n",
       "   3341: True,\n",
       "   3342: False,\n",
       "   3343: True,\n",
       "   3344: True,\n",
       "   3345: True,\n",
       "   3346: True,\n",
       "   3347: False,\n",
       "   3348: False,\n",
       "   3349: True,\n",
       "   3350: True,\n",
       "   3351: False,\n",
       "   3352: False,\n",
       "   3353: True,\n",
       "   3354: True,\n",
       "   3355: False,\n",
       "   3356: False,\n",
       "   3357: True,\n",
       "   3358: False,\n",
       "   3359: False,\n",
       "   3360: True,\n",
       "   3361: False,\n",
       "   3362: False,\n",
       "   3363: False,\n",
       "   3364: True,\n",
       "   3365: True,\n",
       "   3367: False,\n",
       "   3368: False,\n",
       "   3371: True,\n",
       "   3372: False,\n",
       "   3374: False,\n",
       "   3375: False,\n",
       "   3376: True,\n",
       "   3377: True,\n",
       "   3378: True,\n",
       "   ...},\n",
       "  0.7671114937580714],\n",
       " 'di-mi-zo': [{10: False}, 0.0],\n",
       " 'qa-ra-su-ti-jo': [{11: False, 964: False, 3176: False}, 0.0],\n",
       " 'si-da-jo': [{12: False, 132: False, 3399: True}, 0.3333333333333333],\n",
       " 'RI': [{13: True,\n",
       "   5093: True,\n",
       "   5094: True,\n",
       "   5095: True,\n",
       "   5096: True,\n",
       "   5097: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5100: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5103: True,\n",
       "   5104: True,\n",
       "   5105: True,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5142: True,\n",
       "   5736: False},\n",
       "  0.9743589743589743],\n",
       " 'O': [{13: True,\n",
       "   5081: True,\n",
       "   5093: True,\n",
       "   5094: False,\n",
       "   5095: True,\n",
       "   5096: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5100: True,\n",
       "   5101: True,\n",
       "   5102: False,\n",
       "   5103: False,\n",
       "   5104: True,\n",
       "   5105: False,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5142: False,\n",
       "   5332: True,\n",
       "   5333: True,\n",
       "   5334: True,\n",
       "   5336: True,\n",
       "   5337: True,\n",
       "   5338: True,\n",
       "   5339: True,\n",
       "   5340: True,\n",
       "   5341: True,\n",
       "   5342: True,\n",
       "   5343: True,\n",
       "   5365: False,\n",
       "   5377: True,\n",
       "   5389: True,\n",
       "   5565: True},\n",
       "  0.9041095890410958],\n",
       " '2': [{13: False,\n",
       "   63: True,\n",
       "   71: True,\n",
       "   93: False,\n",
       "   96: False,\n",
       "   103: True,\n",
       "   104: True,\n",
       "   105: False,\n",
       "   113: True,\n",
       "   119: True,\n",
       "   129: True,\n",
       "   131: False,\n",
       "   135: True,\n",
       "   136: True,\n",
       "   142: True,\n",
       "   143: True,\n",
       "   152: False,\n",
       "   160: False,\n",
       "   163: False,\n",
       "   165: False,\n",
       "   172: True,\n",
       "   174: True,\n",
       "   186: False,\n",
       "   190: False,\n",
       "   201: False,\n",
       "   212: True,\n",
       "   213: True,\n",
       "   229: True,\n",
       "   254: False,\n",
       "   256: True,\n",
       "   273: False,\n",
       "   275: True,\n",
       "   276: True,\n",
       "   279: True,\n",
       "   284: True,\n",
       "   286: True,\n",
       "   289: True,\n",
       "   290: True,\n",
       "   292: False,\n",
       "   294: True,\n",
       "   312: True,\n",
       "   314: True,\n",
       "   315: True,\n",
       "   316: True,\n",
       "   319: True,\n",
       "   326: True,\n",
       "   332: True,\n",
       "   344: True,\n",
       "   361: True,\n",
       "   375: True,\n",
       "   378: True,\n",
       "   388: False,\n",
       "   399: True,\n",
       "   409: True,\n",
       "   462: True,\n",
       "   481: True,\n",
       "   495: False,\n",
       "   546: True,\n",
       "   549: True,\n",
       "   551: True,\n",
       "   553: True,\n",
       "   554: False,\n",
       "   561: False,\n",
       "   566: True,\n",
       "   582: True,\n",
       "   586: False,\n",
       "   598: True,\n",
       "   603: False,\n",
       "   610: True,\n",
       "   620: True,\n",
       "   641: False,\n",
       "   647: False,\n",
       "   656: True,\n",
       "   665: False,\n",
       "   666: True,\n",
       "   677: True,\n",
       "   706: False,\n",
       "   723: False,\n",
       "   962: True,\n",
       "   1092: True,\n",
       "   1096: True,\n",
       "   1122: False,\n",
       "   1182: True,\n",
       "   1263: False,\n",
       "   1273: False,\n",
       "   1296: False,\n",
       "   1411: False,\n",
       "   1483: False,\n",
       "   1562: False,\n",
       "   1615: False,\n",
       "   1634: False,\n",
       "   1635: False,\n",
       "   1642: False,\n",
       "   1687: False,\n",
       "   1688: False,\n",
       "   1713: False,\n",
       "   1766: False,\n",
       "   1810: False,\n",
       "   1854: False,\n",
       "   1862: False,\n",
       "   1880: False,\n",
       "   1882: False,\n",
       "   1883: False,\n",
       "   1979: False,\n",
       "   1992: False,\n",
       "   2050: False,\n",
       "   2057: False,\n",
       "   2059: False,\n",
       "   2119: False,\n",
       "   2120: False,\n",
       "   2121: False,\n",
       "   2148: False,\n",
       "   2308: False,\n",
       "   2316: True,\n",
       "   2326: True,\n",
       "   2328: True,\n",
       "   2329: True,\n",
       "   2330: False,\n",
       "   2331: False,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2336: True,\n",
       "   2339: True,\n",
       "   2340: True,\n",
       "   2352: True,\n",
       "   2353: True,\n",
       "   2356: False,\n",
       "   2360: True,\n",
       "   2367: True,\n",
       "   2368: True,\n",
       "   2369: True,\n",
       "   2372: True,\n",
       "   2374: False,\n",
       "   2382: True,\n",
       "   2425: True,\n",
       "   2436: False,\n",
       "   2489: True,\n",
       "   2490: True,\n",
       "   2499: False,\n",
       "   2501: True,\n",
       "   2503: True,\n",
       "   2504: False,\n",
       "   2507: True,\n",
       "   2517: True,\n",
       "   2522: True,\n",
       "   2525: False,\n",
       "   2528: False,\n",
       "   2530: False,\n",
       "   2546: False,\n",
       "   2548: False,\n",
       "   2550: True,\n",
       "   2551: True,\n",
       "   2554: False,\n",
       "   2556: True,\n",
       "   2557: True,\n",
       "   2558: True,\n",
       "   2559: True,\n",
       "   2563: False,\n",
       "   2564: True,\n",
       "   2568: False,\n",
       "   2570: True,\n",
       "   2572: True,\n",
       "   2574: True,\n",
       "   2575: True,\n",
       "   2576: False,\n",
       "   2583: True,\n",
       "   2587: True,\n",
       "   2589: True,\n",
       "   2591: True,\n",
       "   2594: True,\n",
       "   2599: True,\n",
       "   2603: True,\n",
       "   2612: False,\n",
       "   2614: False,\n",
       "   2627: False,\n",
       "   2632: True,\n",
       "   2644: True,\n",
       "   2647: False,\n",
       "   2648: True,\n",
       "   2650: True,\n",
       "   2653: False,\n",
       "   2654: True,\n",
       "   2662: True,\n",
       "   2669: True,\n",
       "   2672: False,\n",
       "   2684: False,\n",
       "   2685: False,\n",
       "   2690: True,\n",
       "   2702: True,\n",
       "   2706: True,\n",
       "   2716: True,\n",
       "   2717: True,\n",
       "   2746: True,\n",
       "   2758: False,\n",
       "   2765: False,\n",
       "   2771: False,\n",
       "   2772: False,\n",
       "   2797: True,\n",
       "   2800: False,\n",
       "   2813: True,\n",
       "   2826: True,\n",
       "   2866: True,\n",
       "   2869: True,\n",
       "   2872: True,\n",
       "   2914: False,\n",
       "   3054: True,\n",
       "   3111: True,\n",
       "   3134: False,\n",
       "   3158: True,\n",
       "   3177: True,\n",
       "   3185: True,\n",
       "   3187: True,\n",
       "   3198: True,\n",
       "   3204: True,\n",
       "   3211: True,\n",
       "   3214: True,\n",
       "   3217: True,\n",
       "   3225: True,\n",
       "   3227: True,\n",
       "   3244: True,\n",
       "   3249: True,\n",
       "   3250: True,\n",
       "   3254: True,\n",
       "   3255: True,\n",
       "   3256: True,\n",
       "   3260: False,\n",
       "   3284: True,\n",
       "   3285: True,\n",
       "   3288: True,\n",
       "   3290: True,\n",
       "   3294: False,\n",
       "   3296: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3318: True,\n",
       "   3319: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3332: True,\n",
       "   3339: True,\n",
       "   3369: True,\n",
       "   3373: False,\n",
       "   3379: False,\n",
       "   3381: True,\n",
       "   3388: False,\n",
       "   3392: True,\n",
       "   3394: True,\n",
       "   3397: True,\n",
       "   3400: False,\n",
       "   3401: True,\n",
       "   3406: False,\n",
       "   3408: False,\n",
       "   3410: False,\n",
       "   3414: True,\n",
       "   3425: True,\n",
       "   3428: True,\n",
       "   3444: False,\n",
       "   3447: True,\n",
       "   3455: True,\n",
       "   3469: True,\n",
       "   3501: False,\n",
       "   3544: True,\n",
       "   3545: False,\n",
       "   3575: False,\n",
       "   3581: False,\n",
       "   3609: True,\n",
       "   3678: False,\n",
       "   3735: False,\n",
       "   3757: False,\n",
       "   3761: False,\n",
       "   3790: False,\n",
       "   3797: False,\n",
       "   3819: False,\n",
       "   3841: False,\n",
       "   3842: False,\n",
       "   3843: False,\n",
       "   3844: False,\n",
       "   3865: False,\n",
       "   3870: True,\n",
       "   3872: True,\n",
       "   3875: False,\n",
       "   3882: True,\n",
       "   3885: False,\n",
       "   3900: True,\n",
       "   3901: True,\n",
       "   3906: True,\n",
       "   3907: True,\n",
       "   3908: True,\n",
       "   3915: True,\n",
       "   3916: True,\n",
       "   3926: True,\n",
       "   3932: False,\n",
       "   3933: False,\n",
       "   3935: True,\n",
       "   3936: True,\n",
       "   3937: True,\n",
       "   3941: False,\n",
       "   3942: True,\n",
       "   3945: True,\n",
       "   3951: True,\n",
       "   3955: True,\n",
       "   3956: True,\n",
       "   3970: True,\n",
       "   3972: False,\n",
       "   3992: True,\n",
       "   3997: True,\n",
       "   3998: True,\n",
       "   4002: True,\n",
       "   4008: True,\n",
       "   4034: False,\n",
       "   4039: True,\n",
       "   4042: False,\n",
       "   4055: False,\n",
       "   4059: True,\n",
       "   4067: True,\n",
       "   4088: True,\n",
       "   4093: True,\n",
       "   4094: True,\n",
       "   4098: True,\n",
       "   4099: True,\n",
       "   4100: True,\n",
       "   4101: True,\n",
       "   4104: True,\n",
       "   4106: True,\n",
       "   4107: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4125: True,\n",
       "   4131: True,\n",
       "   4138: True,\n",
       "   4145: False,\n",
       "   4151: False,\n",
       "   4160: True,\n",
       "   4162: True,\n",
       "   4165: True,\n",
       "   4174: True,\n",
       "   4177: False,\n",
       "   4178: False,\n",
       "   4181: False,\n",
       "   4182: True,\n",
       "   4185: False,\n",
       "   4186: True,\n",
       "   4187: True,\n",
       "   4193: True,\n",
       "   4201: True,\n",
       "   4203: True,\n",
       "   4213: False,\n",
       "   4227: True,\n",
       "   4249: True,\n",
       "   4250: True,\n",
       "   4268: False,\n",
       "   4269: True,\n",
       "   4270: True,\n",
       "   4275: True,\n",
       "   4280: True,\n",
       "   4285: True,\n",
       "   4286: True,\n",
       "   4289: True,\n",
       "   4300: True,\n",
       "   4305: True,\n",
       "   4326: False,\n",
       "   4339: False,\n",
       "   4358: True,\n",
       "   4363: True,\n",
       "   4376: True,\n",
       "   4377: False,\n",
       "   4378: False,\n",
       "   4381: True,\n",
       "   4399: True,\n",
       "   4401: False,\n",
       "   4402: False,\n",
       "   4409: False,\n",
       "   4412: True,\n",
       "   4417: True,\n",
       "   4421: True,\n",
       "   4439: True,\n",
       "   4443: True,\n",
       "   4456: True,\n",
       "   4457: True,\n",
       "   4458: False,\n",
       "   4465: True,\n",
       "   4466: True,\n",
       "   4467: True,\n",
       "   4504: False,\n",
       "   4509: False,\n",
       "   4559: True,\n",
       "   4564: True,\n",
       "   4569: True,\n",
       "   4570: True,\n",
       "   4572: True,\n",
       "   4585: True,\n",
       "   4586: True,\n",
       "   4592: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4603: False,\n",
       "   4605: True,\n",
       "   4613: True,\n",
       "   4616: True,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4628: False,\n",
       "   4630: False,\n",
       "   4639: True,\n",
       "   4651: True,\n",
       "   4664: True,\n",
       "   4667: True,\n",
       "   4668: True,\n",
       "   4689: False,\n",
       "   4695: True,\n",
       "   4698: True,\n",
       "   4699: True,\n",
       "   4702: False,\n",
       "   4707: True,\n",
       "   4709: True,\n",
       "   4715: True,\n",
       "   4716: True,\n",
       "   4731: True,\n",
       "   4732: True,\n",
       "   4734: True,\n",
       "   4759: False,\n",
       "   4769: True,\n",
       "   4772: True,\n",
       "   4783: True,\n",
       "   4784: True,\n",
       "   4791: True,\n",
       "   4793: True,\n",
       "   4794: True,\n",
       "   4797: True,\n",
       "   4798: True,\n",
       "   4805: True,\n",
       "   4808: True,\n",
       "   4812: True,\n",
       "   4814: False,\n",
       "   4815: True,\n",
       "   4817: True,\n",
       "   4818: True,\n",
       "   4821: True,\n",
       "   4824: False,\n",
       "   4825: True,\n",
       "   4826: True,\n",
       "   4828: False,\n",
       "   4834: True,\n",
       "   4846: True,\n",
       "   4852: True,\n",
       "   4855: True,\n",
       "   4856: True,\n",
       "   4862: True,\n",
       "   4865: True,\n",
       "   4882: True,\n",
       "   4887: True,\n",
       "   4890: True,\n",
       "   4898: True,\n",
       "   4902: True,\n",
       "   4903: True,\n",
       "   4908: True,\n",
       "   4909: False,\n",
       "   4916: True,\n",
       "   4918: True,\n",
       "   4921: False,\n",
       "   4922: True,\n",
       "   4926: True,\n",
       "   4928: True,\n",
       "   4931: True,\n",
       "   4932: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4935: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4939: True,\n",
       "   4940: True,\n",
       "   4944: True,\n",
       "   4946: True,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   4955: True,\n",
       "   4961: True,\n",
       "   4962: True,\n",
       "   4964: True,\n",
       "   4966: True,\n",
       "   4967: True,\n",
       "   4970: True,\n",
       "   4972: False,\n",
       "   4976: True,\n",
       "   4982: True,\n",
       "   4983: False,\n",
       "   4984: True,\n",
       "   4985: True,\n",
       "   4986: True,\n",
       "   4987: True,\n",
       "   4990: True,\n",
       "   4997: False,\n",
       "   5003: True,\n",
       "   5005: False,\n",
       "   5008: True,\n",
       "   5010: True,\n",
       "   5013: True,\n",
       "   5017: True,\n",
       "   5020: True,\n",
       "   5024: True,\n",
       "   5025: True,\n",
       "   5052: True,\n",
       "   5053: True,\n",
       "   5057: True,\n",
       "   5059: True,\n",
       "   5063: False,\n",
       "   5065: True,\n",
       "   5066: True,\n",
       "   5070: True,\n",
       "   5071: True,\n",
       "   5072: True,\n",
       "   5074: True,\n",
       "   5075: True,\n",
       "   5076: True,\n",
       "   5077: True,\n",
       "   5078: True,\n",
       "   5081: True,\n",
       "   5093: True,\n",
       "   5094: True,\n",
       "   5095: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5115: True,\n",
       "   5124: True,\n",
       "   5128: True,\n",
       "   5131: False,\n",
       "   5142: True,\n",
       "   5144: True,\n",
       "   5151: False,\n",
       "   5158: True,\n",
       "   5163: True,\n",
       "   5228: True,\n",
       "   5260: True,\n",
       "   5262: False,\n",
       "   5264: False,\n",
       "   5274: True,\n",
       "   5277: True,\n",
       "   5282: True,\n",
       "   5284: True,\n",
       "   5285: True,\n",
       "   5291: True,\n",
       "   5293: True,\n",
       "   5298: True,\n",
       "   5300: False,\n",
       "   5305: True,\n",
       "   5309: True,\n",
       "   5311: True,\n",
       "   5314: True,\n",
       "   5318: True,\n",
       "   5321: True,\n",
       "   5332: True,\n",
       "   5333: True,\n",
       "   5334: True,\n",
       "   5336: True,\n",
       "   5337: True,\n",
       "   5338: True,\n",
       "   5339: True,\n",
       "   5340: True,\n",
       "   5341: True,\n",
       "   5342: True,\n",
       "   5343: True,\n",
       "   5344: True,\n",
       "   5348: True,\n",
       "   5353: True,\n",
       "   5354: True,\n",
       "   5357: True,\n",
       "   5358: True,\n",
       "   5359: True,\n",
       "   5360: True,\n",
       "   5361: True,\n",
       "   5366: True,\n",
       "   5369: True,\n",
       "   5372: True,\n",
       "   5373: True,\n",
       "   5374: True,\n",
       "   5376: True,\n",
       "   5377: False,\n",
       "   5378: True,\n",
       "   5379: True,\n",
       "   5382: True,\n",
       "   5384: True,\n",
       "   5385: True,\n",
       "   5386: True,\n",
       "   5389: True,\n",
       "   5392: True,\n",
       "   5393: True,\n",
       "   5395: True,\n",
       "   5399: False,\n",
       "   5401: True,\n",
       "   5403: True,\n",
       "   5408: True,\n",
       "   5409: True,\n",
       "   5412: False,\n",
       "   5495: False,\n",
       "   5504: False,\n",
       "   5576: False,\n",
       "   5621: False,\n",
       "   5720: True},\n",
       "  0.7867867867867868],\n",
       " 'KE': [{13: True,\n",
       "   4205: True,\n",
       "   5093: True,\n",
       "   5094: True,\n",
       "   5095: True,\n",
       "   5096: True,\n",
       "   5097: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5100: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5103: True,\n",
       "   5104: True,\n",
       "   5105: True,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5377: True},\n",
       "  1.0],\n",
       " '*146': [{13: True,\n",
       "   766: False,\n",
       "   767: False,\n",
       "   768: True,\n",
       "   771: False,\n",
       "   773: False,\n",
       "   4465: True,\n",
       "   4466: True,\n",
       "   4467: True,\n",
       "   4468: True,\n",
       "   4470: True,\n",
       "   4471: True,\n",
       "   4473: True,\n",
       "   4475: True,\n",
       "   4517: True,\n",
       "   4698: True,\n",
       "   5093: True,\n",
       "   5094: True,\n",
       "   5095: True,\n",
       "   5096: True,\n",
       "   5097: True,\n",
       "   5098: True,\n",
       "   5099: False,\n",
       "   5100: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5103: True,\n",
       "   5104: True,\n",
       "   5105: True,\n",
       "   5106: True,\n",
       "   5107: True,\n",
       "   5108: True,\n",
       "   5109: True,\n",
       "   5110: True,\n",
       "   5111: True,\n",
       "   5112: False,\n",
       "   5114: True,\n",
       "   5115: True,\n",
       "   5116: False,\n",
       "   5117: True,\n",
       "   5118: True,\n",
       "   5119: True,\n",
       "   5120: True,\n",
       "   5121: True,\n",
       "   5122: True,\n",
       "   5123: True,\n",
       "   5124: True,\n",
       "   5125: False,\n",
       "   5126: False,\n",
       "   5127: True,\n",
       "   5128: False,\n",
       "   5129: True,\n",
       "   5130: True,\n",
       "   5131: True,\n",
       "   5132: True,\n",
       "   5133: False,\n",
       "   5134: True,\n",
       "   5135: False,\n",
       "   5136: True,\n",
       "   5137: True,\n",
       "   5138: True,\n",
       "   5139: True,\n",
       "   5140: True,\n",
       "   5141: False,\n",
       "   5143: True,\n",
       "   5144: True,\n",
       "   5145: True,\n",
       "   5146: True,\n",
       "   5147: False,\n",
       "   5148: True,\n",
       "   5149: True,\n",
       "   5150: True,\n",
       "   5151: True,\n",
       "   5152: True,\n",
       "   5153: True,\n",
       "   5154: True,\n",
       "   5155: True,\n",
       "   5362: True,\n",
       "   5367: True,\n",
       "   5373: True,\n",
       "   5374: True,\n",
       "   5381: True,\n",
       "   5386: True,\n",
       "   5395: True},\n",
       "  0.8776978417266187],\n",
       " 'wi-pi-o': [{14: True}, 1.0],\n",
       " 'N': [{14: True,\n",
       "   67: True,\n",
       "   72: False,\n",
       "   74: True,\n",
       "   79: True,\n",
       "   80: False,\n",
       "   81: True,\n",
       "   84: True,\n",
       "   85: True,\n",
       "   87: True,\n",
       "   89: True,\n",
       "   93: False,\n",
       "   95: True,\n",
       "   102: False,\n",
       "   103: False,\n",
       "   106: False,\n",
       "   135: True,\n",
       "   172: False,\n",
       "   212: False,\n",
       "   229: False,\n",
       "   231: False,\n",
       "   1902: False,\n",
       "   2758: True,\n",
       "   2812: False,\n",
       "   4141: True,\n",
       "   4142: True,\n",
       "   4185: True,\n",
       "   4286: True,\n",
       "   4378: False,\n",
       "   4421: True,\n",
       "   5056: True,\n",
       "   5057: True,\n",
       "   5059: True,\n",
       "   5065: True,\n",
       "   5071: True,\n",
       "   5072: True,\n",
       "   5074: True,\n",
       "   5075: True,\n",
       "   5076: True,\n",
       "   5077: True,\n",
       "   5093: True,\n",
       "   5107: True,\n",
       "   5363: True},\n",
       "  0.8360655737704918],\n",
       " 'e': [{15: False,\n",
       "   37: False,\n",
       "   598: True,\n",
       "   697: False,\n",
       "   762: False,\n",
       "   916: False,\n",
       "   954: False,\n",
       "   1032: False,\n",
       "   1087: False,\n",
       "   1203: False,\n",
       "   1212: False,\n",
       "   1216: False,\n",
       "   1341: False,\n",
       "   1406: False,\n",
       "   1473: False,\n",
       "   1482: False,\n",
       "   1569: False,\n",
       "   1607: False,\n",
       "   1839: False,\n",
       "   1908: False,\n",
       "   2190: False,\n",
       "   2191: False,\n",
       "   2580: False,\n",
       "   2589: False,\n",
       "   2599: False,\n",
       "   2636: False,\n",
       "   2648: False,\n",
       "   2672: False,\n",
       "   2687: False,\n",
       "   2708: False,\n",
       "   2714: False,\n",
       "   2732: False,\n",
       "   2748: False,\n",
       "   3587: True,\n",
       "   3623: False,\n",
       "   3629: False,\n",
       "   3907: False,\n",
       "   4284: False,\n",
       "   4452: False,\n",
       "   4453: False,\n",
       "   4464: True,\n",
       "   4729: False,\n",
       "   4766: False,\n",
       "   4791: False,\n",
       "   4818: False,\n",
       "   4956: False,\n",
       "   5045: False,\n",
       "   5071: False,\n",
       "   5146: False,\n",
       "   5224: False,\n",
       "   5226: False,\n",
       "   5264: False,\n",
       "   5377: False,\n",
       "   5412: False,\n",
       "   5484: False,\n",
       "   5519: False,\n",
       "   5558: False,\n",
       "   5675: False,\n",
       "   5737: False},\n",
       "  0.21333333333333335],\n",
       " 'ku': [{16: False,\n",
       "   51: False,\n",
       "   962: False,\n",
       "   1115: False,\n",
       "   1310: False,\n",
       "   1403: False,\n",
       "   1590: False,\n",
       "   1752: False,\n",
       "   1916: False,\n",
       "   1967: False,\n",
       "   2074: False,\n",
       "   2701: False,\n",
       "   2752: False,\n",
       "   3902: False,\n",
       "   4212: False,\n",
       "   5372: False},\n",
       "  0.0],\n",
       " 'ta': [{17: False,\n",
       "   171: False,\n",
       "   279: False,\n",
       "   403: False,\n",
       "   572: False,\n",
       "   624: False,\n",
       "   661: False,\n",
       "   750: False,\n",
       "   990: False,\n",
       "   1048: False,\n",
       "   1091: False,\n",
       "   1104: False,\n",
       "   1293: False,\n",
       "   1300: False,\n",
       "   1304: False,\n",
       "   1362: False,\n",
       "   1420: False,\n",
       "   1448: False,\n",
       "   1540: False,\n",
       "   1598: False,\n",
       "   1669: False,\n",
       "   1670: False,\n",
       "   1754: False,\n",
       "   1844: False,\n",
       "   1923: False,\n",
       "   2004: False,\n",
       "   2031: False,\n",
       "   2170: False,\n",
       "   2173: False,\n",
       "   2180: False,\n",
       "   2402: True,\n",
       "   2689: False,\n",
       "   2707: False,\n",
       "   2775: True,\n",
       "   3504: False,\n",
       "   3534: False,\n",
       "   3556: False,\n",
       "   4255: False,\n",
       "   4265: False,\n",
       "   4713: False,\n",
       "   4723: False,\n",
       "   4752: False,\n",
       "   4767: False,\n",
       "   4775: False,\n",
       "   5058: False,\n",
       "   5409: False,\n",
       "   5419: False,\n",
       "   5553: False,\n",
       "   5554: False,\n",
       "   5560: False,\n",
       "   5738: False},\n",
       "  0.038461538461538464],\n",
       " 'wa-ko': [{19: False, 2624: False}, 0.0],\n",
       " 'jo': [{24: False,\n",
       "   27: False,\n",
       "   34: False,\n",
       "   75: False,\n",
       "   84: False,\n",
       "   109: False,\n",
       "   123: False,\n",
       "   303: False,\n",
       "   320: False,\n",
       "   337: False,\n",
       "   368: False,\n",
       "   386: False,\n",
       "   398: False,\n",
       "   413: False,\n",
       "   454: False,\n",
       "   467: False,\n",
       "   672: False,\n",
       "   703: False,\n",
       "   712: False,\n",
       "   759: False,\n",
       "   763: False,\n",
       "   847: True,\n",
       "   1232: False,\n",
       "   1271: False,\n",
       "   1473: False,\n",
       "   1528: False,\n",
       "   1562: False,\n",
       "   1575: False,\n",
       "   1578: False,\n",
       "   1761: False,\n",
       "   1792: False,\n",
       "   1857: False,\n",
       "   1909: False,\n",
       "   1910: False,\n",
       "   2018: False,\n",
       "   2030: False,\n",
       "   2103: False,\n",
       "   2117: False,\n",
       "   2133: False,\n",
       "   2143: False,\n",
       "   2305: False,\n",
       "   2560: False,\n",
       "   2587: False,\n",
       "   2647: False,\n",
       "   2669: False,\n",
       "   2682: False,\n",
       "   2708: False,\n",
       "   2721: False,\n",
       "   2738: False,\n",
       "   2740: False,\n",
       "   2761: False,\n",
       "   2936: False,\n",
       "   2992: False,\n",
       "   3249: False,\n",
       "   3520: False,\n",
       "   3531: False,\n",
       "   3547: False,\n",
       "   3636: False,\n",
       "   3641: False,\n",
       "   3762: False,\n",
       "   4096: False,\n",
       "   4099: False,\n",
       "   4137: False,\n",
       "   4180: False,\n",
       "   4214: False,\n",
       "   4224: False,\n",
       "   4234: False,\n",
       "   4242: False,\n",
       "   4292: False,\n",
       "   4487: False,\n",
       "   4504: False,\n",
       "   4509: False,\n",
       "   4699: False,\n",
       "   4707: False,\n",
       "   4717: False,\n",
       "   4738: False,\n",
       "   5041: False,\n",
       "   5075: False,\n",
       "   5077: False,\n",
       "   5111: False,\n",
       "   5150: False,\n",
       "   5207: False,\n",
       "   5393: False,\n",
       "   5435: False,\n",
       "   5454: False,\n",
       "   5537: False,\n",
       "   5542: False,\n",
       "   5572: False,\n",
       "   5673: False,\n",
       "   5679: False,\n",
       "   5680: False,\n",
       "   5681: False},\n",
       "  0.010869565217391304],\n",
       " 'ta-jo': [{25: False, 5076: False}, 0.0],\n",
       " 'sa-na-so': [{26: False}, 0.0],\n",
       " 'du': [{29: False,\n",
       "   837: False,\n",
       "   1347: False,\n",
       "   1960: False,\n",
       "   2013: False,\n",
       "   2014: False,\n",
       "   2205: False,\n",
       "   2304: False,\n",
       "   2733: False,\n",
       "   3802: False,\n",
       "   5756: False},\n",
       "  0.0],\n",
       " 'ko-ri-jo': [{30: False, 3090: True}, 0.5],\n",
       " 'zo': [{32: False,\n",
       "   361: False,\n",
       "   702: False,\n",
       "   1121: False,\n",
       "   1197: False,\n",
       "   1760: False,\n",
       "   2101: False,\n",
       "   2311: False,\n",
       "   3136: False,\n",
       "   4318: False},\n",
       "  0.0],\n",
       " 'e-si': [{33: False}, 0.0],\n",
       " 'o-pe': [{33: False, 1373: False, 2364: False, 2708: True, 4729: False}, 0.2],\n",
       " 'i-we': [{34: True, 2839: False}, 0.5],\n",
       " 'ro': [{36: False,\n",
       "   79: False,\n",
       "   586: False,\n",
       "   610: False,\n",
       "   714: False,\n",
       "   882: False,\n",
       "   922: False,\n",
       "   1429: False,\n",
       "   1593: False,\n",
       "   1724: False,\n",
       "   1752: False,\n",
       "   1843: False,\n",
       "   2075: False,\n",
       "   2140: False,\n",
       "   2171: False,\n",
       "   2407: False,\n",
       "   2651: False,\n",
       "   2683: False,\n",
       "   2899: False,\n",
       "   3011: False,\n",
       "   3024: False,\n",
       "   3029: False,\n",
       "   3041: False,\n",
       "   3242: False,\n",
       "   3436: False,\n",
       "   3499: False,\n",
       "   3550: False,\n",
       "   3658: False,\n",
       "   3659: False,\n",
       "   3906: False,\n",
       "   3909: False,\n",
       "   3917: False,\n",
       "   4127: False,\n",
       "   4179: False,\n",
       "   4248: False,\n",
       "   4267: False,\n",
       "   4722: False,\n",
       "   4732: False,\n",
       "   5030: False,\n",
       "   5068: False,\n",
       "   5144: False,\n",
       "   5405: False,\n",
       "   5623: False},\n",
       "  0.0],\n",
       " 'e-ri': [{38: False,\n",
       "   579: False,\n",
       "   676: False,\n",
       "   4282: False,\n",
       "   4988: False,\n",
       "   5461: False},\n",
       "  0.0],\n",
       " 'no-ro': [{40: False}, 0.0],\n",
       " 'ra-ma': [{41: False}, 0.0],\n",
       " 'te-we': [{42: False, 721: False, 1484: False, 2386: False, 4319: False},\n",
       "  0.0],\n",
       " 'tu': [{43: False,\n",
       "   944: True,\n",
       "   1279: False,\n",
       "   1604: False,\n",
       "   2652: True,\n",
       "   2653: True,\n",
       "   2654: True,\n",
       "   2659: True,\n",
       "   3956: False,\n",
       "   4422: False,\n",
       "   5412: False,\n",
       "   5681: False},\n",
       "  0.5],\n",
       " 'ja': [{48: False,\n",
       "   54: False,\n",
       "   63: False,\n",
       "   136: False,\n",
       "   391: False,\n",
       "   406: False,\n",
       "   521: False,\n",
       "   617: False,\n",
       "   713: False,\n",
       "   744: False,\n",
       "   955: False,\n",
       "   973: False,\n",
       "   977: False,\n",
       "   1010: False,\n",
       "   1059: False,\n",
       "   1501: False,\n",
       "   1541: False,\n",
       "   1573: False,\n",
       "   1680: False,\n",
       "   1712: False,\n",
       "   1739: False,\n",
       "   1740: False,\n",
       "   1741: False,\n",
       "   1743: False,\n",
       "   1790: False,\n",
       "   1830: False,\n",
       "   1831: False,\n",
       "   1864: False,\n",
       "   1865: False,\n",
       "   1870: False,\n",
       "   1962: False,\n",
       "   1998: False,\n",
       "   2016: False,\n",
       "   2017: False,\n",
       "   2067: False,\n",
       "   2068: False,\n",
       "   2150: False,\n",
       "   2161: False,\n",
       "   2170: False,\n",
       "   2182: False,\n",
       "   2389: False,\n",
       "   2435: False,\n",
       "   2501: False,\n",
       "   2511: False,\n",
       "   2585: False,\n",
       "   2601: False,\n",
       "   2614: False,\n",
       "   2617: False,\n",
       "   2623: False,\n",
       "   2624: False,\n",
       "   2654: False,\n",
       "   2655: False,\n",
       "   2827: False,\n",
       "   2839: False,\n",
       "   2917: False,\n",
       "   2918: False,\n",
       "   3042: False,\n",
       "   3129: False,\n",
       "   3212: False,\n",
       "   3391: False,\n",
       "   3746: False,\n",
       "   3908: False,\n",
       "   4080: False,\n",
       "   4384: False,\n",
       "   4418: False,\n",
       "   4457: False,\n",
       "   4459: False,\n",
       "   4462: False,\n",
       "   4475: False,\n",
       "   4692: False,\n",
       "   4696: False,\n",
       "   4723: False,\n",
       "   4737: False,\n",
       "   5075: False,\n",
       "   5233: False,\n",
       "   5411: False,\n",
       "   5415: False,\n",
       "   5578: False,\n",
       "   5700: False},\n",
       "  0.0],\n",
       " 'ka': [{50: False,\n",
       "   129: False,\n",
       "   225: False,\n",
       "   484: False,\n",
       "   570: False,\n",
       "   724: False,\n",
       "   926: False,\n",
       "   1205: False,\n",
       "   1215: False,\n",
       "   1248: False,\n",
       "   1524: False,\n",
       "   1579: False,\n",
       "   1580: False,\n",
       "   1722: False,\n",
       "   1744: False,\n",
       "   1794: False,\n",
       "   1867: False,\n",
       "   1934: False,\n",
       "   2020: False,\n",
       "   2021: False,\n",
       "   2064: False,\n",
       "   2069: False,\n",
       "   2134: False,\n",
       "   2339: False,\n",
       "   2411: True,\n",
       "   2417: False,\n",
       "   2780: False,\n",
       "   3291: True,\n",
       "   3917: False,\n",
       "   4261: False,\n",
       "   4271: False,\n",
       "   4273: False,\n",
       "   4331: False,\n",
       "   4720: True,\n",
       "   4766: False,\n",
       "   5375: False,\n",
       "   5384: True,\n",
       "   5412: False,\n",
       "   5466: False,\n",
       "   5538: False,\n",
       "   5703: False,\n",
       "   5729: False,\n",
       "   5740: False,\n",
       "   5754: False,\n",
       "   5763: False,\n",
       "   5768: False,\n",
       "   5770: False},\n",
       "  0.08163265306122448],\n",
       " 'ra-ro': [{52: False, 3102: False}, 0.0],\n",
       " 'de': [{55: False,\n",
       "   1420: False,\n",
       "   1433: False,\n",
       "   1466: False,\n",
       "   1523: False,\n",
       "   1571: False,\n",
       "   1735: False,\n",
       "   1742: False,\n",
       "   2130: False,\n",
       "   2268: False,\n",
       "   2317: False,\n",
       "   2414: False,\n",
       "   2487: False,\n",
       "   2550: True,\n",
       "   2559: True,\n",
       "   2591: False,\n",
       "   2595: False,\n",
       "   2626: False,\n",
       "   2714: False,\n",
       "   3987: False,\n",
       "   3992: False,\n",
       "   4001: False,\n",
       "   4019: False,\n",
       "   4076: False,\n",
       "   4101: False,\n",
       "   4118: False,\n",
       "   4120: False,\n",
       "   4933: True,\n",
       "   5002: False,\n",
       "   5127: False,\n",
       "   5265: True,\n",
       "   5266: True,\n",
       "   5420: False,\n",
       "   5571: False},\n",
       "  0.16666666666666666],\n",
       " 'qa': [{58: False,\n",
       "   241: False,\n",
       "   743: False,\n",
       "   745: False,\n",
       "   993: False,\n",
       "   1031: False,\n",
       "   1117: False,\n",
       "   1339: False,\n",
       "   1391: False,\n",
       "   1459: False,\n",
       "   1592: False,\n",
       "   1646: False,\n",
       "   1651: False,\n",
       "   1919: False,\n",
       "   1926: False,\n",
       "   3512: False,\n",
       "   3603: False,\n",
       "   4018: False,\n",
       "   4079: False,\n",
       "   4505: False,\n",
       "   5553: False},\n",
       "  0.0],\n",
       " 'ka-ta-ra': [{59: True}, 1.0],\n",
       " 'CROC': [{59: True,\n",
       "   60: True,\n",
       "   61: True,\n",
       "   62: True,\n",
       "   63: True,\n",
       "   64: False,\n",
       "   65: True,\n",
       "   66: True,\n",
       "   67: False,\n",
       "   68: False,\n",
       "   69: False,\n",
       "   71: True,\n",
       "   72: True,\n",
       "   73: True,\n",
       "   74: True,\n",
       "   75: True,\n",
       "   76: True,\n",
       "   77: True,\n",
       "   78: True,\n",
       "   79: True,\n",
       "   80: True,\n",
       "   81: False,\n",
       "   83: False,\n",
       "   84: True,\n",
       "   85: True,\n",
       "   87: False,\n",
       "   88: False,\n",
       "   89: False,\n",
       "   90: True,\n",
       "   91: False,\n",
       "   92: False,\n",
       "   93: False,\n",
       "   94: False,\n",
       "   95: False,\n",
       "   97: False,\n",
       "   106: False,\n",
       "   107: False,\n",
       "   109: False,\n",
       "   110: False,\n",
       "   114: False,\n",
       "   117: False,\n",
       "   2812: True},\n",
       "  0.5],\n",
       " 'QI': [{59: True, 63: True, 64: True, 69: True}, 1.0],\n",
       " 'sa-ma-da': [{60: True}, 1.0],\n",
       " 'RO': [{60: False, 61: True, 64: True, 99: False, 213: True}, 0.6],\n",
       " 'to': [{61: False,\n",
       "   210: False,\n",
       "   223: False,\n",
       "   379: True,\n",
       "   456: False,\n",
       "   574: False,\n",
       "   686: False,\n",
       "   697: False,\n",
       "   716: False,\n",
       "   900: False,\n",
       "   963: False,\n",
       "   1080: False,\n",
       "   1153: False,\n",
       "   1198: False,\n",
       "   1245: False,\n",
       "   1379: False,\n",
       "   1423: False,\n",
       "   1603: False,\n",
       "   1700: False,\n",
       "   1757: False,\n",
       "   1803: False,\n",
       "   1872: False,\n",
       "   1873: False,\n",
       "   2144: False,\n",
       "   2175: False,\n",
       "   2671: False,\n",
       "   2677: False,\n",
       "   2684: False,\n",
       "   2690: False,\n",
       "   2701: False,\n",
       "   2887: False,\n",
       "   3026: False,\n",
       "   3030: False,\n",
       "   3132: False,\n",
       "   3418: False,\n",
       "   3419: False,\n",
       "   3443: False,\n",
       "   3543: False,\n",
       "   3658: False,\n",
       "   3736: False,\n",
       "   3754: False,\n",
       "   3774: False,\n",
       "   3828: False,\n",
       "   3874: False,\n",
       "   3906: False,\n",
       "   4151: False,\n",
       "   4200: False,\n",
       "   4202: False,\n",
       "   4211: False,\n",
       "   4221: False,\n",
       "   4235: False,\n",
       "   4497: False,\n",
       "   4709: False,\n",
       "   4717: False,\n",
       "   4778: False,\n",
       "   4801: True,\n",
       "   4948: True,\n",
       "   4960: False,\n",
       "   5146: False,\n",
       "   5389: True,\n",
       "   5730: False},\n",
       "  0.06451612903225806],\n",
       " 'ni-ja': [{62: False,\n",
       "   598: False,\n",
       "   689: False,\n",
       "   1290: False,\n",
       "   1799: False,\n",
       "   1921: False,\n",
       "   2043: False,\n",
       "   4052: False,\n",
       "   4168: False},\n",
       "  0.0],\n",
       " 'P': [{63: True,\n",
       "   66: True,\n",
       "   67: True,\n",
       "   71: True,\n",
       "   73: True,\n",
       "   77: True,\n",
       "   78: True,\n",
       "   82: True,\n",
       "   83: True,\n",
       "   86: False,\n",
       "   90: False,\n",
       "   93: True,\n",
       "   96: True,\n",
       "   97: True,\n",
       "   98: True,\n",
       "   104: False,\n",
       "   105: True,\n",
       "   108: True,\n",
       "   110: False,\n",
       "   111: False,\n",
       "   113: False,\n",
       "   115: True,\n",
       "   118: True,\n",
       "   135: True,\n",
       "   136: True,\n",
       "   143: True,\n",
       "   213: True,\n",
       "   215: True,\n",
       "   216: False,\n",
       "   221: True,\n",
       "   224: False,\n",
       "   228: True,\n",
       "   229: True,\n",
       "   779: True,\n",
       "   1577: True,\n",
       "   2133: False,\n",
       "   2308: False,\n",
       "   2333: True,\n",
       "   4278: True,\n",
       "   4285: True,\n",
       "   4286: True,\n",
       "   5056: True,\n",
       "   5077: True,\n",
       "   5084: True},\n",
       "  0.8484848484848485],\n",
       " 're': [{64: False,\n",
       "   1463: False,\n",
       "   1520: False,\n",
       "   1542: False,\n",
       "   1653: False,\n",
       "   4358: True,\n",
       "   4409: True,\n",
       "   5144: False,\n",
       "   5472: False,\n",
       "   5556: False,\n",
       "   5672: False},\n",
       "  0.18181818181818182],\n",
       " 'da-wo': [{65: True,\n",
       "   683: True,\n",
       "   734: False,\n",
       "   758: False,\n",
       "   1246: True,\n",
       "   1293: False,\n",
       "   1375: True,\n",
       "   1407: True,\n",
       "   1828: True,\n",
       "   1995: False,\n",
       "   2560: True,\n",
       "   2829: True,\n",
       "   2878: True,\n",
       "   2906: False,\n",
       "   2975: True,\n",
       "   2976: True,\n",
       "   2977: True,\n",
       "   3000: True,\n",
       "   3025: True,\n",
       "   3031: True,\n",
       "   3050: True,\n",
       "   3061: True,\n",
       "   3105: True,\n",
       "   3106: True,\n",
       "   3112: True,\n",
       "   3128: True,\n",
       "   3134: True,\n",
       "   3144: True,\n",
       "   3158: True,\n",
       "   3194: True,\n",
       "   3221: True,\n",
       "   3238: False,\n",
       "   3240: True,\n",
       "   3387: True,\n",
       "   3475: True,\n",
       "   3562: True,\n",
       "   3563: True,\n",
       "   3583: False,\n",
       "   3584: True,\n",
       "   3654: True,\n",
       "   3686: True,\n",
       "   3687: True,\n",
       "   3708: False,\n",
       "   3736: True,\n",
       "   3911: True,\n",
       "   4464: True},\n",
       "  0.8260869565217391],\n",
       " 'Q': [{65: True, 99: True, 216: True, 230: True, 4285: True}, 1.0],\n",
       " 'ti-ja-no': [{66: False, 5481: False}, 0.0],\n",
       " 'so': [{68: False,\n",
       "   594: False,\n",
       "   652: False,\n",
       "   669: False,\n",
       "   703: False,\n",
       "   1321: False,\n",
       "   1344: False,\n",
       "   1719: False,\n",
       "   1746: False,\n",
       "   1941: False,\n",
       "   1944: False,\n",
       "   2649: False,\n",
       "   3214: False,\n",
       "   3395: False,\n",
       "   3562: False,\n",
       "   3570: False,\n",
       "   3685: False,\n",
       "   4303: False,\n",
       "   4305: False,\n",
       "   4337: False,\n",
       "   4351: False,\n",
       "   4993: False,\n",
       "   5034: False},\n",
       "  0.0],\n",
       " 'a-u-ta-na': [{72: True}, 1.0],\n",
       " 'da-na-pi': [{72: True}, 1.0],\n",
       " 'jo-zo': [{73: False}, 0.0],\n",
       " 'o': [{73: True,\n",
       "   77: True,\n",
       "   78: True,\n",
       "   82: False,\n",
       "   98: False,\n",
       "   118: False,\n",
       "   134: False,\n",
       "   135: True,\n",
       "   158: True,\n",
       "   164: True,\n",
       "   175: True,\n",
       "   177: True,\n",
       "   182: True,\n",
       "   194: False,\n",
       "   196: True,\n",
       "   198: True,\n",
       "   201: True,\n",
       "   202: False,\n",
       "   208: False,\n",
       "   467: False,\n",
       "   477: False,\n",
       "   517: True,\n",
       "   666: True,\n",
       "   672: False,\n",
       "   890: False,\n",
       "   950: False,\n",
       "   963: False,\n",
       "   1093: True,\n",
       "   1650: False,\n",
       "   1734: False,\n",
       "   1811: False,\n",
       "   1872: False,\n",
       "   2211: False,\n",
       "   2323: False,\n",
       "   2667: False,\n",
       "   2832: False,\n",
       "   2877: True,\n",
       "   2883: True,\n",
       "   2884: False,\n",
       "   2900: True,\n",
       "   2902: True,\n",
       "   2908: True,\n",
       "   2914: True,\n",
       "   2938: True,\n",
       "   3139: True,\n",
       "   3141: True,\n",
       "   3142: True,\n",
       "   3144: True,\n",
       "   3145: True,\n",
       "   3146: True,\n",
       "   3149: True,\n",
       "   3151: True,\n",
       "   3152: True,\n",
       "   3155: True,\n",
       "   3158: True,\n",
       "   3159: True,\n",
       "   3163: False,\n",
       "   3165: True,\n",
       "   3167: True,\n",
       "   3168: True,\n",
       "   3170: True,\n",
       "   3217: True,\n",
       "   3218: True,\n",
       "   3219: True,\n",
       "   3220: True,\n",
       "   3221: True,\n",
       "   3222: False,\n",
       "   3223: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3226: True,\n",
       "   3227: True,\n",
       "   3228: True,\n",
       "   3229: True,\n",
       "   3230: True,\n",
       "   3231: True,\n",
       "   3232: True,\n",
       "   3233: True,\n",
       "   3234: True,\n",
       "   3235: True,\n",
       "   3236: True,\n",
       "   3237: True,\n",
       "   3238: True,\n",
       "   3239: True,\n",
       "   3240: True,\n",
       "   3241: True,\n",
       "   3242: True,\n",
       "   3243: True,\n",
       "   3244: True,\n",
       "   3245: True,\n",
       "   3246: True,\n",
       "   3247: True,\n",
       "   3248: True,\n",
       "   3249: True,\n",
       "   3250: True,\n",
       "   3251: True,\n",
       "   3252: True,\n",
       "   3253: True,\n",
       "   3254: True,\n",
       "   3256: True,\n",
       "   3285: True,\n",
       "   3288: True,\n",
       "   3290: True,\n",
       "   3295: True,\n",
       "   3304: True,\n",
       "   3306: True,\n",
       "   3310: True,\n",
       "   3311: False,\n",
       "   3316: True,\n",
       "   3319: True,\n",
       "   3320: True,\n",
       "   3321: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3325: True,\n",
       "   3326: True,\n",
       "   3327: True,\n",
       "   3328: True,\n",
       "   3329: True,\n",
       "   3333: True,\n",
       "   3335: False,\n",
       "   3336: True,\n",
       "   3337: False,\n",
       "   3339: True,\n",
       "   3341: True,\n",
       "   3342: True,\n",
       "   3343: True,\n",
       "   3344: True,\n",
       "   3349: True,\n",
       "   3360: True,\n",
       "   3364: False,\n",
       "   3365: True,\n",
       "   3366: True,\n",
       "   3369: False,\n",
       "   3375: True,\n",
       "   3376: True,\n",
       "   3377: True,\n",
       "   3378: True,\n",
       "   3379: True,\n",
       "   3380: True,\n",
       "   3381: True,\n",
       "   3382: True,\n",
       "   3383: True,\n",
       "   3384: True,\n",
       "   3385: False,\n",
       "   3386: True,\n",
       "   3388: True,\n",
       "   3389: True,\n",
       "   3390: True,\n",
       "   3391: True,\n",
       "   3392: True,\n",
       "   3393: True,\n",
       "   3394: False,\n",
       "   3395: True,\n",
       "   3396: True,\n",
       "   3397: True,\n",
       "   3398: True,\n",
       "   3399: True,\n",
       "   3400: True,\n",
       "   3401: True,\n",
       "   3402: True,\n",
       "   3403: True,\n",
       "   3404: True,\n",
       "   3405: True,\n",
       "   3406: True,\n",
       "   3409: True,\n",
       "   3410: True,\n",
       "   3411: True,\n",
       "   3413: True,\n",
       "   3414: True,\n",
       "   3415: True,\n",
       "   3416: False,\n",
       "   3417: False,\n",
       "   3419: False,\n",
       "   3420: True,\n",
       "   3421: True,\n",
       "   3422: True,\n",
       "   3423: True,\n",
       "   3425: True,\n",
       "   3427: False,\n",
       "   3429: True,\n",
       "   3430: False,\n",
       "   3431: True,\n",
       "   3433: False,\n",
       "   3435: False,\n",
       "   3441: False,\n",
       "   3443: True,\n",
       "   3444: False,\n",
       "   3445: True,\n",
       "   3449: False,\n",
       "   3451: False,\n",
       "   3487: False,\n",
       "   3488: True,\n",
       "   3490: True,\n",
       "   3491: True,\n",
       "   3494: True,\n",
       "   3500: False,\n",
       "   3524: True,\n",
       "   3525: True,\n",
       "   3529: True,\n",
       "   3532: True,\n",
       "   3535: True,\n",
       "   3538: False,\n",
       "   3547: True,\n",
       "   3574: False,\n",
       "   3579: True,\n",
       "   3597: True,\n",
       "   3633: True,\n",
       "   3660: True,\n",
       "   3670: False,\n",
       "   3689: True,\n",
       "   3702: True,\n",
       "   3704: False,\n",
       "   3710: True,\n",
       "   3749: True,\n",
       "   3758: True,\n",
       "   3767: True,\n",
       "   4043: False,\n",
       "   4126: False,\n",
       "   4142: True,\n",
       "   4151: False,\n",
       "   4165: True,\n",
       "   4166: True,\n",
       "   4176: False,\n",
       "   4178: True,\n",
       "   4181: True,\n",
       "   4185: True,\n",
       "   4262: False,\n",
       "   4284: False,\n",
       "   4409: True,\n",
       "   4432: False,\n",
       "   4438: True,\n",
       "   4439: True,\n",
       "   4450: False,\n",
       "   4477: True,\n",
       "   4492: True,\n",
       "   4502: True,\n",
       "   4509: False,\n",
       "   4655: True,\n",
       "   4669: True,\n",
       "   4673: False,\n",
       "   4689: True,\n",
       "   4717: False,\n",
       "   4722: False,\n",
       "   4723: False,\n",
       "   4729: False,\n",
       "   4958: False,\n",
       "   5094: False,\n",
       "   5095: True,\n",
       "   5099: True,\n",
       "   5101: True,\n",
       "   5102: True,\n",
       "   5106: True,\n",
       "   5109: True,\n",
       "   5137: False,\n",
       "   5251: False,\n",
       "   5269: False,\n",
       "   5294: False,\n",
       "   5358: False,\n",
       "   5373: True,\n",
       "   5392: True,\n",
       "   5399: False,\n",
       "   5407: False,\n",
       "   5412: True,\n",
       "   5531: False,\n",
       "   5538: False,\n",
       "   5568: False,\n",
       "   5575: False,\n",
       "   5674: False},\n",
       "  0.7492163009404389],\n",
       " 'nu-po': [{74: False}, 0.0],\n",
       " 'a-nu-ko': [{74: True, 2828: True, 3113: True, 3140: True, 3372: False},\n",
       "  0.8333333333333334],\n",
       " 'sa-ma-ri-jo': [{75: True, 2949: True}, 1.0],\n",
       " 'ma-ki-nu-wo': [{76: False}, 0.0],\n",
       " 'wo': [{77: False,\n",
       "   411: False,\n",
       "   412: False,\n",
       "   598: False,\n",
       "   635: False,\n",
       "   677: False,\n",
       "   686: False,\n",
       "   690: False,\n",
       "   697: False,\n",
       "   1088: False,\n",
       "   1113: False,\n",
       "   1227: False,\n",
       "   1348: False,\n",
       "   1589: False,\n",
       "   1672: False,\n",
       "   1705: False,\n",
       "   1806: False,\n",
       "   2065: False,\n",
       "   2082: False,\n",
       "   2093: False,\n",
       "   2137: False,\n",
       "   2178: False,\n",
       "   2222: False,\n",
       "   2586: False,\n",
       "   2590: False,\n",
       "   2708: False,\n",
       "   2732: False,\n",
       "   2795: False,\n",
       "   2854: False,\n",
       "   2928: False,\n",
       "   3540: False,\n",
       "   3698: False,\n",
       "   3905: False,\n",
       "   4169: False,\n",
       "   4294: False,\n",
       "   4757: False,\n",
       "   4766: False,\n",
       "   4780: False,\n",
       "   4895: False,\n",
       "   5134: False,\n",
       "   5541: False,\n",
       "   5573: False},\n",
       "  0.0],\n",
       " 'si-du-wo': [{80: False}, 0.0],\n",
       " 'qa-sa-ro-we': [{83: True, 3101: True, 3548: True, 3876: False}, 0.75],\n",
       " 'tu-qa-ni-ja-so': [{84: True, 3092: True}, 1.0],\n",
       " 'e-u-na-wo': [{85: True, 2681: True, 2707: True, 3569: True}, 1.0],\n",
       " 'ra': [{92: False,\n",
       "   166: False,\n",
       "   626: False,\n",
       "   651: False,\n",
       "   752: False,\n",
       "   975: False,\n",
       "   1149: False,\n",
       "   1215: False,\n",
       "   1328: False,\n",
       "   1445: False,\n",
       "   1450: False,\n",
       "   1530: False,\n",
       "   1750: False,\n",
       "   2034: False,\n",
       "   2510: False,\n",
       "   2527: False,\n",
       "   2879: False,\n",
       "   3046: False,\n",
       "   3119: False,\n",
       "   3207: False,\n",
       "   3222: False,\n",
       "   3277: False,\n",
       "   3405: False,\n",
       "   3541: False,\n",
       "   3775: False,\n",
       "   3814: False,\n",
       "   3872: False,\n",
       "   4078: False,\n",
       "   4201: False,\n",
       "   4282: False,\n",
       "   4297: False,\n",
       "   4326: False,\n",
       "   4336: False,\n",
       "   4411: False,\n",
       "   4779: False,\n",
       "   5016: True},\n",
       "  0.027777777777777776],\n",
       " 'wa-to': [{93: True,\n",
       "   2776: True,\n",
       "   2866: True,\n",
       "   5639: True,\n",
       "   5642: True,\n",
       "   5644: True,\n",
       "   5645: True,\n",
       "   5646: True,\n",
       "   5647: True,\n",
       "   5664: True,\n",
       "   5668: False},\n",
       "  0.9230769230769231],\n",
       " 'ri-no': [{93: True, 219: True, 1420: True, 4285: True, 5259: True}, 1.0],\n",
       " 'ra-to': [{101: False,\n",
       "   686: False,\n",
       "   747: False,\n",
       "   935: True,\n",
       "   2037: False,\n",
       "   2038: False,\n",
       "   2979: True,\n",
       "   2997: True,\n",
       "   3015: True,\n",
       "   3022: True,\n",
       "   3067: True,\n",
       "   3068: True,\n",
       "   3069: False,\n",
       "   3117: True,\n",
       "   3118: True,\n",
       "   3124: True,\n",
       "   3179: True,\n",
       "   3205: True,\n",
       "   3260: True,\n",
       "   3275: True,\n",
       "   3278: True,\n",
       "   3468: False,\n",
       "   3479: True,\n",
       "   3564: False,\n",
       "   3565: True,\n",
       "   3566: True,\n",
       "   3612: True,\n",
       "   3630: True,\n",
       "   3641: True,\n",
       "   3644: True,\n",
       "   3743: True,\n",
       "   3813: False},\n",
       "  0.71875],\n",
       " '*167': [{119: False}, 0.0],\n",
       " 'L': [{119: True,\n",
       "   120: False,\n",
       "   121: False,\n",
       "   122: True,\n",
       "   217: True,\n",
       "   225: True,\n",
       "   4482: True,\n",
       "   5055: True,\n",
       "   5058: False,\n",
       "   5060: True,\n",
       "   5061: True,\n",
       "   5062: True,\n",
       "   5064: True,\n",
       "   5066: True,\n",
       "   5069: True,\n",
       "   5070: True},\n",
       "  0.8125],\n",
       " 're-o': [{121: False, 305: False}, 0.0],\n",
       " '*167+PE': [{122: False, 123: False}, 0.0],\n",
       " 'AES': [{123: True,\n",
       "   285: True,\n",
       "   906: True,\n",
       "   4230: True,\n",
       "   4285: True,\n",
       "   5056: True,\n",
       "   5057: True,\n",
       "   5058: True,\n",
       "   5059: True,\n",
       "   5060: True,\n",
       "   5061: True,\n",
       "   5062: True,\n",
       "   5063: True,\n",
       "   5064: False,\n",
       "   5065: True,\n",
       "   5066: True,\n",
       "   5067: True,\n",
       "   5068: True,\n",
       "   5069: True,\n",
       "   5070: True,\n",
       "   5071: True,\n",
       "   5072: True,\n",
       "   5073: True,\n",
       "   5074: True,\n",
       "   5075: True,\n",
       "   5076: True,\n",
       "   5358: True},\n",
       "  0.9385964912280702],\n",
       " 'a-ka': [{124: False,\n",
       "   661: False,\n",
       "   689: False,\n",
       "   2901: False,\n",
       "   2930: True,\n",
       "   2931: True,\n",
       "   2932: True,\n",
       "   2933: True,\n",
       "   2934: True,\n",
       "   2935: True,\n",
       "   2936: True,\n",
       "   2986: True,\n",
       "   3011: True,\n",
       "   3036: True,\n",
       "   3484: True,\n",
       "   3720: True,\n",
       "   4732: True},\n",
       "  0.7647058823529411],\n",
       " 'jo-jo': [{124: False,\n",
       "   746: False,\n",
       "   1911: False,\n",
       "   2019: False,\n",
       "   2616: False,\n",
       "   3481: False},\n",
       "  0.0],\n",
       " 'me-no': [{124: False,\n",
       "   732: False,\n",
       "   889: False,\n",
       "   1227: True,\n",
       "   1437: False,\n",
       "   2717: False,\n",
       "   3901: False,\n",
       "   4008: False,\n",
       "   4088: True,\n",
       "   4089: True,\n",
       "   4091: True,\n",
       "   4092: True,\n",
       "   4093: True,\n",
       "   4094: True,\n",
       "   4095: True,\n",
       "   4096: True,\n",
       "   4098: True,\n",
       "   4162: False,\n",
       "   4174: True,\n",
       "   4198: True,\n",
       "   4213: False,\n",
       "   4216: False,\n",
       "   4220: False,\n",
       "   4237: False,\n",
       "   4470: False,\n",
       "   4473: True,\n",
       "   4956: False,\n",
       "   5025: True,\n",
       "   5061: False,\n",
       "   5076: False},\n",
       "  0.4666666666666667],\n",
       " 'da-phu-ri-to-jo': [{124: True, 4196: True}, 1.0],\n",
       " 'po-ti-ni-ja': [{124: False,\n",
       "   1194: False,\n",
       "   2370: True,\n",
       "   4196: True,\n",
       "   4471: False,\n",
       "   4732: True,\n",
       "   4741: True,\n",
       "   5011: True,\n",
       "   5026: True,\n",
       "   5032: True,\n",
       "   5036: True,\n",
       "   5037: True,\n",
       "   5357: True,\n",
       "   5377: False},\n",
       "  0.7333333333333333],\n",
       " 'ri': [{124: True,\n",
       "   162: False,\n",
       "   271: False,\n",
       "   431: False,\n",
       "   616: False,\n",
       "   649: False,\n",
       "   697: False,\n",
       "   1013: False,\n",
       "   1059: False,\n",
       "   1320: False,\n",
       "   1668: False,\n",
       "   1776: False,\n",
       "   1933: False,\n",
       "   2336: True,\n",
       "   2486: False,\n",
       "   2812: True,\n",
       "   3660: False,\n",
       "   3883: False,\n",
       "   3906: False},\n",
       "  0.15789473684210525],\n",
       " '*166+WE': [{124: True,\n",
       "   125: True,\n",
       "   126: False,\n",
       "   5089: False,\n",
       "   5262: True,\n",
       "   5263: True,\n",
       "   5367: True,\n",
       "   5374: True},\n",
       "  0.75],\n",
       " 'e-to-ro-qa-ta': [{125: True, 592: True}, 1.0],\n",
       " 'ko-ro-to': [{127: True, 128: True, 129: True, 2340: True}, 1.0],\n",
       " 'o-nu-ka': [{127: False, 4434: True, 4437: True}, 0.6666666666666666],\n",
       " 'LANA': [{127: True,\n",
       "   129: True,\n",
       "   130: True,\n",
       "   131: True,\n",
       "   132: True,\n",
       "   133: True,\n",
       "   135: True,\n",
       "   136: True,\n",
       "   137: False,\n",
       "   138: True,\n",
       "   139: True,\n",
       "   140: True,\n",
       "   141: True,\n",
       "   142: True,\n",
       "   143: True,\n",
       "   144: True,\n",
       "   145: True,\n",
       "   146: True,\n",
       "   148: True,\n",
       "   149: True,\n",
       "   150: False,\n",
       "   151: True,\n",
       "   152: True,\n",
       "   153: False,\n",
       "   155: False,\n",
       "   156: False,\n",
       "   157: False,\n",
       "   158: True,\n",
       "   159: False,\n",
       "   160: True,\n",
       "   161: False,\n",
       "   162: True,\n",
       "   163: True,\n",
       "   164: True,\n",
       "   165: True,\n",
       "   166: True,\n",
       "   167: True,\n",
       "   168: True,\n",
       "   169: True,\n",
       "   170: True,\n",
       "   171: True,\n",
       "   172: True,\n",
       "   173: False,\n",
       "   174: False,\n",
       "   175: False,\n",
       "   176: True,\n",
       "   177: False,\n",
       "   178: True,\n",
       "   179: True,\n",
       "   180: False,\n",
       "   181: False,\n",
       "   182: False,\n",
       "   183: False,\n",
       "   184: False,\n",
       "   185: False,\n",
       "   186: False,\n",
       "   187: False,\n",
       "   188: False,\n",
       "   189: True,\n",
       "   191: False,\n",
       "   192: False,\n",
       "   193: False,\n",
       "   194: False,\n",
       "   195: True,\n",
       "   196: True,\n",
       "   197: False,\n",
       "   198: False,\n",
       "   199: False,\n",
       "   201: False,\n",
       "   202: False,\n",
       "   203: False,\n",
       "   204: False,\n",
       "   205: False,\n",
       "   206: False,\n",
       "   208: True,\n",
       "   209: False,\n",
       "   561: False,\n",
       "   736: False,\n",
       "   774: True,\n",
       "   783: True,\n",
       "   1714: True,\n",
       "   1766: False,\n",
       "   1823: False,\n",
       "   2338: True,\n",
       "   2339: False,\n",
       "   2340: True,\n",
       "   2341: True,\n",
       "   2342: True,\n",
       "   2344: True,\n",
       "   2345: True,\n",
       "   2346: True,\n",
       "   2347: True,\n",
       "   2348: False,\n",
       "   2350: True,\n",
       "   2352: True,\n",
       "   2353: True,\n",
       "   2354: False,\n",
       "   2355: True,\n",
       "   2356: True,\n",
       "   2358: False,\n",
       "   2359: True,\n",
       "   2360: True,\n",
       "   2361: True,\n",
       "   2362: True,\n",
       "   2891: False,\n",
       "   3303: False,\n",
       "   3304: True,\n",
       "   3305: True,\n",
       "   3306: True,\n",
       "   3308: True,\n",
       "   3309: True,\n",
       "   3310: True,\n",
       "   3311: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3317: True,\n",
       "   3318: True,\n",
       "   3319: True,\n",
       "   3320: True,\n",
       "   3321: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3325: True,\n",
       "   3326: True,\n",
       "   3327: True,\n",
       "   3328: True,\n",
       "   3329: True,\n",
       "   3332: False,\n",
       "   3333: True,\n",
       "   3334: True,\n",
       "   3335: True,\n",
       "   3336: True,\n",
       "   3337: True,\n",
       "   3339: True,\n",
       "   3340: True,\n",
       "   3341: True,\n",
       "   3342: True,\n",
       "   3343: False,\n",
       "   3344: False,\n",
       "   3345: False,\n",
       "   3346: True,\n",
       "   3348: True,\n",
       "   3349: True,\n",
       "   3350: True,\n",
       "   3351: True,\n",
       "   3352: True,\n",
       "   3353: False,\n",
       "   3354: False,\n",
       "   3355: True,\n",
       "   3356: True,\n",
       "   3357: True,\n",
       "   3358: True,\n",
       "   3359: True,\n",
       "   3360: True,\n",
       "   3361: True,\n",
       "   3362: True,\n",
       "   3363: False,\n",
       "   3364: True,\n",
       "   3365: True,\n",
       "   3366: True,\n",
       "   3367: True,\n",
       "   3368: True,\n",
       "   3369: True,\n",
       "   3371: False,\n",
       "   3375: True,\n",
       "   3376: False,\n",
       "   3379: True,\n",
       "   3380: True,\n",
       "   3381: True,\n",
       "   3385: True,\n",
       "   3388: True,\n",
       "   3389: True,\n",
       "   3390: True,\n",
       "   3392: True,\n",
       "   3393: True,\n",
       "   3394: True,\n",
       "   3395: False,\n",
       "   3396: True,\n",
       "   3397: False,\n",
       "   3398: True,\n",
       "   3399: True,\n",
       "   3400: True,\n",
       "   3401: True,\n",
       "   3402: True,\n",
       "   3403: True,\n",
       "   3404: True,\n",
       "   3405: True,\n",
       "   3406: True,\n",
       "   3407: True,\n",
       "   3408: True,\n",
       "   3409: False,\n",
       "   3410: True,\n",
       "   3414: True,\n",
       "   3418: False,\n",
       "   3420: True,\n",
       "   3425: True,\n",
       "   3426: False,\n",
       "   3427: True,\n",
       "   3428: False,\n",
       "   3429: False,\n",
       "   3438: True,\n",
       "   3440: True,\n",
       "   3441: False,\n",
       "   3442: False,\n",
       "   3444: True,\n",
       "   3447: False,\n",
       "   3510: False,\n",
       "   3511: False,\n",
       "   3512: True,\n",
       "   3514: True,\n",
       "   3517: True,\n",
       "   3518: True,\n",
       "   4216: True,\n",
       "   4270: True,\n",
       "   4276: True,\n",
       "   4299: False,\n",
       "   4375: False,\n",
       "   4376: False,\n",
       "   4377: True,\n",
       "   4378: True,\n",
       "   4379: True,\n",
       "   4380: False,\n",
       "   4381: True,\n",
       "   4384: False,\n",
       "   4386: True,\n",
       "   4392: True,\n",
       "   4395: True,\n",
       "   4399: True,\n",
       "   4400: True,\n",
       "   4401: True,\n",
       "   4402: True,\n",
       "   4403: True,\n",
       "   4404: True,\n",
       "   4407: False,\n",
       "   4408: True,\n",
       "   4409: True,\n",
       "   4411: False,\n",
       "   4412: False,\n",
       "   4413: True,\n",
       "   4414: True,\n",
       "   4415: False,\n",
       "   4417: True,\n",
       "   4419: True,\n",
       "   4421: False,\n",
       "   4464: True,\n",
       "   4466: True,\n",
       "   4467: True,\n",
       "   4504: True,\n",
       "   4698: True,\n",
       "   5086: False,\n",
       "   5088: False,\n",
       "   5090: False,\n",
       "   5374: False,\n",
       "   5378: True,\n",
       "   5379: True,\n",
       "   5381: True,\n",
       "   5382: True,\n",
       "   5384: True,\n",
       "   5386: True},\n",
       "  0.7127659574468085],\n",
       " 'ta-ma': [{128: False}, 0.0],\n",
       " 'pa-i-to': [{130: True,\n",
       "   148: True,\n",
       "   239: True,\n",
       "   461: True,\n",
       "   576: True,\n",
       "   731: False,\n",
       "   916: False,\n",
       "   1182: False,\n",
       "   1203: True,\n",
       "   1206: False,\n",
       "   1350: True,\n",
       "   1387: False,\n",
       "   2875: True,\n",
       "   2950: True,\n",
       "   2951: False,\n",
       "   2953: True,\n",
       "   2954: True,\n",
       "   2955: True,\n",
       "   2956: True,\n",
       "   2957: True,\n",
       "   2984: True,\n",
       "   2988: True,\n",
       "   2993: True,\n",
       "   3009: True,\n",
       "   3049: True,\n",
       "   3062: True,\n",
       "   3063: True,\n",
       "   3103: False,\n",
       "   3113: True,\n",
       "   3114: True,\n",
       "   3177: True,\n",
       "   3178: True,\n",
       "   3193: False,\n",
       "   3201: True,\n",
       "   3209: True,\n",
       "   3217: True,\n",
       "   3288: True,\n",
       "   3461: True,\n",
       "   3465: True,\n",
       "   3475: True,\n",
       "   3542: True,\n",
       "   3546: False,\n",
       "   3549: True,\n",
       "   3626: True,\n",
       "   3628: True,\n",
       "   3633: True,\n",
       "   3787: True,\n",
       "   3804: True,\n",
       "   3862: True,\n",
       "   3907: True,\n",
       "   4131: True,\n",
       "   4254: True,\n",
       "   4459: True},\n",
       "  0.8301886792452831],\n",
       " 'we-we-si-jo-jo': [{130: True,\n",
       "   175: True,\n",
       "   1197: True,\n",
       "   1429: True,\n",
       "   2561: True,\n",
       "   2952: True,\n",
       "   3062: True,\n",
       "   3063: True,\n",
       "   3066: True,\n",
       "   3103: True,\n",
       "   3113: True,\n",
       "   3144: True,\n",
       "   3145: True,\n",
       "   3177: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3250: True,\n",
       "   3626: True,\n",
       "   4420: True},\n",
       "  1.0],\n",
       " 'o-pi': [{131: True,\n",
       "   132: False,\n",
       "   134: True,\n",
       "   141: True,\n",
       "   142: True,\n",
       "   629: True,\n",
       "   697: True,\n",
       "   795: True,\n",
       "   918: False,\n",
       "   1061: False,\n",
       "   1256: False,\n",
       "   1264: False,\n",
       "   1274: True,\n",
       "   1350: False,\n",
       "   1711: False,\n",
       "   1713: True,\n",
       "   1717: True,\n",
       "   1912: False,\n",
       "   2303: True,\n",
       "   2678: True,\n",
       "   3961: True,\n",
       "   4274: True,\n",
       "   4283: True,\n",
       "   4301: False,\n",
       "   4460: True,\n",
       "   4464: True,\n",
       "   4477: True,\n",
       "   4676: True,\n",
       "   4679: True,\n",
       "   4681: True,\n",
       "   4686: True},\n",
       "  0.725],\n",
       " 'ti-mu-nu-we': [{131: True, 4467: False}, 0.5],\n",
       " 'no-nu-we': [{132: True}, 1.0],\n",
       " 'a-ti-pa-mo': [{132: True, 5058: True, 5071: True}, 1.0],\n",
       " 'pe-re': [{132: True, 4744: True, 5357: True, 5372: False, 5391: True},\n",
       "  0.8888888888888888],\n",
       " 'po-ro-to': [{132: True}, 1.0],\n",
       " 'a-po-te': [{132: True}, 1.0],\n",
       " 'ri-jo-ni-jo': [{133: True, 2115: False}, 0.5],\n",
       " 'e-ze-to': [{133: True}, 1.0],\n",
       " 'to-ro-qo': [{133: True}, 1.0],\n",
       " 'a-to-mo-na': [{133: True, 142: True}, 1.0],\n",
       " 'su-mo-no-qe': [{133: True}, 1.0],\n",
       " 'ki-si': [{134: False, 703: False, 1443: False}, 0.0],\n",
       " 'ke-me-no': [{135: False, 1186: False}, 0.0],\n",
       " 'au-u-te': [{135: True}, 1.0],\n",
       " 'a-pe-i-si': [{135: True}, 1.0],\n",
       " 'ke-re-wa': [{135: False, 978: True}, 0.5],\n",
       " 'o-mu-ka-ra': [{135: True}, 1.0],\n",
       " 'ku-pi-ri-ja': [{136: True}, 1.0],\n",
       " 'ke': [{136: False,\n",
       "   223: False,\n",
       "   598: False,\n",
       "   697: False,\n",
       "   753: False,\n",
       "   1107: False,\n",
       "   1145: False,\n",
       "   1275: False,\n",
       "   1292: False,\n",
       "   1516: False,\n",
       "   1556: False,\n",
       "   1796: False,\n",
       "   1801: False,\n",
       "   1847: False,\n",
       "   2070: False,\n",
       "   2674: True,\n",
       "   2814: False,\n",
       "   3661: False,\n",
       "   3957: False,\n",
       "   4231: True,\n",
       "   4278: False,\n",
       "   4466: False,\n",
       "   5225: True,\n",
       "   5244: True,\n",
       "   5399: False,\n",
       "   5515: False,\n",
       "   5559: False,\n",
       "   5561: False,\n",
       "   5570: False},\n",
       "  0.21875],\n",
       " 'sa-mu': [{136: False}, 0.0],\n",
       " 'qo-ja-te': [{136: True, 137: True}, 1.0],\n",
       " 'e-na-po-na': [{137: True}, 1.0],\n",
       " 'o-nu': [{137: True, 2303: False, 4449: False, 4452: False}, 0.25],\n",
       " 'pa-i-ti-jo': [{137: True}, 1.0],\n",
       " 'e-ti-wa-ja-qe': [{137: True}, 1.0],\n",
       " 'a-pu-do-ke': [{137: True}, 1.0],\n",
       " 'ti-ra': [{137: True, 139: True, 1924: False}, 0.6666666666666666],\n",
       " 'o-nu-ke': [{138: True, 779: True, 4464: True, 4467: True}, 1.0],\n",
       " 'a-mi-ke-te-to': [{139: True}, 1.0],\n",
       " 'ne-ki-ri-si': [{139: True}, 1.0],\n",
       " 'e-ki': [{140: True}, 1.0],\n",
       " 'ri-jo-ni-ja': [{140: True, 2562: True, 4383: True, 4457: True}, 1.0],\n",
       " 'to-sa': [{140: True,\n",
       "   250: True,\n",
       "   1194: True,\n",
       "   1195: False,\n",
       "   1237: True,\n",
       "   1369: False,\n",
       "   1372: False,\n",
       "   1414: True,\n",
       "   1601: False,\n",
       "   2399: False,\n",
       "   2654: True,\n",
       "   2661: True,\n",
       "   2784: True,\n",
       "   3415: True,\n",
       "   3511: True,\n",
       "   3513: False,\n",
       "   3516: True,\n",
       "   4165: True,\n",
       "   4304: True,\n",
       "   4343: True,\n",
       "   4389: True,\n",
       "   4390: True,\n",
       "   4434: False,\n",
       "   4436: True,\n",
       "   4439: True,\n",
       "   4464: True,\n",
       "   4482: True,\n",
       "   4933: True,\n",
       "   4960: True,\n",
       "   5229: True,\n",
       "   5299: True,\n",
       "   5316: True,\n",
       "   5325: True,\n",
       "   5531: True},\n",
       "  0.8157894736842105],\n",
       " 'ne-wo': [{141: True,\n",
       "   1204: True,\n",
       "   2361: True,\n",
       "   3956: True,\n",
       "   3968: False,\n",
       "   4725: True,\n",
       "   5007: True,\n",
       "   5064: False,\n",
       "   5335: True},\n",
       "  0.7777777777777778],\n",
       " 'po-po': [{141: True,\n",
       "   1711: False,\n",
       "   4266: True,\n",
       "   4274: True,\n",
       "   4283: False,\n",
       "   4464: True},\n",
       "  0.6666666666666666],\n",
       " 'po-ro-i-ra': [{142: True}, 1.0],\n",
       " 'ne-we': [{143: False}, 0.0],\n",
       " 'pe-ko-to': [{143: True,\n",
       "   4358: True,\n",
       "   4380: True,\n",
       "   4381: True,\n",
       "   4389: True,\n",
       "   4390: True,\n",
       "   4409: True},\n",
       "  1.0],\n",
       " '*1641': [{143: False}, 0.0],\n",
       " 'e-pi-ro-pa-ja': [{143: False}, 0.0],\n",
       " 'o-du-we': [{143: True}, 1.0],\n",
       " 'te-o-po-ri-ja': [{143: True, 4163: True}, 1.0],\n",
       " 'i-jo-te': [{143: False, 2320: True, 2744: False, 4694: True}, 0.5],\n",
       " 'ku-su-a-ta-o': [{143: True}, 1.0],\n",
       " 'jo-du-mi': [{143: False}, 0.0],\n",
       " 'wo-ke': [{143: True, 5335: True}, 1.0],\n",
       " 'a-*65-na': [{144: True}, 1.0],\n",
       " 'e-re-u-ti-ja': [{144: True, 145: True, 146: False, 4514: True}, 0.75],\n",
       " 'ta-wa-ko-to': [{145: True}, 1.0],\n",
       " 'ka-ra': [{148: False,\n",
       "   1201: False,\n",
       "   2333: False,\n",
       "   2380: False,\n",
       "   5155: False,\n",
       "   5393: False},\n",
       "  0.0],\n",
       " 'a-me-a': [{148: True, 2958: True}, 1.0],\n",
       " 'PE': [{149: True,\n",
       "   150: True,\n",
       "   2330: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   5084: True,\n",
       "   5377: True,\n",
       "   5392: True},\n",
       "  1.0],\n",
       " 'wi-ri-za': [{151: False, 176: True, 5378: True}, 0.6666666666666666],\n",
       " 'ki-ri-ta-i': [{152: True}, 1.0],\n",
       " 'ni-ki-jo': [{153: False}, 0.0],\n",
       " 'nu-we': [{155: False}, 0.0],\n",
       " 'o-na': [{155: True,\n",
       "   3997: True,\n",
       "   4466: True,\n",
       "   4807: True,\n",
       "   4812: True,\n",
       "   5362: True},\n",
       "  1.0],\n",
       " 'ne': [{167: False,\n",
       "   1136: False,\n",
       "   1836: False,\n",
       "   2201: False,\n",
       "   2303: False,\n",
       "   2562: True,\n",
       "   2563: True,\n",
       "   2573: True,\n",
       "   2593: True,\n",
       "   2594: True,\n",
       "   2600: True,\n",
       "   2622: False,\n",
       "   2652: True,\n",
       "   2776: True,\n",
       "   2846: True,\n",
       "   3298: True,\n",
       "   3317: True,\n",
       "   3318: True,\n",
       "   3480: True,\n",
       "   3511: True,\n",
       "   4057: False,\n",
       "   4376: False,\n",
       "   4457: False,\n",
       "   4709: False,\n",
       "   4765: False,\n",
       "   4780: False,\n",
       "   5117: False,\n",
       "   5553: False,\n",
       "   5668: False},\n",
       "  0.627906976744186],\n",
       " 'we': [{170: False,\n",
       "   687: False,\n",
       "   1734: False,\n",
       "   1749: False,\n",
       "   1859: False,\n",
       "   2046: False,\n",
       "   2079: False,\n",
       "   2080: False,\n",
       "   2081: False,\n",
       "   2342: False,\n",
       "   2417: False,\n",
       "   2773: False,\n",
       "   2846: True,\n",
       "   3607: False,\n",
       "   3786: False,\n",
       "   4238: False,\n",
       "   4314: False,\n",
       "   4818: False,\n",
       "   4987: False,\n",
       "   5046: False,\n",
       "   5060: False,\n",
       "   5395: False,\n",
       "   5437: False,\n",
       "   5557: False,\n",
       "   5569: False},\n",
       "  0.037037037037037035],\n",
       " 'da-i-ra': [{171: True, 676: True}, 1.0],\n",
       " 'su-ri-mo': [{176: True,\n",
       "   235: True,\n",
       "   1438: False,\n",
       "   2644: True,\n",
       "   2939: True,\n",
       "   2952: True,\n",
       "   3056: True,\n",
       "   3102: True,\n",
       "   3115: True,\n",
       "   3173: True,\n",
       "   3198: True,\n",
       "   3206: True,\n",
       "   3211: True,\n",
       "   3253: True,\n",
       "   3281: True,\n",
       "   3285: True,\n",
       "   3286: True,\n",
       "   3287: True,\n",
       "   3296: True,\n",
       "   3312: True,\n",
       "   3401: True,\n",
       "   3471: False,\n",
       "   3552: False,\n",
       "   3553: False,\n",
       "   3634: True,\n",
       "   3637: True,\n",
       "   3651: True,\n",
       "   3664: False,\n",
       "   3738: False,\n",
       "   4268: True},\n",
       "  0.8],\n",
       " 'ne-a': [{178: False, 222: False}, 0.0],\n",
       " 'OVISm': [{193: True,\n",
       "   603: True,\n",
       "   735: False,\n",
       "   740: True,\n",
       "   742: True,\n",
       "   748: True,\n",
       "   749: True,\n",
       "   754: False,\n",
       "   902: True,\n",
       "   1627: True,\n",
       "   1813: False,\n",
       "   2323: True,\n",
       "   2773: True,\n",
       "   2779: True,\n",
       "   2780: True,\n",
       "   2782: True,\n",
       "   2784: True,\n",
       "   2786: True,\n",
       "   2828: True,\n",
       "   2831: False,\n",
       "   2838: True,\n",
       "   2845: False,\n",
       "   2866: True,\n",
       "   2867: True,\n",
       "   2868: True,\n",
       "   2869: True,\n",
       "   2870: True,\n",
       "   2875: True,\n",
       "   2877: True,\n",
       "   2878: True,\n",
       "   2879: True,\n",
       "   2880: True,\n",
       "   2881: False,\n",
       "   2882: True,\n",
       "   2883: False,\n",
       "   2884: True,\n",
       "   2885: False,\n",
       "   2887: True,\n",
       "   2888: False,\n",
       "   2889: False,\n",
       "   2890: False,\n",
       "   2891: True,\n",
       "   2892: True,\n",
       "   2893: False,\n",
       "   2894: True,\n",
       "   2895: True,\n",
       "   2896: False,\n",
       "   2899: False,\n",
       "   2900: True,\n",
       "   2901: True,\n",
       "   2902: False,\n",
       "   2903: False,\n",
       "   2905: False,\n",
       "   2907: True,\n",
       "   2908: False,\n",
       "   2909: False,\n",
       "   2910: True,\n",
       "   2915: False,\n",
       "   2916: True,\n",
       "   2918: True,\n",
       "   2919: True,\n",
       "   2920: False,\n",
       "   2922: True,\n",
       "   2924: False,\n",
       "   2928: False,\n",
       "   2930: True,\n",
       "   2931: True,\n",
       "   2932: True,\n",
       "   2933: True,\n",
       "   2934: True,\n",
       "   2935: True,\n",
       "   2936: False,\n",
       "   2937: True,\n",
       "   2938: True,\n",
       "   2939: True,\n",
       "   2940: True,\n",
       "   2941: True,\n",
       "   2942: True,\n",
       "   2943: True,\n",
       "   2944: True,\n",
       "   2945: True,\n",
       "   2946: True,\n",
       "   2947: True,\n",
       "   2948: True,\n",
       "   2949: True,\n",
       "   2950: True,\n",
       "   2951: True,\n",
       "   2952: True,\n",
       "   2953: True,\n",
       "   2954: True,\n",
       "   2955: True,\n",
       "   2956: True,\n",
       "   2957: True,\n",
       "   2958: True,\n",
       "   2959: True,\n",
       "   2960: True,\n",
       "   2961: True,\n",
       "   2962: True,\n",
       "   2963: True,\n",
       "   2964: True,\n",
       "   2965: False,\n",
       "   2966: True,\n",
       "   2967: True,\n",
       "   2968: True,\n",
       "   2969: True,\n",
       "   2970: True,\n",
       "   2971: True,\n",
       "   2972: True,\n",
       "   2973: True,\n",
       "   2974: True,\n",
       "   2975: True,\n",
       "   2976: True,\n",
       "   2977: True,\n",
       "   2978: True,\n",
       "   2979: True,\n",
       "   2980: True,\n",
       "   2981: True,\n",
       "   2982: True,\n",
       "   2983: True,\n",
       "   2984: True,\n",
       "   2985: True,\n",
       "   2986: True,\n",
       "   2987: True,\n",
       "   2988: True,\n",
       "   2989: True,\n",
       "   2990: True,\n",
       "   2991: True,\n",
       "   2992: True,\n",
       "   2993: True,\n",
       "   2994: True,\n",
       "   2995: True,\n",
       "   2996: True,\n",
       "   2997: True,\n",
       "   2998: True,\n",
       "   2999: True,\n",
       "   3000: True,\n",
       "   3001: True,\n",
       "   3002: True,\n",
       "   3003: True,\n",
       "   3004: True,\n",
       "   3005: True,\n",
       "   3006: True,\n",
       "   3007: True,\n",
       "   3008: True,\n",
       "   3009: True,\n",
       "   3010: True,\n",
       "   3011: True,\n",
       "   3012: True,\n",
       "   3013: True,\n",
       "   3014: True,\n",
       "   3015: True,\n",
       "   3016: True,\n",
       "   3017: True,\n",
       "   3018: True,\n",
       "   3019: True,\n",
       "   3020: True,\n",
       "   3021: True,\n",
       "   3022: True,\n",
       "   3023: True,\n",
       "   3024: True,\n",
       "   3025: True,\n",
       "   3026: True,\n",
       "   3027: True,\n",
       "   3028: False,\n",
       "   3029: True,\n",
       "   3030: True,\n",
       "   3031: True,\n",
       "   3032: False,\n",
       "   3033: True,\n",
       "   3034: True,\n",
       "   3035: False,\n",
       "   3036: True,\n",
       "   3037: False,\n",
       "   3038: True,\n",
       "   3039: True,\n",
       "   3040: True,\n",
       "   3041: True,\n",
       "   3042: True,\n",
       "   3043: False,\n",
       "   3044: True,\n",
       "   3045: True,\n",
       "   3046: False,\n",
       "   3047: False,\n",
       "   3048: True,\n",
       "   3049: True,\n",
       "   3050: True,\n",
       "   3051: True,\n",
       "   3052: False,\n",
       "   3054: True,\n",
       "   3055: True,\n",
       "   3056: True,\n",
       "   3057: True,\n",
       "   3058: True,\n",
       "   3059: True,\n",
       "   3060: True,\n",
       "   3061: True,\n",
       "   3062: True,\n",
       "   3063: True,\n",
       "   3064: True,\n",
       "   3065: True,\n",
       "   3066: True,\n",
       "   3067: True,\n",
       "   3068: True,\n",
       "   3069: True,\n",
       "   3070: True,\n",
       "   3071: True,\n",
       "   3072: True,\n",
       "   3073: True,\n",
       "   3074: True,\n",
       "   3075: True,\n",
       "   3076: True,\n",
       "   3077: True,\n",
       "   3078: True,\n",
       "   3079: True,\n",
       "   3080: True,\n",
       "   3081: True,\n",
       "   3082: True,\n",
       "   3083: True,\n",
       "   3084: True,\n",
       "   3085: True,\n",
       "   3086: True,\n",
       "   3087: True,\n",
       "   3088: True,\n",
       "   3089: True,\n",
       "   3090: True,\n",
       "   3091: True,\n",
       "   3092: True,\n",
       "   3093: True,\n",
       "   3094: True,\n",
       "   3095: True,\n",
       "   3096: True,\n",
       "   3097: True,\n",
       "   3098: True,\n",
       "   3099: True,\n",
       "   3100: True,\n",
       "   3101: True,\n",
       "   3102: True,\n",
       "   3103: True,\n",
       "   3104: True,\n",
       "   3105: True,\n",
       "   3106: False,\n",
       "   3107: True,\n",
       "   3108: True,\n",
       "   3109: True,\n",
       "   3110: True,\n",
       "   3111: True,\n",
       "   3112: True,\n",
       "   3113: True,\n",
       "   3114: True,\n",
       "   3115: True,\n",
       "   3116: True,\n",
       "   3117: True,\n",
       "   3118: True,\n",
       "   3119: False,\n",
       "   3120: False,\n",
       "   3121: True,\n",
       "   3122: True,\n",
       "   3123: True,\n",
       "   3124: True,\n",
       "   3126: True,\n",
       "   3127: False,\n",
       "   3128: True,\n",
       "   3129: False,\n",
       "   3130: True,\n",
       "   3131: True,\n",
       "   3132: True,\n",
       "   3134: False,\n",
       "   3135: True,\n",
       "   3136: True,\n",
       "   3137: True,\n",
       "   3138: True,\n",
       "   3139: True,\n",
       "   3140: True,\n",
       "   3141: True,\n",
       "   3142: True,\n",
       "   3143: True,\n",
       "   3144: True,\n",
       "   3145: True,\n",
       "   3146: True,\n",
       "   3147: True,\n",
       "   3148: True,\n",
       "   3149: True,\n",
       "   3150: True,\n",
       "   3151: True,\n",
       "   3152: True,\n",
       "   3153: True,\n",
       "   3154: True,\n",
       "   3155: True,\n",
       "   3156: True,\n",
       "   3157: True,\n",
       "   3158: True,\n",
       "   3159: True,\n",
       "   3160: True,\n",
       "   3161: True,\n",
       "   3162: False,\n",
       "   3163: True,\n",
       "   3164: True,\n",
       "   3165: True,\n",
       "   3166: True,\n",
       "   3167: True,\n",
       "   3168: True,\n",
       "   3169: True,\n",
       "   3170: True,\n",
       "   3171: True,\n",
       "   3172: True,\n",
       "   3173: True,\n",
       "   3175: True,\n",
       "   3176: True,\n",
       "   3177: True,\n",
       "   3178: True,\n",
       "   3179: True,\n",
       "   3180: True,\n",
       "   3181: True,\n",
       "   3182: True,\n",
       "   3183: True,\n",
       "   3184: True,\n",
       "   3185: True,\n",
       "   3186: True,\n",
       "   3187: True,\n",
       "   3188: True,\n",
       "   3189: True,\n",
       "   3190: True,\n",
       "   3191: True,\n",
       "   3192: True,\n",
       "   3193: True,\n",
       "   3194: True,\n",
       "   3195: True,\n",
       "   3196: True,\n",
       "   3197: True,\n",
       "   3198: True,\n",
       "   3199: True,\n",
       "   3200: True,\n",
       "   3201: True,\n",
       "   3202: False,\n",
       "   3203: True,\n",
       "   3204: True,\n",
       "   3205: True,\n",
       "   3206: True,\n",
       "   3207: True,\n",
       "   3208: True,\n",
       "   3209: True,\n",
       "   3210: True,\n",
       "   3211: True,\n",
       "   3212: True,\n",
       "   3213: False,\n",
       "   3214: True,\n",
       "   3215: True,\n",
       "   3216: True,\n",
       "   3217: True,\n",
       "   3218: True,\n",
       "   3219: True,\n",
       "   3220: True,\n",
       "   3221: True,\n",
       "   3222: True,\n",
       "   3223: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3226: True,\n",
       "   3227: True,\n",
       "   3228: True,\n",
       "   3229: True,\n",
       "   3230: True,\n",
       "   3231: True,\n",
       "   3232: True,\n",
       "   3233: True,\n",
       "   3234: True,\n",
       "   3235: True,\n",
       "   3236: True,\n",
       "   3237: True,\n",
       "   3238: True,\n",
       "   3239: True,\n",
       "   3240: True,\n",
       "   3241: True,\n",
       "   3242: True,\n",
       "   3243: True,\n",
       "   3244: True,\n",
       "   3245: True,\n",
       "   3246: True,\n",
       "   3247: True,\n",
       "   3248: True,\n",
       "   3249: True,\n",
       "   3250: True,\n",
       "   3251: True,\n",
       "   3252: True,\n",
       "   3253: True,\n",
       "   3254: True,\n",
       "   3255: True,\n",
       "   3256: True,\n",
       "   3257: True,\n",
       "   3258: True,\n",
       "   3259: True,\n",
       "   3260: True,\n",
       "   3261: True,\n",
       "   3262: True,\n",
       "   3263: True,\n",
       "   3264: True,\n",
       "   3265: True,\n",
       "   3266: True,\n",
       "   3267: True,\n",
       "   3268: True,\n",
       "   3269: True,\n",
       "   3270: True,\n",
       "   3271: True,\n",
       "   3272: True,\n",
       "   3273: True,\n",
       "   3275: True,\n",
       "   3276: True,\n",
       "   3277: True,\n",
       "   3278: True,\n",
       "   3279: True,\n",
       "   3280: True,\n",
       "   3281: True,\n",
       "   3282: True,\n",
       "   3283: False,\n",
       "   3284: True,\n",
       "   3285: True,\n",
       "   3286: True,\n",
       "   3287: False,\n",
       "   3288: True,\n",
       "   3289: True,\n",
       "   3290: True,\n",
       "   3291: True,\n",
       "   3292: True,\n",
       "   3293: True,\n",
       "   3294: True,\n",
       "   3295: True,\n",
       "   3296: True,\n",
       "   3297: True,\n",
       "   3298: True,\n",
       "   3299: True,\n",
       "   3300: True,\n",
       "   3301: True,\n",
       "   3302: True,\n",
       "   3303: True,\n",
       "   3304: False,\n",
       "   3305: True,\n",
       "   3306: True,\n",
       "   3307: True,\n",
       "   3308: True,\n",
       "   3309: True,\n",
       "   3310: True,\n",
       "   3311: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3317: True,\n",
       "   3318: True,\n",
       "   3319: True,\n",
       "   3320: True,\n",
       "   3321: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3325: True,\n",
       "   3326: True,\n",
       "   3327: True,\n",
       "   3328: True,\n",
       "   3329: True,\n",
       "   3330: False,\n",
       "   3334: True,\n",
       "   3337: True,\n",
       "   3338: True,\n",
       "   3339: False,\n",
       "   3341: False,\n",
       "   3346: True,\n",
       "   3347: True,\n",
       "   3348: False,\n",
       "   3357: False,\n",
       "   3359: True,\n",
       "   3372: True,\n",
       "   3378: True,\n",
       "   3379: True,\n",
       "   3380: True,\n",
       "   3382: True,\n",
       "   3383: False,\n",
       "   3384: False,\n",
       "   3385: True,\n",
       "   3386: True,\n",
       "   3388: True,\n",
       "   3389: True,\n",
       "   3390: True,\n",
       "   3391: True,\n",
       "   3392: True,\n",
       "   3393: True,\n",
       "   3395: True,\n",
       "   3396: True,\n",
       "   3397: True,\n",
       "   3398: True,\n",
       "   3399: True,\n",
       "   3400: True,\n",
       "   3401: True,\n",
       "   3402: True,\n",
       "   3403: True,\n",
       "   3404: True,\n",
       "   3411: True,\n",
       "   3413: True,\n",
       "   3414: True,\n",
       "   3415: False,\n",
       "   3418: True,\n",
       "   3420: True,\n",
       "   3421: False,\n",
       "   3422: True,\n",
       "   3423: False,\n",
       "   3425: False,\n",
       "   3431: True,\n",
       "   3433: True,\n",
       "   3436: True,\n",
       "   3438: True,\n",
       "   3454: True,\n",
       "   3455: True,\n",
       "   3456: True,\n",
       "   3457: True,\n",
       "   3458: True,\n",
       "   3460: True,\n",
       "   3461: True,\n",
       "   3462: True,\n",
       "   3463: True,\n",
       "   3465: True,\n",
       "   3467: True,\n",
       "   3468: True,\n",
       "   3469: True,\n",
       "   3470: False,\n",
       "   3471: True,\n",
       "   3472: True,\n",
       "   3473: True,\n",
       "   3474: True,\n",
       "   3475: True,\n",
       "   3476: True,\n",
       "   3477: True,\n",
       "   3478: True,\n",
       "   3479: False,\n",
       "   3480: True,\n",
       "   3481: True,\n",
       "   3482: True,\n",
       "   3483: True,\n",
       "   3484: False,\n",
       "   3485: True,\n",
       "   3486: True,\n",
       "   3487: True,\n",
       "   3488: True,\n",
       "   3490: True,\n",
       "   3491: True,\n",
       "   3492: True,\n",
       "   3494: True,\n",
       "   3495: False,\n",
       "   3501: True,\n",
       "   3505: True,\n",
       "   3506: True,\n",
       "   3510: True,\n",
       "   3511: True,\n",
       "   3517: True,\n",
       "   3518: False,\n",
       "   3520: False,\n",
       "   3521: False,\n",
       "   3523: True,\n",
       "   3524: True,\n",
       "   3525: False,\n",
       "   3527: True,\n",
       "   3528: False,\n",
       "   3529: True,\n",
       "   3531: False,\n",
       "   3532: True,\n",
       "   3533: False,\n",
       "   3534: True,\n",
       "   3535: True,\n",
       "   3536: True,\n",
       "   3537: True,\n",
       "   3538: True,\n",
       "   3539: True,\n",
       "   3541: True,\n",
       "   3542: False,\n",
       "   3543: False,\n",
       "   3544: True,\n",
       "   3547: True,\n",
       "   3549: False,\n",
       "   3550: True,\n",
       "   3551: True,\n",
       "   3552: True,\n",
       "   3553: True,\n",
       "   3554: True,\n",
       "   3555: True,\n",
       "   3556: True,\n",
       "   3558: True,\n",
       "   3559: True,\n",
       "   3560: False,\n",
       "   3561: False,\n",
       "   3562: True,\n",
       "   3563: True,\n",
       "   3564: True,\n",
       "   3565: True,\n",
       "   3566: True,\n",
       "   3567: True,\n",
       "   3568: True,\n",
       "   3569: True,\n",
       "   3570: True,\n",
       "   3571: True,\n",
       "   3572: True,\n",
       "   3573: True,\n",
       "   3574: True,\n",
       "   3575: True,\n",
       "   3576: True,\n",
       "   3577: True,\n",
       "   3578: True,\n",
       "   3579: True,\n",
       "   3580: True,\n",
       "   3581: False,\n",
       "   3584: True,\n",
       "   3585: True,\n",
       "   3586: True,\n",
       "   3588: False,\n",
       "   3589: False,\n",
       "   3590: True,\n",
       "   3592: True,\n",
       "   3593: True,\n",
       "   3597: True,\n",
       "   3598: True,\n",
       "   3600: False,\n",
       "   3606: True,\n",
       "   3609: False,\n",
       "   3610: True,\n",
       "   3612: True,\n",
       "   3613: False,\n",
       "   3616: True,\n",
       "   3617: True,\n",
       "   3618: False,\n",
       "   3622: True,\n",
       "   3623: True,\n",
       "   3626: True,\n",
       "   3627: True,\n",
       "   3628: True,\n",
       "   3630: True,\n",
       "   3633: True,\n",
       "   3636: True,\n",
       "   3637: True,\n",
       "   3638: False,\n",
       "   3640: True,\n",
       "   3641: True,\n",
       "   3643: True,\n",
       "   3644: True,\n",
       "   3645: True,\n",
       "   3646: True,\n",
       "   3648: True,\n",
       "   3651: True,\n",
       "   3652: True,\n",
       "   3653: True,\n",
       "   3654: True,\n",
       "   3656: True,\n",
       "   3657: True,\n",
       "   3658: True,\n",
       "   3659: True,\n",
       "   3660: False,\n",
       "   3665: True,\n",
       "   3666: True,\n",
       "   3668: True,\n",
       "   3669: True,\n",
       "   3670: True,\n",
       "   3674: True,\n",
       "   3677: False,\n",
       "   3681: True,\n",
       "   3682: True,\n",
       "   3683: True,\n",
       "   3686: False,\n",
       "   3687: True,\n",
       "   3689: True,\n",
       "   3693: True,\n",
       "   3694: False,\n",
       "   3696: True,\n",
       "   3697: False,\n",
       "   3698: True,\n",
       "   3699: True,\n",
       "   3700: False,\n",
       "   3701: True,\n",
       "   3702: True,\n",
       "   3703: True,\n",
       "   3704: True,\n",
       "   3706: False,\n",
       "   3708: False,\n",
       "   3710: True,\n",
       "   3712: False,\n",
       "   3721: True,\n",
       "   3729: True,\n",
       "   3732: True,\n",
       "   3733: False,\n",
       "   3734: False,\n",
       "   3735: True,\n",
       "   3736: True,\n",
       "   3743: False,\n",
       "   3744: False,\n",
       "   3746: True,\n",
       "   3748: True,\n",
       "   3749: True,\n",
       "   3751: True,\n",
       "   3752: True,\n",
       "   3755: True,\n",
       "   3756: True,\n",
       "   3757: False,\n",
       "   3758: True,\n",
       "   3759: True,\n",
       "   3767: True,\n",
       "   3773: True,\n",
       "   3775: False,\n",
       "   3779: True,\n",
       "   3784: True,\n",
       "   3786: True,\n",
       "   3788: True,\n",
       "   3793: False,\n",
       "   3818: True,\n",
       "   4501: True,\n",
       "   4502: True,\n",
       "   4504: True,\n",
       "   4741: True,\n",
       "   4743: False,\n",
       "   4745: True,\n",
       "   4748: True,\n",
       "   4750: True,\n",
       "   4751: True,\n",
       "   4753: True,\n",
       "   4754: True,\n",
       "   4755: True,\n",
       "   4757: True,\n",
       "   4758: True,\n",
       "   4759: True,\n",
       "   4761: True,\n",
       "   4767: False,\n",
       "   4771: True,\n",
       "   4774: True,\n",
       "   4775: True,\n",
       "   4776: True,\n",
       "   4781: True,\n",
       "   5360: False,\n",
       "   5361: True,\n",
       "   5373: True,\n",
       "   5375: True,\n",
       "   5376: True,\n",
       "   5385: True,\n",
       "   5386: True,\n",
       "   5448: True},\n",
       "  0.888268156424581],\n",
       " 'pa-i-ti-ja': [{210: True,\n",
       "   1356: False,\n",
       "   2577: True,\n",
       "   2654: True,\n",
       "   3514: True,\n",
       "   3871: True,\n",
       "   4170: True,\n",
       "   4394: False,\n",
       "   4456: True},\n",
       "  0.7777777777777778],\n",
       " 'da-wi-ja': [{210: True, 2572: True, 4380: False, 4422: False, 4456: True},\n",
       "  0.6],\n",
       " 'su-ri-mi-jo': [{211: False, 3870: False, 4133: True}, 0.3333333333333333],\n",
       " 'u-ta-ni-jo': [{211: False, 1360: True, 2715: True, 3870: True}, 0.75],\n",
       " 'ti-ri-ti-jo': [{211: False, 3870: True}, 0.5],\n",
       " 'qa-mi-jo': [{211: True, 3870: True}, 1.0],\n",
       " 'pu-si-jo': [{211: True, 2760: True, 3870: True}, 1.0],\n",
       " 'ru-ki-ti-jo': [{211: True,\n",
       "   967: True,\n",
       "   1184: False,\n",
       "   2776: True,\n",
       "   3867: True,\n",
       "   3869: True,\n",
       "   3870: True,\n",
       "   4130: True},\n",
       "  0.875],\n",
       " 'tu-ri-si-jo': [{211: True, 2715: True, 3867: True}, 1.0],\n",
       " 'qa-ra-jo': [{211: True,\n",
       "   620: False,\n",
       "   673: False,\n",
       "   2678: True,\n",
       "   3870: True,\n",
       "   4138: True},\n",
       "  0.6666666666666666],\n",
       " 'mo-ri-wo-do': [{212: True}, 0.75],\n",
       " 'ra-jo': [{213: False, 1150: False, 2644: False, 2750: False, 2876: False},\n",
       "  0.0],\n",
       " 'qa-ra-o': [{213: True}, 1.0],\n",
       " 'jo-a-mi-ni-so-de': [{214: True}, 1.0],\n",
       " 'di-do': [{214: False}, 0.0],\n",
       " 'ku-pe-se-ro': [{214: True}, 1.0],\n",
       " 'me-to-re': [{214: True, 3033: False, 5229: True}, 0.6666666666666666],\n",
       " 'ne-ri-wa-to': [{214: True}, 1.0],\n",
       " 'pi-ri': [{214: False, 1033: False, 1250: False, 4988: False, 5363: False},\n",
       "  0.0],\n",
       " 'no': [{215: False,\n",
       "   259: False,\n",
       "   304: False,\n",
       "   598: False,\n",
       "   648: False,\n",
       "   670: False,\n",
       "   881: False,\n",
       "   949: False,\n",
       "   1096: False,\n",
       "   1106: False,\n",
       "   1131: False,\n",
       "   1238: False,\n",
       "   1332: False,\n",
       "   1464: False,\n",
       "   1517: False,\n",
       "   1667: False,\n",
       "   1683: False,\n",
       "   1684: False,\n",
       "   2028: False,\n",
       "   2130: False,\n",
       "   2218: False,\n",
       "   2430: False,\n",
       "   2459: False,\n",
       "   2460: False,\n",
       "   2461: False,\n",
       "   2462: False,\n",
       "   2463: False,\n",
       "   2464: False,\n",
       "   2473: False,\n",
       "   2478: False,\n",
       "   2529: False,\n",
       "   2678: False,\n",
       "   2681: False,\n",
       "   2773: False,\n",
       "   2811: False,\n",
       "   2852: False,\n",
       "   3410: False,\n",
       "   3417: False,\n",
       "   3668: False,\n",
       "   3804: False,\n",
       "   4012: False,\n",
       "   4029: False,\n",
       "   4122: False,\n",
       "   4235: False,\n",
       "   4407: False,\n",
       "   4723: False,\n",
       "   4730: False,\n",
       "   5260: False,\n",
       "   5395: False,\n",
       "   5746: False,\n",
       "   5753: False,\n",
       "   5758: False},\n",
       "  0.0],\n",
       " 'ka-te-ro': [{217: False}, 0.0],\n",
       " 'e-re-pa-ta': [{225: True}, 1.0],\n",
       " 'o-no': [{227: False,\n",
       "   2342: True,\n",
       "   2343: True,\n",
       "   2826: True,\n",
       "   2827: False,\n",
       "   3942: True,\n",
       "   3943: True,\n",
       "   3955: True,\n",
       "   3965: True,\n",
       "   4007: True,\n",
       "   4289: False,\n",
       "   4466: True,\n",
       "   4698: True,\n",
       "   4723: False,\n",
       "   4729: True,\n",
       "   5381: True,\n",
       "   5395: False,\n",
       "   5537: False},\n",
       "  0.631578947368421],\n",
       " 'u': [{227: False,\n",
       "   399: False,\n",
       "   658: False,\n",
       "   1493: False,\n",
       "   1594: False,\n",
       "   2066: False,\n",
       "   2195: False,\n",
       "   2477: False,\n",
       "   2711: False,\n",
       "   3691: False,\n",
       "   3737: False,\n",
       "   3954: False,\n",
       "   4363: False,\n",
       "   4744: False,\n",
       "   5058: False,\n",
       "   5139: False,\n",
       "   5543: False},\n",
       "  0.0],\n",
       " 'MU': [{233: False, 3942: True, 3964: True, 4010: True}, 0.75],\n",
       " 'ti-ri-to': [{234: False,\n",
       "   616: True,\n",
       "   621: True,\n",
       "   626: True,\n",
       "   630: True,\n",
       "   632: True,\n",
       "   633: True,\n",
       "   636: True,\n",
       "   669: True,\n",
       "   1026: False,\n",
       "   1408: False,\n",
       "   2964: False,\n",
       "   2999: False,\n",
       "   3078: True,\n",
       "   3079: True,\n",
       "   3108: True,\n",
       "   3227: True,\n",
       "   3267: True,\n",
       "   3290: True,\n",
       "   3298: True,\n",
       "   3300: True,\n",
       "   3476: True,\n",
       "   3535: True,\n",
       "   3536: True,\n",
       "   3537: True,\n",
       "   3539: True,\n",
       "   3544: True,\n",
       "   3575: True,\n",
       "   3576: True,\n",
       "   3696: False,\n",
       "   4295: True,\n",
       "   4486: True,\n",
       "   4505: True},\n",
       "  0.8181818181818182],\n",
       " 'di-za-so': [{234: True, 3624: False}, 0.5],\n",
       " '*168+SE': [{234: False,\n",
       "   235: True,\n",
       "   236: True,\n",
       "   237: True,\n",
       "   238: False,\n",
       "   240: True},\n",
       "  0.6666666666666666],\n",
       " 'a-na-re-u': [{235: True}, 1.0],\n",
       " 'qa-ra': [{236: False,\n",
       "   622: True,\n",
       "   627: True,\n",
       "   631: True,\n",
       "   635: True,\n",
       "   640: True,\n",
       "   688: True,\n",
       "   1126: False,\n",
       "   1187: True,\n",
       "   1306: True,\n",
       "   1551: False,\n",
       "   2643: False,\n",
       "   2938: True,\n",
       "   2966: True,\n",
       "   2991: True,\n",
       "   3060: True,\n",
       "   3086: True,\n",
       "   3087: True,\n",
       "   3089: True,\n",
       "   3090: True,\n",
       "   3228: True,\n",
       "   3229: True,\n",
       "   3230: True,\n",
       "   3231: True,\n",
       "   3232: True,\n",
       "   3239: True,\n",
       "   3245: True,\n",
       "   3252: True,\n",
       "   3449: True,\n",
       "   3476: True,\n",
       "   3578: False,\n",
       "   3627: True,\n",
       "   3812: False,\n",
       "   4225: False,\n",
       "   4258: True},\n",
       "  0.8157894736842105],\n",
       " 're-me-to': [{236: True, 3173: False}, 0.5],\n",
       " 'u-ta-no': [{237: False,\n",
       "   973: True,\n",
       "   1260: True,\n",
       "   1742: True,\n",
       "   2671: True,\n",
       "   3054: True,\n",
       "   3205: True,\n",
       "   3302: True,\n",
       "   3412: True,\n",
       "   3485: True,\n",
       "   3530: False,\n",
       "   3570: True,\n",
       "   3571: True,\n",
       "   3572: True,\n",
       "   3573: False,\n",
       "   3574: True,\n",
       "   4004: True,\n",
       "   4092: True},\n",
       "  0.8421052631578947],\n",
       " 'pa-da': [{237: False}, 0.0],\n",
       " 'qa-mo': [{238: True,\n",
       "   962: False,\n",
       "   1904: False,\n",
       "   2910: False,\n",
       "   2978: True,\n",
       "   3055: True,\n",
       "   3294: True,\n",
       "   3295: True,\n",
       "   3485: True,\n",
       "   3648: True,\n",
       "   3653: True,\n",
       "   4132: True},\n",
       "  0.75],\n",
       " 'e-ra': [{239: True,\n",
       "   963: True,\n",
       "   1208: True,\n",
       "   1759: False,\n",
       "   1907: False,\n",
       "   2107: False,\n",
       "   2108: False,\n",
       "   2404: False,\n",
       "   2612: False,\n",
       "   2898: True,\n",
       "   2980: False,\n",
       "   2981: True,\n",
       "   2985: True,\n",
       "   3038: True,\n",
       "   3126: True,\n",
       "   3150: True,\n",
       "   3154: True,\n",
       "   3208: True,\n",
       "   3256: True,\n",
       "   3395: True,\n",
       "   3397: True,\n",
       "   3521: True,\n",
       "   3525: True,\n",
       "   3586: True,\n",
       "   3588: True,\n",
       "   3590: False,\n",
       "   3610: False,\n",
       "   3682: True,\n",
       "   3698: True,\n",
       "   3747: False,\n",
       "   3905: False,\n",
       "   4045: False,\n",
       "   4300: False,\n",
       "   4501: True,\n",
       "   5357: True,\n",
       "   5472: False},\n",
       "  0.6216216216216216],\n",
       " 'ta-ta-ro': [{239: True,\n",
       "   1120: False,\n",
       "   2673: True,\n",
       "   4893: True,\n",
       "   4937: True,\n",
       "   4949: True},\n",
       "  0.8333333333333334],\n",
       " '*168': [{239: False, 605: True}, 0.6666666666666666],\n",
       " 'mu-ka-ra': [{239: True}, 1.0],\n",
       " 'je-ne': [{244: False}, 0.0],\n",
       " 'ZE': [{244: True,\n",
       "   276: True,\n",
       "   279: True,\n",
       "   280: True,\n",
       "   284: True,\n",
       "   286: True,\n",
       "   291: False,\n",
       "   292: False,\n",
       "   302: True,\n",
       "   312: True,\n",
       "   316: True,\n",
       "   319: True,\n",
       "   324: True,\n",
       "   325: True,\n",
       "   332: True,\n",
       "   339: False,\n",
       "   340: True,\n",
       "   344: True,\n",
       "   346: True,\n",
       "   353: False,\n",
       "   359: True,\n",
       "   363: True,\n",
       "   364: True,\n",
       "   365: True,\n",
       "   368: True,\n",
       "   372: True,\n",
       "   376: True,\n",
       "   384: True,\n",
       "   391: True,\n",
       "   396: True,\n",
       "   397: True,\n",
       "   400: True,\n",
       "   404: False,\n",
       "   426: True,\n",
       "   428: False,\n",
       "   429: True,\n",
       "   440: False,\n",
       "   441: True,\n",
       "   448: False,\n",
       "   450: False,\n",
       "   519: True,\n",
       "   555: True,\n",
       "   557: True,\n",
       "   558: True,\n",
       "   559: True,\n",
       "   560: True,\n",
       "   562: True,\n",
       "   563: True,\n",
       "   564: True,\n",
       "   565: True,\n",
       "   566: True,\n",
       "   567: True,\n",
       "   568: True,\n",
       "   569: True,\n",
       "   570: True,\n",
       "   571: True,\n",
       "   572: False,\n",
       "   573: True,\n",
       "   574: True,\n",
       "   575: True,\n",
       "   576: True,\n",
       "   577: True,\n",
       "   578: True,\n",
       "   1157: True,\n",
       "   2794: True,\n",
       "   2802: True,\n",
       "   2819: True,\n",
       "   2834: True,\n",
       "   2840: True,\n",
       "   2846: True,\n",
       "   2847: True,\n",
       "   2848: True,\n",
       "   2849: True,\n",
       "   2850: True,\n",
       "   2852: False,\n",
       "   2853: True,\n",
       "   2855: True,\n",
       "   2860: True,\n",
       "   2861: True,\n",
       "   2862: True,\n",
       "   2865: True,\n",
       "   4230: True,\n",
       "   4737: True,\n",
       "   4738: True,\n",
       "   5295: True,\n",
       "   5297: True,\n",
       "   5298: True,\n",
       "   5299: True,\n",
       "   5300: True,\n",
       "   5301: True,\n",
       "   5303: True,\n",
       "   5304: True,\n",
       "   5305: True,\n",
       "   5306: True,\n",
       "   5307: True,\n",
       "   5308: True,\n",
       "   5309: True,\n",
       "   5310: True,\n",
       "   5311: True,\n",
       "   5312: True,\n",
       "   5313: True,\n",
       "   5314: True,\n",
       "   5315: True,\n",
       "   5316: True,\n",
       "   5317: True,\n",
       "   5318: True,\n",
       "   5319: True,\n",
       "   5320: True,\n",
       "   5321: True,\n",
       "   5322: True,\n",
       "   5323: True,\n",
       "   5324: True,\n",
       "   5325: True,\n",
       "   5326: True,\n",
       "   5327: True,\n",
       "   5328: True,\n",
       "   5329: True,\n",
       "   5331: False,\n",
       "   5335: True,\n",
       "   5339: True,\n",
       "   5369: True,\n",
       "   5401: True,\n",
       "   5714: True},\n",
       "  0.918918918918919],\n",
       " 'e-ke-a': [{245: True, 686: True}, 1.0],\n",
       " 'ka-ka': [{245: True, 5743: False, 5750: False}, 0.3333333333333333],\n",
       " 're-a': [{245: True}, 1.0],\n",
       " 'HAS': [{245: True}, 1.0],\n",
       " 'SAG': [{246: True}, 1.0],\n",
       " 'e-re-pa-te': [{248: True,\n",
       "   249: False,\n",
       "   451: True,\n",
       "   453: True,\n",
       "   458: True,\n",
       "   465: True,\n",
       "   2032: False,\n",
       "   5345: True,\n",
       "   5347: True},\n",
       "  0.7777777777777778],\n",
       " 'de-de-me-na': [{248: True, 271: True, 272: True}, 1.0],\n",
       " 'PUG': [{248: False,\n",
       "   250: True,\n",
       "   251: True,\n",
       "   252: True,\n",
       "   253: False,\n",
       "   254: True,\n",
       "   256: True,\n",
       "   258: True,\n",
       "   264: False,\n",
       "   267: False,\n",
       "   270: True,\n",
       "   271: True,\n",
       "   273: False},\n",
       "  0.6153846153846154],\n",
       " 'zo-wa': [{248: False, 249: True, 909: True, 1216: True}, 0.75],\n",
       " 'e-pi-zo-ta': [{248: True, 249: False, 271: True}, 0.6666666666666666],\n",
       " 'ke-ra': [{248: True,\n",
       "   271: True,\n",
       "   272: True,\n",
       "   274: False,\n",
       "   1581: False,\n",
       "   4865: False,\n",
       "   4953: True},\n",
       "  0.5714285714285714],\n",
       " 'GUP': [{248: False, 249: False, 271: True, 273: False}, 0.25],\n",
       " 'qo-jo': [{249: False}, 0.0],\n",
       " 'pa-ka-na': [{250: True,\n",
       "   251: True,\n",
       "   255: True,\n",
       "   258: True,\n",
       "   259: True,\n",
       "   260: True,\n",
       "   262: True,\n",
       "   263: True,\n",
       "   264: False,\n",
       "   265: False},\n",
       "  0.8],\n",
       " 'ka-si-ko-no': [{251: True,\n",
       "   256: True,\n",
       "   262: True,\n",
       "   266: True,\n",
       "   267: True,\n",
       "   268: True,\n",
       "   269: True,\n",
       "   4702: True},\n",
       "  1.0],\n",
       " 'a-ra-ru-wo-a': [{251: True,\n",
       "   252: True,\n",
       "   253: True,\n",
       "   255: True,\n",
       "   258: True,\n",
       "   260: True,\n",
       "   262: True,\n",
       "   263: True,\n",
       "   264: True,\n",
       "   265: True},\n",
       "  1.0],\n",
       " 'de-so-mo': [{253: True, 258: True, 5142: False}, 0.6666666666666666],\n",
       " 'a-mi-to-no': [{253: True}, 1.0],\n",
       " 'pi-ri-te': [{253: True}, 1.0],\n",
       " 'da-zo': [{257: True, 2684: True}, 1.0],\n",
       " 'pi-ri-je-te': [{257: False, 258: True, 259: True}, 0.6666666666666666],\n",
       " 'ku-ka-ro': [{258: True, 682: False, 2964: True}, 0.6666666666666666],\n",
       " 'te': [{260: False,\n",
       "   664: False,\n",
       "   703: False,\n",
       "   774: True,\n",
       "   1129: False,\n",
       "   1151: False,\n",
       "   1259: False,\n",
       "   1383: False,\n",
       "   1455: False,\n",
       "   1600: False,\n",
       "   1871: False,\n",
       "   2141: False,\n",
       "   2142: False,\n",
       "   2217: False,\n",
       "   2683: True,\n",
       "   2694: True,\n",
       "   3935: True,\n",
       "   3966: False,\n",
       "   4397: False,\n",
       "   4464: False,\n",
       "   4493: False,\n",
       "   4696: False,\n",
       "   4771: False,\n",
       "   5032: False,\n",
       "   5142: False,\n",
       "   5149: False,\n",
       "   5324: False,\n",
       "   5548: False,\n",
       "   5563: False},\n",
       "  0.13793103448275862],\n",
       " 'wi-jo': [{266: False, 947: False, 2681: False, 5141: False}, 0.0],\n",
       " 'o-wa-si-jo': [{267: True}, 1.0],\n",
       " 'u-ta-jo': [{269: False,\n",
       "   687: True,\n",
       "   2943: True,\n",
       "   2944: True,\n",
       "   2945: True,\n",
       "   2947: True,\n",
       "   2993: True,\n",
       "   2995: True,\n",
       "   2996: True,\n",
       "   3002: True,\n",
       "   3112: True,\n",
       "   3123: True,\n",
       "   3141: True,\n",
       "   3142: True,\n",
       "   3155: True,\n",
       "   3201: True,\n",
       "   3218: True,\n",
       "   3220: True,\n",
       "   3243: False,\n",
       "   3559: True,\n",
       "   3586: False,\n",
       "   3590: True,\n",
       "   3597: True,\n",
       "   3688: True},\n",
       "  0.875],\n",
       " 'ki-rya-i-jo': [{275: True}, 1.0],\n",
       " 'TUN': [{275: True,\n",
       "   276: True,\n",
       "   277: True,\n",
       "   279: True,\n",
       "   281: False,\n",
       "   283: False,\n",
       "   284: True,\n",
       "   288: True,\n",
       "   291: True,\n",
       "   293: True,\n",
       "   294: True,\n",
       "   295: True,\n",
       "   296: True,\n",
       "   297: True,\n",
       "   298: False,\n",
       "   299: True,\n",
       "   303: True,\n",
       "   310: False,\n",
       "   311: True,\n",
       "   312: True,\n",
       "   313: True,\n",
       "   314: True,\n",
       "   315: True,\n",
       "   316: True,\n",
       "   318: False,\n",
       "   319: True,\n",
       "   320: True,\n",
       "   321: False,\n",
       "   323: False,\n",
       "   326: True,\n",
       "   328: True,\n",
       "   332: True,\n",
       "   337: True,\n",
       "   338: False,\n",
       "   343: False,\n",
       "   344: False,\n",
       "   345: False,\n",
       "   348: False,\n",
       "   349: False,\n",
       "   350: False,\n",
       "   360: False,\n",
       "   361: True,\n",
       "   367: False,\n",
       "   368: True,\n",
       "   373: False,\n",
       "   375: False,\n",
       "   378: True,\n",
       "   386: False,\n",
       "   389: True,\n",
       "   394: True,\n",
       "   398: False,\n",
       "   399: True,\n",
       "   402: True,\n",
       "   403: True,\n",
       "   405: False,\n",
       "   407: True,\n",
       "   409: False,\n",
       "   411: False,\n",
       "   415: False,\n",
       "   416: False,\n",
       "   417: False,\n",
       "   418: False,\n",
       "   419: False,\n",
       "   430: False,\n",
       "   431: False,\n",
       "   433: False,\n",
       "   434: False},\n",
       "  0.5294117647058824],\n",
       " 'BIG': [{275: True,\n",
       "   276: True,\n",
       "   277: False,\n",
       "   279: True,\n",
       "   281: True,\n",
       "   283: True,\n",
       "   285: True,\n",
       "   287: False,\n",
       "   288: True,\n",
       "   291: True,\n",
       "   292: True,\n",
       "   294: False,\n",
       "   295: False,\n",
       "   296: True,\n",
       "   297: False,\n",
       "   298: True,\n",
       "   300: False,\n",
       "   302: False,\n",
       "   303: True,\n",
       "   304: False,\n",
       "   305: True,\n",
       "   308: False,\n",
       "   309: True,\n",
       "   311: False,\n",
       "   312: True,\n",
       "   313: False,\n",
       "   314: False,\n",
       "   315: False,\n",
       "   316: False,\n",
       "   317: True,\n",
       "   320: False,\n",
       "   322: False,\n",
       "   324: False,\n",
       "   326: False,\n",
       "   327: False,\n",
       "   328: False,\n",
       "   329: False,\n",
       "   330: True,\n",
       "   331: False,\n",
       "   332: True,\n",
       "   333: False,\n",
       "   334: False,\n",
       "   335: False,\n",
       "   337: True,\n",
       "   341: False,\n",
       "   342: False,\n",
       "   344: True,\n",
       "   345: True,\n",
       "   348: True,\n",
       "   349: True,\n",
       "   350: True,\n",
       "   351: False,\n",
       "   352: True,\n",
       "   353: False,\n",
       "   354: False,\n",
       "   355: False,\n",
       "   356: False,\n",
       "   358: False,\n",
       "   360: True,\n",
       "   362: False,\n",
       "   363: False,\n",
       "   365: False,\n",
       "   366: False,\n",
       "   367: False,\n",
       "   368: True,\n",
       "   369: True,\n",
       "   370: False,\n",
       "   373: True,\n",
       "   374: False,\n",
       "   377: False,\n",
       "   379: False,\n",
       "   380: False,\n",
       "   381: False,\n",
       "   383: False,\n",
       "   387: False,\n",
       "   388: False,\n",
       "   389: False,\n",
       "   390: False,\n",
       "   391: False,\n",
       "   392: False,\n",
       "   393: False,\n",
       "   395: False,\n",
       "   401: False,\n",
       "   402: False,\n",
       "   403: False,\n",
       "   406: False,\n",
       "   408: False,\n",
       "   413: False,\n",
       "   419: False,\n",
       "   420: False,\n",
       "   421: False,\n",
       "   422: False,\n",
       "   423: False,\n",
       "   424: False,\n",
       "   425: False,\n",
       "   426: False,\n",
       "   435: False,\n",
       "   436: False,\n",
       "   437: False,\n",
       "   438: False,\n",
       "   439: False,\n",
       "   445: False,\n",
       "   446: False,\n",
       "   447: False,\n",
       "   449: False,\n",
       "   454: True,\n",
       "   983: False,\n",
       "   1068: False,\n",
       "   1079: False,\n",
       "   1782: False},\n",
       "  0.2727272727272727],\n",
       " 'EQU': [{275: True,\n",
       "   276: True,\n",
       "   279: True,\n",
       "   280: True,\n",
       "   282: True,\n",
       "   283: True,\n",
       "   284: True,\n",
       "   286: True,\n",
       "   287: True,\n",
       "   288: True,\n",
       "   289: True,\n",
       "   291: True,\n",
       "   292: True,\n",
       "   301: False,\n",
       "   302: True,\n",
       "   303: False,\n",
       "   312: True,\n",
       "   316: True,\n",
       "   319: False,\n",
       "   322: True,\n",
       "   324: True,\n",
       "   327: False,\n",
       "   330: False,\n",
       "   332: True,\n",
       "   333: False,\n",
       "   334: True,\n",
       "   335: True,\n",
       "   336: False,\n",
       "   339: True,\n",
       "   340: True,\n",
       "   344: True,\n",
       "   345: True,\n",
       "   346: False,\n",
       "   347: False,\n",
       "   350: True,\n",
       "   351: True,\n",
       "   353: True,\n",
       "   354: False,\n",
       "   355: False,\n",
       "   357: False,\n",
       "   359: False,\n",
       "   363: True,\n",
       "   364: False,\n",
       "   365: True,\n",
       "   366: False,\n",
       "   368: True,\n",
       "   370: True,\n",
       "   371: True,\n",
       "   372: False,\n",
       "   373: True,\n",
       "   374: False,\n",
       "   376: True,\n",
       "   377: False,\n",
       "   379: False,\n",
       "   384: False,\n",
       "   385: False,\n",
       "   392: False,\n",
       "   396: False,\n",
       "   400: False,\n",
       "   401: False,\n",
       "   404: True,\n",
       "   408: False,\n",
       "   410: False,\n",
       "   413: False,\n",
       "   425: False,\n",
       "   426: True,\n",
       "   427: False,\n",
       "   428: False,\n",
       "   432: False,\n",
       "   439: False,\n",
       "   440: False,\n",
       "   441: False,\n",
       "   442: False,\n",
       "   443: False,\n",
       "   444: False,\n",
       "   448: False,\n",
       "   450: False,\n",
       "   1060: False,\n",
       "   1068: False,\n",
       "   1074: False,\n",
       "   1157: False,\n",
       "   2826: True,\n",
       "   2827: False,\n",
       "   5295: True},\n",
       "  0.4588235294117647],\n",
       " 'ru-o-wo': [{276: False}, 0.0],\n",
       " 'qa-mi-si-jo': [{277: True}, 1.0],\n",
       " 'a-mi-ni-si': [{279: True, 297: False, 1192: False, 1282: False, 4003: False},\n",
       "  0.2],\n",
       " 'MO': [{282: True,\n",
       "   283: True,\n",
       "   287: True,\n",
       "   289: True,\n",
       "   290: False,\n",
       "   301: True,\n",
       "   334: True,\n",
       "   373: True,\n",
       "   555: False,\n",
       "   558: True,\n",
       "   562: True,\n",
       "   564: True,\n",
       "   566: True,\n",
       "   568: True,\n",
       "   574: True,\n",
       "   2425: True,\n",
       "   5299: False,\n",
       "   5301: True,\n",
       "   5316: True},\n",
       "  0.85],\n",
       " 'me-za-wo': [{284: True}, 1.0],\n",
       " 'DA': [{285: False,\n",
       "   317: True,\n",
       "   613: True,\n",
       "   616: True,\n",
       "   617: True,\n",
       "   618: True,\n",
       "   619: True,\n",
       "   620: False,\n",
       "   621: True,\n",
       "   622: True,\n",
       "   623: True,\n",
       "   624: True,\n",
       "   636: True,\n",
       "   638: False,\n",
       "   640: False,\n",
       "   642: True,\n",
       "   643: True,\n",
       "   644: True,\n",
       "   645: True,\n",
       "   646: True,\n",
       "   647: True,\n",
       "   648: True,\n",
       "   649: True,\n",
       "   650: True,\n",
       "   651: True,\n",
       "   652: True,\n",
       "   654: False,\n",
       "   655: False,\n",
       "   656: False,\n",
       "   657: False,\n",
       "   729: True,\n",
       "   2422: True,\n",
       "   2550: True,\n",
       "   2551: True,\n",
       "   2552: True,\n",
       "   2553: True,\n",
       "   2559: True,\n",
       "   2562: True,\n",
       "   2564: True,\n",
       "   2567: False,\n",
       "   2577: True,\n",
       "   2583: True,\n",
       "   2612: True,\n",
       "   2624: False,\n",
       "   2674: True,\n",
       "   4536: True,\n",
       "   4538: True,\n",
       "   4540: True,\n",
       "   4541: True,\n",
       "   4542: True,\n",
       "   4544: True,\n",
       "   4545: True,\n",
       "   4546: True,\n",
       "   4547: True,\n",
       "   4550: True,\n",
       "   4551: True,\n",
       "   4554: True,\n",
       "   4555: True,\n",
       "   4556: True,\n",
       "   4557: True,\n",
       "   4559: True,\n",
       "   4560: True,\n",
       "   4564: True,\n",
       "   4565: True,\n",
       "   4567: True,\n",
       "   4570: True,\n",
       "   4571: True,\n",
       "   4573: True,\n",
       "   4575: True,\n",
       "   4576: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4589: True,\n",
       "   4591: False,\n",
       "   4592: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: True,\n",
       "   4601: True,\n",
       "   4602: True,\n",
       "   4605: True,\n",
       "   4611: True,\n",
       "   4614: True,\n",
       "   4615: True,\n",
       "   4616: True,\n",
       "   4618: True,\n",
       "   4730: True,\n",
       "   4933: True,\n",
       "   5390: True},\n",
       "  0.8620689655172413],\n",
       " 'pa-di-jo': [{286: True}, 1.0],\n",
       " 'TUN+QE': [{286: True,\n",
       "   289: True,\n",
       "   290: True,\n",
       "   325: False,\n",
       "   412: False,\n",
       "   414: False},\n",
       "  0.5],\n",
       " '*166': [{287: True, 351: False, 383: False, 5081: False, 5625: False}, 0.2],\n",
       " 'ti-ri-jo-qa': [{288: True, 853: False, 980: False}, 0.3333333333333333],\n",
       " 'e-ko': [{288: True, 574: True, 1038: False}, 0.6666666666666666],\n",
       " 'ri-jo': [{290: False,\n",
       "   708: False,\n",
       "   1142: False,\n",
       "   1608: False,\n",
       "   2321: False,\n",
       "   2470: False,\n",
       "   3260: False,\n",
       "   3730: False,\n",
       "   3883: False,\n",
       "   4457: False,\n",
       "   4694: True,\n",
       "   4721: True,\n",
       "   4729: True,\n",
       "   4768: False,\n",
       "   4772: True,\n",
       "   5072: True,\n",
       "   5098: True,\n",
       "   5170: True},\n",
       "  0.3888888888888889],\n",
       " 'o-pi-ri-mi-ni-jo': [{291: True}, 1.0],\n",
       " 'a-qa-ro': [{293: False}, 0.0],\n",
       " 'po-*34-wi-do': [{295: True}, 1.0],\n",
       " 'ku-ru-me-no': [{296: True, 4737: True}, 1.0],\n",
       " 'a-e-da-do-ro': [{297: True, 3901: True}, 1.0],\n",
       " 'me-nu-wa': [{298: True, 661: True, 1056: False, 4729: True}, 0.75],\n",
       " 'a-ko-to': [{299: True, 310: True, 1035: True}, 1.0],\n",
       " 'ta-pa-no': [{300: True, 1442: False, 3901: True}, 0.6666666666666666],\n",
       " 'a-*47': [{302: False}, 0.0],\n",
       " 'phu-re-wa': [{303: True}, 1.0],\n",
       " 'e-ro-e': [{304: True}, 1.0],\n",
       " 'qe-ra-di-ri-jo': [{306: True}, 1.0],\n",
       " '*165': [{306: False,\n",
       "   307: True,\n",
       "   308: True,\n",
       "   309: False,\n",
       "   382: False,\n",
       "   5736: False},\n",
       "  0.3333333333333333],\n",
       " 'pa-re': [{307: True, 309: False, 3437: True}, 0.6666666666666666],\n",
       " 'je-u': [{311: False, 1671: False}, 0.0],\n",
       " 'we-wa-do-ro': [{312: True, 3627: False}, 0.5],\n",
       " 'a-mi-ni-si-jo': [{312: True,\n",
       "   395: False,\n",
       "   402: False,\n",
       "   403: False,\n",
       "   710: True,\n",
       "   848: True,\n",
       "   1248: True,\n",
       "   2642: True},\n",
       "  0.625],\n",
       " 'i-sa-wo': [{313: True}, 1.0],\n",
       " 'pa-wa-wo': [{314: True, 4755: True, 5411: True}, 1.0],\n",
       " 'di-so': [{315: True, 864: False}, 0.5],\n",
       " 'po-*34': [{315: False}, 0.0],\n",
       " 'a-ka-to': [{316: True, 941: False, 2305: False, 3644: True}, 0.5],\n",
       " 'a-re-ka-tu-ru-wo': [{316: True}, 1.0],\n",
       " 'T': [{316: True,\n",
       "   2083: False,\n",
       "   2326: True,\n",
       "   2329: True,\n",
       "   2331: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2335: True,\n",
       "   2375: True,\n",
       "   2504: True,\n",
       "   2505: True,\n",
       "   2506: False,\n",
       "   2517: True,\n",
       "   2523: True,\n",
       "   2537: True,\n",
       "   2545: True,\n",
       "   2553: True,\n",
       "   2643: True,\n",
       "   3865: False,\n",
       "   3867: True,\n",
       "   3870: True,\n",
       "   3872: True,\n",
       "   3875: False,\n",
       "   3878: True,\n",
       "   3881: True,\n",
       "   3882: True,\n",
       "   3890: True,\n",
       "   3891: True,\n",
       "   3900: True,\n",
       "   3901: True,\n",
       "   3902: False,\n",
       "   3903: True,\n",
       "   3907: True,\n",
       "   3908: True,\n",
       "   3912: True,\n",
       "   3915: True,\n",
       "   3916: True,\n",
       "   3921: True,\n",
       "   3924: False,\n",
       "   3925: True,\n",
       "   3926: True,\n",
       "   3929: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4107: True,\n",
       "   4109: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4114: True,\n",
       "   4115: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4124: False,\n",
       "   4125: True,\n",
       "   4126: True,\n",
       "   4129: True,\n",
       "   4130: True,\n",
       "   4131: True,\n",
       "   4133: True,\n",
       "   4136: True,\n",
       "   4145: False,\n",
       "   4147: True,\n",
       "   4148: True,\n",
       "   4151: True,\n",
       "   4159: False,\n",
       "   4160: True,\n",
       "   4162: True,\n",
       "   4163: True,\n",
       "   4177: True,\n",
       "   4179: True,\n",
       "   4181: True,\n",
       "   4187: True,\n",
       "   4508: True,\n",
       "   4515: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4590: True,\n",
       "   4591: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: True,\n",
       "   4598: True,\n",
       "   4600: True,\n",
       "   4602: True,\n",
       "   4603: True,\n",
       "   4605: True,\n",
       "   4609: True,\n",
       "   4611: True,\n",
       "   4613: True,\n",
       "   4616: True,\n",
       "   4618: True,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4628: True,\n",
       "   4702: True,\n",
       "   4787: True,\n",
       "   4788: True,\n",
       "   4789: True,\n",
       "   4791: True,\n",
       "   4792: True,\n",
       "   4793: True,\n",
       "   4794: True,\n",
       "   4795: True,\n",
       "   4797: True,\n",
       "   4798: True,\n",
       "   4799: True,\n",
       "   4800: True,\n",
       "   4801: True,\n",
       "   4805: True,\n",
       "   4806: True,\n",
       "   4808: True,\n",
       "   4809: True,\n",
       "   4810: True,\n",
       "   4811: False,\n",
       "   4812: True,\n",
       "   4813: True,\n",
       "   4815: True,\n",
       "   4816: True,\n",
       "   4818: True,\n",
       "   4819: True,\n",
       "   4820: True,\n",
       "   4822: True,\n",
       "   4823: True,\n",
       "   4824: True,\n",
       "   4826: True,\n",
       "   4828: True,\n",
       "   4829: True,\n",
       "   4830: True,\n",
       "   4831: True,\n",
       "   4832: True,\n",
       "   4833: True,\n",
       "   4834: True,\n",
       "   4835: True,\n",
       "   4836: True,\n",
       "   4837: True,\n",
       "   4838: True,\n",
       "   4839: True,\n",
       "   4840: True,\n",
       "   4841: True,\n",
       "   4844: True,\n",
       "   4845: False,\n",
       "   4846: False,\n",
       "   4851: False,\n",
       "   4853: True,\n",
       "   4854: True,\n",
       "   4855: True,\n",
       "   4856: True,\n",
       "   4857: True,\n",
       "   4858: True,\n",
       "   4860: True,\n",
       "   4861: True,\n",
       "   4862: False,\n",
       "   4863: True,\n",
       "   4864: True,\n",
       "   4865: True,\n",
       "   4866: True,\n",
       "   4867: True,\n",
       "   4868: True,\n",
       "   4869: True,\n",
       "   4870: True,\n",
       "   4871: True,\n",
       "   4873: True,\n",
       "   4874: True,\n",
       "   4875: True,\n",
       "   4877: True,\n",
       "   4880: True,\n",
       "   4881: True,\n",
       "   4882: True,\n",
       "   4883: True,\n",
       "   4884: True,\n",
       "   4885: True,\n",
       "   4886: False,\n",
       "   4887: True,\n",
       "   4889: True,\n",
       "   4890: False,\n",
       "   4894: True,\n",
       "   4898: True,\n",
       "   4899: True,\n",
       "   4901: True,\n",
       "   4902: True,\n",
       "   4904: True,\n",
       "   4905: True,\n",
       "   4906: True,\n",
       "   4907: True,\n",
       "   4908: True,\n",
       "   4909: False,\n",
       "   4916: True,\n",
       "   4918: True,\n",
       "   4919: True,\n",
       "   4920: True,\n",
       "   4921: True,\n",
       "   4922: True,\n",
       "   4926: True,\n",
       "   4927: True,\n",
       "   4928: True,\n",
       "   4929: True,\n",
       "   4930: False,\n",
       "   4931: True,\n",
       "   4932: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4935: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4939: True,\n",
       "   4940: True,\n",
       "   4941: True,\n",
       "   4942: True,\n",
       "   4943: True,\n",
       "   4944: True,\n",
       "   4945: True,\n",
       "   4946: True,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   4955: False,\n",
       "   4956: True,\n",
       "   4961: True,\n",
       "   4962: True,\n",
       "   4963: True,\n",
       "   4964: True,\n",
       "   4965: True,\n",
       "   4966: True,\n",
       "   4967: True,\n",
       "   4968: True,\n",
       "   4969: True,\n",
       "   4970: True,\n",
       "   4971: True,\n",
       "   4972: True,\n",
       "   4973: True,\n",
       "   4974: True,\n",
       "   4975: True,\n",
       "   4976: True,\n",
       "   4977: False,\n",
       "   4978: True,\n",
       "   4982: True,\n",
       "   4983: True,\n",
       "   4984: True,\n",
       "   4985: True,\n",
       "   4986: True,\n",
       "   4987: True,\n",
       "   4990: True,\n",
       "   4991: False,\n",
       "   4992: True,\n",
       "   4997: True,\n",
       "   5359: True,\n",
       "   5360: True,\n",
       "   5361: True,\n",
       "   5373: True,\n",
       "   5375: True,\n",
       "   5376: True,\n",
       "   5378: True,\n",
       "   5379: True,\n",
       "   5380: True,\n",
       "   5382: False,\n",
       "   5384: True,\n",
       "   5385: True,\n",
       "   5386: True,\n",
       "   5389: True},\n",
       "  0.9150442477876106],\n",
       " 'ka-ro-qo': [{317: True, 1037: True, 1237: False, 2320: True, 5413: True},\n",
       "  0.8],\n",
       " 'qa-*83-to': [{317: True}, 1.0],\n",
       " 'pa-ra-ko': [{318: False,\n",
       "   4854: True,\n",
       "   4864: True,\n",
       "   4937: True,\n",
       "   4949: True,\n",
       "   4951: True},\n",
       "  0.8888888888888888],\n",
       " 'pa-wa-so': [{318: True}, 1.0],\n",
       " 'i-ka-se': [{318: True}, 1.0],\n",
       " 'di-ka': [{318: False, 937: False, 2516: False}, 0.0],\n",
       " 'ku-ne': [{318: False, 2329: True}, 0.5],\n",
       " 'a-*64-jo': [{321: True, 4783: True, 4987: True, 5073: True}, 1.0],\n",
       " 'si-mo': [{323: True}, 1.0],\n",
       " 'QE': [{325: True}, 1.0],\n",
       " 'si-jo': [{328: False,\n",
       "   419: False,\n",
       "   632: False,\n",
       "   1284: False,\n",
       "   1295: False,\n",
       "   1453: False,\n",
       "   1998: False,\n",
       "   2707: False,\n",
       "   3134: False,\n",
       "   3710: False,\n",
       "   4222: False},\n",
       "  0.0],\n",
       " 'da-nwa-re': [{330: False, 3096: False}, 0.0],\n",
       " 'wa': [{341: False,\n",
       "   394: False,\n",
       "   680: False,\n",
       "   689: False,\n",
       "   707: False,\n",
       "   732: False,\n",
       "   908: True,\n",
       "   1062: False,\n",
       "   1067: False,\n",
       "   1556: False,\n",
       "   1607: False,\n",
       "   1758: False,\n",
       "   1791: False,\n",
       "   1804: False,\n",
       "   1805: False,\n",
       "   1928: False,\n",
       "   1974: False,\n",
       "   1975: False,\n",
       "   2045: False,\n",
       "   2077: False,\n",
       "   2403: False,\n",
       "   2489: False,\n",
       "   2667: False,\n",
       "   3715: False,\n",
       "   3897: False,\n",
       "   3900: True,\n",
       "   3907: False,\n",
       "   4458: True,\n",
       "   4710: False,\n",
       "   4721: False,\n",
       "   4784: False,\n",
       "   5154: False,\n",
       "   5179: False,\n",
       "   5215: False,\n",
       "   5246: False,\n",
       "   5383: False,\n",
       "   5628: True,\n",
       "   5629: False,\n",
       "   5701: False,\n",
       "   5742: True,\n",
       "   5747: False,\n",
       "   5757: False,\n",
       "   5769: True},\n",
       "  0.13636363636363635],\n",
       " 'pa': [{354: False,\n",
       "   603: True,\n",
       "   950: False,\n",
       "   1077: False,\n",
       "   1337: False,\n",
       "   1350: False,\n",
       "   1476: False,\n",
       "   1969: False,\n",
       "   2203: False,\n",
       "   2307: False,\n",
       "   2554: True,\n",
       "   2558: True,\n",
       "   2563: True,\n",
       "   2591: True,\n",
       "   2595: True,\n",
       "   2773: True,\n",
       "   2780: False,\n",
       "   2781: False,\n",
       "   2878: False,\n",
       "   3128: True,\n",
       "   3154: True,\n",
       "   3161: True,\n",
       "   3162: True,\n",
       "   3169: True,\n",
       "   3172: True,\n",
       "   3173: True,\n",
       "   3175: True,\n",
       "   3176: True,\n",
       "   3177: True,\n",
       "   3178: True,\n",
       "   3179: True,\n",
       "   3180: True,\n",
       "   3181: True,\n",
       "   3182: True,\n",
       "   3183: True,\n",
       "   3184: True,\n",
       "   3185: True,\n",
       "   3186: True,\n",
       "   3187: True,\n",
       "   3188: True,\n",
       "   3189: True,\n",
       "   3190: True,\n",
       "   3191: True,\n",
       "   3192: True,\n",
       "   3193: True,\n",
       "   3194: True,\n",
       "   3195: True,\n",
       "   3196: True,\n",
       "   3197: True,\n",
       "   3198: True,\n",
       "   3199: True,\n",
       "   3200: True,\n",
       "   3201: True,\n",
       "   3202: True,\n",
       "   3203: True,\n",
       "   3204: True,\n",
       "   3205: True,\n",
       "   3206: True,\n",
       "   3207: True,\n",
       "   3208: True,\n",
       "   3209: True,\n",
       "   3210: True,\n",
       "   3211: True,\n",
       "   3212: True,\n",
       "   3213: True,\n",
       "   3214: True,\n",
       "   3215: True,\n",
       "   3216: True,\n",
       "   3286: True,\n",
       "   3287: True,\n",
       "   3288: True,\n",
       "   3292: True,\n",
       "   3293: True,\n",
       "   3294: True,\n",
       "   3295: True,\n",
       "   3296: True,\n",
       "   3297: True,\n",
       "   3534: False,\n",
       "   3537: True,\n",
       "   3612: False,\n",
       "   3683: True,\n",
       "   3698: False,\n",
       "   3699: True,\n",
       "   3734: True,\n",
       "   3775: True,\n",
       "   3788: True,\n",
       "   3814: False,\n",
       "   3818: False,\n",
       "   4464: True,\n",
       "   4501: True,\n",
       "   4779: False,\n",
       "   4873: False,\n",
       "   4955: False,\n",
       "   5045: False,\n",
       "   5076: False,\n",
       "   5520: False,\n",
       "   5563: False,\n",
       "   5739: False},\n",
       "  0.7706422018348624],\n",
       " 'ta-o': [{376: False, 3541: False, 3543: False}, 0.0],\n",
       " 'qo': [{378: False,\n",
       "   1148: False,\n",
       "   1382: False,\n",
       "   1869: False,\n",
       "   2139: False,\n",
       "   2738: False,\n",
       "   3806: False},\n",
       "  0.0],\n",
       " 'a-*47-wi': [{389: False}, 0.0],\n",
       " 'ka-wo': [{391: True}, 1.0],\n",
       " 'EQU+QE': [{397: False}, 0.0],\n",
       " 'to-wo': [{398: True, 1408: False}, 0.5],\n",
       " 'ni-ko': [{407: False}, 0.0],\n",
       " 'na': [{407: False,\n",
       "   516: False,\n",
       "   1085: False,\n",
       "   1404: False,\n",
       "   1467: False,\n",
       "   1589: False,\n",
       "   1745: False,\n",
       "   1835: False,\n",
       "   1920: False,\n",
       "   1968: False,\n",
       "   2333: False,\n",
       "   2654: False,\n",
       "   2663: False,\n",
       "   3194: False,\n",
       "   4203: False,\n",
       "   4476: False,\n",
       "   5357: False,\n",
       "   5412: False,\n",
       "   5431: False,\n",
       "   5533: False},\n",
       "  0.0],\n",
       " 'e-wo': [{414: False}, 0.0],\n",
       " 'a-ra-ru-ja': [{451: False,\n",
       "   453: True,\n",
       "   455: True,\n",
       "   461: True,\n",
       "   465: True,\n",
       "   467: False},\n",
       "  0.6666666666666666],\n",
       " 'a-ni-ja-pi': [{451: True, 455: True, 458: True, 461: True, 465: True}, 1.0],\n",
       " 'wi-ri-ni-jo': [{451: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   456: False,\n",
       "   461: True,\n",
       "   524: True},\n",
       "  0.8333333333333334],\n",
       " 'o-po-qo': [{451: True,\n",
       "   453: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   456: True,\n",
       "   458: True,\n",
       "   459: True,\n",
       "   461: True,\n",
       "   462: True,\n",
       "   465: True,\n",
       "   466: True,\n",
       "   467: True,\n",
       "   505: True,\n",
       "   524: True},\n",
       "  1.0],\n",
       " 'ke-ra-ja-pi': [{451: True,\n",
       "   453: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   456: True,\n",
       "   458: True,\n",
       "   461: True,\n",
       "   462: True,\n",
       "   505: True,\n",
       "   524: True},\n",
       "  1.0],\n",
       " 'o-pi-i-ja-pi': [{451: True,\n",
       "   453: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   456: True,\n",
       "   458: True,\n",
       "   459: True,\n",
       "   460: True,\n",
       "   461: True,\n",
       "   462: True,\n",
       "   465: True,\n",
       "   467: True,\n",
       "   505: True,\n",
       "   524: True},\n",
       "  1.0],\n",
       " 'CUR': [{451: False,\n",
       "   452: False,\n",
       "   453: True,\n",
       "   454: False,\n",
       "   455: False,\n",
       "   456: False,\n",
       "   458: False,\n",
       "   459: True,\n",
       "   460: True,\n",
       "   461: False,\n",
       "   462: True,\n",
       "   463: True,\n",
       "   464: False,\n",
       "   470: False,\n",
       "   475: False,\n",
       "   476: True,\n",
       "   477: True,\n",
       "   478: False,\n",
       "   479: False,\n",
       "   488: False,\n",
       "   490: False,\n",
       "   492: False,\n",
       "   493: False,\n",
       "   494: False,\n",
       "   495: False,\n",
       "   524: True},\n",
       "  0.3076923076923077],\n",
       " 'i-qi-jo': [{451: True, 462: True}, 1.0],\n",
       " 'a-ja-me-no': [{451: True,\n",
       "   5346: True,\n",
       "   5347: True,\n",
       "   5349: True,\n",
       "   5352: True,\n",
       "   5353: True,\n",
       "   5355: True,\n",
       "   5356: True},\n",
       "  1.0],\n",
       " 'a-ra-ro-mo-te-me-no': [{451: True}, 1.0],\n",
       " 'po-ni-ki-jo': [{451: True,\n",
       "   2041: True,\n",
       "   2759: False,\n",
       "   2760: True,\n",
       "   2761: True,\n",
       "   2762: False,\n",
       "   2763: True,\n",
       "   2764: True,\n",
       "   2767: False,\n",
       "   2768: True,\n",
       "   2769: False,\n",
       "   4132: True,\n",
       "   4133: True,\n",
       "   4135: True,\n",
       "   4138: True,\n",
       "   4139: True,\n",
       "   4140: True,\n",
       "   4141: True,\n",
       "   4142: True,\n",
       "   4164: False,\n",
       "   4182: True,\n",
       "   4183: True,\n",
       "   4184: True,\n",
       "   4191: True},\n",
       "  0.8],\n",
       " 'a-u-qe': [{452: True}, 1.0],\n",
       " 'a-re-ta-to': [{452: True}, 1.0],\n",
       " 'o-u-qe': [{452: True,\n",
       "   455: True,\n",
       "   460: True,\n",
       "   463: True,\n",
       "   464: True,\n",
       "   465: True,\n",
       "   4737: True,\n",
       "   4849: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   5399: True},\n",
       "  1.0],\n",
       " 'pte-no': [{452: True, 455: True, 465: True}, 1.0],\n",
       " 'au-ro': [{452: True}, 1.0],\n",
       " 'pe-qa-to': [{452: True, 464: True}, 1.0],\n",
       " 'i-qi-ja': [{452: False,\n",
       "   453: False,\n",
       "   454: False,\n",
       "   455: False,\n",
       "   458: True,\n",
       "   459: False,\n",
       "   461: True,\n",
       "   463: False,\n",
       "   464: False,\n",
       "   465: True,\n",
       "   466: True,\n",
       "   468: True,\n",
       "   471: False,\n",
       "   472: True,\n",
       "   473: True,\n",
       "   474: True,\n",
       "   496: True,\n",
       "   497: False,\n",
       "   498: True,\n",
       "   499: True,\n",
       "   500: False,\n",
       "   501: True,\n",
       "   502: True,\n",
       "   503: False,\n",
       "   504: False,\n",
       "   505: True,\n",
       "   507: False,\n",
       "   508: True,\n",
       "   512: True},\n",
       "  0.5517241379310345],\n",
       " 'a-ra-ro-mo-te-me-na': [{452: True,\n",
       "   453: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   456: True,\n",
       "   458: True,\n",
       "   459: True,\n",
       "   461: True,\n",
       "   462: True,\n",
       "   465: True,\n",
       "   466: True,\n",
       "   467: True,\n",
       "   524: True},\n",
       "  1.0],\n",
       " 'po-ni-ki-ja': [{452: True,\n",
       "   454: True,\n",
       "   455: True,\n",
       "   459: True,\n",
       "   461: True,\n",
       "   465: True,\n",
       "   469: False,\n",
       "   478: True,\n",
       "   484: True,\n",
       "   486: True,\n",
       "   493: True,\n",
       "   505: True},\n",
       "  0.9166666666666666],\n",
       " 'a-ni-ja': [{452: True, 463: True, 464: True, 5369: True}, 1.0],\n",
       " 'po-si': [{452: True,\n",
       "   460: True,\n",
       "   463: True,\n",
       "   464: True,\n",
       "   465: True,\n",
       "   2578: False},\n",
       "  0.8333333333333334],\n",
       " 'e-re-pa-te-jo': [{453: True,\n",
       "   5345: True,\n",
       "   5346: True,\n",
       "   5347: True,\n",
       "   5349: True,\n",
       "   5351: True,\n",
       "   5353: True,\n",
       "   5355: True,\n",
       "   5356: True},\n",
       "  1.0],\n",
       " 'ko-ki-da': [{453: True, 562: True}, 1.0],\n",
       " 'o-pa': [{453: True,\n",
       "   464: True,\n",
       "   498: True,\n",
       "   562: True,\n",
       "   784: True,\n",
       "   787: True,\n",
       "   790: True,\n",
       "   2315: True,\n",
       "   2644: True,\n",
       "   2828: True,\n",
       "   3465: True,\n",
       "   3879: True,\n",
       "   4286: True,\n",
       "   4710: False,\n",
       "   5335: True,\n",
       "   5443: True,\n",
       "   5447: False,\n",
       "   5448: True,\n",
       "   5449: True,\n",
       "   5450: True},\n",
       "  0.9047619047619048],\n",
       " 'a-ja-me-na': [{453: True,\n",
       "   458: True,\n",
       "   459: True,\n",
       "   460: True,\n",
       "   463: True,\n",
       "   499: True,\n",
       "   501: True,\n",
       "   503: True,\n",
       "   504: True,\n",
       "   511: True,\n",
       "   1308: False,\n",
       "   5345: True,\n",
       "   5352: True},\n",
       "  0.9285714285714286],\n",
       " 'i-qo-e-qe': [{454: True, 461: True, 465: True, 524: True}, 1.0],\n",
       " 'ku-do-ni-ja': [{454: True,\n",
       "   750: True,\n",
       "   1354: False,\n",
       "   1541: False,\n",
       "   2791: True,\n",
       "   2829: True,\n",
       "   2867: True,\n",
       "   3953: True,\n",
       "   4123: True,\n",
       "   4375: True,\n",
       "   4419: True},\n",
       "  0.8181818181818182],\n",
       " 'mi-to-we-sa-e': [{454: True}, 1.0],\n",
       " 'a-ra-ru-wo-ja': [{458: False}, 0.0],\n",
       " 'wi-ri-ne-o': [{458: True, 459: True, 465: True, 505: True}, 1.0],\n",
       " 'po-ni-ja-ja': [{458: True}, 1.0],\n",
       " 'ka-ke-ja-pi': [{459: True, 460: True, 467: True}, 1.0],\n",
       " 'e-re-pa': [{460: True, 1340: False, 5401: False}, 0.3333333333333333],\n",
       " 'me-na': [{460: False, 642: False, 3872: True, 4105: True, 4207: True}, 0.6],\n",
       " 'do-we-jo': [{461: True, 465: True, 524: True}, 1.0],\n",
       " 'wi-ri-ne-jo': [{462: True, 466: True}, 1.0],\n",
       " 'mi-to-we-sa': [{462: True, 463: True, 524: True}, 1.0],\n",
       " 'a-ja-me': [{462: True, 508: False}, 0.5],\n",
       " 'a-ra-ro-mo-to-me-na': [{463: False}, 0.0],\n",
       " 'u-po': [{464: True, 5372: True}, 1.0],\n",
       " 'a-ro-mo-te-me-na': [{464: True}, 1.0],\n",
       " 'e-e-si': [{464: False, 2496: True}, 0.5],\n",
       " 'ke-ra-i-ja-pi': [{465: True}, 1.0],\n",
       " 'ai-ki-no-o': [{475: True, 481: True, 2022: False}, 0.6666666666666666],\n",
       " 'pte-re-wa': [{475: True,\n",
       "   480: True,\n",
       "   481: True,\n",
       "   482: False,\n",
       "   483: True,\n",
       "   487: False,\n",
       "   491: False,\n",
       "   559: True,\n",
       "   560: True,\n",
       "   563: True,\n",
       "   569: True,\n",
       "   573: True,\n",
       "   576: True,\n",
       "   577: True},\n",
       "  0.7857142857142857],\n",
       " 'pa-ra-ja': [{475: True,\n",
       "   587: True,\n",
       "   588: True,\n",
       "   4464: True,\n",
       "   5316: True,\n",
       "   5319: True},\n",
       "  1.0],\n",
       " 'e-te-re-ta': [{475: True}, 1.0],\n",
       " 'po-ro-ti-ri': [{475: True}, 1.0],\n",
       " 'po-ni-ke-a': [{476: True}, 1.0],\n",
       " 'wo-ra-we-sa': [{476: True}, 1.0],\n",
       " 'pi': [{479: False,\n",
       "   697: False,\n",
       "   918: False,\n",
       "   1253: False,\n",
       "   1269: False,\n",
       "   1868: False,\n",
       "   2002: False,\n",
       "   2004: False,\n",
       "   2408: False,\n",
       "   2654: False,\n",
       "   4261: False,\n",
       "   4295: False,\n",
       "   4325: False,\n",
       "   4443: False,\n",
       "   5200: False,\n",
       "   5685: False},\n",
       "  0.0],\n",
       " 'e-re-pa-te-jo-pi': [{481: True, 1449: False, 2042: False}, 0.5],\n",
       " 'o-mo-pi': [{481: True}, 1.0],\n",
       " 'e-ka-te-re-ta': [{481: True}, 1.0],\n",
       " 'we-ja': [{483: False, 1409: False, 1574: False, 2024: False}, 0.0],\n",
       " 'ka-ke': [{483: False, 2677: True, 4718: False}, 0.3333333333333333],\n",
       " 'e-wi-su-zo-ko': [{484: True, 485: True}, 1.0],\n",
       " 'e-re-pa-te-o': [{485: True, 923: False}, 0.5],\n",
       " 'o-mo': [{485: False}, 0.0],\n",
       " 'wi': [{487: False,\n",
       "   853: False,\n",
       "   1138: False,\n",
       "   1733: False,\n",
       "   1792: False,\n",
       "   1874: False,\n",
       "   2145: False,\n",
       "   2728: False,\n",
       "   3001: False,\n",
       "   3880: True,\n",
       "   4081: False,\n",
       "   4723: False,\n",
       "   5445: False},\n",
       "  0.07692307692307693],\n",
       " 'pte-re': [{489: False}, 0.0],\n",
       " 'e-ka-te-jo': [{496: True, 1217: True}, 1.0],\n",
       " 'CAPS': [{496: False,\n",
       "   497: True,\n",
       "   498: True,\n",
       "   499: True,\n",
       "   500: True,\n",
       "   501: True,\n",
       "   502: False,\n",
       "   503: True,\n",
       "   504: True,\n",
       "   505: True,\n",
       "   513: False,\n",
       "   514: True,\n",
       "   515: False,\n",
       "   516: False,\n",
       "   517: False,\n",
       "   518: True,\n",
       "   519: True,\n",
       "   520: False,\n",
       "   521: False,\n",
       "   522: False},\n",
       "  0.5714285714285714],\n",
       " 'a-na-i-ta': [{497: True, 510: False}, 0.5],\n",
       " 'a-re-ki-si-to-jo': [{498: True, 556: True}, 1.0],\n",
       " 'a-na-ta': [{498: True}, 1.0],\n",
       " 'a-na-mo-to': [{498: True, 499: True, 500: True, 504: True, 509: False}, 0.8],\n",
       " 'a-mo-ta': [{498: False,\n",
       "   557: True,\n",
       "   559: True,\n",
       "   560: False,\n",
       "   563: False,\n",
       "   568: True,\n",
       "   569: True,\n",
       "   574: False,\n",
       "   576: True,\n",
       "   751: False,\n",
       "   5317: True,\n",
       "   5434: True},\n",
       "  0.5833333333333334],\n",
       " 'a': [{498: False,\n",
       "   556: False,\n",
       "   605: False,\n",
       "   682: False,\n",
       "   683: False,\n",
       "   687: False,\n",
       "   697: False,\n",
       "   712: False,\n",
       "   727: False,\n",
       "   898: False,\n",
       "   995: False,\n",
       "   1039: False,\n",
       "   1055: False,\n",
       "   1110: False,\n",
       "   1195: False,\n",
       "   1205: False,\n",
       "   1213: False,\n",
       "   1242: False,\n",
       "   1257: False,\n",
       "   1260: False,\n",
       "   1306: False,\n",
       "   1308: False,\n",
       "   1357: False,\n",
       "   1410: False,\n",
       "   1529: False,\n",
       "   1566: False,\n",
       "   1586: False,\n",
       "   1661: False,\n",
       "   1736: False,\n",
       "   1787: False,\n",
       "   1808: False,\n",
       "   1824: False,\n",
       "   1825: False,\n",
       "   2004: False,\n",
       "   2007: False,\n",
       "   2078: False,\n",
       "   2417: False,\n",
       "   2430: False,\n",
       "   2469: False,\n",
       "   2488: False,\n",
       "   2692: False,\n",
       "   3190: True,\n",
       "   3289: True,\n",
       "   3370: False,\n",
       "   3662: False,\n",
       "   3777: False,\n",
       "   3974: False,\n",
       "   4011: False,\n",
       "   4066: False,\n",
       "   4122: False,\n",
       "   4143: False,\n",
       "   4267: False,\n",
       "   4316: False,\n",
       "   4411: False,\n",
       "   4710: False,\n",
       "   4729: False,\n",
       "   4754: False,\n",
       "   4756: False,\n",
       "   4758: True,\n",
       "   4766: False,\n",
       "   4775: False,\n",
       "   4987: False,\n",
       "   5057: False,\n",
       "   5211: False,\n",
       "   5357: True,\n",
       "   5622: False,\n",
       "   5682: False},\n",
       "  0.057971014492753624],\n",
       " 'a-na-to': [{500: True, 502: True, 517: True, 520: True}, 1.0],\n",
       " 'me-ta-ke-ku-me-na': [{505: True}, 1.0],\n",
       " 'a-na-mo-ta': [{506: True, 511: False}, 0.5],\n",
       " '*245': [{506: True}, 1.0],\n",
       " 'e-na-ri-po-to': [{513: False}, 0.0],\n",
       " 'po-ro-su-re': [{517: True}, 1.0],\n",
       " 'te-mi-we-te': [{519: False}, 0.0],\n",
       " 'ROTA': [{519: True,\n",
       "   555: True,\n",
       "   556: False,\n",
       "   557: True,\n",
       "   558: True,\n",
       "   559: True,\n",
       "   560: True,\n",
       "   562: True,\n",
       "   563: True,\n",
       "   564: True,\n",
       "   565: True,\n",
       "   566: False,\n",
       "   567: True,\n",
       "   568: True,\n",
       "   569: True,\n",
       "   570: True,\n",
       "   571: True,\n",
       "   572: True,\n",
       "   573: True,\n",
       "   574: True,\n",
       "   575: True,\n",
       "   576: True,\n",
       "   577: True,\n",
       "   578: False,\n",
       "   580: True,\n",
       "   2425: False,\n",
       "   2426: False,\n",
       "   2427: False,\n",
       "   5296: True,\n",
       "   5297: True,\n",
       "   5298: True,\n",
       "   5303: True,\n",
       "   5316: True,\n",
       "   5320: True,\n",
       "   5328: False,\n",
       "   5330: True,\n",
       "   5331: False,\n",
       "   5714: False},\n",
       "  0.8225806451612904],\n",
       " 'o-da-ke-we-ta': [{519: False, 574: True}, 0.6666666666666666],\n",
       " 'a-mo': [{519: True, 1377: False}, 0.5],\n",
       " 'se-to-i-ja': [{524: True,\n",
       "   571: True,\n",
       "   774: True,\n",
       "   1205: True,\n",
       "   1264: True,\n",
       "   1343: True,\n",
       "   2567: True,\n",
       "   2648: False,\n",
       "   2667: True,\n",
       "   2677: True,\n",
       "   2998: True,\n",
       "   3165: True,\n",
       "   3493: True,\n",
       "   3499: True,\n",
       "   3503: True,\n",
       "   3504: False,\n",
       "   3523: True,\n",
       "   3533: True,\n",
       "   3879: True,\n",
       "   4379: True,\n",
       "   4458: True,\n",
       "   4488: True},\n",
       "  0.9130434782608695],\n",
       " 'qe-ryo': [{546: False,\n",
       "   549: True,\n",
       "   551: True,\n",
       "   553: True,\n",
       "   554: True,\n",
       "   4230: True},\n",
       "  0.8333333333333334],\n",
       " 'ko-ru': [{546: True, 549: True, 550: True, 553: True, 2711: False}, 0.8],\n",
       " 'GAL': [{546: True, 549: True, 550: True, 553: True}, 1.0],\n",
       " 'o-pa-wo-ta': [{546: False,\n",
       "   549: True,\n",
       "   553: True,\n",
       "   554: True,\n",
       "   5336: True,\n",
       "   5339: True},\n",
       "  0.8571428571428571],\n",
       " 'e-po-mi-jo': [{549: True, 551: True, 553: True}, 1.0],\n",
       " 'o-pi-ko-ru-si-ja': [{549: False, 553: True}, 0.5],\n",
       " 'to-ra': [{551: True, 553: True, 5420: True}, 1.0],\n",
       " 'e-pi-ko-ru-si-jo': [{551: True}, 1.0],\n",
       " 'pa-ra-wa-jo': [{551: False, 553: True, 5336: True}, 0.6666666666666666],\n",
       " 'a-te-re-te-a': [{555: True}, 1.0],\n",
       " 'pe-te-re-wa': [{555: True}, 1.0],\n",
       " 'te-mi-dwe': [{555: True}, 1.0],\n",
       " 'ka-ki-jo': [{555: True}, 1.0],\n",
       " 'ka-ko-de-ta': [{555: True}, 1.0],\n",
       " 'ki-da-pa': [{555: True}, 1.0],\n",
       " 'te-mi-dwe-ta': [{555: True,\n",
       "   556: True,\n",
       "   560: True,\n",
       "   563: True,\n",
       "   566: True,\n",
       "   568: True,\n",
       "   573: True,\n",
       "   576: True,\n",
       "   577: True,\n",
       "   5318: True,\n",
       "   5319: True},\n",
       "  1.0],\n",
       " 'o-da-tu-we-ta': [{555: True}, 1.0],\n",
       " 'e-ri-ka': [{555: True,\n",
       "   558: True,\n",
       "   562: True,\n",
       "   564: False,\n",
       "   566: True,\n",
       "   567: False,\n",
       "   568: True,\n",
       "   570: True,\n",
       "   574: True,\n",
       "   575: True},\n",
       "  0.8],\n",
       " 'pte-re-e': [{557: False}, 0.0],\n",
       " 'o-da-ku-we-ta': [{557: True, 4296: True}, 1.0],\n",
       " 'e-ri-ko': [{557: False, 912: False, 2730: False}, 0.0],\n",
       " 'o-da-twe-ta': [{558: True,\n",
       "   562: True,\n",
       "   564: True,\n",
       "   569: True,\n",
       "   570: True,\n",
       "   581: False},\n",
       "  0.8333333333333334],\n",
       " 'a-ryo-jo': [{559: True}, 1.0],\n",
       " 'te-mi-dwe-te': [{559: True, 565: True}, 1.0],\n",
       " 'de-do-me-na': [{560: True, 569: True, 570: True, 606: False}, 0.75],\n",
       " 'ne-wa': [{562: True,\n",
       "   577: True,\n",
       "   1414: False,\n",
       "   2345: True,\n",
       "   2787: False,\n",
       "   3513: True,\n",
       "   4775: False,\n",
       "   5121: True,\n",
       "   5325: True,\n",
       "   5369: True},\n",
       "  0.7272727272727273],\n",
       " 'a-ryo-a': [{562: True,\n",
       "   1210: True,\n",
       "   4276: True,\n",
       "   4316: True,\n",
       "   4425: True,\n",
       "   4426: True,\n",
       "   4433: True},\n",
       "  1.0],\n",
       " 'a-re-ki-si-to': [{565: True, 5413: True}, 1.0],\n",
       " 'wo-zo-me-no': [{565: True}, 1.0],\n",
       " 'wo-zo-me-na': [{567: True}, 1.0],\n",
       " 'o-pe-ro': [{571: True,\n",
       "   574: True,\n",
       "   1189: True,\n",
       "   1469: False,\n",
       "   2332: True,\n",
       "   2756: False,\n",
       "   4146: False,\n",
       "   4147: True,\n",
       "   4165: True,\n",
       "   4200: True,\n",
       "   4202: True,\n",
       "   4215: False,\n",
       "   4252: False,\n",
       "   4258: True,\n",
       "   4295: True,\n",
       "   4630: True,\n",
       "   4631: True,\n",
       "   4632: True,\n",
       "   4633: True,\n",
       "   4634: True,\n",
       "   4635: True,\n",
       "   4636: False,\n",
       "   4637: True,\n",
       "   4645: True,\n",
       "   4661: True,\n",
       "   4729: True,\n",
       "   4747: True,\n",
       "   4769: True,\n",
       "   4870: True,\n",
       "   4951: False,\n",
       "   4987: True,\n",
       "   5097: True,\n",
       "   5098: True,\n",
       "   5099: True,\n",
       "   5102: True,\n",
       "   5104: True,\n",
       "   5108: True,\n",
       "   5110: True,\n",
       "   5259: True,\n",
       "   5370: True,\n",
       "   5371: True,\n",
       "   5474: True,\n",
       "   5483: True},\n",
       "  0.8333333333333334],\n",
       " 'a-mo-te': [{571: True}, 1.0],\n",
       " 'pe-ru-si-nwa': [{571: True, 2345: True, 2375: True, 3519: True}, 1.0],\n",
       " 'ta-ra-si-ja': [{571: True,\n",
       "   1535: False,\n",
       "   2344: True,\n",
       "   4389: True,\n",
       "   4390: True,\n",
       "   4457: True,\n",
       "   5057: True,\n",
       "   5058: True,\n",
       "   5059: True,\n",
       "   5060: False,\n",
       "   5061: True,\n",
       "   5062: True,\n",
       "   5063: True,\n",
       "   5064: True,\n",
       "   5065: True,\n",
       "   5066: True,\n",
       "   5067: True,\n",
       "   5068: True,\n",
       "   5069: True,\n",
       "   5070: True,\n",
       "   5071: True,\n",
       "   5074: True,\n",
       "   5076: True},\n",
       "  0.9259259259259259],\n",
       " 'wa-ra-wi-ta': [{572: True}, 1.0],\n",
       " 'o-pe-te-we': [{575: True}, 1.0],\n",
       " 'twe-te': [{580: False}, 0.0],\n",
       " 'ri-ko': [{580: False, 1751: False}, 0.0],\n",
       " 'wo-ra-e': [{582: True}, 1.0],\n",
       " '*253': [{582: True, 583: True}, 1.0],\n",
       " 'wo-ra': [{583: False}, 0.0],\n",
       " 'ka-za': [{583: True}, 1.0],\n",
       " 'ri-*65-no': [{584: True}, 1.0],\n",
       " 'a-pe-re': [{584: True}, 1.0],\n",
       " 'QO': [{584: True}, 1.0],\n",
       " '*178': [{585: True}, 1.0],\n",
       " '*179': [{586: True}, 1.0],\n",
       " 'a-ku-wo': [{587: True}, 1.0],\n",
       " '*256': [{587: False, 588: True, 606: False}, 0.3333333333333333],\n",
       " 'ke-ti-ro': [{589: True, 1072: True, 2980: True, 5061: True}, 1.0],\n",
       " '*180': [{589: True}, 1.0],\n",
       " '*172+KE': [{590: False}, 0.0],\n",
       " 'ko-so-ni-ja': [{591: False}, 0.0],\n",
       " '*246': [{591: True, 599: True, 906: True}, 1.0],\n",
       " 'na-u-do-mo': [{592: False, 5215: True, 5413: True, 5539: False}, 0.5],\n",
       " '*181': [{592: True, 4472: True}, 1.0],\n",
       " '*172+KE+RO2': [{593: True}, 1.0],\n",
       " '*258': [{594: True, 688: True, 2800: False}, 0.5],\n",
       " '*182': [{595: True}, 1.0],\n",
       " 'a-pi-te': [{595: False}, 0.0],\n",
       " 'wi-ja': [{596: False, 2786: False, 5624: False}, 0.0],\n",
       " '*183': [{596: True}, 1.0],\n",
       " 'si-ja-ma-to': [{598: True, 1196: False, 4098: True}, 0.6666666666666666],\n",
       " '*177': [{598: False}, 0.4375],\n",
       " 'me-wo-ni-jo': [{598: True, 4245: True}, 1.0],\n",
       " 'ma': [{598: True,\n",
       "   1132: False,\n",
       "   1323: False,\n",
       "   1521: False,\n",
       "   1548: False,\n",
       "   1586: False,\n",
       "   1649: False,\n",
       "   1999: False,\n",
       "   2136: False,\n",
       "   2654: False,\n",
       "   2709: False,\n",
       "   2809: False,\n",
       "   3130: False,\n",
       "   3338: False,\n",
       "   4731: False,\n",
       "   5063: False,\n",
       "   5077: False,\n",
       "   5264: False,\n",
       "   5551: False,\n",
       "   5620: False},\n",
       "  0.25],\n",
       " 'ai-ki-si-jo': [{598: True}, 1.0],\n",
       " 'ta-u-po-no': [{598: True}, 1.0],\n",
       " 'pu-re-wa': [{598: True}, 1.0],\n",
       " 'pa-na-re-jo': [{598: True, 693: True, 2677: True, 2678: True, 4989: True},\n",
       "  1.0],\n",
       " 'ka-u-ja': [{598: False}, 0.0],\n",
       " 'de-u-ke-ro': [{598: True}, 1.0],\n",
       " 'do-ri-ka-no': [{598: True}, 1.0],\n",
       " 'ai-ki-po': [{598: True}, 1.0],\n",
       " 'a-nu': [{598: False, 696: False, 4006: False}, 0.0],\n",
       " 'e-ke': [{598: False,\n",
       "   621: True,\n",
       "   636: True,\n",
       "   1082: False,\n",
       "   2317: False,\n",
       "   2657: False,\n",
       "   3377: False,\n",
       "   4766: False,\n",
       "   4786: True,\n",
       "   4787: True,\n",
       "   4788: True,\n",
       "   4789: True,\n",
       "   4790: True,\n",
       "   4791: True,\n",
       "   4794: True,\n",
       "   4795: True,\n",
       "   4796: True,\n",
       "   4797: True,\n",
       "   4798: True,\n",
       "   4799: True,\n",
       "   4800: True,\n",
       "   4804: True,\n",
       "   4806: True,\n",
       "   4807: True,\n",
       "   4808: True,\n",
       "   4809: True,\n",
       "   4810: True,\n",
       "   4812: True,\n",
       "   4813: True,\n",
       "   4814: True,\n",
       "   4815: True,\n",
       "   4816: True,\n",
       "   4819: True,\n",
       "   4820: True,\n",
       "   4821: True,\n",
       "   4823: True,\n",
       "   4824: True,\n",
       "   4826: True,\n",
       "   4827: True,\n",
       "   4828: True,\n",
       "   4830: True,\n",
       "   4831: True,\n",
       "   4832: True,\n",
       "   4833: True,\n",
       "   4834: True,\n",
       "   4836: True,\n",
       "   4838: True,\n",
       "   4840: True,\n",
       "   4841: True,\n",
       "   4842: True,\n",
       "   4844: True,\n",
       "   4847: False,\n",
       "   4848: True,\n",
       "   4931: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   4955: True,\n",
       "   4956: False,\n",
       "   4960: True,\n",
       "   4967: True,\n",
       "   5176: True,\n",
       "   5186: True,\n",
       "   5538: False},\n",
       "  0.9411764705882353],\n",
       " 'e-da': [{598: False}, 0.0],\n",
       " 'nu-wo': [{598: False}, 0.0],\n",
       " 'ka-pa-ri-jo': [{598: False, 661: False, 662: True, 805: True}, 0.5],\n",
       " 'we-ka-di-jo': [{598: True, 686: True}, 1.0],\n",
       " 'pi-ri-sa-ta': [{598: True}, 1.0],\n",
       " 'ku-ryo': [{598: True, 2670: True, 4833: True}, 1.0],\n",
       " 'e-ke-me-de': [{598: True, 3172: False, 4726: True, 5077: True}, 0.75],\n",
       " 'se-ri-na-ta': [{598: True}, 1.0],\n",
       " 'pa-ke-ta': [{598: True}, 1.0],\n",
       " 'a-wi-to-do-to': [{598: True}, 1.0],\n",
       " '*190': [{600: True,\n",
       "   603: True,\n",
       "   788: True,\n",
       "   2336: True,\n",
       "   2367: True,\n",
       "   2368: True,\n",
       "   2369: True,\n",
       "   2370: False,\n",
       "   2372: True,\n",
       "   2376: True,\n",
       "   2391: True,\n",
       "   2518: False},\n",
       "  0.8214285714285714],\n",
       " 'KI': [{601: True, 4300: True}, 1.0],\n",
       " 'f': [{602: False, 2772: False, 2811: False, 2812: False}, 0.0],\n",
       " 'E': [{602: True,\n",
       "   2307: True,\n",
       "   2331: True,\n",
       "   5370: True,\n",
       "   5371: True,\n",
       "   5372: True,\n",
       "   5377: True,\n",
       "   5392: True},\n",
       "  1.0],\n",
       " 'sa': [{603: False,\n",
       "   1105: False,\n",
       "   1118: False,\n",
       "   1210: False,\n",
       "   1250: False,\n",
       "   1402: False,\n",
       "   1654: False,\n",
       "   1655: False,\n",
       "   2773: True,\n",
       "   2796: False,\n",
       "   2876: False,\n",
       "   2885: True,\n",
       "   3514: False,\n",
       "   4228: False,\n",
       "   5062: False},\n",
       "  0.17647058823529413],\n",
       " 'OVISf': [{603: True,\n",
       "   736: False,\n",
       "   2159: True,\n",
       "   2324: False,\n",
       "   2325: False,\n",
       "   2772: True,\n",
       "   2779: True,\n",
       "   2780: True,\n",
       "   2783: True,\n",
       "   2784: True,\n",
       "   2786: True,\n",
       "   2787: True,\n",
       "   2795: False,\n",
       "   2828: True,\n",
       "   2866: True,\n",
       "   2867: True,\n",
       "   2868: True,\n",
       "   2869: True,\n",
       "   2872: True,\n",
       "   2874: True,\n",
       "   2879: True,\n",
       "   2880: False,\n",
       "   2883: True,\n",
       "   2884: True,\n",
       "   2906: True,\n",
       "   2913: True,\n",
       "   2921: True,\n",
       "   2923: True,\n",
       "   2929: False,\n",
       "   3046: True,\n",
       "   3054: True,\n",
       "   3055: True,\n",
       "   3056: True,\n",
       "   3057: True,\n",
       "   3058: True,\n",
       "   3059: True,\n",
       "   3060: True,\n",
       "   3061: True,\n",
       "   3062: True,\n",
       "   3063: True,\n",
       "   3064: True,\n",
       "   3065: True,\n",
       "   3066: True,\n",
       "   3067: True,\n",
       "   3068: True,\n",
       "   3069: True,\n",
       "   3070: True,\n",
       "   3071: True,\n",
       "   3072: True,\n",
       "   3073: True,\n",
       "   3074: True,\n",
       "   3075: True,\n",
       "   3076: True,\n",
       "   3077: True,\n",
       "   3078: True,\n",
       "   3079: True,\n",
       "   3080: True,\n",
       "   3081: True,\n",
       "   3082: True,\n",
       "   3083: True,\n",
       "   3084: True,\n",
       "   3085: True,\n",
       "   3086: True,\n",
       "   3087: True,\n",
       "   3088: True,\n",
       "   3089: True,\n",
       "   3090: True,\n",
       "   3091: True,\n",
       "   3092: True,\n",
       "   3093: True,\n",
       "   3094: True,\n",
       "   3095: True,\n",
       "   3096: True,\n",
       "   3097: True,\n",
       "   3098: True,\n",
       "   3099: True,\n",
       "   3100: True,\n",
       "   3101: True,\n",
       "   3102: True,\n",
       "   3103: True,\n",
       "   3104: False,\n",
       "   3105: True,\n",
       "   3106: True,\n",
       "   3107: True,\n",
       "   3108: True,\n",
       "   3109: True,\n",
       "   3110: True,\n",
       "   3111: True,\n",
       "   3112: True,\n",
       "   3113: True,\n",
       "   3114: True,\n",
       "   3115: True,\n",
       "   3116: True,\n",
       "   3117: True,\n",
       "   3118: True,\n",
       "   3119: True,\n",
       "   3120: True,\n",
       "   3121: True,\n",
       "   3122: True,\n",
       "   3123: True,\n",
       "   3124: True,\n",
       "   3125: False,\n",
       "   3126: True,\n",
       "   3127: False,\n",
       "   3128: True,\n",
       "   3129: True,\n",
       "   3130: True,\n",
       "   3131: True,\n",
       "   3132: True,\n",
       "   3133: True,\n",
       "   3134: True,\n",
       "   3135: True,\n",
       "   3136: True,\n",
       "   3145: False,\n",
       "   3173: True,\n",
       "   3175: True,\n",
       "   3176: True,\n",
       "   3177: True,\n",
       "   3178: True,\n",
       "   3179: True,\n",
       "   3180: True,\n",
       "   3181: True,\n",
       "   3182: True,\n",
       "   3183: True,\n",
       "   3184: True,\n",
       "   3185: True,\n",
       "   3186: True,\n",
       "   3187: True,\n",
       "   3188: True,\n",
       "   3189: True,\n",
       "   3190: True,\n",
       "   3191: True,\n",
       "   3192: True,\n",
       "   3193: True,\n",
       "   3194: True,\n",
       "   3195: True,\n",
       "   3196: True,\n",
       "   3197: True,\n",
       "   3198: True,\n",
       "   3199: True,\n",
       "   3200: True,\n",
       "   3201: True,\n",
       "   3202: True,\n",
       "   3203: True,\n",
       "   3204: True,\n",
       "   3205: True,\n",
       "   3206: True,\n",
       "   3207: True,\n",
       "   3208: True,\n",
       "   3209: True,\n",
       "   3210: True,\n",
       "   3211: True,\n",
       "   3212: True,\n",
       "   3213: False,\n",
       "   3214: True,\n",
       "   3215: True,\n",
       "   3216: True,\n",
       "   3217: True,\n",
       "   3218: True,\n",
       "   3219: True,\n",
       "   3220: True,\n",
       "   3221: True,\n",
       "   3222: False,\n",
       "   3223: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3226: True,\n",
       "   3227: True,\n",
       "   3228: True,\n",
       "   3229: True,\n",
       "   3230: True,\n",
       "   3231: True,\n",
       "   3232: True,\n",
       "   3233: True,\n",
       "   3234: True,\n",
       "   3235: True,\n",
       "   3236: True,\n",
       "   3237: True,\n",
       "   3238: True,\n",
       "   3239: True,\n",
       "   3240: True,\n",
       "   3241: True,\n",
       "   3242: True,\n",
       "   3243: True,\n",
       "   3244: True,\n",
       "   3245: True,\n",
       "   3246: True,\n",
       "   3247: True,\n",
       "   3248: True,\n",
       "   3249: True,\n",
       "   3250: True,\n",
       "   3251: True,\n",
       "   3252: True,\n",
       "   3253: True,\n",
       "   3254: True,\n",
       "   3255: True,\n",
       "   3256: True,\n",
       "   3257: True,\n",
       "   3258: True,\n",
       "   3259: True,\n",
       "   3260: True,\n",
       "   3261: True,\n",
       "   3262: True,\n",
       "   3263: True,\n",
       "   3264: True,\n",
       "   3265: True,\n",
       "   3266: True,\n",
       "   3267: True,\n",
       "   3268: True,\n",
       "   3269: True,\n",
       "   3270: True,\n",
       "   3271: True,\n",
       "   3272: True,\n",
       "   3273: True,\n",
       "   3275: True,\n",
       "   3276: True,\n",
       "   3277: True,\n",
       "   3278: True,\n",
       "   3279: True,\n",
       "   3280: True,\n",
       "   3281: True,\n",
       "   3282: True,\n",
       "   3283: True,\n",
       "   3284: True,\n",
       "   3285: True,\n",
       "   3288: True,\n",
       "   3289: True,\n",
       "   3290: True,\n",
       "   3292: True,\n",
       "   3293: True,\n",
       "   3294: True,\n",
       "   3296: True,\n",
       "   3297: True,\n",
       "   3299: True,\n",
       "   3345: False,\n",
       "   3377: True,\n",
       "   3378: True,\n",
       "   3380: False,\n",
       "   3381: True,\n",
       "   3382: True,\n",
       "   3383: True,\n",
       "   3384: True,\n",
       "   3385: True,\n",
       "   3386: True,\n",
       "   3387: False,\n",
       "   3388: True,\n",
       "   3389: True,\n",
       "   3390: True,\n",
       "   3391: True,\n",
       "   3392: True,\n",
       "   3393: True,\n",
       "   3395: True,\n",
       "   3396: True,\n",
       "   3397: True,\n",
       "   3398: True,\n",
       "   3399: True,\n",
       "   3400: True,\n",
       "   3401: True,\n",
       "   3402: True,\n",
       "   3403: True,\n",
       "   3404: True,\n",
       "   3405: True,\n",
       "   3406: True,\n",
       "   3409: False,\n",
       "   3410: True,\n",
       "   3412: True,\n",
       "   3415: False,\n",
       "   3417: True,\n",
       "   3418: True,\n",
       "   3419: True,\n",
       "   3420: True,\n",
       "   3421: False,\n",
       "   3423: False,\n",
       "   3424: True,\n",
       "   3430: True,\n",
       "   3431: True,\n",
       "   3436: True,\n",
       "   3442: False,\n",
       "   3443: True,\n",
       "   3450: False,\n",
       "   3454: True,\n",
       "   3487: True,\n",
       "   3488: True,\n",
       "   3490: True,\n",
       "   3491: True,\n",
       "   3492: True,\n",
       "   3493: False,\n",
       "   3494: True,\n",
       "   3495: True,\n",
       "   3496: True,\n",
       "   3497: False,\n",
       "   3498: False,\n",
       "   3499: True,\n",
       "   3500: True,\n",
       "   3501: True,\n",
       "   3502: True,\n",
       "   3508: True,\n",
       "   3513: False,\n",
       "   3518: False,\n",
       "   3535: True,\n",
       "   3537: True,\n",
       "   3540: True,\n",
       "   3544: True,\n",
       "   3545: True,\n",
       "   3549: True,\n",
       "   3550: False,\n",
       "   3551: True,\n",
       "   3562: True,\n",
       "   3563: True,\n",
       "   3565: True,\n",
       "   3567: True,\n",
       "   3569: False,\n",
       "   3571: True,\n",
       "   3575: False,\n",
       "   3577: True,\n",
       "   3581: True,\n",
       "   3592: False,\n",
       "   3600: True,\n",
       "   3609: True,\n",
       "   3612: True,\n",
       "   3616: True,\n",
       "   3623: True,\n",
       "   3627: True,\n",
       "   3628: False,\n",
       "   3644: True,\n",
       "   3647: True,\n",
       "   3652: True,\n",
       "   3666: True,\n",
       "   3667: True,\n",
       "   3668: True,\n",
       "   3670: True,\n",
       "   3674: False,\n",
       "   3683: True,\n",
       "   3687: True,\n",
       "   3693: True,\n",
       "   3697: True,\n",
       "   3700: True,\n",
       "   3701: True,\n",
       "   3703: True,\n",
       "   3705: True,\n",
       "   3706: True,\n",
       "   3722: True,\n",
       "   3734: True,\n",
       "   3735: True,\n",
       "   3736: False,\n",
       "   3743: True,\n",
       "   3756: False,\n",
       "   3773: True,\n",
       "   3783: True,\n",
       "   3784: True,\n",
       "   3792: True,\n",
       "   3793: True,\n",
       "   3819: False,\n",
       "   4477: True,\n",
       "   4501: True,\n",
       "   4502: True,\n",
       "   4748: True,\n",
       "   4749: True,\n",
       "   4750: True,\n",
       "   4754: False,\n",
       "   4758: True,\n",
       "   4760: False,\n",
       "   4766: True,\n",
       "   4767: True,\n",
       "   4768: False,\n",
       "   4771: True,\n",
       "   4774: True,\n",
       "   4775: True,\n",
       "   4776: False,\n",
       "   4780: False,\n",
       "   5360: True,\n",
       "   5373: True,\n",
       "   5374: True,\n",
       "   5375: True,\n",
       "   5376: True,\n",
       "   5386: True},\n",
       "  0.8925233644859814],\n",
       " 'VIN': [{603: True,\n",
       "   610: True,\n",
       "   611: True,\n",
       "   2374: True,\n",
       "   2375: True,\n",
       "   2377: True,\n",
       "   4104: True,\n",
       "   4106: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4114: True,\n",
       "   4115: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4221: True,\n",
       "   4222: False,\n",
       "   4223: False,\n",
       "   4698: True,\n",
       "   5053: True,\n",
       "   5054: True,\n",
       "   5360: True,\n",
       "   5373: True,\n",
       "   5375: True,\n",
       "   5376: True,\n",
       "   5379: True,\n",
       "   5383: True,\n",
       "   5385: True,\n",
       "   5386: True,\n",
       "   5394: True,\n",
       "   5452: True,\n",
       "   5453: True,\n",
       "   5454: True,\n",
       "   5455: True},\n",
       "  0.9347826086956522],\n",
       " 'OVISx': [{603: False,\n",
       "   1744: False,\n",
       "   2325: False,\n",
       "   2779: False,\n",
       "   2780: False,\n",
       "   2801: False,\n",
       "   2807: False,\n",
       "   2886: False,\n",
       "   2895: False,\n",
       "   2904: False,\n",
       "   2914: False,\n",
       "   2925: False,\n",
       "   2927: False,\n",
       "   2928: False,\n",
       "   3213: False,\n",
       "   3354: False,\n",
       "   3387: False,\n",
       "   3415: True,\n",
       "   3424: True,\n",
       "   3446: False,\n",
       "   3450: False,\n",
       "   3451: False,\n",
       "   3488: False,\n",
       "   3500: False,\n",
       "   3514: True,\n",
       "   3554: False,\n",
       "   3558: False,\n",
       "   3629: False,\n",
       "   3697: True,\n",
       "   3710: False,\n",
       "   3729: False,\n",
       "   3754: False,\n",
       "   3789: False,\n",
       "   3791: False,\n",
       "   3810: False,\n",
       "   4751: False,\n",
       "   4752: True,\n",
       "   4754: True,\n",
       "   4760: False,\n",
       "   5374: False},\n",
       "  0.13953488372093023],\n",
       " '*209VAS': [{604: False,\n",
       "   610: True,\n",
       "   4193: True,\n",
       "   4194: True,\n",
       "   4196: True,\n",
       "   4200: True,\n",
       "   4201: True,\n",
       "   4202: True,\n",
       "   4203: True,\n",
       "   4204: True,\n",
       "   4214: False,\n",
       "   4215: True,\n",
       "   4217: True,\n",
       "   4219: True,\n",
       "   4514: True,\n",
       "   5358: True},\n",
       "  0.8095238095238095],\n",
       " 'me-ri-te-o': [{605: False}, 0.0],\n",
       " 'e-re-ta': [{607: False,\n",
       "   2693: True,\n",
       "   2776: True,\n",
       "   4694: True,\n",
       "   4721: True,\n",
       "   4729: True,\n",
       "   5077: False},\n",
       "  0.7777777777777778],\n",
       " '*259': [{607: True}, 1.0],\n",
       " 're-u-ka': [{608: False,\n",
       "   2331: True,\n",
       "   2333: False,\n",
       "   2334: True,\n",
       "   4256: True,\n",
       "   4441: False},\n",
       "  0.6666666666666666],\n",
       " 'si-ki-ro': [{608: True}, 1.0],\n",
       " 'DI+PTE': [{608: True, 2046: False}, 0.5],\n",
       " 'WA': [{609: False, 5462: True}, 0.5],\n",
       " 'Z': [{610: True,\n",
       "   2329: True,\n",
       "   2330: True,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   3900: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4108: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4113: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4119: True,\n",
       "   4702: True,\n",
       "   4982: False,\n",
       "   4987: True,\n",
       "   5009: True,\n",
       "   5025: True,\n",
       "   5389: True},\n",
       "  0.9433962264150944],\n",
       " 'qi': [{610: False, 1072: True, 3507: False}, 0.3333333333333333],\n",
       " 'V': [{610: False,\n",
       "   2326: True,\n",
       "   2328: True,\n",
       "   2329: False,\n",
       "   2330: False,\n",
       "   2331: True,\n",
       "   2332: True,\n",
       "   2333: True,\n",
       "   2334: True,\n",
       "   2643: True,\n",
       "   3870: True,\n",
       "   3900: True,\n",
       "   3901: True,\n",
       "   3903: True,\n",
       "   3905: True,\n",
       "   3915: False,\n",
       "   3924: True,\n",
       "   3926: True,\n",
       "   3930: True,\n",
       "   3933: True,\n",
       "   3936: True,\n",
       "   3938: True,\n",
       "   3940: True,\n",
       "   3942: True,\n",
       "   3944: True,\n",
       "   3946: True,\n",
       "   3951: True,\n",
       "   3955: True,\n",
       "   3956: True,\n",
       "   3958: True,\n",
       "   3970: True,\n",
       "   3979: True,\n",
       "   3987: True,\n",
       "   3988: True,\n",
       "   3990: True,\n",
       "   4012: True,\n",
       "   4015: True,\n",
       "   4019: True,\n",
       "   4034: True,\n",
       "   4035: True,\n",
       "   4041: True,\n",
       "   4055: True,\n",
       "   4088: True,\n",
       "   4092: True,\n",
       "   4093: False,\n",
       "   4099: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4107: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4114: True,\n",
       "   4115: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4119: True,\n",
       "   4132: True,\n",
       "   4702: True,\n",
       "   4792: True,\n",
       "   4804: True,\n",
       "   4805: True,\n",
       "   4824: True,\n",
       "   4835: True,\n",
       "   4837: True,\n",
       "   4845: True,\n",
       "   4854: True,\n",
       "   4857: True,\n",
       "   4865: True,\n",
       "   4866: True,\n",
       "   4873: True,\n",
       "   4885: True,\n",
       "   4893: True,\n",
       "   4900: True,\n",
       "   4905: False,\n",
       "   4909: True,\n",
       "   4912: True,\n",
       "   4917: True,\n",
       "   4926: True,\n",
       "   4929: True,\n",
       "   4931: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4941: True,\n",
       "   4943: True,\n",
       "   4944: True,\n",
       "   4946: False,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: False,\n",
       "   4956: True,\n",
       "   4961: False,\n",
       "   4962: True,\n",
       "   4963: True,\n",
       "   4964: True,\n",
       "   4965: True,\n",
       "   4966: True,\n",
       "   4968: True,\n",
       "   4969: True,\n",
       "   4970: True,\n",
       "   4971: True,\n",
       "   4972: True,\n",
       "   4973: True,\n",
       "   4974: True,\n",
       "   4975: True,\n",
       "   4976: True,\n",
       "   4982: False,\n",
       "   4983: True,\n",
       "   4984: True,\n",
       "   4985: True,\n",
       "   4986: True,\n",
       "   4987: True,\n",
       "   4990: True,\n",
       "   4991: False,\n",
       "   4992: True,\n",
       "   4997: False,\n",
       "   5002: True,\n",
       "   5003: True,\n",
       "   5005: True,\n",
       "   5007: True,\n",
       "   5008: True,\n",
       "   5010: True,\n",
       "   5011: True,\n",
       "   5012: False,\n",
       "   5013: True,\n",
       "   5014: False,\n",
       "   5017: True,\n",
       "   5018: True,\n",
       "   5020: True,\n",
       "   5021: True,\n",
       "   5023: True,\n",
       "   5027: True,\n",
       "   5028: True,\n",
       "   5029: True,\n",
       "   5030: True,\n",
       "   5031: True,\n",
       "   5034: True,\n",
       "   5036: True,\n",
       "   5037: True,\n",
       "   5038: True,\n",
       "   5040: True,\n",
       "   5043: True,\n",
       "   5048: True,\n",
       "   5052: False,\n",
       "   5359: True,\n",
       "   5360: True,\n",
       "   5361: True,\n",
       "   5373: True,\n",
       "   5374: True,\n",
       "   5375: True,\n",
       "   5376: True,\n",
       "   5382: True,\n",
       "   5384: True,\n",
       "   5385: True,\n",
       "   5386: True,\n",
       "   5387: True,\n",
       "   5388: True,\n",
       "   5389: True},\n",
       "  0.9371859296482412],\n",
       " 'VINb': [{610: True, 5379: True}, 1.0],\n",
       " 'S': [{610: True,\n",
       "   962: True,\n",
       "   2328: True,\n",
       "   2336: True,\n",
       "   2374: True,\n",
       "   3919: False,\n",
       "   3927: True,\n",
       "   3935: True,\n",
       "   3936: True,\n",
       "   3938: True,\n",
       "   3942: True,\n",
       "   3943: True,\n",
       "   3947: True,\n",
       "   3948: True,\n",
       "   3951: True,\n",
       "   3955: True,\n",
       "   3961: True,\n",
       "   3964: True,\n",
       "   3972: True,\n",
       "   3973: True,\n",
       "   3979: True,\n",
       "   3987: True,\n",
       "   3989: True,\n",
       "   3992: True,\n",
       "   3994: True,\n",
       "   3997: True,\n",
       "   3998: True,\n",
       "   4001: True,\n",
       "   4002: True,\n",
       "   4007: False,\n",
       "   4008: True,\n",
       "   4016: True,\n",
       "   4024: True,\n",
       "   4025: True,\n",
       "   4032: True,\n",
       "   4036: True,\n",
       "   4039: True,\n",
       "   4046: False,\n",
       "   4054: False,\n",
       "   4059: True,\n",
       "   4064: True,\n",
       "   4067: False,\n",
       "   4088: True,\n",
       "   4089: True,\n",
       "   4090: True,\n",
       "   4091: True,\n",
       "   4092: True,\n",
       "   4093: True,\n",
       "   4094: True,\n",
       "   4095: True,\n",
       "   4096: True,\n",
       "   4098: True,\n",
       "   4099: True,\n",
       "   4100: True,\n",
       "   4101: True,\n",
       "   4216: False,\n",
       "   4509: True,\n",
       "   4510: True,\n",
       "   5003: True,\n",
       "   5007: True,\n",
       "   5008: True,\n",
       "   5010: True,\n",
       "   5013: True,\n",
       "   5015: True,\n",
       "   5019: True,\n",
       "   5021: True,\n",
       "   5022: True,\n",
       "   5024: True,\n",
       "   5026: True,\n",
       "   5028: True,\n",
       "   5032: True,\n",
       "   5033: True,\n",
       "   5037: True,\n",
       "   5039: True,\n",
       "   5041: False,\n",
       "   5046: False,\n",
       "   5052: False,\n",
       "   5053: True,\n",
       "   5360: True,\n",
       "   5373: True,\n",
       "   5374: True,\n",
       "   5382: False,\n",
       "   5384: True,\n",
       "   5385: True,\n",
       "   5389: True},\n",
       "  0.9090909090909091],\n",
       " 'de-re-u-ko': [{610: True}, 1.0],\n",
       " 'CYP': [{610: True,\n",
       "   1682: True,\n",
       "   3872: True,\n",
       "   4145: True,\n",
       "   4146: True,\n",
       "   4147: True,\n",
       "   4163: True,\n",
       "   4177: False,\n",
       "   4178: True,\n",
       "   4189: True},\n",
       "  0.9285714285714286],\n",
       " 'a-pi-po-re-we': [{610: True}, 1.0],\n",
       " 'i-po-no': [{610: True}, 1.0],\n",
       " '*213VAS': [{610: True, 4243: True, 5357: True}, 1.0],\n",
       " '*212VAS': [{610: True, 5358: True}, 1.0],\n",
       " 'OVIS': [{611: False,\n",
       "   2807: True,\n",
       "   2813: True,\n",
       "   2833: True,\n",
       "   2835: True,\n",
       "   2837: True,\n",
       "   2839: True,\n",
       "   2876: True,\n",
       "   2912: True,\n",
       "   2917: True,\n",
       "   2926: True,\n",
       "   3137: True,\n",
       "   3377: True,\n",
       "   3381: True,\n",
       "   3405: True,\n",
       "   3416: True,\n",
       "   3442: True,\n",
       "   3443: True,\n",
       "   3498: True,\n",
       "   3499: True,\n",
       "   3535: True,\n",
       "   3711: False,\n",
       "   4760: True,\n",
       "   4781: True,\n",
       "   4782: True,\n",
       "   5388: True,\n",
       "   5389: True},\n",
       "  0.9473684210526315],\n",
       " 'SUS': [{611: True,\n",
       "   2777: True,\n",
       "   2790: True,\n",
       "   2792: True,\n",
       "   2809: True,\n",
       "   2810: True,\n",
       "   2832: True,\n",
       "   2866: True,\n",
       "   4741: True,\n",
       "   5376: True},\n",
       "  1.0],\n",
       " 'OLIV': [{611: True,\n",
       "   2374: True,\n",
       "   2375: True,\n",
       "   2657: True,\n",
       "   3868: True,\n",
       "   3869: True,\n",
       "   3872: False,\n",
       "   3902: False,\n",
       "   3907: True,\n",
       "   3908: True,\n",
       "   3916: True,\n",
       "   3918: True,\n",
       "   3923: True,\n",
       "   4982: False,\n",
       "   4983: True,\n",
       "   4985: True,\n",
       "   4990: True,\n",
       "   4991: True,\n",
       "   4997: False,\n",
       "   5359: True,\n",
       "   5373: True,\n",
       "   5376: True},\n",
       "  0.8611111111111112],\n",
       " 'NI': [{611: True,\n",
       "   2329: True,\n",
       "   2374: True,\n",
       "   2375: True,\n",
       "   2429: False,\n",
       "   3907: True,\n",
       "   3914: True,\n",
       "   3915: True,\n",
       "   3920: False,\n",
       "   3922: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4107: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4115: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4589: True,\n",
       "   4590: True,\n",
       "   4591: True,\n",
       "   4592: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: True,\n",
       "   4598: True,\n",
       "   4599: True,\n",
       "   4600: False,\n",
       "   4601: True,\n",
       "   4602: True,\n",
       "   4603: True,\n",
       "   4604: True,\n",
       "   4605: True,\n",
       "   4606: True,\n",
       "   4609: False,\n",
       "   4611: True,\n",
       "   4613: True,\n",
       "   4614: False,\n",
       "   4616: False,\n",
       "   4618: True,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4627: True,\n",
       "   4628: True,\n",
       "   4698: True,\n",
       "   4978: True,\n",
       "   4979: True,\n",
       "   4980: True,\n",
       "   4986: True,\n",
       "   5362: True,\n",
       "   5373: True,\n",
       "   5375: True,\n",
       "   5384: True,\n",
       "   5395: True,\n",
       "   5397: True},\n",
       "  0.9210526315789473],\n",
       " 'ka-wi-ja': [{613: False}, 0.0],\n",
       " 'i-wa-ka': [{616: True, 661: True, 5057: True, 5371: True}, 1.0],\n",
       " 'ra-mo': [{616: True}, 1.0],\n",
       " 'qe-da-do-ro': [{617: True, 3234: True}, 1.0],\n",
       " 'pa-na-so': [{617: True, 650: False, 1231: True, 3873: True, 4518: True},\n",
       "  0.8],\n",
       " 'pa-to-ro': [{618: True}, 1.0],\n",
       " 'ta-na-ti': [{619: False}, 0.0],\n",
       " 'PA': [{620: True,\n",
       "   621: True,\n",
       "   622: True,\n",
       "   623: True,\n",
       "   641: True,\n",
       "   656: True,\n",
       "   657: False,\n",
       "   5332: True,\n",
       "   5333: True,\n",
       "   5334: True,\n",
       "   5337: True,\n",
       "   5338: True,\n",
       "   5339: True,\n",
       "   5340: True,\n",
       "   5341: True,\n",
       "   5342: True,\n",
       "   5343: True,\n",
       "   5392: True},\n",
       "  0.9090909090909091],\n",
       " 'phu-ru-da-ro': [{620: True}, 1.0],\n",
       " 'di-ra': [{620: True}, 1.0],\n",
       " 'e-te-do-mo': [{620: True, 4827: True, 4933: True, 4936: True, 4949: True},\n",
       "  1.0],\n",
       " 'ki-te': [{620: False}, 0.0],\n",
       " 'da-ru-*56': [{620: True}, 1.0],\n",
       " 'ke-ke-me-na': [{621: True,\n",
       "   629: True,\n",
       "   1716: False,\n",
       "   4791: True,\n",
       "   4801: True,\n",
       "   4804: True,\n",
       "   4808: True,\n",
       "   4812: True,\n",
       "   4822: False,\n",
       "   4823: True,\n",
       "   4826: True,\n",
       "   4828: True,\n",
       "   4844: True,\n",
       "   4853: True,\n",
       "   4856: True,\n",
       "   4860: True,\n",
       "   4861: False,\n",
       "   4862: True,\n",
       "   4863: True,\n",
       "   4864: True,\n",
       "   4866: True,\n",
       "   4869: True,\n",
       "   4871: True,\n",
       "   4873: True,\n",
       "   4875: True,\n",
       "   4876: True,\n",
       "   4878: True,\n",
       "   4879: True,\n",
       "   4881: True,\n",
       "   4882: True,\n",
       "   4885: True,\n",
       "   4886: True,\n",
       "   4888: True,\n",
       "   4889: True,\n",
       "   4890: True,\n",
       "   4891: True,\n",
       "   4893: False,\n",
       "   4896: False,\n",
       "   4898: True,\n",
       "   4899: True,\n",
       "   4900: True,\n",
       "   4901: True,\n",
       "   4902: True,\n",
       "   4903: True,\n",
       "   4904: True,\n",
       "   4906: True,\n",
       "   4911: False,\n",
       "   4912: True,\n",
       "   4914: False,\n",
       "   4916: True,\n",
       "   4917: True,\n",
       "   4918: True,\n",
       "   4919: True,\n",
       "   4920: True,\n",
       "   4921: True,\n",
       "   4923: True,\n",
       "   4924: False,\n",
       "   4925: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True},\n",
       "  0.911504424778761],\n",
       " 'do': [{621: False,\n",
       "   1112: False,\n",
       "   2708: False,\n",
       "   4048: False,\n",
       "   4158: False,\n",
       "   5084: False,\n",
       "   5386: False},\n",
       "  0.0],\n",
       " 'pu-te': [{621: True, 630: False, 639: True, 653: False, 2677: True}, 0.6],\n",
       " 'wo-we-u': [{622: True, 2779: True, 4682: True}, 1.0],\n",
       " 'ku-ka-da-ro': [{622: True}, 1.0],\n",
       " 'pi-di-jo': [{622: True}, 1.0],\n",
       " 'te-wa-te-u': [{623: False, 640: False}, 0.0],\n",
       " 'ta-ko-ro': [{623: False}, 0.0],\n",
       " 'a-ke-re-mo': [{624: True}, 1.0],\n",
       " 'te-re-ta': [{625: True,\n",
       "   626: True,\n",
       "   627: False,\n",
       "   631: False,\n",
       "   1231: True,\n",
       "   2645: True,\n",
       "   2742: False,\n",
       "   4849: True,\n",
       "   4928: False,\n",
       "   4933: True,\n",
       "   4937: True,\n",
       "   4951: True,\n",
       "   4956: True,\n",
       "   4959: True},\n",
       "  0.7333333333333333],\n",
       " 'ke-ma-qe-me': [{625: True}, 1.0],\n",
       " 'me-ra': [{625: True}, 1.0],\n",
       " 'ko-do': [{625: True, 4824: True, 4840: True, 4841: True, 5410: True}, 1.0],\n",
       " 'da-*22-to': [{625: True,\n",
       "   956: True,\n",
       "   1355: True,\n",
       "   2564: True,\n",
       "   2667: True,\n",
       "   2801: True,\n",
       "   2829: True,\n",
       "   2943: True,\n",
       "   2949: True,\n",
       "   2989: True,\n",
       "   3001: True,\n",
       "   3043: True,\n",
       "   3107: True,\n",
       "   3127: True,\n",
       "   3141: True,\n",
       "   3142: True,\n",
       "   3143: True,\n",
       "   3175: True,\n",
       "   3176: True,\n",
       "   3197: True,\n",
       "   3223: True,\n",
       "   3224: True,\n",
       "   3225: True,\n",
       "   3306: True,\n",
       "   3474: True,\n",
       "   3524: True,\n",
       "   3526: True,\n",
       "   3528: False,\n",
       "   3560: True,\n",
       "   3576: True,\n",
       "   3622: True,\n",
       "   3767: True,\n",
       "   3906: True,\n",
       "   4147: True,\n",
       "   4481: False,\n",
       "   5628: True},\n",
       "  0.9302325581395349],\n",
       " 'ke-nu-wa-so': [{625: True}, 1.0],\n",
       " 'wo-ne': [{626: False, 664: False}, 0.0],\n",
       " 'da-jo': [{627: False, 3917: False}, 0.0],\n",
       " 'ko-to-i-na': [{628: True, 634: True, 636: True}, 1.0],\n",
       " 'e-ri-ke-re-we': [{628: True}, 1.0],\n",
       " 'e-ke-pu-te-ri-ja': [{628: True, 634: True}, 1.0],\n",
       " 'po-to-ri-ka-ta': [{629: True}, 1.0],\n",
       " 'do-wo': [{629: False}, 0.0],\n",
       " 'e-ko-so': [{629: True,\n",
       "   757: False,\n",
       "   1330: False,\n",
       "   2792: True,\n",
       "   2947: True,\n",
       "   2967: True,\n",
       "   2968: True,\n",
       "   2969: True,\n",
       "   2970: True,\n",
       "   3023: True,\n",
       "   3029: True,\n",
       "   3066: True,\n",
       "   3091: True,\n",
       "   3149: True,\n",
       "   3184: True,\n",
       "   3220: True,\n",
       "   3244: True,\n",
       "   3249: True,\n",
       "   3301: True,\n",
       "   3305: True,\n",
       "   3382: False,\n",
       "   3383: False,\n",
       "   3384: True,\n",
       "   3386: True,\n",
       "   3391: True,\n",
       "   3415: False,\n",
       "   3436: True,\n",
       "   3437: True,\n",
       "   3467: True,\n",
       "   3473: True,\n",
       "   3522: True,\n",
       "   3559: True,\n",
       "   3561: True,\n",
       "   3579: True,\n",
       "   3605: True,\n",
       "   3652: True,\n",
       "   3656: True,\n",
       "   3695: True,\n",
       "   3702: False,\n",
       "   3709: True,\n",
       "   3713: True,\n",
       "   3721: True,\n",
       "   3902: True,\n",
       "   4272: True,\n",
       "   4460: False},\n",
       "  0.8444444444444444],\n",
       " 'ai-ki-wa-to': [{630: True}, 1.0],\n",
       " 'a-ri-ja-wo': [{631: True}, 1.0],\n",
       " 'pu': [{632: False,\n",
       "   633: False,\n",
       "   686: False,\n",
       "   1405: False,\n",
       "   2012: False,\n",
       "   2244: False,\n",
       "   5050: False,\n",
       "   5752: False},\n",
       "  0.0],\n",
       " 'i-ra-ta': [{633: True, 4934: True, 4943: True}, 1.0],\n",
       " 'pe-ri-qo-ta': [{634: True, 5068: True}, 1.0],\n",
       " 'nwa-jo': [{635: False}, 0.0],\n",
       " 'pe-ri-je-ja': [{636: True}, 1.0],\n",
       " 'pu-te-ri-ja': [{636: True}, 1.0],\n",
       " 'te-wa-jo': [{637: False, 2836: True, 3430: True, 4987: True}, 0.75],\n",
       " 'te-ro-ri-jo': [{638: True}, 1.0],\n",
       " 'phu-to': [{638: True}, 1.0],\n",
       " 'me-ta-no-re': [{638: True}, 1.0],\n",
       " 'mi-ni-so': [{638: True}, 1.0],\n",
       " 'e-ke-da-mo': [{638: True, 4755: True}, 1.0],\n",
       " 'ka-da-i-to': [{639: True}, 1.0],\n",
       " '*83-re-te': [{640: True}, 1.0],\n",
       " '*65-no': [{644: False, 1957: False}, 0.0],\n",
       " 'te-nu': [{645: True}, 1.0],\n",
       " 'ku-no': [{647: False}, 0.0],\n",
       " 'ru-po': [{651: False}, 0.0],\n",
       " 'a-ta-na-po-ti-ni-ja': [{658: True}, 1.0],\n",
       " 'e-nu-wa-ri-jo': [{658: True}, 1.0],\n",
       " 'pa-ja-wo-ne': [{658: True}, 1.0],\n",
       " 'po-se-da-o-ne': [{658: True,\n",
       "   4962: True,\n",
       "   4963: True,\n",
       "   4964: True,\n",
       "   4965: True,\n",
       "   4968: True,\n",
       "   4969: True,\n",
       "   4971: True,\n",
       "   4973: True,\n",
       "   4974: True,\n",
       "   4975: True,\n",
       "   5000: True,\n",
       "   5020: True,\n",
       "   5025: True,\n",
       "   5374: True,\n",
       "   5386: True},\n",
       "  0.9411764705882353],\n",
       " 'e-ri-nu-we': [{658: True}, 1.0],\n",
       " 'pe-ro': [{658: True, 1567: False}, 0.5],\n",
       " 'ko-no-si-jo': [{659: True,\n",
       "   686: False,\n",
       "   967: True,\n",
       "   2641: True,\n",
       "   2674: True,\n",
       "   2728: True},\n",
       "  0.8333333333333334],\n",
       " 'e-qe-a-o': [{659: True}, 1.0],\n",
       " 'a-to-mo': [{659: True,\n",
       "   2790: True,\n",
       "   4737: True,\n",
       "   5073: True,\n",
       "   5075: False,\n",
       "   5077: True},\n",
       "  0.8333333333333334],\n",
       " 'e-u-da-mo': [{660: True, 2707: True, 5646: True}, 1.0],\n",
       " 'e-we-wa-ta': [{660: True}, 1.0],\n",
       " 'wo-di-jo': [{661: True, 5064: True}, 1.0],\n",
       " 'qo-ta': [{661: False, 2512: False, 3432: False, 4040: False, 5203: True},\n",
       "  0.2],\n",
       " 'a-ni-o-ko': [{661: True}, 1.0],\n",
       " 'ti-jo': [{661: False,\n",
       "   2677: False,\n",
       "   2988: False,\n",
       "   3653: False,\n",
       "   3868: False,\n",
       "   4136: False,\n",
       "   4181: False,\n",
       "   4286: False,\n",
       "   4354: False},\n",
       "  0.0],\n",
       " 'ki-si-wi-jo': [{661: False}, 0.0],\n",
       " 'wi-da-jo': [{661: True}, 1.0],\n",
       " 'di-we-so': [{661: True}, 1.0],\n",
       " 'o-ku-na-wo': [{661: True}, 1.0],\n",
       " 'i-to-ma': [{661: False, 1044: False}, 0.0],\n",
       " 'a-ke-u': [{661: True, 3255: True, 3559: False}, 0.6666666666666666],\n",
       " 'pe-ri-ta': [{661: True, 4239: True}, 1.0],\n",
       " 'te-wa': [{662: False, 1325: False, 4710: False}, 0.0],\n",
       " 'pa-ze': [{663: True}, 1.0],\n",
       " 'pe-da': [{663: True, 3991: True}, 1.0],\n",
       " 'wa-tu': [{663: True, 1220: False, 4955: True, 5357: True}, 0.8],\n",
       " 'ne-jo': [{664: False}, 0.0],\n",
       " 'u-o': [{664: False}, 0.0],\n",
       " 'a-pe-o': [{664: False, 4696: True}, 0.5],\n",
       " 'po-ru-da-si-jo': [{665: True, 4738: True}, 1.0],\n",
       " 'ha-ke-te-re': [{665: True}, 1.0],\n",
       " 'ta-mo': [{666: False}, 0.0],\n",
       " 'u-wo-qe-ne': [{666: True}, 1.0],\n",
       " 'u-du-ru-wo': [{666: True}, 1.0],\n",
       " 'we-re-we': [{666: True, 2776: True}, 1.0],\n",
       " 'ku-pa-sa': [{666: True}, 1.0],\n",
       " 'ka-ta-ra-pi': [{666: True}, 1.0],\n",
       " 'a-ke-to-ro': [{666: True}, 1.0],\n",
       " 'to-ni': [{666: True, 2911: False}, 0.5],\n",
       " 'we-we-ro': [{667: True, 3942: True}, 1.0],\n",
       " 'e-u-ru-qo-ta': [{667: True}, 1.0],\n",
       " 'na-e-si-jo': [{667: True, 5071: True}, 1.0],\n",
       " 'da-te-wa': [{667: True}, 1.0],\n",
       " 'te-ra-pe-te': [{667: True}, 1.0],\n",
       " 'to-ko-so-ta': [{668: True}, 1.0],\n",
       " 'a-te-u-ke': [{668: True}, 1.0],\n",
       " 'ru-ki-to': [{669: True,\n",
       "   688: True,\n",
       "   1351: False,\n",
       "   1389: False,\n",
       "   2971: True,\n",
       "   2972: True,\n",
       "   2973: True,\n",
       "   3003: True,\n",
       "   3092: True,\n",
       "   3093: True,\n",
       "   3094: True,\n",
       "   3109: True,\n",
       "   3164: True,\n",
       "   3169: True,\n",
       "   3171: True,\n",
       "   3185: True,\n",
       "   3186: True,\n",
       "   3187: True,\n",
       "   3188: True,\n",
       "   3189: True,\n",
       "   3190: True,\n",
       "   3233: True,\n",
       "   3234: True,\n",
       "   3268: True,\n",
       "   3269: True,\n",
       "   3272: True,\n",
       "   3292: True,\n",
       "   3293: True,\n",
       "   3458: True,\n",
       "   3484: True,\n",
       "   3580: True,\n",
       "   3597: True,\n",
       "   3616: True,\n",
       "   3646: True,\n",
       "   3681: True,\n",
       "   3716: True,\n",
       "   3751: True,\n",
       "   3944: True},\n",
       "  0.9473684210526315],\n",
       " 'phu-te-re': [{669: True, 5197: True}, 1.0],\n",
       " 'wa-si-ro': [{669: True}, 1.0],\n",
       " 'ra-pte-re': [{669: True, 3988: True, 4707: True, 4712: True, 4715: True},\n",
       "  1.0],\n",
       " 'su-pa-ta': [{670: False}, 0.0],\n",
       " 'wo-de-wi-jo': [{671: True, 4095: True, 4098: True}, 1.0],\n",
       " 'to-pe-za': [{671: True, 5345: True, 5351: True, 5353: True}, 1.0],\n",
       " 'o-u-ki-te-mi': [{671: True}, 1.0],\n",
       " 'a-pe-ti-rya': [{671: True}, 1.0],\n",
       " 'o-u-te-mi': [{671: True}, 1.0],\n",
       " 'e-pi': [{671: True, 5042: False, 5373: True}, 0.6666666666666666],\n",
       " 'i-ku-wo-i-pi': [{671: True}, 1.0],\n",
       " 'a-pi-re-we': [{672: True}, 1.0],\n",
       " 'da-mi-ni-jo': [{672: False,\n",
       "   1232: True,\n",
       "   2941: True,\n",
       "   2984: True,\n",
       "   3138: True,\n",
       "   3139: True,\n",
       "   3140: True,\n",
       "   3257: True,\n",
       "   3258: True,\n",
       "   3259: True,\n",
       "   3327: True,\n",
       "   3328: True,\n",
       "   3329: True,\n",
       "   3330: True,\n",
       "   3337: True,\n",
       "   3341: True,\n",
       "   3347: True,\n",
       "   3372: True,\n",
       "   3529: True,\n",
       "   3538: True,\n",
       "   3555: True,\n",
       "   3618: True,\n",
       "   4721: False},\n",
       "  0.9130434782608695],\n",
       " 'po-mi-jo': [{672: False}, 0.0],\n",
       " 'mo-ni-ko': [{672: True, 2971: True}, 1.0],\n",
       " 'wi-to': [{672: False}, 0.0],\n",
       " 'ku-ka-so': [{673: True, 2688: False}, 0.5],\n",
       " 'ja-pa-ra-ro': [{673: False, 681: True}, 0.5],\n",
       " 'e-ra-jo': [{674: False, 3990: True}, 0.5],\n",
       " 'pu-da-so': [{674: False, 3164: True}, 0.5],\n",
       " 'qa-ra-i-so': [{675: True, 3649: False}, 0.5],\n",
       " 'si-ra-no': [{675: True}, 1.0],\n",
       " 'i-ra-ko-to': [{675: True}, 1.0],\n",
       " 'wa-na-ta-jo': [{675: True,\n",
       "   4863: True,\n",
       "   4933: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4949: True,\n",
       "   5073: True},\n",
       "  0.7777777777777778],\n",
       " 'du-phu-ra-zo': [{676: True, 2957: True}, 1.0],\n",
       " 'qe-ro': [{676: True, 708: False, 2669: True, 3072: True}, 0.75],\n",
       " 'su-ko': [{676: True, 1369: False, 4849: False, 4951: True}, 0.5],\n",
       " 'di-zo': [{676: True, 697: True, 2465: False, 2681: True}, 0.75],\n",
       " 'pu-ri': [{676: True, 2707: True, 2853: True, 3682: False, 4509: True}, 0.8],\n",
       " 'wi-da-ma-ro': [{676: True, 3487: True}, 1.0],\n",
       " 'o-ro-qa': [{676: True}, 1.0],\n",
       " 'wa-je': [{676: True, 2690: False}, 0.5],\n",
       " 'pe-ri-ro-qo': [{676: True}, 1.0],\n",
       " 'da-*83-jo': [{676: True, 860: True, 3157: True}, 1.0],\n",
       " 'da': [{676: False,\n",
       "   1137: False,\n",
       "   1167: False,\n",
       "   1664: False,\n",
       "   1710: False,\n",
       "   2010: False,\n",
       "   2258: False,\n",
       "   2333: False,\n",
       "   2653: False,\n",
       "   3308: False,\n",
       "   3378: False,\n",
       "   3619: False,\n",
       "   3691: False,\n",
       "   3742: False,\n",
       "   4479: True,\n",
       "   5686: False},\n",
       "  0.0625],\n",
       " 'o-du': [{676: True, 2690: False}, 0.5],\n",
       " 'te-ja-ro': [{676: True, 1286: False, 1626: True}, 0.6666666666666666],\n",
       " 'e-u-ko-ro': [{677: True, 2974: True, 3108: False, 3175: True}, 0.75],\n",
       " 'qe-ra-jo': [{677: True, 899: True}, 1.0],\n",
       " 'a-mi-nwa': [{677: False}, 0.0],\n",
       " 'nu-to': [{677: True, 707: True}, 1.0],\n",
       " 'mi-ru-ro': [{677: True, 707: True, 2677: True, 2776: True, 2943: True}, 1.0],\n",
       " 'me-to': [{677: False}, 0.0],\n",
       " 'ko-wo': [{677: True,\n",
       "   1932: False,\n",
       "   2083: False,\n",
       "   2353: True,\n",
       "   2489: True,\n",
       "   2490: True,\n",
       "   2492: True,\n",
       "   2496: True,\n",
       "   2497: True,\n",
       "   2499: True,\n",
       "   2500: True,\n",
       "   2501: True,\n",
       "   2503: True,\n",
       "   2504: False,\n",
       "   2506: True,\n",
       "   2507: True,\n",
       "   2509: True,\n",
       "   2517: True,\n",
       "   2520: True,\n",
       "   2522: True,\n",
       "   2523: True,\n",
       "   2524: False,\n",
       "   2528: False,\n",
       "   2543: False,\n",
       "   2544: True,\n",
       "   2549: True,\n",
       "   2550: True,\n",
       "   2551: True,\n",
       "   2552: True,\n",
       "   2553: False,\n",
       "   2554: True,\n",
       "   2555: True,\n",
       "   2557: True,\n",
       "   2558: True,\n",
       "   2559: True,\n",
       "   2560: True,\n",
       "   2561: False,\n",
       "   2562: True,\n",
       "   2563: True,\n",
       "   2564: True,\n",
       "   2566: True,\n",
       "   2567: True,\n",
       "   2569: False,\n",
       "   2570: True,\n",
       "   2572: True,\n",
       "   2573: True,\n",
       "   2577: True,\n",
       "   2578: True,\n",
       "   2584: True,\n",
       "   2587: True,\n",
       "   2588: True,\n",
       "   2593: True,\n",
       "   2598: True,\n",
       "   2600: False,\n",
       "   2602: True,\n",
       "   2603: False,\n",
       "   2606: False,\n",
       "   2607: True,\n",
       "   2609: False,\n",
       "   2612: True,\n",
       "   2613: False,\n",
       "   2619: False,\n",
       "   2620: False,\n",
       "   2629: True,\n",
       "   2630: True,\n",
       "   2634: True,\n",
       "   2643: True,\n",
       "   2646: True,\n",
       "   2652: True,\n",
       "   2654: True,\n",
       "   2657: True,\n",
       "   2661: True,\n",
       "   2662: True,\n",
       "   2757: True,\n",
       "   4511: True,\n",
       "   4536: True,\n",
       "   4538: True,\n",
       "   4539: True,\n",
       "   4540: True,\n",
       "   4541: True,\n",
       "   4542: True,\n",
       "   4543: True,\n",
       "   4544: True,\n",
       "   4545: True,\n",
       "   4546: True,\n",
       "   4548: True,\n",
       "   4549: True,\n",
       "   4550: True,\n",
       "   4551: True,\n",
       "   4553: True,\n",
       "   4554: True,\n",
       "   4555: True,\n",
       "   4556: True,\n",
       "   4557: True,\n",
       "   4558: True,\n",
       "   4559: True,\n",
       "   4560: True,\n",
       "   4561: True,\n",
       "   4562: True,\n",
       "   4563: True,\n",
       "   4564: True,\n",
       "   4565: True,\n",
       "   4566: True,\n",
       "   4567: True,\n",
       "   4569: True,\n",
       "   4570: True,\n",
       "   4571: True,\n",
       "   4572: True,\n",
       "   4573: True,\n",
       "   4574: True,\n",
       "   4575: True,\n",
       "   4576: True,\n",
       "   4578: True,\n",
       "   4579: False,\n",
       "   4585: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4589: True,\n",
       "   4590: True,\n",
       "   4591: True,\n",
       "   4592: True,\n",
       "   4594: False,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: False,\n",
       "   4599: False,\n",
       "   4601: True,\n",
       "   4602: True,\n",
       "   4603: True,\n",
       "   4604: True,\n",
       "   4605: True,\n",
       "   4606: True,\n",
       "   4609: True,\n",
       "   4611: True,\n",
       "   4612: True,\n",
       "   4613: True,\n",
       "   4618: True,\n",
       "   4619: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4624: True,\n",
       "   4638: True,\n",
       "   4639: True,\n",
       "   4640: True,\n",
       "   4642: True,\n",
       "   4643: True,\n",
       "   4644: True,\n",
       "   4645: True,\n",
       "   4646: True,\n",
       "   4647: True,\n",
       "   4648: True,\n",
       "   4649: True,\n",
       "   4650: True,\n",
       "   4651: True,\n",
       "   4652: True,\n",
       "   4653: True,\n",
       "   4654: True,\n",
       "   4655: True,\n",
       "   4656: True,\n",
       "   4657: True,\n",
       "   4658: True,\n",
       "   4659: True,\n",
       "   4660: True,\n",
       "   4661: True,\n",
       "   4662: True,\n",
       "   4663: True,\n",
       "   4664: True,\n",
       "   4665: True,\n",
       "   4666: True,\n",
       "   4667: True,\n",
       "   4668: True,\n",
       "   4669: True,\n",
       "   4670: True,\n",
       "   4671: True,\n",
       "   4672: True,\n",
       "   4673: True,\n",
       "   4674: True,\n",
       "   4682: True,\n",
       "   4692: True,\n",
       "   4706: True,\n",
       "   4711: True,\n",
       "   5385: True,\n",
       "   5493: False},\n",
       "  0.9039301310043668],\n",
       " 'na-ko-to': [{678: False}, 0.0],\n",
       " 'de-so': [{678: False, 5659: False, 5728: False}, 0.0],\n",
       " 'dwo-jo': [{679: False, 1520: True}, 0.5],\n",
       " 'pa-ki': [{679: False}, 0.0],\n",
       " 'na-po': [{679: False}, 0.0],\n",
       " 'ru-ki': [{679: False, 1922: False, 4060: False}, 0.0],\n",
       " 'a-nu-mo': [{680: True, 3018: True, 3239: True}, 1.0],\n",
       " 'ta-za-ro': [{680: True, 2846: True, 3054: True}, 1.0],\n",
       " '*47-ti-jo': [{680: True, 4233: False}, 0.5],\n",
       " 'ja-ma-ra': [{680: True}, 1.0],\n",
       " 'pa-ja': [{680: False, 729: False}, 0.0],\n",
       " 'po-mi-ni-jo': [{680: True}, 1.0],\n",
       " 'wa-du-na': [{680: True, 697: True}, 1.0],\n",
       " 'no-mo': [{681: False}, 0.0],\n",
       " 'tu-ti': [{681: True, 3629: True}, 1.0],\n",
       " 'ki-ti-jo': [{681: False}, 0.0],\n",
       " 'do-ti-jo': [{682: True, 2741: True, 2891: True}, 1.0],\n",
       " '*49-sa-ro': [{682: True}, 1.0],\n",
       " 'po-ti': [{682: False, 2368: False, 4239: False}, 0.0],\n",
       " 'ra-ku': [{682: True}, 1.0],\n",
       " 'ra-te-me': [{682: True}, 1.0],\n",
       " '*56': [{682: False,\n",
       "   760: False,\n",
       "   1862: False,\n",
       "   2084: False,\n",
       "   2179: False,\n",
       "   2481: False,\n",
       "   2704: False,\n",
       "   2707: False,\n",
       "   3800: False,\n",
       "   5717: False,\n",
       "   5741: False},\n",
       "  0.0],\n",
       " 'e-re-dwo-e': [{683: False, 2671: True}, 0.5],\n",
       " 'i': [{683: False,\n",
       "   883: False,\n",
       "   928: False,\n",
       "   1545: False,\n",
       "   1570: False,\n",
       "   1648: False,\n",
       "   1662: False,\n",
       "   1679: False,\n",
       "   1863: False,\n",
       "   2001: False,\n",
       "   2071: False,\n",
       "   2100: False,\n",
       "   2318: False,\n",
       "   2372: False,\n",
       "   2663: False,\n",
       "   2674: False,\n",
       "   2709: False,\n",
       "   3887: False,\n",
       "   3910: False,\n",
       "   3917: False,\n",
       "   4111: False,\n",
       "   4491: False,\n",
       "   4842: False,\n",
       "   5142: False,\n",
       "   5375: False},\n",
       "  0.0],\n",
       " 'sa-ma-ru': [{683: False}, 0.0],\n",
       " 'pe-ri-to-wo': [{683: True, 834: True, 1748: True}, 1.0],\n",
       " 'ne-o': [{683: False, 2651: False}, 0.0],\n",
       " 'ja-ma-ta-ro': [{683: False}, 0.0],\n",
       " 'ta-de-so': [{683: True,\n",
       "   1428: False,\n",
       "   2671: True,\n",
       "   3244: True,\n",
       "   3268: True,\n",
       "   5658: True},\n",
       "  0.8333333333333334],\n",
       " 'ja-po': [{683: True}, 1.0],\n",
       " 'po': [{683: False,\n",
       "   1454: False,\n",
       "   1476: False,\n",
       "   1481: False,\n",
       "   1530: False,\n",
       "   1543: False,\n",
       "   1590: False,\n",
       "   1591: False,\n",
       "   2031: False,\n",
       "   2716: True,\n",
       "   2720: True,\n",
       "   2736: True,\n",
       "   2746: True,\n",
       "   2784: False,\n",
       "   4277: False,\n",
       "   4316: False,\n",
       "   4326: True,\n",
       "   4332: True,\n",
       "   5719: False},\n",
       "  0.3157894736842105],\n",
       " 'pe-to-me': [{683: False}, 0.0],\n",
       " 'ko-to': [{683: False, 890: True, 2780: True}, 0.6666666666666666],\n",
       " 'pa-ro': [{683: True,\n",
       "   1057: False,\n",
       "   1219: True,\n",
       "   1840: False,\n",
       "   1858: False,\n",
       "   2166: False,\n",
       "   2190: False,\n",
       "   2497: True,\n",
       "   2778: False,\n",
       "   2781: True,\n",
       "   2782: True,\n",
       "   2783: True,\n",
       "   2786: True,\n",
       "   2822: False,\n",
       "   3310: True,\n",
       "   4297: True,\n",
       "   4434: True,\n",
       "   4452: True,\n",
       "   4453: True,\n",
       "   4464: True,\n",
       "   4703: True,\n",
       "   4714: True,\n",
       "   4740: True,\n",
       "   4748: True,\n",
       "   4749: True,\n",
       "   4750: False,\n",
       "   4754: True,\n",
       "   4759: True,\n",
       "   4765: False,\n",
       "   4768: True,\n",
       "   4770: True,\n",
       "   4771: True,\n",
       "   4777: True,\n",
       "   4778: False,\n",
       "   4779: False,\n",
       "   4787: True,\n",
       "   4788: True,\n",
       "   4789: True,\n",
       "   4790: False,\n",
       "   4791: True,\n",
       "   4796: False,\n",
       "   4797: True,\n",
       "   4798: True,\n",
       "   4800: True,\n",
       "   4806: True,\n",
       "   4807: True,\n",
       "   4814: True,\n",
       "   4816: True,\n",
       "   4819: True,\n",
       "   4821: True,\n",
       "   4827: True,\n",
       "   4833: True,\n",
       "   4834: True,\n",
       "   4838: True,\n",
       "   4840: True,\n",
       "   4841: True,\n",
       "   4842: True,\n",
       "   4848: False,\n",
       "   4853: True,\n",
       "   4854: False,\n",
       "   4858: True,\n",
       "   4860: True,\n",
       "   4861: True,\n",
       "   4863: True,\n",
       "   4866: True,\n",
       "   4876: True,\n",
       "   4880: True,\n",
       "   4882: True,\n",
       "   4885: True,\n",
       "   4886: True,\n",
       "   4890: True,\n",
       "   4891: True,\n",
       "   4893: True,\n",
       "   4894: True,\n",
       "   4895: True,\n",
       "   4896: True,\n",
       "   4897: True,\n",
       "   4898: False,\n",
       "   4902: False,\n",
       "   4903: False,\n",
       "   4904: True,\n",
       "   4905: True,\n",
       "   4906: True,\n",
       "   4907: False,\n",
       "   4912: False,\n",
       "   4916: True,\n",
       "   4917: True,\n",
       "   4918: True,\n",
       "   4920: False,\n",
       "   4921: True,\n",
       "   4922: True,\n",
       "   4934: True,\n",
       "   4935: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4940: True,\n",
       "   4941: True,\n",
       "   4943: True,\n",
       "   4944: True,\n",
       "   4946: True,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   5001: True,\n",
       "   5119: True,\n",
       "   5129: True,\n",
       "   5143: True,\n",
       "   5155: True,\n",
       "   5265: True,\n",
       "   5266: True,\n",
       "   5376: True,\n",
       "   5393: True,\n",
       "   5394: False,\n",
       "   5410: True,\n",
       "   5465: True},\n",
       "  0.9097744360902256],\n",
       " 'e-re-pa-to': [{684: True, 5319: True}, 1.0],\n",
       " 'ka-ra-ma-to': [{684: True}, 1.0],\n",
       " 'ka-so': [{684: True, 749: False, 4770: True}, 0.6666666666666666],\n",
       " 'ke-ma-ta': [{684: True}, 1.0],\n",
       " 'po-ti-ro': [{685: False,\n",
       "   691: True,\n",
       "   692: True,\n",
       "   693: False,\n",
       "   694: True,\n",
       "   695: True,\n",
       "   700: False,\n",
       "   717: True,\n",
       "   722: False},\n",
       "  0.5555555555555556],\n",
       " 'wa-wi': [{685: True}, 1.0],\n",
       " 'a-mu-ta-wo-qe': [{685: False}, 0.0],\n",
       " 'da-*22-ti-ja': [{685: True, 691: True, 1714: True, 4123: True, 4423: True},\n",
       "  1.0],\n",
       " 'e-mi-ja-ta': [{686: True}, 1.0],\n",
       " 'ka-mu-ko-to': [{686: False}, 0.0],\n",
       " 'ke-ra-ja': [{686: True}, 1.0],\n",
       " 'ma-ke-ra': [{686: True}, 1.0],\n",
       " 'de-ro': [{686: False}, 0.0],\n",
       " 'ma-ti-ko': [{686: True, 897: False}, 0.5],\n",
       " 'a-ti-ka': [{686: True}, 1.0],\n",
       " 'we-u': [{686: False}, 0.0],\n",
       " 'qa-da-ro': [{686: True}, 1.0],\n",
       " 'ka-sa-no': [{686: True}, 1.0],\n",
       " 'to-ro-ki-no': [{686: True}, 1.0],\n",
       " 'no-re': [{686: False, 2806: False}, 0.0],\n",
       " 'wi-ri-ki-no': [{686: True}, 1.0],\n",
       " 'do-ti-jo-no': [{686: False}, 0.0],\n",
       " 'si-ra-pe-te-so': [{687: True}, 1.0],\n",
       " 'ka-pu-ro': [{687: True}, 1.0],\n",
       " 'ka-na-po-to': [{687: True}, 1.0],\n",
       " 'pi-ma': [{687: False}, 0.0],\n",
       " 'ru-ro': [{687: True, 4737: True, 5077: True}, 1.0],\n",
       " 'ta-u-ro': [{687: True}, 1.0],\n",
       " 'ja-sa-ro': [{687: True}, 1.0],\n",
       " 'a-ke-re-mo-no': [{688: True}, 1.0],\n",
       " 'ko-re-te': [{688: True,\n",
       "   2776: True,\n",
       "   4737: True,\n",
       "   5072: True,\n",
       "   5077: True,\n",
       "   5260: True,\n",
       "   5264: True},\n",
       "  0.8809523809523809],\n",
       " 'po-ro-ko-re-te': [{688: True, 5072: True, 5077: True}, 1.0],\n",
       " 'ma-na-je-u': [{689: False}, 0.0],\n",
       " 'te-ra': [{689: False}, 0.0],\n",
       " 'e-da-e': [{689: False}, 0.0],\n",
       " 'te-te-u': [{689: True}, 1.0],\n",
       " 'do-ri-ka-o': [{689: True, 5077: True}, 1.0],\n",
       " 'ka-ke-u': [{689: True, 4720: True, 5070: True, 5071: True, 5260: False},\n",
       "  0.8333333333333334],\n",
       " 'ka-ta': [{689: False, 1029: False, 3483: False, 4702: False}, 0.0],\n",
       " 'wi-pi-no-o': [{689: True}, 1.0],\n",
       " 'se': [{689: False,\n",
       "   698: False,\n",
       "   1057: False,\n",
       "   1492: False,\n",
       "   2681: False,\n",
       "   2714: False,\n",
       "   2728: False,\n",
       "   3487: True,\n",
       "   3488: True,\n",
       "   3490: False,\n",
       "   3491: False,\n",
       "   3492: True,\n",
       "   3494: True,\n",
       "   3496: False,\n",
       "   3500: False,\n",
       "   3508: True,\n",
       "   4502: True},\n",
       "  0.35294117647058826],\n",
       " 'pu-ra': [{689: False, 2712: False}, 0.0],\n",
       " 'wo-ka-re': [{690: True}, 1.0],\n",
       " 'pi-ra-ki-jo': [{691: True}, 1.0],\n",
       " 'pe-ri-jo-ta-qe': [{691: True}, 1.0],\n",
       " 'ka-di-ti-ja': [{692: False, 726: False}, 0.0],\n",
       " 'ku-da-jo-qe': [{693: True}, 1.0],\n",
       " 'e-wa-ko-ro': [{694: True, 5643: True, 5669: True, 5670: True}, 1.0],\n",
       " 'pi-ra-ka-wo-qe': [{694: True}, 1.0],\n",
       " 'ki-ra-di-ja': [{694: True}, 1.0],\n",
       " 'da-i-wo-wo': [{695: True}, 1.0],\n",
       " 'to-no-qe': [{695: True}, 1.0],\n",
       " 'pa-si-ja': [{695: False}, 0.0],\n",
       " 'ta-ra-nu': [{696: True,\n",
       "   5346: True,\n",
       "   5347: True,\n",
       "   5349: True,\n",
       "   5352: True,\n",
       "   5355: True,\n",
       "   5356: True},\n",
       "  0.8235294117647058],\n",
       " 'qe': [{696: False,\n",
       "   698: False,\n",
       "   924: False,\n",
       "   1019: False,\n",
       "   1067: False,\n",
       "   1101: False,\n",
       "   2382: False,\n",
       "   2417: False,\n",
       "   2729: False,\n",
       "   2773: False,\n",
       "   2784: False,\n",
       "   2855: False,\n",
       "   2860: False,\n",
       "   2864: False,\n",
       "   4723: False,\n",
       "   5264: False,\n",
       "   5565: False},\n",
       "  0.0],\n",
       " 'i-jo': [{697: True,\n",
       "   1070: False,\n",
       "   3868: False,\n",
       "   4723: False,\n",
       "   4724: True,\n",
       "   5114: False,\n",
       "   5725: False},\n",
       "  0.5],\n",
       " 'sa-ka-ri-jo': [{697: False}, 0.0],\n",
       " 'pi-ma-na-ro': [{697: True, 2681: True}, 1.0],\n",
       " 'zo-wi-jo': [{697: True, 4748: True}, 1.0],\n",
       " 'a-tu-qo-te-ra-to': [{697: True}, 1.0],\n",
       " 'ri-zo': [{697: True,\n",
       "   714: True,\n",
       "   2678: True,\n",
       "   2681: True,\n",
       "   2708: True,\n",
       "   4710: True},\n",
       "  1.0],\n",
       " 'pi-ro-i-ta': [{697: True}, 1.0],\n",
       " 'pa-ka': [{697: True, 4982: True}, 1.0],\n",
       " 'di-wa-jo': [{697: True}, 1.0],\n",
       " 'o-na-se-u': [{697: True, 4732: True, 5064: True, 5066: True, 5070: True},\n",
       "  1.0],\n",
       " 'wi-du': [{697: False, 2678: False}, 0.0],\n",
       " 'za': [{697: False,\n",
       "   1469: False,\n",
       "   2556: True,\n",
       "   2627: True,\n",
       "   3487: True,\n",
       "   3490: True,\n",
       "   3494: True,\n",
       "   3495: True,\n",
       "   3501: True,\n",
       "   3506: True,\n",
       "   4327: False,\n",
       "   4502: True},\n",
       "  0.75],\n",
       " 'ma-no-ne': [{698: False}, 0.0],\n",
       " 'po-da': [{698: False}, 0.6666666666666666],\n",
       " 'do-ma': [{698: False}, 0.0],\n",
       " 'ki-si-wo': [{698: False}, 0.0],\n",
       " 'si-mi-te-u': [{700: True, 2646: True}, 1.0],\n",
       " 'a-ra-ko-qe': [{700: True}, 1.0],\n",
       " 'ti-ja': [{700: False, 3292: True, 3907: False, 5711: False}, 0.25],\n",
       " 'to-i-je': [{701: False}, 0.0],\n",
       " 'ru-ko': [{701: False, 5269: True}, 0.5],\n",
       " 'no-du': [{701: False}, 0.0],\n",
       " 'da-na-jo': [{701: False, 3099: True}, 0.5],\n",
       " 'me': [{702: False,\n",
       "   1057: False,\n",
       "   1116: False,\n",
       "   1291: False,\n",
       "   1333: False,\n",
       "   1448: False,\n",
       "   1461: False,\n",
       "   1526: False,\n",
       "   1666: False,\n",
       "   2072: False,\n",
       "   2552: False,\n",
       "   2567: True,\n",
       "   2581: False,\n",
       "   2584: False,\n",
       "   2587: True,\n",
       "   2598: False,\n",
       "   2607: False,\n",
       "   2612: False,\n",
       "   2625: False,\n",
       "   3954: False,\n",
       "   4532: False},\n",
       "  0.08695652173913043],\n",
       " 'do-wa': [{702: True}, 1.0],\n",
       " 'ka-na-po': [{703: False}, 0.0],\n",
       " 'qa-na': [{704: False}, 0.0],\n",
       " 'ka-sa': [{704: False, 4753: False}, 0.0],\n",
       " 'we-to-ro': [{705: False}, 0.0],\n",
       " 'ja-ro': [{706: False, 5564: False}, 0.0],\n",
       " 'ro-we': [{706: False}, 0.0],\n",
       " 'se-me-ni': [{706: False}, 0.0],\n",
       " 'o-da-ra-o': [{707: True}, 1.0],\n",
       " 'ku-ke-mo': [{708: False}, 0.0],\n",
       " 'ke-ti-ra-wo': [{709: False}, 0.0],\n",
       " 'wo-do': [{709: True, 978: True}, 1.0],\n",
       " 'u-su': [{710: True}, 1.0],\n",
       " 'ta-qa-ra-ti': [{710: True}, 1.0],\n",
       " 'ku-ma-to': [{710: False}, 0.0],\n",
       " 'ma-ro': [{711: False,\n",
       "   1487: False,\n",
       "   2728: False,\n",
       "   3143: False,\n",
       "   4748: True,\n",
       "   4758: True},\n",
       "  0.42857142857142855],\n",
       " 'o-ru': [{711: False}, 0.0],\n",
       " 'qe-re-ma-o': [{711: True, 5277: True}, 1.0],\n",
       " 'me-ja': [{713: False}, 0.0],\n",
       " 'ko': [{715: False,\n",
       "   723: False,\n",
       "   915: False,\n",
       "   1072: False,\n",
       "   1409: False,\n",
       "   1425: False,\n",
       "   1477: False,\n",
       "   1583: False,\n",
       "   1584: False,\n",
       "   1723: False,\n",
       "   1797: False,\n",
       "   1800: False,\n",
       "   1866: False,\n",
       "   1912: False,\n",
       "   1913: False,\n",
       "   1963: False,\n",
       "   1964: False,\n",
       "   1965: False,\n",
       "   2023: False,\n",
       "   2207: False,\n",
       "   2331: True,\n",
       "   2335: False,\n",
       "   2375: True,\n",
       "   2418: False,\n",
       "   2419: False,\n",
       "   2495: False,\n",
       "   2519: False,\n",
       "   2529: False,\n",
       "   2532: False,\n",
       "   2534: False,\n",
       "   2538: False,\n",
       "   2539: False,\n",
       "   2581: False,\n",
       "   2598: False,\n",
       "   2618: False,\n",
       "   2621: False,\n",
       "   2651: True,\n",
       "   2652: True,\n",
       "   2677: False,\n",
       "   2706: False,\n",
       "   2849: False,\n",
       "   2905: True,\n",
       "   3741: False,\n",
       "   3992: False,\n",
       "   4011: False,\n",
       "   4154: True,\n",
       "   4161: True,\n",
       "   4358: True,\n",
       "   4396: False,\n",
       "   4515: False,\n",
       "   4707: False,\n",
       "   4717: False,\n",
       "   4723: False,\n",
       "   4763: False,\n",
       "   4766: False,\n",
       "   4769: False,\n",
       "   4780: False,\n",
       "   5062: False,\n",
       "   5078: False,\n",
       "   5382: True,\n",
       "   5417: False,\n",
       "   5435: False,\n",
       "   5704: False,\n",
       "   5762: False},\n",
       "  0.14925373134328357],\n",
       " 'di-pi-ja': [{717: True}, 1.0],\n",
       " 'a-qi-ta': [{718: False}, 0.0],\n",
       " 'ki-ma-to': [{718: False}, 0.0],\n",
       " 'e-u-po-ro-wo': [{718: False, 5064: True, 5068: True}, 0.6666666666666666],\n",
       " 'a-pa-ta-wa-ja': [{719: True}, 1.0],\n",
       " 'ra-tu': [{720: False}, 0.0],\n",
       " 'pe-re-ta': [{724: False, 5066: True, 5070: True}, 0.6666666666666666],\n",
       " 'je-u-qe': [{725: False}, 0.0],\n",
       " 'ti': [{727: False,\n",
       "   735: False,\n",
       "   894: False,\n",
       "   1080: False,\n",
       "   1105: False,\n",
       "   1419: False,\n",
       "   1725: False,\n",
       "   1959: False,\n",
       "   2076: False,\n",
       "   2114: False,\n",
       "   2194: False,\n",
       "   2317: False,\n",
       "   3204: False,\n",
       "   3280: False,\n",
       "   3687: False,\n",
       "   3830: False,\n",
       "   4688: False,\n",
       "   5083: False,\n",
       "   5139: False,\n",
       "   5389: False,\n",
       "   5671: False},\n",
       "  0.0],\n",
       " 'a-mu-ta-wo': [{728: False, 5260: True}, 0.5],\n",
       " 'a-phu-ka': [{729: True, 1009: False, 4725: True}, 0.6666666666666666],\n",
       " 'za-mi-so': [{729: True}, 1.0],\n",
       " 'ku-ta-to': [{729: True,\n",
       "   939: False,\n",
       "   1189: True,\n",
       "   1593: False,\n",
       "   2829: True,\n",
       "   2940: True,\n",
       "   2941: True,\n",
       "   2942: True,\n",
       "   3004: True,\n",
       "   3006: True,\n",
       "   3012: True,\n",
       "   3014: True,\n",
       "   3016: True,\n",
       "   3024: True,\n",
       "   3034: True,\n",
       "   3057: True,\n",
       "   3058: False,\n",
       "   3059: True,\n",
       "   3121: True,\n",
       "   3122: True,\n",
       "   3138: True,\n",
       "   3139: True,\n",
       "   3140: True,\n",
       "   3161: True,\n",
       "   3168: True,\n",
       "   3202: True,\n",
       "   3218: True,\n",
       "   3219: True,\n",
       "   3242: True,\n",
       "   3246: True,\n",
       "   3250: True,\n",
       "   3257: True,\n",
       "   3258: True,\n",
       "   3259: True,\n",
       "   3315: True,\n",
       "   3316: True,\n",
       "   3317: False,\n",
       "   3318: True,\n",
       "   3319: True,\n",
       "   3320: False,\n",
       "   3321: True,\n",
       "   3322: True,\n",
       "   3323: True,\n",
       "   3324: True,\n",
       "   3325: True,\n",
       "   3326: True,\n",
       "   3334: False,\n",
       "   3338: True,\n",
       "   3381: True,\n",
       "   3457: True,\n",
       "   3460: True,\n",
       "   3464: True,\n",
       "   3527: False,\n",
       "   3554: True,\n",
       "   3555: True,\n",
       "   3556: False,\n",
       "   3557: True,\n",
       "   3558: True,\n",
       "   3585: True,\n",
       "   3615: True,\n",
       "   3618: True,\n",
       "   3640: False,\n",
       "   3655: False,\n",
       "   3659: True,\n",
       "   3660: True,\n",
       "   3668: True,\n",
       "   4144: True,\n",
       "   4147: True,\n",
       "   4480: True,\n",
       "   4483: True},\n",
       "  0.8571428571428571],\n",
       " 'o-pe-re-ra': [{730: False}, 0.0],\n",
       " 'me-na-qe': [{730: False}, 0.0],\n",
       " 'e-ke-pi': [{733: False}, 0.0],\n",
       " 'a-ra': [{733: False}, 0.0],\n",
       " 'ku-ta-ti-jo': [{737: False, 1474: False, 4134: True, 4152: False}, 0.2],\n",
       " 'da-*22-ti': [{739: False, 1318: False, 2009: False, 4453: False}, 0.0],\n",
       " 'da-mi': [{741: False, 4229: False}, 0.0],\n",
       " 'si-we': [{748: False}, 0.0],\n",
       " 'ra-ze': [{748: False}, 0.0],\n",
       " 'o-a-pu': [{756: False}, 0.0],\n",
       " 'tu-na-no': [{756: True,\n",
       "   4379: True,\n",
       "   4380: True,\n",
       "   4381: True,\n",
       "   4382: True,\n",
       "   4383: True,\n",
       "   4384: True,\n",
       "   4385: True,\n",
       "   4386: True,\n",
       "   4388: True,\n",
       "   4389: True,\n",
       "   4390: False,\n",
       "   4393: True,\n",
       "   4395: True,\n",
       "   4399: True,\n",
       "   4401: True,\n",
       "   4404: False,\n",
       "   4408: True,\n",
       "   4409: True,\n",
       "   4412: False,\n",
       "   4414: True,\n",
       "   4420: False,\n",
       "   4464: False},\n",
       "  0.782608695652174],\n",
       " 'ki-ri-jo-te': [{757: False,\n",
       "   2953: True,\n",
       "   2959: True,\n",
       "   2961: True,\n",
       "   2974: True,\n",
       "   2992: True,\n",
       "   3018: True,\n",
       "   3020: False,\n",
       "   3025: True,\n",
       "   3029: True,\n",
       "   3042: True,\n",
       "   3044: True,\n",
       "   3113: True,\n",
       "   3119: True,\n",
       "   3150: True,\n",
       "   3158: True,\n",
       "   3167: True,\n",
       "   3277: True,\n",
       "   3589: True,\n",
       "   3750: True,\n",
       "   4501: False},\n",
       "  0.8571428571428571],\n",
       " 'ki': [{764: False,\n",
       "   927: False,\n",
       "   1246: False,\n",
       "   1534: False,\n",
       "   1582: False,\n",
       "   1627: False,\n",
       "   1914: False,\n",
       "   1915: False,\n",
       "   2158: False,\n",
       "   2379: False,\n",
       "   2816: True,\n",
       "   2926: False,\n",
       "   3298: True,\n",
       "   3299: True,\n",
       "   3300: True,\n",
       "   3301: True,\n",
       "   3302: True,\n",
       "   3303: True,\n",
       "   3317: True,\n",
       "   3318: True,\n",
       "   3377: True,\n",
       "   3378: True,\n",
       "   3379: True,\n",
       "   3380: True,\n",
       "   3381: True,\n",
       "   3382: True,\n",
       "   3383: True,\n",
       "   3384: True,\n",
       "   3385: True,\n",
       "   3386: True,\n",
       "   3387: True,\n",
       "   3388: True,\n",
       "   3389: True,\n",
       "   3390: True,\n",
       "   3391: True,\n",
       "   3392: True,\n",
       "   3393: True,\n",
       "   3395: True,\n",
       "   3396: True,\n",
       "   3397: True,\n",
       "   3398: True,\n",
       "   3399: True,\n",
       "   3400: True,\n",
       "   3401: True,\n",
       "   3402: True,\n",
       "   3403: True,\n",
       "   3404: True,\n",
       "   3405: True,\n",
       "   3411: True,\n",
       "   3412: False,\n",
       "   3413: True,\n",
       "   3414: False,\n",
       "   3415: True,\n",
       "   3416: True,\n",
       "   3418: True,\n",
       "   3420: False,\n",
       "   3421: True,\n",
       "   3422: True,\n",
       "   3423: True,\n",
       "   3431: True,\n",
       "   3433: True,\n",
       "   3436: True,\n",
       "   3438: True,\n",
       "   3442: True,\n",
       "   3443: True,\n",
       "   3445: False,\n",
       "   3450: False,\n",
       "   3451: True,\n",
       "   3454: False,\n",
       "   3487: True,\n",
       "   3488: True,\n",
       "   3490: True,\n",
       "   3491: True,\n",
       "   3492: False,\n",
       "   3494: True,\n",
       "   3495: True,\n",
       "   3497: False,\n",
       "   3498: True,\n",
       "   3499: True,\n",
       "   3500: True,\n",
       "   3501: True,\n",
       "   3506: False,\n",
       "   3827: False,\n",
       "   4444: False,\n",
       "   4502: True,\n",
       "   4722: False,\n",
       "   5077: False,\n",
       "   5146: False},\n",
       "  0.75],\n",
       " 'ai-wo-re-u': [{765: False}, 0.0],\n",
       " 'si': [{765: False,\n",
       "   1247: False,\n",
       "   1421: False,\n",
       "   1821: False,\n",
       "   2309: False,\n",
       "   2654: False,\n",
       "   2659: False,\n",
       "   2787: False,\n",
       "   3034: False,\n",
       "   3203: False,\n",
       "   3970: False,\n",
       "   4182: False,\n",
       "   4225: False,\n",
       "   4273: False,\n",
       "   4322: False,\n",
       "   4958: False,\n",
       "   5552: False},\n",
       "  0.0],\n",
       " 'do-ke': [{765: True, 774: True, 1188: False, 2112: True, 5381: False}, 0.6],\n",
       " 'me-sa-to': [{766: True, 767: True, 769: True, 770: False}, 0.75],\n",
       " 'sa-ma-ja-so': [{772: True, 3632: True}, 1.0],\n",
       " 'ki-ri-ta-de': [{774: True, 1665: True}, 1.0],\n",
       " 'pi-mo-no': [{775: True, 2303: True}, 1.0],\n",
       " 'na-ki-zo': [{775: True}, 1.0],\n",
       " 'pa-wo': [{775: True}, 1.0],\n",
       " 'VIR': [{776: True,\n",
       "   782: True,\n",
       "   2217: False,\n",
       "   2219: False,\n",
       "   2227: False,\n",
       "   2316: True,\n",
       "   2317: True,\n",
       "   2318: False,\n",
       "   2320: True,\n",
       "   2321: True,\n",
       "   2322: True,\n",
       "   2417: False,\n",
       "   2489: True,\n",
       "   2490: True,\n",
       "   2491: True,\n",
       "   2492: True,\n",
       "   2493: True,\n",
       "   2494: True,\n",
       "   2495: False,\n",
       "   2572: True,\n",
       "   2639: False,\n",
       "   2640: True,\n",
       "   2641: True,\n",
       "   2642: True,\n",
       "   2643: True,\n",
       "   2644: True,\n",
       "   2645: True,\n",
       "   2646: True,\n",
       "   2647: True,\n",
       "   2648: True,\n",
       "   2649: True,\n",
       "   2667: True,\n",
       "   2668: False,\n",
       "   2669: False,\n",
       "   2670: False,\n",
       "   2671: True,\n",
       "   2672: True,\n",
       "   2673: True,\n",
       "   2674: True,\n",
       "   2675: True,\n",
       "   2676: False,\n",
       "   2677: True,\n",
       "   2678: True,\n",
       "   2679: True,\n",
       "   2680: True,\n",
       "   2681: True,\n",
       "   2682: True,\n",
       "   2683: False,\n",
       "   2684: True,\n",
       "   2685: True,\n",
       "   2686: True,\n",
       "   2687: True,\n",
       "   2688: True,\n",
       "   2689: True,\n",
       "   2690: True,\n",
       "   2691: True,\n",
       "   2692: True,\n",
       "   2693: False,\n",
       "   2694: True,\n",
       "   2695: False,\n",
       "   2696: True,\n",
       "   2697: False,\n",
       "   2698: True,\n",
       "   2699: True,\n",
       "   2700: True,\n",
       "   2701: False,\n",
       "   2702: True,\n",
       "   2703: True,\n",
       "   2704: False,\n",
       "   2705: True,\n",
       "   2706: True,\n",
       "   2707: True,\n",
       "   2708: True,\n",
       "   2709: True,\n",
       "   2711: False,\n",
       "   2712: True,\n",
       "   2713: True,\n",
       "   2714: True,\n",
       "   2715: True,\n",
       "   2716: True,\n",
       "   2717: True,\n",
       "   2718: True,\n",
       "   2719: True,\n",
       "   2720: True,\n",
       "   2721: True,\n",
       "   2722: True,\n",
       "   2723: True,\n",
       "   2724: True,\n",
       "   2726: True,\n",
       "   2727: True,\n",
       "   2728: True,\n",
       "   2729: False,\n",
       "   2731: True,\n",
       "   2732: True,\n",
       "   2734: True,\n",
       "   2735: False,\n",
       "   2736: True,\n",
       "   2737: False,\n",
       "   2738: False,\n",
       "   2739: False,\n",
       "   2740: True,\n",
       "   2741: True,\n",
       "   2742: True,\n",
       "   2743: True,\n",
       "   2744: True,\n",
       "   2745: True,\n",
       "   2746: True,\n",
       "   2747: False,\n",
       "   2748: False,\n",
       "   2749: True,\n",
       "   2751: False,\n",
       "   2753: False,\n",
       "   2755: False,\n",
       "   2756: True,\n",
       "   2757: True,\n",
       "   2758: True,\n",
       "   2763: False,\n",
       "   2764: True,\n",
       "   2765: False,\n",
       "   2768: False,\n",
       "   2770: False,\n",
       "   2895: False,\n",
       "   3780: False,\n",
       "   4235: True,\n",
       "   4606: True,\n",
       "   4630: True,\n",
       "   4631: True,\n",
       "   4632: True,\n",
       "   4633: True,\n",
       "   4634: True,\n",
       "   4635: True,\n",
       "   4636: True,\n",
       "   4637: True,\n",
       "   4638: True,\n",
       "   4639: True,\n",
       "   4640: True,\n",
       "   4641: True,\n",
       "   4642: True,\n",
       "   4643: True,\n",
       "   4644: True,\n",
       "   4645: True,\n",
       "   4647: True,\n",
       "   4648: False,\n",
       "   4649: True,\n",
       "   4650: True,\n",
       "   4651: True,\n",
       "   4652: True,\n",
       "   4653: True,\n",
       "   4654: True,\n",
       "   4655: True,\n",
       "   4656: True,\n",
       "   4657: True,\n",
       "   4658: True,\n",
       "   4659: True,\n",
       "   4660: True,\n",
       "   4661: True,\n",
       "   4662: True,\n",
       "   4663: True,\n",
       "   4664: True,\n",
       "   4665: True,\n",
       "   4666: True,\n",
       "   4667: True,\n",
       "   4668: True,\n",
       "   4669: True,\n",
       "   4670: True,\n",
       "   4671: True,\n",
       "   4672: True,\n",
       "   4673: True,\n",
       "   4674: True,\n",
       "   4675: False,\n",
       "   4676: True,\n",
       "   4677: True,\n",
       "   4678: True,\n",
       "   4679: True,\n",
       "   4681: True,\n",
       "   4682: True,\n",
       "   4683: True,\n",
       "   4685: True,\n",
       "   4686: True,\n",
       "   4687: True,\n",
       "   4690: True,\n",
       "   4691: False,\n",
       "   4693: False,\n",
       "   4694: True,\n",
       "   4695: True,\n",
       "   4696: True,\n",
       "   4697: True,\n",
       "   4698: True,\n",
       "   4699: True,\n",
       "   4700: True,\n",
       "   4701: True,\n",
       "   4702: True,\n",
       "   4703: True,\n",
       "   4704: True,\n",
       "   4705: True,\n",
       "   4706: True,\n",
       "   4707: True,\n",
       "   4708: True,\n",
       "   4709: True,\n",
       "   4710: True,\n",
       "   4712: True,\n",
       "   4713: True,\n",
       "   4714: False,\n",
       "   4715: True,\n",
       "   4716: True,\n",
       "   4717: True,\n",
       "   4718: True,\n",
       "   4719: True,\n",
       "   4721: True,\n",
       "   4722: True,\n",
       "   4723: False,\n",
       "   4724: True,\n",
       "   4725: True,\n",
       "   4726: True,\n",
       "   4727: True,\n",
       "   4728: True,\n",
       "   4729: True,\n",
       "   4730: True,\n",
       "   4731: True,\n",
       "   4732: True,\n",
       "   4733: True,\n",
       "   4734: True,\n",
       "   4735: False,\n",
       "   4736: False,\n",
       "   4738: True,\n",
       "   4769: True,\n",
       "   4933: True,\n",
       "   4959: True,\n",
       "   4982: True,\n",
       "   5062: True,\n",
       "   5357: True,\n",
       "   5384: True,\n",
       "   5719: True},\n",
       "  0.8526946107784431],\n",
       " 'MUL': [{777: True,\n",
       "   1931: False,\n",
       "   2489: True,\n",
       "   2490: True,\n",
       "   2493: True,\n",
       "   2494: True,\n",
       "   2495: True,\n",
       "   2496: True,\n",
       "   2499: False,\n",
       "   2500: False,\n",
       "   2501: True,\n",
       "   2502: True,\n",
       "   2503: True,\n",
       "   2506: True,\n",
       "   2507: True,\n",
       "   2508: False,\n",
       "   2509: True,\n",
       "   2510: True,\n",
       "   2512: False,\n",
       "   2516: True,\n",
       "   2518: True,\n",
       "   2519: True,\n",
       "   2521: True,\n",
       "   2523: True,\n",
       "   2529: True,\n",
       "   2530: False,\n",
       "   2531: False,\n",
       "   2533: False,\n",
       "   2534: True,\n",
       "   2535: False,\n",
       "   2550: True,\n",
       "   2551: True,\n",
       "   2552: True,\n",
       "   2553: True,\n",
       "   2554: True,\n",
       "   2555: True,\n",
       "   2556: True,\n",
       "   2557: True,\n",
       "   2558: True,\n",
       "   2559: True,\n",
       "   2560: True,\n",
       "   2564: False,\n",
       "   2566: True,\n",
       "   2572: True,\n",
       "   2573: True,\n",
       "   2574: True,\n",
       "   2575: True,\n",
       "   2583: False,\n",
       "   2591: True,\n",
       "   2593: True,\n",
       "   2595: True,\n",
       "   2597: True,\n",
       "   2599: False,\n",
       "   2601: True,\n",
       "   2606: True,\n",
       "   2612: False,\n",
       "   2613: False,\n",
       "   2614: True,\n",
       "   2615: False,\n",
       "   2616: True,\n",
       "   2617: True,\n",
       "   2619: True,\n",
       "   2623: False,\n",
       "   2627: True,\n",
       "   2630: True,\n",
       "   2637: False,\n",
       "   2638: False,\n",
       "   2646: True,\n",
       "   2650: True,\n",
       "   2651: False,\n",
       "   2652: True,\n",
       "   2653: True,\n",
       "   2654: True,\n",
       "   2655: True,\n",
       "   2656: True,\n",
       "   2658: True,\n",
       "   2659: True,\n",
       "   2661: True,\n",
       "   4536: True,\n",
       "   4538: True,\n",
       "   4539: True,\n",
       "   4540: True,\n",
       "   4541: True,\n",
       "   4542: True,\n",
       "   4543: True,\n",
       "   4544: True,\n",
       "   4545: True,\n",
       "   4546: True,\n",
       "   4547: True,\n",
       "   4548: True,\n",
       "   4549: True,\n",
       "   4550: True,\n",
       "   4551: True,\n",
       "   4552: True,\n",
       "   4553: True,\n",
       "   4554: True,\n",
       "   4555: True,\n",
       "   4556: True,\n",
       "   4557: True,\n",
       "   4558: True,\n",
       "   4560: True,\n",
       "   4561: True,\n",
       "   4562: True,\n",
       "   4563: True,\n",
       "   4564: True,\n",
       "   4565: True,\n",
       "   4566: True,\n",
       "   4567: True,\n",
       "   4568: True,\n",
       "   4569: True,\n",
       "   4570: True,\n",
       "   4571: True,\n",
       "   4572: True,\n",
       "   4573: True,\n",
       "   4574: True,\n",
       "   4575: True,\n",
       "   4576: True,\n",
       "   4577: True,\n",
       "   4580: True,\n",
       "   4581: True,\n",
       "   4583: False,\n",
       "   4584: False,\n",
       "   4585: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4589: True,\n",
       "   4590: True,\n",
       "   4591: True,\n",
       "   4592: True,\n",
       "   4593: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: True,\n",
       "   4598: False,\n",
       "   4599: True,\n",
       "   4601: True,\n",
       "   4603: True,\n",
       "   4604: True,\n",
       "   4605: True,\n",
       "   4607: False,\n",
       "   4608: True,\n",
       "   4609: False,\n",
       "   4610: False,\n",
       "   4611: True,\n",
       "   4612: True,\n",
       "   4613: True,\n",
       "   4618: True,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4625: True,\n",
       "   4684: True,\n",
       "   4688: True,\n",
       "   4689: True,\n",
       "   4711: False,\n",
       "   4720: True,\n",
       "   5357: True,\n",
       "   5417: True,\n",
       "   5432: True},\n",
       "  0.8699186991869918],\n",
       " 'ta-to-mo': [{779: True, 4747: True, 4769: True, 5408: True}, 1.0],\n",
       " 'ne-ki': [{783: False}, 0.0],\n",
       " 'ri-de': [{783: False}, 0.0],\n",
       " 'JAC': [{784: True, 787: True}, 1.0],\n",
       " 'pa-ta-ja': [{784: True, 787: True, 904: True}, 1.0],\n",
       " 'TELAI+TE': [{785: True, 4311: True, 4379: True}, 1.0],\n",
       " 'te-pa': [{785: True, 1241: False, 4302: False}, 0.3333333333333333],\n",
       " 'ko-we': [{790: True, 1609: False}, 0.5],\n",
       " 'GRA': [{791: True,\n",
       "   2312: True,\n",
       "   2321: True,\n",
       "   2326: True,\n",
       "   2327: True,\n",
       "   2375: True,\n",
       "   2379: False,\n",
       "   2422: True,\n",
       "   2504: True,\n",
       "   2505: True,\n",
       "   2506: True,\n",
       "   2517: True,\n",
       "   2523: True,\n",
       "   2528: True,\n",
       "   2530: False,\n",
       "   2536: False,\n",
       "   2537: False,\n",
       "   3862: True,\n",
       "   3863: False,\n",
       "   3864: True,\n",
       "   3865: True,\n",
       "   3866: False,\n",
       "   3867: True,\n",
       "   3868: True,\n",
       "   3869: False,\n",
       "   3870: True,\n",
       "   3871: True,\n",
       "   3872: False,\n",
       "   3873: False,\n",
       "   3874: True,\n",
       "   3875: True,\n",
       "   3876: True,\n",
       "   3877: True,\n",
       "   3878: True,\n",
       "   3879: False,\n",
       "   3880: True,\n",
       "   3881: True,\n",
       "   3882: False,\n",
       "   3883: False,\n",
       "   3885: False,\n",
       "   3886: True,\n",
       "   3887: False,\n",
       "   3888: False,\n",
       "   3889: True,\n",
       "   3891: False,\n",
       "   3892: True,\n",
       "   3893: False,\n",
       "   3894: True,\n",
       "   3897: False,\n",
       "   3898: False,\n",
       "   3902: True,\n",
       "   3908: True,\n",
       "   3909: True,\n",
       "   3910: True,\n",
       "   3911: True,\n",
       "   3912: True,\n",
       "   3913: True,\n",
       "   3915: False,\n",
       "   3918: False,\n",
       "   4508: True,\n",
       "   4586: True,\n",
       "   4587: True,\n",
       "   4588: True,\n",
       "   4589: True,\n",
       "   4590: True,\n",
       "   4591: True,\n",
       "   4592: True,\n",
       "   4594: True,\n",
       "   4595: True,\n",
       "   4596: True,\n",
       "   4597: True,\n",
       "   4598: True,\n",
       "   4599: True,\n",
       "   4600: False,\n",
       "   4601: True,\n",
       "   4602: True,\n",
       "   4603: True,\n",
       "   4604: True,\n",
       "   4605: True,\n",
       "   4606: True,\n",
       "   4609: False,\n",
       "   4611: True,\n",
       "   4613: True,\n",
       "   4614: False,\n",
       "   4616: False,\n",
       "   4618: True,\n",
       "   4619: True,\n",
       "   4620: True,\n",
       "   4621: True,\n",
       "   4622: True,\n",
       "   4627: True,\n",
       "   4628: True,\n",
       "   4629: False,\n",
       "   4702: True,\n",
       "   4786: False,\n",
       "   4787: True,\n",
       "   4788: True,\n",
       "   4789: True,\n",
       "   4790: True,\n",
       "   4791: True,\n",
       "   4792: True,\n",
       "   4793: True,\n",
       "   4794: True,\n",
       "   4795: True,\n",
       "   4797: True,\n",
       "   4798: True,\n",
       "   4799: True,\n",
       "   4800: False,\n",
       "   4801: True,\n",
       "   4802: True,\n",
       "   4803: True,\n",
       "   4804: True,\n",
       "   4805: True,\n",
       "   4806: True,\n",
       "   4807: True,\n",
       "   4808: True,\n",
       "   4809: True,\n",
       "   4810: True,\n",
       "   4811: True,\n",
       "   4812: True,\n",
       "   4813: True,\n",
       "   4814: True,\n",
       "   4815: True,\n",
       "   4816: True,\n",
       "   4817: True,\n",
       "   4818: True,\n",
       "   4819: True,\n",
       "   4820: True,\n",
       "   4821: True,\n",
       "   4822: True,\n",
       "   4823: True,\n",
       "   4824: True,\n",
       "   4825: True,\n",
       "   4826: True,\n",
       "   4827: True,\n",
       "   4828: False,\n",
       "   4829: True,\n",
       "   4830: True,\n",
       "   4831: True,\n",
       "   4832: True,\n",
       "   4833: True,\n",
       "   4834: True,\n",
       "   4835: True,\n",
       "   4836: True,\n",
       "   4837: True,\n",
       "   4838: True,\n",
       "   4839: True,\n",
       "   4840: True,\n",
       "   4841: True,\n",
       "   4842: True,\n",
       "   4843: False,\n",
       "   4844: True,\n",
       "   4845: True,\n",
       "   4846: True,\n",
       "   4847: True,\n",
       "   4848: True,\n",
       "   4849: True,\n",
       "   4850: True,\n",
       "   4851: False,\n",
       "   4852: True,\n",
       "   4853: True,\n",
       "   4854: True,\n",
       "   4855: True,\n",
       "   4856: True,\n",
       "   4857: True,\n",
       "   4858: True,\n",
       "   4859: False,\n",
       "   4860: True,\n",
       "   4861: True,\n",
       "   4862: False,\n",
       "   4863: True,\n",
       "   4864: True,\n",
       "   4865: True,\n",
       "   4866: True,\n",
       "   4867: True,\n",
       "   4868: True,\n",
       "   4869: True,\n",
       "   4870: True,\n",
       "   4871: True,\n",
       "   4873: True,\n",
       "   4874: True,\n",
       "   4875: True,\n",
       "   4876: True,\n",
       "   4877: True,\n",
       "   4878: True,\n",
       "   4879: True,\n",
       "   4880: True,\n",
       "   4881: True,\n",
       "   4882: True,\n",
       "   4883: True,\n",
       "   4884: True,\n",
       "   4885: True,\n",
       "   4886: True,\n",
       "   4887: True,\n",
       "   4888: True,\n",
       "   4889: True,\n",
       "   4890: True,\n",
       "   4891: True,\n",
       "   4892: True,\n",
       "   4893: True,\n",
       "   4894: True,\n",
       "   4895: True,\n",
       "   4896: False,\n",
       "   4897: True,\n",
       "   4898: True,\n",
       "   4899: True,\n",
       "   4900: True,\n",
       "   4901: True,\n",
       "   4902: True,\n",
       "   4903: True,\n",
       "   4904: True,\n",
       "   4905: False,\n",
       "   4906: True,\n",
       "   4907: False,\n",
       "   4908: True,\n",
       "   4909: True,\n",
       "   4911: True,\n",
       "   4912: True,\n",
       "   4913: True,\n",
       "   4914: True,\n",
       "   4915: True,\n",
       "   4916: True,\n",
       "   4917: True,\n",
       "   4918: True,\n",
       "   4919: True,\n",
       "   4920: True,\n",
       "   4921: True,\n",
       "   4922: False,\n",
       "   4923: True,\n",
       "   4924: True,\n",
       "   4925: True,\n",
       "   4926: True,\n",
       "   4927: True,\n",
       "   4928: True,\n",
       "   4929: True,\n",
       "   4930: True,\n",
       "   4931: True,\n",
       "   4932: True,\n",
       "   4933: True,\n",
       "   4934: True,\n",
       "   4935: True,\n",
       "   4936: True,\n",
       "   4937: True,\n",
       "   4938: True,\n",
       "   4939: True,\n",
       "   4940: True,\n",
       "   4941: True,\n",
       "   4942: True,\n",
       "   4943: True,\n",
       "   4944: True,\n",
       "   4945: True,\n",
       "   4946: True,\n",
       "   4947: True,\n",
       "   4948: True,\n",
       "   4949: True,\n",
       "   4950: True,\n",
       "   4951: True,\n",
       "   4953: True,\n",
       "   4954: True,\n",
       "   4955: True,\n",
       "   4956: True,\n",
       "   4957: True,\n",
       "   4959: True,\n",
       "   4960: True,\n",
       "   4961: True,\n",
       "   4962: True,\n",
       "   4963: True,\n",
       "   4964: True,\n",
       "   4965: True,\n",
       "   4966: True,\n",
       "   4967: True,\n",
       "   4968: True,\n",
       "   4969: True,\n",
       "   4970: True,\n",
       "   4971: True,\n",
       "   4972: True,\n",
       "   4973: True,\n",
       "   4974: True,\n",
       "   4975: True,\n",
       "   4978: True,\n",
       "   4979: True,\n",
       "   4980: True,\n",
       "   4981: True,\n",
       "   5362: True,\n",
       "   5364: False,\n",
       "   5380: False,\n",
       "   5381: True,\n",
       "   5385: False,\n",
       "   5392: True,\n",
       "   5394: True,\n",
       "   5395: True,\n",
       "   5397: True},\n",
       "  0.9079861111111112],\n",
       " 'e-po': [{792: True, 2839: True, 5411: True}, 1.0],\n",
       " 'ku-wa-ta': [{795: True}, 1.0],\n",
       " 'a-nu-wi-ko': [{795: True, 2677: True}, 1.0],\n",
       " 'ko-ro-sa-no': [{796: True}, 1.0],\n",
       " 'po-ro-tu-qo-no': [{797: True}, 1.0],\n",
       " 'i-mo-ro-ne-u': [{798: True, 5076: True}, 1.0],\n",
       " 'mi-ka-ta': [{800: True,\n",
       "   4700: True,\n",
       "   4719: True,\n",
       "   4883: True,\n",
       "   4951: True,\n",
       "   4984: True},\n",
       "  1.0],\n",
       " 'pe-ka-wo': [{802: True, 939: False}, 0.5],\n",
       " 'mi-ka-to': [{803: True}, 1.0],\n",
       " 'wa-na-ka': [{806: True,\n",
       "   888: False,\n",
       "   2470: False,\n",
       "   5176: True,\n",
       "   5235: False,\n",
       "   5350: True},\n",
       "  0.5],\n",
       " 'po-ru-ka-to': [{807: True}, 1.0],\n",
       " 'a-ka-to-wa': [{808: True}, 1.0],\n",
       " 'ka-no': [{809: False, 5073: False}, 0.0],\n",
       " 'e-wi-ta-jo': [{810: True}, 1.0],\n",
       " 'a-ki-re-u': [{811: True}, 1.0],\n",
       " 'za-ki-ri-jo': [{812: True}, 1.0],\n",
       " 'qe-wa': [{813: False}, 0.0],\n",
       " 'da-o-ta': [{814: True}, 1.0],\n",
       " 'o-ko-te': [{815: True, 890: True, 1022: False}, 0.6666666666666666],\n",
       " 'e-u-o-mo': [{816: False}, 0.0],\n",
       " 'e-pi-da-o': [{817: True}, 1.0],\n",
       " 'ri-to-wo': [{819: False}, 0.0],\n",
       " 'a-ka-i-jo': [{820: True,\n",
       "   887: True,\n",
       "   3217: True,\n",
       "   3549: True,\n",
       "   3901: False,\n",
       "   5078: True},\n",
       "  0.8333333333333334],\n",
       " 'wo-ra-ke-re': [{821: False}, 0.0],\n",
       " 'a-no-qo-ta': [{822: True,\n",
       "   1239: False,\n",
       "   2555: False,\n",
       "   2650: True,\n",
       "   2898: True,\n",
       "   2972: True,\n",
       "   3525: True},\n",
       "  0.7142857142857143],\n",
       " 'ai': [{822: False,\n",
       "   2015: False,\n",
       "   2039: False,\n",
       "   2674: False,\n",
       "   2781: True,\n",
       "   3305: False,\n",
       "   3714: False,\n",
       "   3776: False,\n",
       "   4995: False},\n",
       "  0.1111111111111111],\n",
       " 'je': [{822: False, 2835: False, 5751: True}, 0.3333333333333333],\n",
       " 'pe-ri-to': [{823: True}, 1.0],\n",
       " 'a-pi-da-ta': [{824: True}, 1.0],\n",
       " 'po-ru-te-we': [{825: True}, 1.0],\n",
       " 'wa-ke-i-jo': [{826: False, 833: True}, 0.5],\n",
       " 'di': [{826: False,\n",
       "   949: True,\n",
       "   1016: False,\n",
       "   1111: False,\n",
       "   2498: True,\n",
       "   2554: True,\n",
       "   2556: True,\n",
       "   2558: True,\n",
       "   2559: True,\n",
       "   2560: False,\n",
       "   2562: True,\n",
       "   2563: True,\n",
       "   2564: True,\n",
       "   2566: True,\n",
       "   2567: True,\n",
       "   2586: True,\n",
       "   2591: True,\n",
       "   2593: True,\n",
       "   2594: True,\n",
       "   2595: False,\n",
       "   2613: False,\n",
       "   2622: False,\n",
       "   2627: True,\n",
       "   2652: True,\n",
       "   2658: False,\n",
       "   2661: False,\n",
       "   2662: True,\n",
       "   2757: True,\n",
       "   4723: False,\n",
       "   5063: False},\n",
       "  0.7380952380952381],\n",
       " 'sa-u-ko': [{827: True}, 1.0],\n",
       " 'pi-ra-qo': [{828: True}, 1.0],\n",
       " 'pe-re-wa-ta': [{829: True, 4703: True}, 1.0],\n",
       " 'a-ro-wo-ta': [{830: True}, 1.0],\n",
       " 'a-ne-te-wa': [{831: True}, 1.0],\n",
       " 'te-re-ja-wo': [{832: True}, 1.0],\n",
       " 'a-pi-re-jo': [{835: True}, 1.0],\n",
       " 'pu-mo-ne': [{836: False}, 0.0],\n",
       " 'ta-ra-sa-ta': [{837: True}, 1.0],\n",
       " 'a-ke-ra-no': [{839: True}, 1.0],\n",
       " 'pa-pa-ro': [{840: True, 975: True, 1095: False, 4773: True, 4776: True},\n",
       "  0.8],\n",
       " 'a-re-jo': [{841: False}, 0.0],\n",
       " 'pa-sa-ko-me-no': [{842: True}, 1.0],\n",
       " 'e-re-pa-i-ro': [{843: True}, 1.0],\n",
       " 'a-ne-u-da': [{844: True}, 1.0],\n",
       " 'di-wo-a-ne': [{845: False}, 0.0],\n",
       " 're-ka-se-ra-wo': [{846: False}, 0.0],\n",
       " 'du-pi-jo': [{847: True}, 1.0],\n",
       " 'wo-ro-to-qo': [{849: True}, 1.0],\n",
       " 'me-ta-ri-ko-wo': [{850: True}, 1.0],\n",
       " 'di-wi-ja-wo': [{851: True, 5186: True}, 1.0],\n",
       " 'e-we-de-u': [{854: True, 4138: True}, 1.0],\n",
       " 'a-ke-ra-wo': [{855: True, 4770: True, 5393: False}, 0.6666666666666666],\n",
       " 'a-ta-no': [{857: True, 2670: False, 2681: True}, 0.6666666666666666],\n",
       " 'ku-ru-ka': [{858: True}, 1.0],\n",
       " 'ka-na-a-po': [{861: True}, 1.0],\n",
       " 'ne-to': [{862: False, 2396: False}, 0.0],\n",
       " 'wa-si-jo': [{863: False, 2434: False}, 0.0],\n",
       " 'ra-wo': [{865: False, 3608: False, 4275: False, 4457: False}, 0.0],\n",
       " 'ma-ra': [{867: False, 1045: True, 4758: True}, 0.75],\n",
       " 'ka-sa-to': [{868: True, 2336: True, 2347: True, 4700: True, 5058: True},\n",
       "  1.0],\n",
       " 'ai-wa-ta': [{870: True}, 1.0],\n",
       " 'a-re-i': [{871: False}, 0.0],\n",
       " 'pa-ra-ne': [{872: True}, 1.0],\n",
       " 'a-pi': [{873: False, 1264: False, 4123: True, 5354: True}, 0.5],\n",
       " 'ra-ko': [{874: False, 1079: False, 2326: False, 3064: False}, 0.0],\n",
       " 'te-ta': [{875: False}, 0.0],\n",
       " 'nwa-re': [{876: False}, 0.0],\n",
       " 'ne-u': [{877: False, 1084: False, 2967: False, 5070: False}, 0.0],\n",
       " 'we-wa-ta': [{878: False}, 0.0],\n",
       " 're-wa': [{879: False, 1494: False}, 0.0],\n",
       " 'i-ta': [{880: False, 1060: False, 1266: False}, 0.0],\n",
       " 'qe-na': [{884: False}, 0.0],\n",
       " 'ka-di-ja': [{885: False}, 0.0],\n",
       " 'qe-re': [{886: False, 4717: False}, 0.0],\n",
       " 'we-ro': [{887: False, 2677: False, 2692: False}, 0.0],\n",
       " 'da-te': [{889: False}, 0.0],\n",
       " 'pe-qe-u': [{890: True, 3901: False, 5068: True}, 0.6666666666666666],\n",
       " 'ke-phu-je-u': [{891: True}, 1.0],\n",
       " 'a-ta-ti-nu': [{892: True, 3901: True}, 1.0],\n",
       " 'si-wa': [{892: False}, 0.0],\n",
       " 'ka-ra-na-ta': [{895: True, 1075: False}, 0.5],\n",
       " 'ra-wo-ti-jo': [{896: True, 2830: True}, 1.0],\n",
       " 'ti-ma': [{898: True}, 1.0],\n",
       " 'pa-ta': [{901: False, 2337: True, 2784: False, 4748: True}, 0.5],\n",
       " '*185': [{903: True}, 1.0],\n",
       " 'ma-se': [{903: True, 986: False}, 0.5],\n",
       " '*257': [{905: True}, 1.0],\n",
       " 'ke-ni-qa': [{906: True}, 1.0],\n",
       " 'a-sa-mi-to': [{906: True}, 1.0],\n",
       " 'e-sa-re-wi-ja': [{908: False,\n",
       "   4730: True,\n",
       "   5104: True,\n",
       "   5264: False,\n",
       "   5411: True},\n",
       "  0.6],\n",
       " '*260': [{908: True, 2175: False}, 0.5],\n",
       " 'tu-ri-jo': [{909: True, 5068: True}, 1.0],\n",
       " '*22-ja-ro': [{911: True}, 1.0],\n",
       " 'qa-ka-na-pi': [{912: False}, 0.0],\n",
       " 'si-ja-phu': [{913: False}, 0.0],\n",
       " 'pa-pu': [{914: False}, 0.0],\n",
       " 'pa-ra-ku': [{915: False, 4752: True}, 0.5],\n",
       " 'wa-ta': [{918: False, 5546: False}, 0.0],\n",
       " 'o-pe-re': [{919: False}, 0.0],\n",
       " 'ra-i-ja': [{920: False}, 0.0],\n",
       " 'na-pu': [{921: False}, 0.0],\n",
       " 'su-ra': [{922: False}, 0.0],\n",
       " 'ka-ta-ro': [{922: True, 2404: True}, 1.0],\n",
       " 'ku-ri-sa-to': [{922: True, 4695: True, 4747: True, 5069: True}, 1.0],\n",
       " 'ra-wi': [{922: False, 1034: False}, 0.0],\n",
       " '*22-je-mi': [{930: False}, 0.0],\n",
       " 'pa-ta-re': [{935: False, 1432: False}, 0.0],\n",
       " 'o-ka': [{936: False,\n",
       "   4718: True,\n",
       "   4724: True,\n",
       "   4725: True,\n",
       "   4726: True,\n",
       "   4727: True,\n",
       "   4961: False,\n",
       "   4967: True,\n",
       "   4973: True,\n",
       "   5372: True},\n",
       "  0.8666666666666667],\n",
       " 'ku-ka-ra-re': [{938: False}, 0.0],\n",
       " 'a-ka-me-ne': [{940: False}, 0.0],\n",
       " 'ka-wi': [{942: False, 2754: False}, 0.0],\n",
       " 'ru-po-to': [{943: False}, 0.0],\n",
       " 'ki-je-u': [{944: True}, 1.0],\n",
       " 'a-pi-ja-re': [{944: True, 1364: False}, 0.5],\n",
       " 'di-wi-je-ja': [{945: True}, 1.0],\n",
       " 'di-wi-ja': [{945: True, 4720: True}, 1.0],\n",
       " 'ki-si-wi-je-ja': [{946: True}, 1.0],\n",
       " 'pe-rya-wo': [{946: False}, 0.0],\n",
       " '*47-da': [{948: False}, 0.0],\n",
       " 'mi-ko': [{948: False}, 0.0],\n",
       " 'i-ja': [{949: True, 2651: False}, 0.5],\n",
       " 'i-*65-ke-o': [{949: True}, 1.0],\n",
       " 'ma-ki': [{950: True}, 1.0],\n",
       " 'a-ku-di-ri-jo': [{951: True, 3149: True}, 1.0],\n",
       " 'a-ku-di-ri': [{952: False}, 0.0],\n",
       " 'o-ke-te': [{953: False}, 0.0],\n",
       " 'a-e': [{953: False, 5288: False}, 0.0],\n",
       " 'e-ro-e-o': [{953: True}, 1.0],\n",
       " 'ne-ri-jo': [{954: False}, 0.0],\n",
       " 'qe-re-wa': [{955: True, 981: False}, 0.5],\n",
       " 'wo-no-da': [{956: False, 1078: False}, 0.0],\n",
       " 'da-pu-ri-to': [{957: False}, 0.0],\n",
       " 'pa-ze-qe': [{957: True}, 1.0],\n",
       " 'ke-wo': [{957: False}, 0.0],\n",
       " '*47-ta-qo': [{957: False}, 0.0],\n",
       " '*47': [{957: False}, 0.0],\n",
       " 'pu-ra-ko': [{958: False}, 0.0],\n",
       " 'pu-ko-ro': [{959: False, 4700: True, 4988: True, 5063: False}, 0.5],\n",
       " 'i-ke-se': [{960: True}, 1.0],\n",
       " 'ku-ta-i-to': [{961: True, 2776: True}, 1.0],\n",
       " 'a-pi-do-ro': [{961: False}, 0.0],\n",
       " 'ma-u-do': [{961: False}, 0.0],\n",
       " 'i-ja-wo-ne': [{961: False, 2701: False}, 0.0],\n",
       " 'ro-a': [{962: True}, 1.0],\n",
       " 'OLE': [{962: False,\n",
       "   1018: False,\n",
       "   1226: True,\n",
       "   2314: True,\n",
       "   3927: True,\n",
       "   3932: True,\n",
       "   3934: True,\n",
       "   3935: True,\n",
       "   3936: True,\n",
       "   3937: True,\n",
       "   3938: True,\n",
       "   3939: True,\n",
       "   3940: True,\n",
       "   3941: True,\n",
       "   3942: True,\n",
       "   3943: True,\n",
       "   3944: True,\n",
       "   3945: True,\n",
       "   3946: True,\n",
       "   3947: True,\n",
       "   3948: True,\n",
       "   3949: True,\n",
       "   3950: True,\n",
       "   3951: True,\n",
       "   3952: True,\n",
       "   3955: True,\n",
       "   3956: True,\n",
       "   3957: False,\n",
       "   3958: True,\n",
       "   3961: True,\n",
       "   3962: False,\n",
       "   3963: False,\n",
       "   3964: True,\n",
       "   3965: True,\n",
       "   3968: True,\n",
       "   3969: True,\n",
       "   3970: True,\n",
       "   3971: False,\n",
       "   3972: True,\n",
       "   3973: True,\n",
       "   3975: True,\n",
       "   3984: True,\n",
       "   3988: True,\n",
       "   3989: True,\n",
       "   3990: True,\n",
       "   3992: True,\n",
       "   3993: True,\n",
       "   3994: True,\n",
       "   3995: True,\n",
       "   3997: True,\n",
       "   3998: True,\n",
       "   3999: False,\n",
       "   4001: True,\n",
       "   4007: True,\n",
       "   4008: True,\n",
       "   4009: True,\n",
       "   4012: True,\n",
       "   4014: False,\n",
       "   4015: True,\n",
       "   4016: True,\n",
       "   4019: False,\n",
       "   4024: True,\n",
       "   4026: False,\n",
       "   4029: False,\n",
       "   4031: True,\n",
       "   4032: False,\n",
       "   4033: True,\n",
       "   4035: False,\n",
       "   4036: True,\n",
       "   4037: True,\n",
       "   4039: False,\n",
       "   4040: False,\n",
       "   4042: True,\n",
       "   4043: True,\n",
       "   4049: False,\n",
       "   4051: False,\n",
       "   4052: True,\n",
       "   4059: False,\n",
       "   4076: False,\n",
       "   4088: True,\n",
       "   4089: True,\n",
       "   4090: True,\n",
       "   4091: True,\n",
       "   4092: True,\n",
       "   4093: True,\n",
       "   4095: True,\n",
       "   4098: True,\n",
       "   4099: True,\n",
       "   4100: True,\n",
       "   4101: True,\n",
       "   4102: True,\n",
       "   4104: True,\n",
       "   4105: True,\n",
       "   4106: True,\n",
       "   4109: True,\n",
       "   4110: True,\n",
       "   4111: True,\n",
       "   4112: True,\n",
       "   4116: True,\n",
       "   4117: True,\n",
       "   4118: True,\n",
       "   4500: True,\n",
       "   4509: True,\n",
       "   4510: True,\n",
       "   5002: False,\n",
       "   5004: True,\n",
       "   5009: True,\n",
       "   5014: True,\n",
       "   5015: True,\n",
       "   5032: False,\n",
       "   5039: True,\n",
       "   5041: True,\n",
       "   5042: True,\n",
       "   5043: False,\n",
       "   5044: True,\n",
       "   5045: True,\n",
       "   5047: True,\n",
       "   5049: True,\n",
       "   5050: True},\n",
       "  0.8466666666666667],\n",
       " 'tu-ni-ja': [{963: True,\n",
       "   1390: True,\n",
       "   1424: True,\n",
       "   2563: False,\n",
       "   2652: True,\n",
       "   2882: False,\n",
       "   2965: False,\n",
       "   3002: True,\n",
       "   3013: True,\n",
       "   3083: True,\n",
       "   3084: True,\n",
       "   3085: True,\n",
       "   3116: False,\n",
       "   3172: True,\n",
       "   3203: True,\n",
       "   3279: True,\n",
       "   3291: True,\n",
       "   3442: True,\n",
       "   3577: True,\n",
       "   3632: False,\n",
       "   3677: True,\n",
       "   3690: True,\n",
       "   3815: True,\n",
       "   4395: False,\n",
       "   4456: True,\n",
       "   4458: True,\n",
       "   4460: True},\n",
       "  0.7777777777777778],\n",
       " 'ri-u-no': [{963: True}, 1.0],\n",
       " 'ra-wa-ke-si': [{964: False}, 0.0],\n",
       " 'ma-ke-ra-mo': [{964: False}, 0.0],\n",
       " 'i-da-ra-ta': [{964: True}, 1.0],\n",
       " 'a-qi-ra': [{965: True, 985: False}, 0.5],\n",
       " 'e-u-ru-da-mo': [{965: True}, 1.0],\n",
       " 'so-tu-wo-no': [{965: False}, 0.0],\n",
       " 'do-ri-wo': [{966: True}, 1.0],\n",
       " 'ku-do-ni-jo': [{968: False, 4205: False}, 0.0],\n",
       " 'a-re-u-ke': [{969: False}, 0.0],\n",
       " 'ko-ma-ra': [{970: False}, 0.0],\n",
       " 'a-re-se': [{971: False}, 0.0],\n",
       " 'wo-no-qi': [{972: False}, 0.0],\n",
       " 'du-ni-jo': [{974: False,\n",
       "   2677: True,\n",
       "   4141: True,\n",
       "   4683: True,\n",
       "   4705: True,\n",
       "   4791: False,\n",
       "   4830: True,\n",
       "   4853: True,\n",
       "   4954: True,\n",
       "   4985: True,\n",
       "   5376: True},\n",
       "  0.8333333333333334],\n",
       " 'ma-ti-ri': [{976: False}, 0.0],\n",
       " 'na-u-si-ke-re': [{977: False}, 0.0],\n",
       " 'te-u-to-ri-*65': [{979: False}, 0.0],\n",
       " 'e-ru-ti-ri-jo': [{982: False}, 0.0],\n",
       " 'ta-ra-i': [{983: True}, 1.0],\n",
       " 'e-ko-so-wo-ko': [{984: False}, 0.0],\n",
       " 'e-na-i-jo': [{987: False}, 0.0],\n",
       " 'e-ri-ta-ri-jo': [{988: False}, 0.0],\n",
       " 'ra-ke-re-we': [{989: False}, 0.0],\n",
       " 'a-ke': [{990: False, 2348: False, 2374: True, 5357: True},\n",
       "  0.6666666666666666],\n",
       " 'do-qi': [{991: False}, 0.0],\n",
       " 'po-ki': [{992: False, 5529: False}, 0.0],\n",
       " 'pa-no': [{993: False, 1222: False, 2708: False}, 0.0],\n",
       " 'se-ke-ru-pa-ko': [{994: False}, 0.0],\n",
       " 'e-we-wa': [{995: False}, 0.0],\n",
       " 'ru-ki-ti-ja': [{996: False, 4464: True}, 0.5],\n",
       " 'e-pi-ta': [{997: False}, 0.0],\n",
       " 'te-me-u': [{998: False}, 0.0],\n",
       " 'a-pa-te': [{999: False}, 0.0],\n",
       " 'wo-ni-jo': [{1000: False}, 0.0],\n",
       " 'a-ri-we-we': [{1001: False, 3901: False}, 0.0],\n",
       " 'a-ta-wo-ne': [{1002: False}, 0.0],\n",
       " 'we-wa': [{1003: False}, 0.0],\n",
       " 'no-da-ma': [{1004: False}, 0.0],\n",
       " 'e-ri-ra-i': [{1005: False}, 0.0],\n",
       " 'pa-*34-so': [{1006: False}, 0.0],\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq_data = {}\n",
    "for seq, docs_list in sequence_data.items():\n",
    "    complete_count = 0\n",
    "    app_count = 0\n",
    "    docs_dict = {}\n",
    "    for seq_info in docs_list:\n",
    "        complete = seq_info[1]\n",
    "        docs_dict[seq_info[0]] = complete\n",
    "        if complete:\n",
    "            complete_count += 1\n",
    "        app_count += 1\n",
    "    new_seq_data[seq] = [docs_dict, complete_count / app_count]\n",
    "    \n",
    "sequence_data = new_seq_data\n",
    "sequence_data # Dictionary{seq: Tuple[Dictionary{doc_id: complete}, complete_count]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{4955: False}, 0.5],\n",
       " [{3402: False}, 0.0],\n",
       " [{4464: True}, 1.0],\n",
       " [{820: True, 887: True, 3217: True, 3549: True, 3901: False, 5078: True},\n",
       "  0.8333333333333334])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data['a-de-te'], sequence_data['*18-jo'], sequence_data['*56-po-so'], sequence_data['a-ka-i-jo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstraction for Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Completeness(Enum):\n",
    "    INCOMPLETE = 0\n",
    "    MOSTLY_INCOMPLETE = 1\n",
    "    UNCERTAIN = 2\n",
    "    MOSTLY_COMPLETE = 3\n",
    "    COMPLETE = 4\n",
    "\n",
    "def classify_completeness(p):\n",
    "    if p <= 0.0: #0.0\n",
    "        return Completeness.INCOMPLETE\n",
    "    if p <= 1/3: #0.0 -> 0.33\n",
    "        return Completeness.MOSTLY_INCOMPLETE\n",
    "    if p < 2/3: #0.33 -> 0.67\n",
    "        return Completeness.UNCERTAIN\n",
    "    if p < 1.0: #0.67 -> 1.0\n",
    "        return Completeness.MOSTLY_COMPLETE\n",
    "    return Completeness.COMPLETE #1.0\n",
    " \n",
    "    \n",
    "@dataclass(frozen=True, order=True)\n",
    "class WordWithCompleteness:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "    completeness: Completeness\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordWithCompleteness(lang='transliterated_linear_b', form='*18-jo', idx=0, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*18-to-no', idx=1, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*22-ja-ro', idx=2, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*22-je-mi', idx=3, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*22-jo', idx=4, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*22-ri-ta-ro', idx=5, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ka', idx=6, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ka-te-re', idx=7, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ke-ja', idx=8, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ke-te-si', idx=9, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ke-u', idx=10, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ki-no-o', idx=11, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-ra-ka-te-ra', idx=12, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-te', idx=13, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-to', idx=14, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-to-pi', idx=15, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34-zo', idx=16, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-da', idx=17, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-da-de', idx=18, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-de', idx=19, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-ku-to-de', idx=20, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-so', idx=21, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-so-de', idx=22, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-ta-qo', idx=23, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47-ti-jo', idx=24, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*49-sa-ro', idx=25, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*49-so', idx=26, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*49-wo', idx=27, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-du-nu-ka', idx=28, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-i-ti', idx=29, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-i-ti-je', idx=30, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-jo', idx=31, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we', idx=32, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we-e', idx=33, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we-i', idx=34, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we-i-ja-qe', idx=35, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we-i-jo', idx=36, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ko-we-qe', idx=37, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-na-ro', idx=38, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ni-sa-ta', idx=39, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-po-so', idx=40, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ra-ku-ja', idx=41, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ri-to', idx=42, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ryo', idx=43, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-so-jo', idx=44, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-ti', idx=45, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-wo', idx=46, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56-za', idx=47, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*63-o-wa', idx=48, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*64-jo', idx=49, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*65-no', idx=50, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*82-de', idx=51, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*83-re-jo-de', idx=52, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*83-re-te', idx=53, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a', idx=54, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*34-ka', idx=55, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*34-to', idx=56, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*47', idx=57, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*47-wi', idx=58, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*56-da-ro', idx=59, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*56-no', idx=60, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*64-ja', idx=61, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*64-ja-o', idx=62, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*64-jo', idx=63, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*65-ma', idx=64, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*65-ma-na-ke', idx=65, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*65-na', idx=66, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-*79', idx=67, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ma-jo', idx=68, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ma-o', idx=69, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ma-o-jo', idx=70, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ra-ko', idx=71, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ra-ro', idx=72, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ra-te-ja', idx=73, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=74, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-wa-si-jo', idx=75, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-da-wo-ne', idx=76, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-de-me-we', idx=77, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-de-rya', idx=78, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-de-te', idx=79, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-de-te-re', idx=80, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-*22-sa', idx=81, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-je-wo', idx=82, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-nwa-ta', idx=83, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-ri-ja-pi', idx=84, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-ri-ja-pi-qe', idx=85, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-ri-ja-te-qe', idx=86, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-di-ri-jo', idx=87, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-do-we', idx=88, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-du', idx=89, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-du-po-to', idx=90, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-du-ru-po-to', idx=91, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e', idx=92, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-da-do-ro', idx=93, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-ri-qe', idx=94, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-ri-qo', idx=95, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-ri-qo-ta', idx=96, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-se-wa', idx=97, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-e-ti-to', idx=98, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-i-qe-u', idx=99, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-i-qe-we', idx=100, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-i-qe-wo', idx=101, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ja', idx=102, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ja-me', idx=103, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ja-me-na', idx=104, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ja-me-no', idx=105, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-jo', idx=106, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka', idx=107, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-de', idx=108, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-i-jo', idx=109, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ma-jo', idx=110, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ma-wo', idx=111, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-me-ne', idx=112, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-mu', idx=113, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-na-jo', idx=114, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ra-no', idx=115, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-re-u', idx=116, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-re-u-te', idx=117, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-sa-no', idx=118, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-sa-no-qe', idx=119, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-si-jo-ne', idx=120, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ta-jo', idx=121, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ta-jo-jo', idx=122, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-ta-ra-te-so-de', idx=123, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-to', idx=124, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-to-wa', idx=125, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-wi-ja-de', idx=126, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-wo', idx=127, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-wo-ne', idx=128, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke', idx=129, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-e', idx=130, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-e-to', idx=131, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ha', idx=132, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-i', idx=133, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-o', idx=134, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-o-jo', idx=135, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-qe', idx=136, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ra-no', idx=137, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ra-wo', idx=138, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re', idx=139, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-mo', idx=140, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-mo-no', idx=141, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-se', idx=142, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-u', idx=143, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-u-te', idx=144, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-wa', idx=145, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-wa-de', idx=146, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-wa-qe', idx=147, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-we', idx=148, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-wi-ja', idx=149, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-wi-jo', idx=150, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ro', idx=151, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-rya-te', idx=152, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-si', idx=153, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-si-ja', idx=154, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ta', idx=155, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-te', idx=156, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-te-re', idx=157, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-jo', idx=158, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-ri', idx=159, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-ri-ja', idx=160, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-ri-ja-i', idx=161, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-rya', idx=162, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-rya-o', idx=163, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-to-ro', idx=164, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-u', idx=165, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-wa-ta', idx=166, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-wa-to', idx=167, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-wo', idx=168, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki', idx=169, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-re-u', idx=170, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-re-we', idx=171, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ri', idx=172, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ri-ja', idx=173, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ri-jo', idx=174, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ti-to', idx=175, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-to', idx=176, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-to-jo', idx=177, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-wa-ta', idx=178, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-wo-ni-jo', idx=179, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko', idx=180, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-i-da', idx=181, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-mo-ni-jo', idx=182, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ra', idx=183, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ra-ja', idx=184, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ra-jo', idx=185, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro', idx=186, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-da-mo', idx=187, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-da-mo-jo', idx=188, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-qo-ro', idx=189, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-ta', idx=190, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-we', idx=191, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-we-e', idx=192, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-we-i', idx=193, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-so-ne', idx=194, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-so-ne-qe', idx=195, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-so-ta', idx=196, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-so-ta-o', idx=197, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-te-u', idx=198, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-to', idx=199, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-to-no', idx=200, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-to-wo', idx=201, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-we-i-ja', idx=202, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-wo', idx=203, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-di-ri', idx=204, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-di-ri-jo', idx=205, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-na-i', idx=206, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-ri-jo', idx=207, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-ro', idx=208, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-tu-ru-wo', idx=209, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-wo', idx=210, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma', idx=211, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-ko-to', idx=212, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-no', idx=213, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-ru-ta', idx=214, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-ru-ta-o', idx=215, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-ti', idx=216, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-to-wo', idx=217, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ma-tu-na', idx=218, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me', idx=219, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-a', idx=220, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-ja', idx=221, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-ja-si', idx=222, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-ja-to', idx=223, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-no', idx=224, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-me-to', idx=225, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ja', idx=226, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ke-te-to', idx=227, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ni-si', idx=228, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ni-si-ja', idx=229, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ni-si-jo', idx=230, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ni-so', idx=231, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ni-so-de', idx=232, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-nu-wa-ta', idx=233, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-nwa', idx=234, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-si-ja', idx=235, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-to-no', idx=236, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo', idx=237, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-i-je-to', idx=238, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-ra-ma', idx=239, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-si', idx=240, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-ta', idx=241, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-ta-jo', idx=242, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-te', idx=243, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-te-jo-na-de', idx=244, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-te-re', idx=245, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-te-wi-ja', idx=246, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mo-te-wo', idx=247, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mu', idx=248, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mu-ta-wo', idx=249, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mu-ta-wo-no', idx=250, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-mu-ta-wo-qe', idx=251, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-*82', idx=252, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-i-ta', idx=253, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-ka', idx=254, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-ka-te', idx=255, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-ke-e', idx=256, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-ki-ti', idx=257, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-mo-ta', idx=258, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-mo-to', idx=259, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-pe-we', idx=260, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-pu-ke', idx=261, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-qo-ta', idx=262, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-re-u', idx=263, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-ta', idx=264, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-te-u', idx=265, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-na-to', idx=266, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne', idx=267, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-ha', idx=268, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-mo', idx=269, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-mo-i-je-re-ja', idx=270, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-o', idx=271, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-ra-to', idx=272, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-ta-de', idx=273, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-te-wa', idx=274, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-u-da', idx=275, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ne-u-te', idx=276, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ni-ja', idx=277, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ni-ja-e-e-ro-pa-jo-qe-ro-sa', idx=278, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ni-ja-pi', idx=279, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ni-ja-to', idx=280, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ni-o-ko', idx=281, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no', idx=282, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-ke-wa', idx=283, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-ke-we', idx=284, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-me-de', idx=285, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-no', idx=286, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-po', idx=287, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-qa-si-ja', idx=288, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-qo', idx=289, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-qo-ta', idx=290, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-qo-ta-o', idx=291, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-ra-ta', idx=292, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-ta', idx=293, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-we', idx=294, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-wo-to', idx=295, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-ze-we', idx=296, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-no-zo-jo', idx=297, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu', idx=298, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-ko', idx=299, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-mo', idx=300, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-to', idx=301, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-to-jo', idx=302, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-wa', idx=303, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-wa-to', idx=304, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-nu-wi-ko', idx=305, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-o-ri-jo', idx=306, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-o-ri-me-ne', idx=307, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-o-ze-jo', idx=308, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa', idx=309, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-i-ti-jo', idx=310, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-je-u', idx=311, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ni-jo', idx=312, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-re-u-pi', idx=313, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ri-ka-na-we-ja', idx=314, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-sa', idx=315, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-sa-ki-jo', idx=316, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-si-jo-jo', idx=317, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ta', idx=318, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ta-wa', idx=319, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ta-wa-ja', idx=320, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ta-wa-jo', idx=321, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-te', idx=322, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-to', idx=323, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-to-re', idx=324, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-u-ro', idx=325, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-a-sa', idx=326, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-do-ke', idx=327, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-e-ke', idx=328, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-e-si', idx=329, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-i-ja', idx=330, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-i-si', idx=331, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-ke-e', idx=332, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-ke-i-jo', idx=333, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-ne-wo', idx=334, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-o', idx=335, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-o-te', idx=336, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-re', idx=337, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-ri-ta-wo', idx=338, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-te-me-ne', idx=339, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-te-u', idx=340, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-ti-rya', idx=341, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-we-de', idx=342, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-phu-de', idx=343, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-phu-ja', idx=344, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-phu-ka', idx=345, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-phu-ka-ne', idx=346, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-phu-we', idx=347, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi', idx=348, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-da-ta', idx=349, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-do-ra', idx=350, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-do-ro', idx=351, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-e-ke', idx=352, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-e-ra', idx=353, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ha-ro', idx=354, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ja-ko-ro-jo', idx=355, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ja-re', idx=356, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-je', idx=357, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-je-ta', idx=358, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-jo', idx=359, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-jo-to', idx=360, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ka-ra-do-jo', idx=361, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ke-ne-a', idx=362, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-me-de', idx=363, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-me-de-o', idx=364, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-no-e-wi-jo', idx=365, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-o-to', idx=366, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-po-re-we', idx=367, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-i-ta', idx=368, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-o', idx=369, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-ro', idx=370, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-ro-i', idx=371, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-ta', idx=372, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-ta-o', idx=373, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-qo-to', idx=374, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ra-wo', idx=375, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-re-jo', idx=376, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-re-we', idx=377, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-te', idx=378, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-te-ja', idx=379, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-te-wa', idx=380, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-wa-to', idx=381, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-po-ne-we', idx=382, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-po-re-we', idx=383, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-po-te', idx=384, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-po-te-ro-te', idx=385, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu', idx=386, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-da-se-we', idx=387, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-do', idx=388, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-do-ke', idx=389, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-do-si', idx=390, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-do-so-mo', idx=391, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ka', idx=392, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ke', idx=393, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ki', idx=394, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ki-si', idx=395, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ko', idx=396, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ko-wo-ko', idx=397, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ne-we', idx=398, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qa-ro', idx=399, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qa-to', idx=400, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qe-mo', idx=401, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi', idx=402, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ja-i', idx=403, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ra', idx=404, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ro', idx=405, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ru', idx=406, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ta', idx=407, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ti-ta', idx=408, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-zo-we', idx=409, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-qo-ta', idx=410, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra', idx=411, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-da-jo', idx=412, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-i-jo', idx=413, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ka-jo', idx=414, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ka-te-ja', idx=415, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ka-te-ja-o', idx=416, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ko', idx=417, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ko-qe', idx=418, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-na-ro', idx=419, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ro-mo-te-me-na', idx=420, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ro-mo-te-me-no', idx=421, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ro-mo-to-me-na', idx=422, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ru-ja', idx=423, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ru-wo-a', idx=424, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ru-wo-ja', idx=425, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-si-jo', idx=426, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-te-ja-o', idx=427, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-wo', idx=428, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re', idx=429, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-i', idx=430, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-i-jo', idx=431, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ja', idx=432, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-jo', idx=433, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ka-sa-da-ra-ka', idx=434, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ka-tu-ru-wo', idx=435, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ke-se', idx=436, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ke-se-u', idx=437, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ki-si-to', idx=438, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ki-si-to-jo', idx=439, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ko-to-re', idx=440, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ku-tu-ru-no-ne', idx=441, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ku-tu-ru-wo', idx=442, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ku-tu-ru-wo-no', idx=443, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-me-ne', idx=444, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-pa-te', idx=445, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-pa-zo-o', idx=446, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-po-zo-o', idx=447, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ro', idx=448, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-sa-ni-e', idx=449, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-se', idx=450, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-se-si', idx=451, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-su-ti-jo', idx=452, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ta-to', idx=453, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ta-wo', idx=454, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-te-re-u', idx=455, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-to-to', idx=456, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-tya', idx=457, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-u-ke', idx=458, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-wo', idx=459, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-re-zo-me-ne', idx=460, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri', idx=461, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-ja-to', idx=462, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-ja-wo', idx=463, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-ja-wo-ne', idx=464, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-ke-u', idx=465, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-ko', idx=466, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-qa', idx=467, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-qo', idx=468, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-we-we', idx=469, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-wo', idx=470, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-wo-ne', idx=471, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro', idx=472, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-do-ro-o', idx=473, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-ja', idx=474, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-je-u', idx=475, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-ka', idx=476, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-mo', idx=477, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-mo-te-me-na', idx=478, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-pa', idx=479, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-po', idx=480, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-ta', idx=481, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-te', idx=482, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-u-ra', idx=483, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-we', idx=484, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-wo', idx=485, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-wo-ta', idx=486, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-za', idx=487, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-zo', idx=488, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ryo-a', idx=489, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ryo-e', idx=490, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ryo-jo', idx=491, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-ma-o', idx=492, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-ma-to', idx=493, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-mi', idx=494, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-mi-to', idx=495, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-pi', idx=496, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-ro', idx=497, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-ti-ja', idx=498, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-se-e', idx=499, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-si-ja-ti-ja', idx=500, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-si-to-po-qo', idx=501, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-si-wi-ja', idx=502, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-si-wi-jo', idx=503, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-so-na', idx=504, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-so-qi-je-ja', idx=505, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-so-qi-jo', idx=506, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-so-ta-o', idx=507, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ma-ne-u', idx=508, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ma-ne-we', idx=509, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ma-no-we', idx=510, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ma-ta', idx=511, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-na-po-ti-ni-ja', idx=512, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-no', idx=513, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-no-re', idx=514, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-no-ro', idx=515, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-o', idx=516, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-o-jo', idx=517, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-qe', idx=518, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ra', idx=519, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ra-qe', idx=520, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ra-si-jo', idx=521, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ro', idx=522, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ro-we', idx=523, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ti-nu', idx=524, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-tu-ro', idx=525, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-wo', idx=526, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-wo-ne', idx=527, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-wo-ne-jo', idx=528, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ze-u', idx=529, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-i-ja-ta', idx=530, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-jo', idx=531, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-mi-to', idx=532, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-mo', idx=533, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-ra-wo', idx=534, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-re-e-te-jo', idx=535, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-re-te-a', idx=536, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-re-wi-ja', idx=537, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-u-ke', idx=538, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-we', idx=539, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-we-i', idx=540, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-te-wo-jo', idx=541, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti', idx=542, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ja', idx=543, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ja-wo', idx=544, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-jo', idx=545, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ka', idx=546, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ke-ne-ja', idx=547, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-mi-te', idx=548, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-pa-mo', idx=549, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ri-ja', idx=550, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ro', idx=551, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ro-qe', idx=552, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ta', idx=553, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to', idx=554, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-me-ja', idx=555, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-mo', idx=556, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-mo-na', idx=557, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-po-qo', idx=558, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-po-qo-i', idx=559, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-ro-qo', idx=560, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-to-wo', idx=561, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-tu-ko', idx=562, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-tu-qo-ta', idx=563, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-tu-qo-te-ra-to', idx=564, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-u-qe', idx=565, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-u-ta-na', idx=566, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa', idx=567, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-ne-u', idx=568, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-ra-ka-na', idx=569, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-ra-ka-na-o', idx=570, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-si-ja', idx=571, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-so', idx=572, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-ta', idx=573, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wa-ti-ka-ra', idx=574, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-we-ke-se-u', idx=575, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-we-ke-se-we', idx=576, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-we-u-pi', idx=577, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wi-je-mo', idx=578, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wi-to-do-to', idx=579, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wo-i-jo', idx=580, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wo-ro', idx=581, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-wo-ti-jo', idx=582, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ze-o', idx=583, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ze-ta', idx=584, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ze-ti-ri-ja', idx=585, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-ze-to', idx=586, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='a-zo', idx=587, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai', idx=588, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ai-ta', idx=589, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-du-ro', idx=590, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-du-wo-na', idx=591, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ka-na-jo', idx=592, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ka-ra', idx=593, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ka-sa-ma', idx=594, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ke-ta', idx=595, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ke-u', idx=596, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ke-wa-ta', idx=597, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ke-wa-to', idx=598, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki', idx=599, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-de-ja', idx=600, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-e-we', idx=601, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-e-wo', idx=602, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-ha-ri-jo', idx=603, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-no-o', idx=604, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-pa-ta', idx=605, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-po', idx=606, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-po-de', idx=607, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-si-jo', idx=608, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-wa-ro', idx=609, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki-wa-to', idx=610, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ko-ta', idx=611, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ku-pi-ti-jo', idx=612, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ku-ta', idx=613, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-me-wa', idx=614, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-mi', idx=615, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-mi-re-we', idx=616, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ni-jo', idx=617, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-nu-me-no', idx=618, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-pu-ke-ne-ja', idx=619, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-rya', idx=620, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-sa', idx=621, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-se-wa', idx=622, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-se-we', idx=623, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-so-ni-jo', idx=624, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta', idx=625, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta-jo', idx=626, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta-re-u-si', idx=627, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta-ro', idx=628, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta-ro-we', idx=629, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-te', idx=630, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-te-re', idx=631, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ti-jo-qe', idx=632, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ti-jo-qo', idx=633, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ti-nu', idx=634, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-to', idx=635, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-tu-ti-ja', idx=636, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wa', idx=637, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wa-ja', idx=638, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wa-ta', idx=639, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wa-to', idx=640, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wi-jo', idx=641, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wo-di-jo-no', idx=642, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wo-re-u', idx=643, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wo-ro', idx=644, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-wo-ro-qe', idx=645, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-za', idx=646, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-ze', idx=647, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-zo-ro-qe', idx=648, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ai-zo-wo', idx=649, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au', idx=650, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ai-ta', idx=651, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-de-pi', idx=652, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-de-we-sa-qe', idx=653, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ja-to', idx=654, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ke-i-ja-te-we', idx=655, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ke-i-ja-te-wo', idx=656, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ke-wa', idx=657, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ri-jo', idx=658, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ri-mo-de', idx=659, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ro', idx=660, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-ta-mo', idx=661, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-te', idx=662, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-te-ra', idx=663, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to', idx=664, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to-*34-ta-ra', idx=665, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to-a', idx=666, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to-ai-ta', idx=667, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to-ha-ta', idx=668, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-to-jo', idx=669, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-tya', idx=670, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-u-te', idx=671, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='au-wi-ja-to', idx=672, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da', idx=673, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22', idx=674, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-ti', idx=675, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-ti-ja', idx=676, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-ti-jo', idx=677, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-to', idx=678, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-to-qe', idx=679, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*83', idx=680, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*83-ja', idx=681, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*83-ja-de', idx=682, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*83-ja-i', idx=683, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-*83-jo', idx=684, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-da-re-jo-de', idx=685, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i', idx=686, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-ja-ke-re-u', idx=687, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-mi-so', idx=688, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-pi-ta', idx=689, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-qo-ta', idx=690, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-ra', idx=691, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-ta-ra-ro', idx=692, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-wo-wo', idx=693, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-i-ze-to', idx=694, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ja-ro', idx=695, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-je', idx=696, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-je-we', idx=697, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-jo', idx=698, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ka-ja-pi', idx=699, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ka-sa-na-ta', idx=700, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ko-ro', idx=701, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ko-ro-i', idx=702, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ko-so', idx=703, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ma', idx=704, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ma-o-te', idx=705, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ma-so', idx=706, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ma-te', idx=707, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mi', idx=708, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mi-jo', idx=709, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mi-ni-ja', idx=710, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mi-ni-jo', idx=711, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mo', idx=712, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mo-de-mi', idx=713, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mo-ko', idx=714, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-mo-ko-ro', idx=715, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na', idx=716, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na-jo', idx=717, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na-ko', idx=718, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na-mo', idx=719, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na-pi', idx=720, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-na-ro', idx=721, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-nu-wa-a-ri', idx=722, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-nu-wo', idx=723, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-nwa', idx=724, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-nwa-re', idx=725, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-o-ta', idx=726, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-phu-ra-zo', idx=727, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-phu-ri-to-jo', idx=728, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-pu', idx=729, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-pu-ri-to', idx=730, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-qo-ta', idx=731, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ra', idx=732, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ra-ko', idx=733, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ra-mu-ro', idx=734, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ro', idx=735, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ru-*56', idx=736, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-so', idx=737, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-so-de', idx=738, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-so-mo', idx=739, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ta-ja-ro', idx=740, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-ta-ra-mo', idx=741, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-te', idx=742, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-te-ne-ja', idx=743, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-te-wa', idx=744, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-te-we-ja', idx=745, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-to-re-u', idx=746, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-to-ro', idx=747, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-u-da-ro', idx=748, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-u-ta-ro', idx=749, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-wa-no', idx=750, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-we-ro', idx=751, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-we-u-pi', idx=752, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-wi', idx=753, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-wi-ja', idx=754, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-wi-jo', idx=755, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-wo', idx=756, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='da-zo', idx=757, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de', idx=758, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-a', idx=759, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-a-ta', idx=760, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-de-me-na', idx=761, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-de-me-no', idx=762, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-di-ku-ja', idx=763, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-do-me-na', idx=764, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-do-wa-re-we', idx=765, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ka-sa-to', idx=766, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ke', idx=767, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ke-se-u', idx=768, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ki-si-wo', idx=769, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ki-si-wo-jo', idx=770, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ko-to', idx=771, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ko-to-jo', idx=772, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ku-tu-wo-ko', idx=773, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ma-si', idx=774, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-me-o-te', idx=775, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-mi-ni-ja', idx=776, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-mi-ni-jo', idx=777, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-mo-qe', idx=778, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ni-mo', idx=779, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ra', idx=780, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ra-wo', idx=781, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-re-u-ko', idx=782, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-ro', idx=783, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-sa-to', idx=784, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-so', idx=785, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-so-mo', idx=786, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u', idx=787, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-jo-i', idx=788, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ka-ri-jo', idx=789, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ke-ro', idx=790, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ki-jo', idx=791, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ki-jo-jo', idx=792, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ki-jo-qe', idx=793, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-wa-pi', idx=794, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-we-ra', idx=795, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-we-ro', idx=796, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-we-ro-ai-ko-ra', idx=797, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-we-ro-ai-ko-ra-i-ja', idx=798, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='de-wi-jo', idx=799, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di', idx=800, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-*18', idx=801, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-*65-pa-ta', idx=802, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-*79-nu', idx=803, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-da', idx=804, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-da-ka', idx=805, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-da-ka-re', idx=806, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-da-ma-o', idx=807, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-de', idx=808, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-de-ro', idx=809, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-do', idx=810, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-do-si', idx=811, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-du-me', idx=812, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-du-mo', idx=813, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-jo', idx=814, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ka', idx=815, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ka-ta-de', idx=816, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ka-ta-jo', idx=817, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ka-ta-ro', idx=818, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ki', idx=819, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ki-mo', idx=820, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ki-nu-wo', idx=821, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ko-na-re-ja', idx=822, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ko-na-ro', idx=823, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ko-to', idx=824, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-mi', idx=825, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-mi-zo', idx=826, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-no-zo', idx=827, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-nu-wa-ta', idx=828, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pa', idx=829, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pa-e', idx=830, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pa-te', idx=831, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pi-ja', idx=832, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pi-si-je-wi-jo', idx=833, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pi-si-jo', idx=834, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pi-si-jo-i', idx=835, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pte', idx=836, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pte-ra', idx=837, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pte-ra-po-ro', idx=838, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-pte-rai', idx=839, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-qa-ra', idx=840, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-qo', idx=841, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-qo-te', idx=842, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ra', idx=843, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ra-po-ro', idx=844, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ra-qo', idx=845, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ra-wo-no', idx=846, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ri-mi-jo', idx=847, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ri-wa-sa', idx=848, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ro', idx=849, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-so', idx=850, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-ta-ka-so', idx=851, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-u-ja', idx=852, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-u-ja-jo-qe', idx=853, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-u-jo', idx=854, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wa-jo', idx=855, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-we', idx=856, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-we-se-ja', idx=857, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-we-so', idx=858, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-ja', idx=859, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-ja-ta', idx=860, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-ja-wo', idx=861, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-je-ja', idx=862, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-je-u', idx=863, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-je-we', idx=864, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-jo', idx=865, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-jo-de', idx=866, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-jo-jo', idx=867, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wi-pa-ra', idx=868, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wo', idx=869, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wo-a-ne', idx=870, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wo-nu-so', idx=871, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wo-nu-so-jo', idx=872, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-wo-pu-ka-ta', idx=873, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-za', idx=874, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-za-so', idx=875, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='di-zo', idx=876, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do', idx=877, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-e', idx=878, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-e-ra', idx=879, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-e-ro', idx=880, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-e-ro-i', idx=881, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-e-ro-jo', idx=882, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-jo', idx=883, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ka-ma', idx=884, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ka-ma-i', idx=885, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ke', idx=886, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ke-ko-o-ke-ne', idx=887, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ke-ko-o-ke-ne-i', idx=888, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ke-u', idx=889, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ma', idx=890, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ni-ja', idx=891, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-po-ta', idx=892, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qe-ja', idx=893, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qe-u', idx=894, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qi', idx=895, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qo-no', idx=896, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qo-ro', idx=897, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-qo-so', idx=898, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ra-qe', idx=899, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-re-we', idx=900, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri', idx=901, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri-je-we', idx=902, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri-jo', idx=903, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri-ka-no', idx=904, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri-ka-o', idx=905, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ri-wo', idx=906, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ro', idx=907, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ro-jo', idx=908, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ro-jo-jo', idx=909, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ro-me-u', idx=910, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ro-qo', idx=911, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-se', idx=912, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-si', idx=913, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-si-mi-ja', idx=914, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-si-mi-jo-qe', idx=915, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-so-mo', idx=916, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ti', idx=917, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ti-ja', idx=918, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ti-jo', idx=919, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-ti-jo-no', idx=920, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-u-te-ke', idx=921, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-wa', idx=922, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-we-i', idx=923, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-we-jo', idx=924, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-we-jo-qe', idx=925, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-we-na', idx=926, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='do-wo', idx=927, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du', idx=928, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ko-so', idx=929, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ma', idx=930, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ma-te-qe', idx=931, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ma-ti', idx=932, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ni', idx=933, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ni-ja', idx=934, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ni-jo', idx=935, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ni-jo-jo', idx=936, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-phu-ra-zo', idx=937, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-phu-so', idx=938, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-pi-jo', idx=939, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-re-u', idx=940, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ri', idx=941, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ro', idx=942, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ru', idx=943, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ru-po', idx=944, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ru-to-mo', idx=945, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ru-wo-qo', idx=946, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-sa-ni', idx=947, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-ta-so', idx=948, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-to', idx=949, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-tu-wa', idx=950, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wi-ja', idx=951, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-jo', idx=952, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-jo-jo', idx=953, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-pe', idx=954, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-u-pi', idx=955, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-u-pi-de', idx=956, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='dwo', idx=957, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='dwo-jo', idx=958, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e', idx=959, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-*18', idx=960, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-*65-to', idx=961, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-*83', idx=962, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-da', idx=963, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-da-e', idx=964, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-da-e-u', idx=965, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-da-e-wo', idx=966, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-de', idx=967, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-do-mo-ne-u', idx=968, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-do-mo-ne-we', idx=969, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-e-si', idx=970, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-e-to', idx=971, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-i-ja-si', idx=972, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-ma-pi', idx=973, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-ma-te-qe', idx=974, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-no', idx=975, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-ra', idx=976, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-ra-e-we', idx=977, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-sa-te', idx=978, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-te-jo', idx=979, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-te-re-ta', idx=980, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke', idx=981, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-a', idx=982, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-da-mo', idx=983, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-de-mi', idx=984, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-e', idx=985, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-i-ja', idx=986, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-i-ja-ta', idx=987, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-i-jo-jo', idx=988, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ja', idx=989, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-jo-to', idx=990, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-me-de', idx=991, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-na-to', idx=992, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ne', idx=993, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-pi', idx=994, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-pu-te-ri-ja', idx=995, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-qe', idx=996, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ra-ne', idx=997, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ri-ja-wo', idx=998, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ri-jo-na', idx=999, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, sequence_data, numerals, lang=\"transliterated_linear_b\"):\n",
    "        self.word_list = set()\n",
    "        self.log_start = 0\n",
    "        logo_list = set()\n",
    "        self.lang = lang\n",
    "\n",
    "        for word in sequence_data.keys():\n",
    "            if (word.startswith(\"*\") and not \"-\" in word) or word[0].isupper() or word in {\"1\", \"2\"}:\n",
    "                logo_list.add(word)\n",
    "            else:\n",
    "                self.word_list.add(word)\n",
    "        nums = set()\n",
    "        for num_list in numerals.values():\n",
    "            for num in num_list:\n",
    "                nums.add(num)\n",
    "        nums=sorted(list(nums), key=lambda x:int(x))\n",
    "\n",
    "        self.word_list = list(sorted(self.word_list))\n",
    "        self.log_start = len(self.word_list)\n",
    "        logo_list = list(sorted(logo_list))\n",
    "        self.word_list.extend(logo_list)\n",
    "        self.word_list.extend(nums)\n",
    "        \n",
    "        self.build_logo_vocab(sequence_data)\n",
    "        self.build(sequence_data, nums)\n",
    "\n",
    "    def build(self, sequence_data, numerals):\n",
    "        self.form2idx = {w: i for i, w in enumerate(self.word_list)}\n",
    "        word_list = [WordWithCompleteness(self.lang, w, i, classify_completeness(sequence_data[w][1]) if i < len(self.word_list) - len(numerals) else Completeness.COMPLETE) for i, w in enumerate(self.word_list)]\n",
    "        self.word_list = word_list\n",
    "\n",
    "    @property\n",
    "    def get_words(self):\n",
    "        return self.word_list[:self.log_start]\n",
    "\n",
    "    @property\n",
    "    def get_logograms(self):\n",
    "        return self.word_list[self.log_start:]\n",
    "\n",
    "    @property\n",
    "    def get_vocabulary(self):\n",
    "        return self.word_list\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.word_list[idx]\n",
    "\n",
    "    def get_form_idx(self, form, status=False):\n",
    "        #status: True means that it is a word, False means it is a Logogram/Numeral\n",
    "        if status:\n",
    "            return self.form2idx[form], self.form2idx[form] < self.log_start\n",
    "        return self.form2idx[form]\n",
    "\n",
    "    def build_logo_vocab(self, sequence_data):\n",
    "        self.logogram_vocab = {}\n",
    "        mappings = {\n",
    "            \"AES\": \"\",\n",
    "            \"ARG\":\"\",\n",
    "            \"AUR\": \"\",\n",
    "            \"BOS\": \"\",\n",
    "            \"BOSf\": \"\",\n",
    "            \"BOSm\": \"\",\n",
    "            \"CAP\": \"\",\n",
    "            \"CAPf\": \"\",\n",
    "            \"CAPm\": \"\",\n",
    "            \"CERV\": \"\",\n",
    "            \"CORN\": \"\",\n",
    "            \"CROC\": \"\",\n",
    "            \"CYP\": \"\",\n",
    "            \"EQU\": \"\",\n",
    "            \"EQUf\": \"\",\n",
    "            \"EQUm\": \"\",\n",
    "            \"FAR\": \"\",\n",
    "            \"GAL\": \"\",\n",
    "            \"GRA\": \"\",\n",
    "            \"HAS\": \"\",\n",
    "            \"HORD\": \"\",\n",
    "            \"LANA\": \"\",\n",
    "            \"LUNA\": \"\",\n",
    "            \"OLIV\": \"\",\n",
    "            \"OVIS\": \"\",\n",
    "            \"OVISf\": \"\",\n",
    "            \"OVISm\": \"\",\n",
    "            \"SAG\": \"\",\n",
    "            \"SUS\": \"\",\n",
    "            \"SUSf\": \"\",\n",
    "            \"SUSm\": \"\",\n",
    "            \"TELA\": \"\",\n",
    "            \"VIN\": \"\",\n",
    "            \"JAC\": \"\",\n",
    "            \"BIG\": \"\",\n",
    "            \"AROM\": \"\",\n",
    "            \"ARB\": \"\",\n",
    "            \"ALV\": \"\",\n",
    "            \"ARM\": \"\",\n",
    "            \"CUR\": \"\",\n",
    "            \"MUL\": \"\",\n",
    "            \"OLE\": \"\",\n",
    "            \"PUG\": \"\",\n",
    "            \"ROTA\": \"\",\n",
    "            \"TUN\": \"\",\n",
    "            \"VIR\": \"\",\n",
    "            \"TELAI\": \"\",\n",
    "            \"TELHA\": \"\",\n",
    "            \"CAPS\": \"\",\n",
    "            \"VAS\": \"\",\n",
    "            \"A+RE+PA\": \"\",\n",
    "            \"KA+NA+KO\": \"\",\n",
    "            \"KA+PO\": \"\", \n",
    "            \"ME+RI\": \"\",\n",
    "            \"TU+RYO\": \"\",\n",
    "            \"NI\": \"\",\n",
    "            \"MO\": \"\",\n",
    "            \"ZE\": \"\",\n",
    "            \"KO\": \"\",\n",
    "            \"SA\": \"\",\n",
    "            \"KU\": \"\",\n",
    "            \"SE\": \"\",\n",
    "            \"MA\": \"\",\n",
    "            \"GUP\": \"\"\n",
    "        }\n",
    "        for word in sequence_data.keys():\n",
    "              if word in mappings: #correctly translate logograms from latin\n",
    "                  self.logogram_vocab[word] = mappings[word]\n",
    "              #handle variants\n",
    "              elif \"VAS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VAS\"]\n",
    "              elif \"AROM+CYP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"CYP\"]\n",
    "              elif \"AROM+KO\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"KO\"]\n",
    "              elif \"BOS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"BOS\"]\n",
    "              elif \"CAP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"CAP\"]\n",
    "              elif \"CYP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"CYP\"]\n",
    "              elif \"EQU\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"EQU\"]\n",
    "              elif \"GRA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"GRA\"]\n",
    "              elif \"OLE\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OLE\"]\n",
    "              elif \"OLIV\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OLIV\"]\n",
    "              elif \"OVIS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OVIS\"]\n",
    "              elif \"SUS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"SUS\"]\n",
    "              elif \"ROTA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"ROTA\"]\n",
    "              elif \"TUN\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TUN\"]\n",
    "              elif \"VIR\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VIR\"]\n",
    "              elif \"TELAI\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELAI\"]\n",
    "              elif \"TELHA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELHA\"]\n",
    "              elif \"TELA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELA\"]\n",
    "              elif \"VIN\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VIN\"]\n",
    "\n",
    "\n",
    "my_voc = Vocabulary(sequence_data, nums)\n",
    "my_voc.get_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordWithCompleteness(lang='transliterated_linear_b', form='*18-jo', idx=0, completeness=<Completeness.INCOMPLETE: 0>) [{3402: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*22-je-mi', idx=3, completeness=<Completeness.INCOMPLETE: 0>) [{930: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*22-jo', idx=4, completeness=<Completeness.INCOMPLETE: 0>) [{1069: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*34-ka', idx=6, completeness=<Completeness.INCOMPLETE: 0>) [{4454: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*47-da', idx=17, completeness=<Completeness.INCOMPLETE: 0>) [{948: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*47-de', idx=19, completeness=<Completeness.INCOMPLETE: 0>) [{1798: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*47-so', idx=21, completeness=<Completeness.INCOMPLETE: 0>) [{2131: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*47-ta-qo', idx=23, completeness=<Completeness.INCOMPLETE: 0>) [{957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*49-so', idx=26, completeness=<Completeness.INCOMPLETE: 0>) [{3319: False, 3585: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-i-ti-je', idx=30, completeness=<Completeness.INCOMPLETE: 0>) [{4027: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-jo', idx=31, completeness=<Completeness.INCOMPLETE: 0>) [{4181: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-na-ro', idx=38, completeness=<Completeness.INCOMPLETE: 0>) [{3387: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-so-jo', idx=44, completeness=<Completeness.INCOMPLETE: 0>) [{2514: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-wo', idx=46, completeness=<Completeness.INCOMPLETE: 0>) [{3738: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56-za', idx=47, completeness=<Completeness.INCOMPLETE: 0>) [{2663: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*64-jo', idx=49, completeness=<Completeness.INCOMPLETE: 0>) [{1283: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*65-no', idx=50, completeness=<Completeness.INCOMPLETE: 0>) [{644: False, 1957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*83-re-jo-de', idx=52, completeness=<Completeness.INCOMPLETE: 0>) [{4148: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-*47', idx=57, completeness=<Completeness.INCOMPLETE: 0>) [{302: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-*47-wi', idx=58, completeness=<Completeness.INCOMPLETE: 0>) [{389: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-*65-ma', idx=64, completeness=<Completeness.INCOMPLETE: 0>) [{4113: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-da-ra-ko', idx=71, completeness=<Completeness.INCOMPLETE: 0>) [{1219: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-da-wo-ne', idx=76, completeness=<Completeness.INCOMPLETE: 0>) [{2701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-di-*22-sa', idx=81, completeness=<Completeness.INCOMPLETE: 0>) [{3907: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-di-je-wo', idx=82, completeness=<Completeness.INCOMPLETE: 0>) [{2875: False, 2882: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-du', idx=89, completeness=<Completeness.INCOMPLETE: 0>) [{1826: False, 2732: False, 4253: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-e', idx=92, completeness=<Completeness.INCOMPLETE: 0>) [{953: False, 5288: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-e-ti-to', idx=98, completeness=<Completeness.INCOMPLETE: 0>) [{5004: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ja', idx=102, completeness=<Completeness.INCOMPLETE: 0>) [{1958: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-jo', idx=106, completeness=<Completeness.INCOMPLETE: 0>) [{1109: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-me-ne', idx=112, completeness=<Completeness.INCOMPLETE: 0>) [{940: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ka-mu', idx=113, completeness=<Completeness.INCOMPLETE: 0>) [{1015: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-re-u', idx=143, completeness=<Completeness.INCOMPLETE: 0>) [{4763: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-si', idx=153, completeness=<Completeness.INCOMPLETE: 0>) [{4732: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ke-ti-ri', idx=159, completeness=<Completeness.INCOMPLETE: 0>) [{2351: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ki', idx=169, completeness=<Completeness.INCOMPLETE: 0>) [{2712: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ri', idx=172, completeness=<Completeness.INCOMPLETE: 0>) [{5112: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ki-ri-jo', idx=174, completeness=<Completeness.INCOMPLETE: 0>) [{5123: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ko', idx=180, completeness=<Completeness.INCOMPLETE: 0>) [{5419: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-ro-we-i', idx=193, completeness=<Completeness.INCOMPLETE: 0>) [{2862: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ko-we-i-ja', idx=202, completeness=<Completeness.INCOMPLETE: 0>) [{4300: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-di-ri', idx=204, completeness=<Completeness.INCOMPLETE: 0>) [{952: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-ri-jo', idx=207, completeness=<Completeness.INCOMPLETE: 0>) [{2675: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ku-tu-ru-wo', idx=209, completeness=<Completeness.INCOMPLETE: 0>) [{3957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-me', idx=219, completeness=<Completeness.INCOMPLETE: 0>) [{1287: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-ja', idx=226, completeness=<Completeness.INCOMPLETE: 0>) [{5413: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-mi-nwa', idx=234, completeness=<Completeness.INCOMPLETE: 0>) [{677: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-mu', idx=248, completeness=<Completeness.INCOMPLETE: 0>) [{2006: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-mu-ta-wo-qe', idx=251, completeness=<Completeness.INCOMPLETE: 0>) [{685: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-na-pe-we', idx=260, completeness=<Completeness.INCOMPLETE: 0>) [{1023: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ne', idx=267, completeness=<Completeness.INCOMPLETE: 0>) [{1061: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-no-qo', idx=289, completeness=<Completeness.INCOMPLETE: 0>) [{2980: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-nu', idx=298, completeness=<Completeness.INCOMPLETE: 0>) [{598: False, 696: False, 4006: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa', idx=309, completeness=<Completeness.INCOMPLETE: 0>) [{1398: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-ni-jo', idx=312, completeness=<Completeness.INCOMPLETE: 0>) [{3638: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-sa', idx=315, completeness=<Completeness.INCOMPLETE: 0>) [{5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-te', idx=322, completeness=<Completeness.INCOMPLETE: 0>) [{999: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-to', idx=323, completeness=<Completeness.INCOMPLETE: 0>) [{4211: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pa-to-re', idx=324, completeness=<Completeness.INCOMPLETE: 0>) [{4058: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pe-e-si', idx=329, completeness=<Completeness.INCOMPLETE: 0>) [{4722: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-do-ro', idx=351, completeness=<Completeness.INCOMPLETE: 0>) [{961: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ja-ko-ro-jo', idx=355, completeness=<Completeness.INCOMPLETE: 0>) [{2719: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-je', idx=357, completeness=<Completeness.INCOMPLETE: 0>) [{2364: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-ke-ne-a', idx=362, completeness=<Completeness.INCOMPLETE: 0>) [{5245: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-te', idx=378, completeness=<Completeness.INCOMPLETE: 0>) [{595: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-do', idx=388, completeness=<Completeness.INCOMPLETE: 0>) [{1198: False, 3967: False, 4005: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ki', idx=394, completeness=<Completeness.INCOMPLETE: 0>) [{1827: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ki-si', idx=395, completeness=<Completeness.INCOMPLETE: 0>) [{5565: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-pu-ko', idx=396, completeness=<Completeness.INCOMPLETE: 0>) [{5711: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-qa-ro', idx=399, completeness=<Completeness.INCOMPLETE: 0>) [{293: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-qi', idx=402, completeness=<Completeness.INCOMPLETE: 0>) [{2339: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ta', idx=407, completeness=<Completeness.INCOMPLETE: 0>) [{718: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-qi-ti-ta', idx=408, completeness=<Completeness.INCOMPLETE: 0>) [{2338: False, 2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ra', idx=411, completeness=<Completeness.INCOMPLETE: 0>) [{733: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ro-mo-to-me-na', idx=422, completeness=<Completeness.INCOMPLETE: 0>) [{463: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-ru-wo-ja', idx=425, completeness=<Completeness.INCOMPLETE: 0>) [{458: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ra-wo', idx=428, completeness=<Completeness.INCOMPLETE: 0>) [{5721: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-i', idx=430, completeness=<Completeness.INCOMPLETE: 0>) [{871: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-jo', idx=433, completeness=<Completeness.INCOMPLETE: 0>) [{841: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ke-se', idx=436, completeness=<Completeness.INCOMPLETE: 0>) [{2329: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-se', idx=450, completeness=<Completeness.INCOMPLETE: 0>) [{971: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-ta-wo', idx=454, completeness=<Completeness.INCOMPLETE: 0>) [{2676: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-te-re-u', idx=455, completeness=<Completeness.INCOMPLETE: 0>) [{2685: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-re-u-ke', idx=458, completeness=<Completeness.INCOMPLETE: 0>) [{969: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ri', idx=461, completeness=<Completeness.INCOMPLETE: 0>) [{5407: False, 5428: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ri-we-we', idx=469, completeness=<Completeness.INCOMPLETE: 0>) [{1001: False, 3901: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ro', idx=472, completeness=<Completeness.INCOMPLETE: 0>) [{5082: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-mo', idx=477, completeness=<Completeness.INCOMPLETE: 0>) [{2330: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ro-ta', idx=481, completeness=<Completeness.INCOMPLETE: 0>) [{5081: False, 5084: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-sa-mi', idx=494, completeness=<Completeness.INCOMPLETE: 0>) [{1215: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-ma-no-we', idx=510, completeness=<Completeness.INCOMPLETE: 0>) [{2450: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ta-wo-ne', idx=527, completeness=<Completeness.INCOMPLETE: 0>) [{1002: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-te-we-i', idx=540, completeness=<Completeness.INCOMPLETE: 0>) [{1735: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ti', idx=542, completeness=<Completeness.INCOMPLETE: 0>) [{1051: False, 2677: False, 4988: False, 5414: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-ti-ri-ja', idx=550, completeness=<Completeness.INCOMPLETE: 0>) [{4677: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-to-po-qo-i', idx=559, completeness=<Completeness.INCOMPLETE: 0>) [{2349: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-to-wo', idx=561, completeness=<Completeness.INCOMPLETE: 0>) [{2748: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-wa', idx=567, completeness=<Completeness.INCOMPLETE: 0>) [{1053: False, 2008: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-wi-je-mo', idx=578, completeness=<Completeness.INCOMPLETE: 0>) [{1391: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-wo-ro', idx=581, completeness=<Completeness.INCOMPLETE: 0>) [{2708: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='a-zo', idx=587, completeness=<Completeness.INCOMPLETE: 0>) [{4766: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ai-ta', idx=589, completeness=<Completeness.INCOMPLETE: 0>) [{5630: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-du-ro', idx=590, completeness=<Completeness.INCOMPLETE: 0>) [{2754: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ki', idx=599, completeness=<Completeness.INCOMPLETE: 0>) [{4723: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ku-ta', idx=613, completeness=<Completeness.INCOMPLETE: 0>) [{2862: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-mi', idx=615, completeness=<Completeness.INCOMPLETE: 0>) [{4077: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-rya', idx=620, completeness=<Completeness.INCOMPLETE: 0>) [{2398: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-sa', idx=621, completeness=<Completeness.INCOMPLETE: 0>) [{5397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-se-wa', idx=622, completeness=<Completeness.INCOMPLETE: 0>) [{4723: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ta', idx=625, completeness=<Completeness.INCOMPLETE: 0>) [{2449: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ti-nu', idx=634, completeness=<Completeness.INCOMPLETE: 0>) [{1694: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-tu-ti-ja', idx=636, completeness=<Completeness.INCOMPLETE: 0>) [{4320: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-wo-re-u', idx=643, completeness=<Completeness.INCOMPLETE: 0>) [{765: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-ze', idx=647, completeness=<Completeness.INCOMPLETE: 0>) [{2048: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ai-zo-wo', idx=649, completeness=<Completeness.INCOMPLETE: 0>) [{4766: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='au-to-a', idx=666, completeness=<Completeness.INCOMPLETE: 0>) [{4778: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-*22', idx=674, completeness=<Completeness.INCOMPLETE: 0>) [{1395: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-*22-ti', idx=675, completeness=<Completeness.INCOMPLETE: 0>) [{739: False, 1318: False, 2009: False, 4453: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-*83', idx=680, completeness=<Completeness.INCOMPLETE: 0>) [{1378: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-*83-ja-i', idx=683, completeness=<Completeness.INCOMPLETE: 0>) [{3869: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-i', idx=686, completeness=<Completeness.INCOMPLETE: 0>) [{1100: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-je', idx=696, completeness=<Completeness.INCOMPLETE: 0>) [{5437: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-je-we', idx=697, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-jo', idx=698, completeness=<Completeness.INCOMPLETE: 0>) [{627: False, 3917: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-ma', idx=704, completeness=<Completeness.INCOMPLETE: 0>) [{1326: False, 3718: False, 5469: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-ma-o-te', idx=705, completeness=<Completeness.INCOMPLETE: 0>) [{1239: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-mi', idx=708, completeness=<Completeness.INCOMPLETE: 0>) [{741: False, 4229: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-mo-ko', idx=714, completeness=<Completeness.INCOMPLETE: 0>) [{1501: False, 4457: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-na-ro', idx=721, completeness=<Completeness.INCOMPLETE: 0>) [{3209: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-nu-wa-a-ri', idx=722, completeness=<Completeness.INCOMPLETE: 0>) [{5155: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-nwa-re', idx=725, completeness=<Completeness.INCOMPLETE: 0>) [{330: False, 3096: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-pu', idx=729, completeness=<Completeness.INCOMPLETE: 0>) [{1749: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-pu-ri-to', idx=730, completeness=<Completeness.INCOMPLETE: 0>) [{957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-ra', idx=732, completeness=<Completeness.INCOMPLETE: 0>) [{1660: False, 2331: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-ro', idx=735, completeness=<Completeness.INCOMPLETE: 0>) [{2669: False, 3305: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-so-de', idx=738, completeness=<Completeness.INCOMPLETE: 0>) [{3958: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='da-te', idx=742, completeness=<Completeness.INCOMPLETE: 0>) [{889: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-a', idx=759, completeness=<Completeness.INCOMPLETE: 0>) [{1370: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-a-ta', idx=760, completeness=<Completeness.INCOMPLETE: 0>) [{3160: False, 3263: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-di-ku-ja', idx=763, completeness=<Completeness.INCOMPLETE: 0>) [{2551: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-ke', idx=767, completeness=<Completeness.INCOMPLETE: 0>) [{5555: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-ku-tu-wo-ko', idx=773, completeness=<Completeness.INCOMPLETE: 0>) [{5395: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-ra', idx=780, completeness=<Completeness.INCOMPLETE: 0>) [{4240: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-ra-wo', idx=781, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-ro', idx=783, completeness=<Completeness.INCOMPLETE: 0>) [{686: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-sa-to', idx=784, completeness=<Completeness.INCOMPLETE: 0>) [{1491: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-so', idx=785, completeness=<Completeness.INCOMPLETE: 0>) [{678: False, 5659: False, 5728: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-u', idx=787, completeness=<Completeness.INCOMPLETE: 0>) [{2959: False, 4848: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-u-ki-jo', idx=791, completeness=<Completeness.INCOMPLETE: 0>) [{4034: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-wa-pi', idx=794, completeness=<Completeness.INCOMPLETE: 0>) [{4228: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='de-we-ro-ai-ko-ra', idx=797, completeness=<Completeness.INCOMPLETE: 0>) [{5430: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-*18', idx=801, completeness=<Completeness.INCOMPLETE: 0>) [{1419: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-*79-nu', idx=803, completeness=<Completeness.INCOMPLETE: 0>) [{3620: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-da', idx=804, completeness=<Completeness.INCOMPLETE: 0>) [{2577: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-da-ka', idx=805, completeness=<Completeness.INCOMPLETE: 0>) [{2577: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-de', idx=808, completeness=<Completeness.INCOMPLETE: 0>) [{2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-do', idx=810, completeness=<Completeness.INCOMPLETE: 0>) [{214: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-jo', idx=814, completeness=<Completeness.INCOMPLETE: 0>) [{1789: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-ka', idx=815, completeness=<Completeness.INCOMPLETE: 0>) [{318: False, 937: False, 2516: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-ki', idx=819, completeness=<Completeness.INCOMPLETE: 0>) [{1041: False, 3684: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-ki-mo', idx=820, completeness=<Completeness.INCOMPLETE: 0>) [{1385: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-mi', idx=825, completeness=<Completeness.INCOMPLETE: 0>) [{2663: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-mi-zo', idx=826, completeness=<Completeness.INCOMPLETE: 0>) [{10: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-pa-te', idx=831, completeness=<Completeness.INCOMPLETE: 0>) [{3917: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-pte', idx=836, completeness=<Completeness.INCOMPLETE: 0>) [{2429: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-qa-ra', idx=840, completeness=<Completeness.INCOMPLETE: 0>) [{2651: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-ra-wo-no', idx=846, completeness=<Completeness.INCOMPLETE: 0>) [{5071: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-ri-wa-sa', idx=848, completeness=<Completeness.INCOMPLETE: 0>) [{4730: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-wo-a-ne', idx=870, completeness=<Completeness.INCOMPLETE: 0>) [{845: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='di-za', idx=874, completeness=<Completeness.INCOMPLETE: 0>) [{3625: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do', idx=877, completeness=<Completeness.INCOMPLETE: 0>) [{621: False, 1112: False, 2708: False, 4048: False, 4158: False, 5084: False, 5386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-e', idx=878, completeness=<Completeness.INCOMPLETE: 0>) [{1748: False, 2174: False, 4680: False, 4879: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-jo', idx=883, completeness=<Completeness.INCOMPLETE: 0>) [{2380: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ke-ko-o-ke-ne-i', idx=888, completeness=<Completeness.INCOMPLETE: 0>) [{2369: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ke-u', idx=889, completeness=<Completeness.INCOMPLETE: 0>) [{4235: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ma', idx=890, completeness=<Completeness.INCOMPLETE: 0>) [{698: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ni-ja', idx=891, completeness=<Completeness.INCOMPLETE: 0>) [{3978: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-qi', idx=895, completeness=<Completeness.INCOMPLETE: 0>) [{991: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ri', idx=901, completeness=<Completeness.INCOMPLETE: 0>) [{1102: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ro', idx=907, completeness=<Completeness.INCOMPLETE: 0>) [{1331: False, 3600: False, 5429: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-si', idx=913, completeness=<Completeness.INCOMPLETE: 0>) [{1452: False, 4173: False, 4322: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ti', idx=917, completeness=<Completeness.INCOMPLETE: 0>) [{1738: False, 2304: False, 3576: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-ti-jo-no', idx=920, completeness=<Completeness.INCOMPLETE: 0>) [{686: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-u-te-ke', idx=921, completeness=<Completeness.INCOMPLETE: 0>) [{2317: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-we-i', idx=923, completeness=<Completeness.INCOMPLETE: 0>) [{3913: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-we-na', idx=926, completeness=<Completeness.INCOMPLETE: 0>) [{5264: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='do-wo', idx=927, completeness=<Completeness.INCOMPLETE: 0>) [{629: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du', idx=928, completeness=<Completeness.INCOMPLETE: 0>) [{29: False, 837: False, 1347: False, 1960: False, 2013: False, 2014: False, 2205: False, 2304: False, 2733: False, 3802: False, 5756: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-ko-so', idx=929, completeness=<Completeness.INCOMPLETE: 0>) [{5062: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-ma-ti', idx=932, completeness=<Completeness.INCOMPLETE: 0>) [{5264: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-ro', idx=942, completeness=<Completeness.INCOMPLETE: 0>) [{3179: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-ru', idx=943, completeness=<Completeness.INCOMPLETE: 0>) [{2677: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-ru-wo-qo', idx=946, completeness=<Completeness.INCOMPLETE: 0>) [{5386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-tu-wa', idx=950, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-wi-ja', idx=951, completeness=<Completeness.INCOMPLETE: 0>) [{2578: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='du-wo-pe', idx=954, completeness=<Completeness.INCOMPLETE: 0>) [{1064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-*18', idx=960, completeness=<Completeness.INCOMPLETE: 0>) [{1486: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-da', idx=963, completeness=<Completeness.INCOMPLETE: 0>) [{598: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-da-e', idx=964, completeness=<Completeness.INCOMPLETE: 0>) [{689: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-de', idx=967, completeness=<Completeness.INCOMPLETE: 0>) [{5078: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-i-ja-si', idx=972, completeness=<Completeness.INCOMPLETE: 0>) [{2712: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-no', idx=975, completeness=<Completeness.INCOMPLETE: 0>) [{5070: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ka-sa-te', idx=978, completeness=<Completeness.INCOMPLETE: 0>) [{5273: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-pi', idx=994, completeness=<Completeness.INCOMPLETE: 0>) [{733: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ri-ja-wo', idx=998, completeness=<Completeness.INCOMPLETE: 0>) [{5274: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ke-ri-jo-na', idx=999, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ko-so-wo-ko', idx=1025, completeness=<Completeness.INCOMPLETE: 0>) [{984: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-na-i', idx=1039, completeness=<Completeness.INCOMPLETE: 0>) [{1028: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-na-i-jo', idx=1040, completeness=<Completeness.INCOMPLETE: 0>) [{987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-na-ri-po-to', idx=1043, completeness=<Completeness.INCOMPLETE: 0>) [{513: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ne-o', idx=1048, completeness=<Completeness.INCOMPLETE: 0>) [{2674: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ne-re-ja', idx=1050, completeness=<Completeness.INCOMPLETE: 0>) [{2569: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-o-te', idx=1066, completeness=<Completeness.INCOMPLETE: 0>) [{4722: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pa-ta', idx=1070, completeness=<Completeness.INCOMPLETE: 0>) [{5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pe', idx=1071, completeness=<Completeness.INCOMPLETE: 0>) [{5476: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pi-ja-ta-ni-ja', idx=1082, completeness=<Completeness.INCOMPLETE: 0>) [{4638: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pi-ke-to', idx=1086, completeness=<Completeness.INCOMPLETE: 0>) [{3892: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pi-ro-pa-ja', idx=1096, completeness=<Completeness.INCOMPLETE: 0>) [{143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-pi-ta', idx=1097, completeness=<Completeness.INCOMPLETE: 0>) [{997: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-po-me-ne-u', idx=1104, completeness=<Completeness.INCOMPLETE: 0>) [{5260: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qa', idx=1110, completeness=<Completeness.INCOMPLETE: 0>) [{4760: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qa-na-qe', idx=1112, completeness=<Completeness.INCOMPLETE: 0>) [{5362: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qe-qi', idx=1117, completeness=<Completeness.INCOMPLETE: 0>) [{5707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qe-ra-wo', idx=1118, completeness=<Completeness.INCOMPLETE: 0>) [{2729: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qo', idx=1125, completeness=<Completeness.INCOMPLETE: 0>) [{5040: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-qo-me-ne', idx=1126, completeness=<Completeness.INCOMPLETE: 0>) [{5049: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ra-wa', idx=1147, completeness=<Completeness.INCOMPLETE: 0>) [{3907: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-re', idx=1150, completeness=<Completeness.INCOMPLETE: 0>) [{4672: False, 5246: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-re-ke', idx=1158, completeness=<Completeness.INCOMPLETE: 0>) [{2709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-re-u', idx=1175, completeness=<Completeness.INCOMPLETE: 0>) [{5207: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-re-u-te', idx=1177, completeness=<Completeness.INCOMPLETE: 0>) [{2682: False, 5235: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-re-u-te-ri', idx=1180, completeness=<Completeness.INCOMPLETE: 0>) [{4696: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ri', idx=1185, completeness=<Completeness.INCOMPLETE: 0>) [{38: False, 579: False, 676: False, 4282: False, 4988: False, 5461: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ri-ko', idx=1190, completeness=<Completeness.INCOMPLETE: 0>) [{557: False, 912: False, 2730: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ri-ra-i', idx=1200, completeness=<Completeness.INCOMPLETE: 0>) [{1005: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ri-ta-ri-jo', idx=1204, completeness=<Completeness.INCOMPLETE: 0>) [{988: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ri-ti-qi', idx=1205, completeness=<Completeness.INCOMPLETE: 0>) [{2710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ru-ta-jo', idx=1219, completeness=<Completeness.INCOMPLETE: 0>) [{5070: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ru-ti-ri-jo', idx=1222, completeness=<Completeness.INCOMPLETE: 0>) [{982: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-se', idx=1231, completeness=<Completeness.INCOMPLETE: 0>) [{1316: False, 1971: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-si', idx=1234, completeness=<Completeness.INCOMPLETE: 0>) [{33: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ta', idx=1237, completeness=<Completeness.INCOMPLETE: 0>) [{2308: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-te-ja', idx=1246, completeness=<Completeness.INCOMPLETE: 0>) [{3953: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-te-re-u', idx=1250, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-ti-wa-i', idx=1266, completeness=<Completeness.INCOMPLETE: 0>) [{5453: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u', idx=1278, completeness=<Completeness.INCOMPLETE: 0>) [{1042: False, 2661: False, 4767: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-ka-ri', idx=1285, completeness=<Completeness.INCOMPLETE: 0>) [{2710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-ko', idx=1288, completeness=<Completeness.INCOMPLETE: 0>) [{1568: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-ko-me', idx=1289, completeness=<Completeness.INCOMPLETE: 0>) [{3901: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-no', idx=1298, completeness=<Completeness.INCOMPLETE: 0>) [{1827: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-o-mo', idx=1299, completeness=<Completeness.INCOMPLETE: 0>) [{816: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-u-ro-wa', idx=1304, completeness=<Completeness.INCOMPLETE: 0>) [{1188: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-wa', idx=1314, completeness=<Completeness.INCOMPLETE: 0>) [{1144: False, 1368: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-we', idx=1317, completeness=<Completeness.INCOMPLETE: 0>) [{5397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-we-da-si', idx=1318, completeness=<Completeness.INCOMPLETE: 0>) [{5160: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-we-o-si', idx=1321, completeness=<Completeness.INCOMPLETE: 0>) [{1516: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-we-wa', idx=1323, completeness=<Completeness.INCOMPLETE: 0>) [{995: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-we-za-no', idx=1325, completeness=<Completeness.INCOMPLETE: 0>) [{4732: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-wi-ta', idx=1331, completeness=<Completeness.INCOMPLETE: 0>) [{2713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-wi-to-wo', idx=1336, completeness=<Completeness.INCOMPLETE: 0>) [{2714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-wo', idx=1337, completeness=<Completeness.INCOMPLETE: 0>) [{414: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='e-wo-ta-o', idx=1340, completeness=<Completeness.INCOMPLETE: 0>) [{4757: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='f', idx=1343, completeness=<Completeness.INCOMPLETE: 0>) [{602: False, 2772: False, 2811: False, 2812: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ha', idx=1344, completeness=<Completeness.INCOMPLETE: 0>) [{5399: False, 5574: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ha-ke-wo-a-ki-ri', idx=1350, completeness=<Completeness.INCOMPLETE: 0>) [{5231: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ha-te-we', idx=1368, completeness=<Completeness.INCOMPLETE: 0>) [{5149: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i', idx=1371, completeness=<Completeness.INCOMPLETE: 0>) [{683: False, 883: False, 928: False, 1545: False, 1570: False, 1648: False, 1662: False, 1679: False, 1863: False, 2001: False, 2071: False, 2100: False, 2318: False, 2372: False, 2663: False, 2674: False, 2709: False, 3887: False, 3910: False, 3917: False, 4111: False, 4491: False, 4842: False, 5142: False, 5375: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-da-wo', idx=1377, completeness=<Completeness.INCOMPLETE: 0>) [{1010: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ja-ni', idx=1384, completeness=<Completeness.INCOMPLETE: 0>) [{1945: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ja-wo-ne', idx=1387, completeness=<Completeness.INCOMPLETE: 0>) [{961: False, 2701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-je', idx=1388, completeness=<Completeness.INCOMPLETE: 0>) [{3917: False, 3991: False, 4019: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ka', idx=1401, completeness=<Completeness.INCOMPLETE: 0>) [{1346: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ke', idx=1404, completeness=<Completeness.INCOMPLETE: 0>) [{4778: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ko', idx=1408, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-no', idx=1425, completeness=<Completeness.INCOMPLETE: 0>) [{5146: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-pa', idx=1426, completeness=<Completeness.INCOMPLETE: 0>) [{3132: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-pe-ne-o', idx=1430, completeness=<Completeness.INCOMPLETE: 0>) [{5491: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-pi', idx=1434, completeness=<Completeness.INCOMPLETE: 0>) [{4769: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-po-qa', idx=1437, completeness=<Completeness.INCOMPLETE: 0>) [{4224: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-pu-ma', idx=1438, completeness=<Completeness.INCOMPLETE: 0>) [{5392: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-qa-ne', idx=1439, completeness=<Completeness.INCOMPLETE: 0>) [{5465: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-qe', idx=1440, completeness=<Completeness.INCOMPLETE: 0>) [{4989: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-sa-ma', idx=1453, completeness=<Completeness.INCOMPLETE: 0>) [{5468: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-se-re', idx=1456, completeness=<Completeness.INCOMPLETE: 0>) [{1052: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-so', idx=1458, completeness=<Completeness.INCOMPLETE: 0>) [{3303: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-ta', idx=1461, completeness=<Completeness.INCOMPLETE: 0>) [{880: False, 1060: False, 1266: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-to-ma', idx=1474, completeness=<Completeness.INCOMPLETE: 0>) [{661: False, 1044: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='i-tya', idx=1476, completeness=<Completeness.INCOMPLETE: 0>) [{2640: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja', idx=1487, completeness=<Completeness.INCOMPLETE: 0>) [{48: False, 54: False, 63: False, 136: False, 391: False, 406: False, 521: False, 617: False, 713: False, 744: False, 955: False, 973: False, 977: False, 1010: False, 1059: False, 1501: False, 1541: False, 1573: False, 1680: False, 1712: False, 1739: False, 1740: False, 1741: False, 1743: False, 1790: False, 1830: False, 1831: False, 1864: False, 1865: False, 1870: False, 1962: False, 1998: False, 2016: False, 2017: False, 2067: False, 2068: False, 2150: False, 2161: False, 2170: False, 2182: False, 2389: False, 2435: False, 2501: False, 2511: False, 2585: False, 2601: False, 2614: False, 2617: False, 2623: False, 2624: False, 2654: False, 2655: False, 2827: False, 2839: False, 2917: False, 2918: False, 3042: False, 3129: False, 3212: False, 3391: False, 3746: False, 3908: False, 4080: False, 4384: False, 4418: False, 4457: False, 4459: False, 4462: False, 4475: False, 4692: False, 4696: False, 4723: False, 4737: False, 5075: False, 5233: False, 5411: False, 5415: False, 5578: False, 5700: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-*18', idx=1488, completeness=<Completeness.INCOMPLETE: 0>) [{4478: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-de', idx=1489, completeness=<Completeness.INCOMPLETE: 0>) [{2163: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-do', idx=1490, completeness=<Completeness.INCOMPLETE: 0>) [{4159: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-do-ro', idx=1491, completeness=<Completeness.INCOMPLETE: 0>) [{4714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ko', idx=1493, completeness=<Completeness.INCOMPLETE: 0>) [{2132: False, 2162: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ma-ta-ro', idx=1495, completeness=<Completeness.INCOMPLETE: 0>) [{683: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-me', idx=1496, completeness=<Completeness.INCOMPLETE: 0>) [{1114: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-mi-nu', idx=1497, completeness=<Completeness.INCOMPLETE: 0>) [{2658: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-mu-ta', idx=1498, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-na-ti', idx=1499, completeness=<Completeness.INCOMPLETE: 0>) [{3930: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-no', idx=1500, completeness=<Completeness.INCOMPLETE: 0>) [{2406: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-no-ri', idx=1501, completeness=<Completeness.INCOMPLETE: 0>) [{1485: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-o', idx=1502, completeness=<Completeness.INCOMPLETE: 0>) [{4300: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-phu-wi-ja', idx=1505, completeness=<Completeness.INCOMPLETE: 0>) [{4392: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ro', idx=1510, completeness=<Completeness.INCOMPLETE: 0>) [{706: False, 5564: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-sa', idx=1512, completeness=<Completeness.INCOMPLETE: 0>) [{2835: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-se', idx=1515, completeness=<Completeness.INCOMPLETE: 0>) [{3773: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-si-ja', idx=1516, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ta', idx=1517, completeness=<Completeness.INCOMPLETE: 0>) [{2417: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ti', idx=1518, completeness=<Completeness.INCOMPLETE: 0>) [{2484: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-ti-ko', idx=1519, completeness=<Completeness.INCOMPLETE: 0>) [{2479: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-to', idx=1521, completeness=<Completeness.INCOMPLETE: 0>) [{2526: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ja-we-jo', idx=1522, completeness=<Completeness.INCOMPLETE: 0>) [{2801: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-mo', idx=1524, completeness=<Completeness.INCOMPLETE: 0>) [{1446: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-ne', idx=1525, completeness=<Completeness.INCOMPLETE: 0>) [{244: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-ra', idx=1526, completeness=<Completeness.INCOMPLETE: 0>) [{1795: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-re-u', idx=1527, completeness=<Completeness.INCOMPLETE: 0>) [{2807: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-u', idx=1528, completeness=<Completeness.INCOMPLETE: 0>) [{311: False, 1671: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-u-qe', idx=1529, completeness=<Completeness.INCOMPLETE: 0>) [{725: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-wa', idx=1530, completeness=<Completeness.INCOMPLETE: 0>) [{1017: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='je-we', idx=1531, completeness=<Completeness.INCOMPLETE: 0>) [{4063: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-a', idx=1534, completeness=<Completeness.INCOMPLETE: 0>) [{1793: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-du-mi', idx=1538, completeness=<Completeness.INCOMPLETE: 0>) [{143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-e', idx=1539, completeness=<Completeness.INCOMPLETE: 0>) [{1576: False, 2606: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-i', idx=1541, completeness=<Completeness.INCOMPLETE: 0>) [{5045: False, 5488: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-jo', idx=1543, completeness=<Completeness.INCOMPLETE: 0>) [{124: False, 746: False, 1911: False, 2019: False, 2616: False, 3481: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-ko', idx=1544, completeness=<Completeness.INCOMPLETE: 0>) [{3561: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-pi-do-ja', idx=1546, completeness=<Completeness.INCOMPLETE: 0>) [{5416: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-qe', idx=1548, completeness=<Completeness.INCOMPLETE: 0>) [{2421: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-qi', idx=1549, completeness=<Completeness.INCOMPLETE: 0>) [{5391: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-ro', idx=1550, completeness=<Completeness.INCOMPLETE: 0>) [{3282: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-si', idx=1551, completeness=<Completeness.INCOMPLETE: 0>) [{2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-te', idx=1552, completeness=<Completeness.INCOMPLETE: 0>) [{2780: False, 3973: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='jo-zo', idx=1554, completeness=<Completeness.INCOMPLETE: 0>) [{73: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-a-ki-ri', idx=1559, completeness=<Completeness.INCOMPLETE: 0>) [{5136: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-da-ra-so', idx=1566, completeness=<Completeness.INCOMPLETE: 0>) [{4509: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-de', idx=1569, completeness=<Completeness.INCOMPLETE: 0>) [{1108: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-di-ja', idx=1570, completeness=<Completeness.INCOMPLETE: 0>) [{885: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-di-ti-ja', idx=1571, completeness=<Completeness.INCOMPLETE: 0>) [{692: False, 726: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-i-jo', idx=1577, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-mu-ko-to', idx=1602, completeness=<Completeness.INCOMPLETE: 0>) [{686: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-na-pe', idx=1605, completeness=<Completeness.INCOMPLETE: 0>) [{2351: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-na-po', idx=1610, completeness=<Completeness.INCOMPLETE: 0>) [{703: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ni', idx=1616, completeness=<Completeness.INCOMPLETE: 0>) [{4780: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-no', idx=1618, completeness=<Completeness.INCOMPLETE: 0>) [{809: False, 5073: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-pa-rya-do', idx=1627, completeness=<Completeness.INCOMPLETE: 0>) [{4661: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-pa-si-ja', idx=1628, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-phu-sa-jo', idx=1632, completeness=<Completeness.INCOMPLETE: 0>) [{1231: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra', idx=1637, completeness=<Completeness.INCOMPLETE: 0>) [{148: False, 1201: False, 2333: False, 2380: False, 5155: False, 5393: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra-*82', idx=1639, completeness=<Completeness.INCOMPLETE: 0>) [{5062: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra-e-i-jo', idx=1644, completeness=<Completeness.INCOMPLETE: 0>) [{4099: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra-i-no', idx=1648, completeness=<Completeness.INCOMPLETE: 0>) [{2731: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra-se', idx=1656, completeness=<Completeness.INCOMPLETE: 0>) [{2390: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ra-te-ra', idx=1660, completeness=<Completeness.INCOMPLETE: 0>) [{2374: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-re', idx=1674, completeness=<Completeness.INCOMPLETE: 0>) [{2192: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-re-u', idx=1675, completeness=<Completeness.INCOMPLETE: 0>) [{4738: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ru-ka', idx=1683, completeness=<Completeness.INCOMPLETE: 0>) [{5724: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-sa', idx=1689, completeness=<Completeness.INCOMPLETE: 0>) [{704: False, 4753: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-sa-ta', idx=1692, completeness=<Completeness.INCOMPLETE: 0>) [{5143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-si-da', idx=1694, completeness=<Completeness.INCOMPLETE: 0>) [{4707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-ta', idx=1697, completeness=<Completeness.INCOMPLETE: 0>) [{689: False, 1029: False, 3483: False, 4702: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-te', idx=1708, completeness=<Completeness.INCOMPLETE: 0>) [{5130: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-te-ri-ja', idx=1709, completeness=<Completeness.INCOMPLETE: 0>) [{2795: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-te-ro', idx=1710, completeness=<Completeness.INCOMPLETE: 0>) [{217: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-tu-re-wi', idx=1715, completeness=<Completeness.INCOMPLETE: 0>) [{1237: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-tya', idx=1719, completeness=<Completeness.INCOMPLETE: 0>) [{1531: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-u-ja', idx=1722, completeness=<Completeness.INCOMPLETE: 0>) [{598: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-u-so', idx=1724, completeness=<Completeness.INCOMPLETE: 0>) [{5073: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-wa-ro', idx=1728, completeness=<Completeness.INCOMPLETE: 0>) [{3233: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-wi', idx=1730, completeness=<Completeness.INCOMPLETE: 0>) [{942: False, 2754: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-wi-ja', idx=1731, completeness=<Completeness.INCOMPLETE: 0>) [{613: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ka-wo-no', idx=1735, completeness=<Completeness.INCOMPLETE: 0>) [{2403: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-da-si', idx=1740, completeness=<Completeness.INCOMPLETE: 0>) [{5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-do-jo-no', idx=1742, completeness=<Completeness.INCOMPLETE: 0>) [{5114: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-do-ro', idx=1743, completeness=<Completeness.INCOMPLETE: 0>) [{1207: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ja', idx=1750, completeness=<Completeness.INCOMPLETE: 0>) [{1145: False, 2559: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ka-u-me-no', idx=1752, completeness=<Completeness.INCOMPLETE: 0>) [{5344: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ki-do', idx=1761, completeness=<Completeness.INCOMPLETE: 0>) [{5223: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-me', idx=1769, completeness=<Completeness.INCOMPLETE: 0>) [{1371: False, 4955: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-me-no', idx=1771, completeness=<Completeness.INCOMPLETE: 0>) [{135: False, 1186: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-mo', idx=1774, completeness=<Completeness.INCOMPLETE: 0>) [{2885: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ne', idx=1775, completeness=<Completeness.INCOMPLETE: 0>) [{1285: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ni-qe', idx=1777, completeness=<Completeness.INCOMPLETE: 0>) [{2386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ni-qe-te-we', idx=1778, completeness=<Completeness.INCOMPLETE: 0>) [{1217: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-nu', idx=1779, completeness=<Completeness.INCOMPLETE: 0>) [{1086: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-re-to', idx=1811, completeness=<Completeness.INCOMPLETE: 0>) [{2673: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ro-u-te', idx=1822, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False, 5723: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-rya', idx=1827, completeness=<Completeness.INCOMPLETE: 0>) [{4960: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-rya-u-na', idx=1828, completeness=<Completeness.INCOMPLETE: 0>) [{5386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-se-ni-wi-jo', idx=1833, completeness=<Completeness.INCOMPLETE: 0>) [{5032: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-si-jo', idx=1836, completeness=<Completeness.INCOMPLETE: 0>) [{1013: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-te-re', idx=1838, completeness=<Completeness.INCOMPLETE: 0>) [{5408: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-ti-ra-wo', idx=1840, completeness=<Completeness.INCOMPLETE: 0>) [{709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-to-ri', idx=1843, completeness=<Completeness.INCOMPLETE: 0>) [{1012: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-u', idx=1846, completeness=<Completeness.INCOMPLETE: 0>) [{2687: False, 3724: False, 4773: False, 4780: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-wa', idx=1851, completeness=<Completeness.INCOMPLETE: 0>) [{1077: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-wa-o', idx=1852, completeness=<Completeness.INCOMPLETE: 0>) [{5483: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-we-da', idx=1853, completeness=<Completeness.INCOMPLETE: 0>) [{2668: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ke-wo', idx=1857, completeness=<Completeness.INCOMPLETE: 0>) [{957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-jo-ne-we', idx=1870, completeness=<Completeness.INCOMPLETE: 0>) [{4342: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-jo-qe-u', idx=1871, completeness=<Completeness.INCOMPLETE: 0>) [{4704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ki-jo', idx=1874, completeness=<Completeness.INCOMPLETE: 0>) [{3004: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ma-to', idx=1878, completeness=<Completeness.INCOMPLETE: 0>) [{718: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ne-u', idx=1881, completeness=<Completeness.INCOMPLETE: 0>) [{2676: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ni', idx=1882, completeness=<Completeness.INCOMPLETE: 0>) [{1020: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ni-*56', idx=1883, completeness=<Completeness.INCOMPLETE: 0>) [{2345: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-nu-wa', idx=1888, completeness=<Completeness.INCOMPLETE: 0>) [{2704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ri', idx=1892, completeness=<Completeness.INCOMPLETE: 0>) [{1054: False, 1269: False, 2707: False, 5525: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ri-jo-de', idx=1896, completeness=<Completeness.INCOMPLETE: 0>) [{4119: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ri-te-wi-ja-pi', idx=1906, completeness=<Completeness.INCOMPLETE: 0>) [{5397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ro', idx=1908, completeness=<Completeness.INCOMPLETE: 0>) [{1693: False, 4007: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-sa', idx=1910, completeness=<Completeness.INCOMPLETE: 0>) [{3602: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-sa-qo', idx=1912, completeness=<Completeness.INCOMPLETE: 0>) [{3635: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-se', idx=1913, completeness=<Completeness.INCOMPLETE: 0>) [{1066: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-si', idx=1914, completeness=<Completeness.INCOMPLETE: 0>) [{134: False, 703: False, 1443: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-si-wi-jo', idx=1918, completeness=<Completeness.INCOMPLETE: 0>) [{661: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-si-wo', idx=1919, completeness=<Completeness.INCOMPLETE: 0>) [{698: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-so', idx=1920, completeness=<Completeness.INCOMPLETE: 0>) [{3038: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-su', idx=1921, completeness=<Completeness.INCOMPLETE: 0>) [{2280: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-te', idx=1925, completeness=<Completeness.INCOMPLETE: 0>) [{620: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ti-jo', idx=1927, completeness=<Completeness.INCOMPLETE: 0>) [{681: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ti-sa', idx=1930, completeness=<Completeness.INCOMPLETE: 0>) [{1426: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ti-se', idx=1931, completeness=<Completeness.INCOMPLETE: 0>) [{1431: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-ti-to', idx=1933, completeness=<Completeness.INCOMPLETE: 0>) [{5205: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-to-ni-ja', idx=1937, completeness=<Completeness.INCOMPLETE: 0>) [{4351: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-to-pi', idx=1938, completeness=<Completeness.INCOMPLETE: 0>) [{4445: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-wa', idx=1941, completeness=<Completeness.INCOMPLETE: 0>) [{5415: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-wa-ra', idx=1942, completeness=<Completeness.INCOMPLETE: 0>) [{5416: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ki-wi-ri', idx=1944, completeness=<Completeness.INCOMPLETE: 0>) [{5078: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-de', idx=1952, completeness=<Completeness.INCOMPLETE: 0>) [{4219: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-jo', idx=1959, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ki', idx=1961, completeness=<Completeness.INCOMPLETE: 0>) [{2682: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ki-te-jo', idx=1966, completeness=<Completeness.INCOMPLETE: 0>) [{5165: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ko', idx=1967, completeness=<Completeness.INCOMPLETE: 0>) [{2686: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ma-ra', idx=1971, completeness=<Completeness.INCOMPLETE: 0>) [{970: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-me-no', idx=1978, completeness=<Completeness.INCOMPLETE: 0>) [{2674: False, 2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-pe-re-we', idx=1994, completeness=<Completeness.INCOMPLETE: 0>) [{4026: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-pu', idx=1999, completeness=<Completeness.INCOMPLETE: 0>) [{1543: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ri-ja', idx=2009, completeness=<Completeness.INCOMPLETE: 0>) [{4157: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ro-ka', idx=2022, completeness=<Completeness.INCOMPLETE: 0>) [{2582: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ro-ki-no', idx=2025, completeness=<Completeness.INCOMPLETE: 0>) [{1222: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ru-we-ja', idx=2040, completeness=<Completeness.INCOMPLETE: 0>) [{4257: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-si-ja', idx=2043, completeness=<Completeness.INCOMPLETE: 0>) [{4462: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-so-ni-ja', idx=2047, completeness=<Completeness.INCOMPLETE: 0>) [{591: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-so-u-to-qe', idx=2049, completeness=<Completeness.INCOMPLETE: 0>) [{2850: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-ta-o', idx=2050, completeness=<Completeness.INCOMPLETE: 0>) [{3542: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-to-no-ko', idx=2063, completeness=<Completeness.INCOMPLETE: 0>) [{4854: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-to-wa', idx=2066, completeness=<Completeness.INCOMPLETE: 0>) [{5064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-u-ta-po', idx=2073, completeness=<Completeness.INCOMPLETE: 0>) [{5625: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-we-jo', idx=2078, completeness=<Completeness.INCOMPLETE: 0>) [{3307: False, 3528: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ko-wo-a', idx=2081, completeness=<Completeness.INCOMPLETE: 0>) [{5710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku', idx=2083, completeness=<Completeness.INCOMPLETE: 0>) [{16: False, 51: False, 962: False, 1115: False, 1310: False, 1403: False, 1590: False, 1752: False, 1916: False, 1967: False, 2074: False, 2701: False, 2752: False, 3902: False, 4212: False, 5372: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-da-ra-ro', idx=2087, completeness=<Completeness.INCOMPLETE: 0>) [{4271: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-do-ni-jo', idx=2091, completeness=<Completeness.INCOMPLETE: 0>) [{968: False, 4205: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-i-so', idx=2092, completeness=<Completeness.INCOMPLETE: 0>) [{3021: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ka-ra-re', idx=2099, completeness=<Completeness.INCOMPLETE: 0>) [{938: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ke-mo', idx=2103, completeness=<Completeness.INCOMPLETE: 0>) [{708: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ku-mo', idx=2108, completeness=<Completeness.INCOMPLETE: 0>) [{1533: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ma-to', idx=2109, completeness=<Completeness.INCOMPLETE: 0>) [{710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-mi-no-jo', idx=2112, completeness=<Completeness.INCOMPLETE: 0>) [{2330: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-mi-ro', idx=2113, completeness=<Completeness.INCOMPLETE: 0>) [{3434: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-na', idx=2117, completeness=<Completeness.INCOMPLETE: 0>) [{2026: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-na-je', idx=2119, completeness=<Completeness.INCOMPLETE: 0>) [{2029: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-no', idx=2124, completeness=<Completeness.INCOMPLETE: 0>) [{647: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-no-o', idx=2125, completeness=<Completeness.INCOMPLETE: 0>) [{2787: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-pa', idx=2126, completeness=<Completeness.INCOMPLETE: 0>) [{1433: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-re', idx=2141, completeness=<Completeness.INCOMPLETE: 0>) [{1397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ru', idx=2146, completeness=<Completeness.INCOMPLETE: 0>) [{1030: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ru-ni-ta', idx=2152, completeness=<Completeness.INCOMPLETE: 0>) [{1245: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ru-no', idx=2153, completeness=<Completeness.INCOMPLETE: 0>) [{2674: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ru-so-no', idx=2159, completeness=<Completeness.INCOMPLETE: 0>) [{4377: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-su-pa-ta', idx=2171, completeness=<Completeness.INCOMPLETE: 0>) [{3511: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ta', idx=2174, completeness=<Completeness.INCOMPLETE: 0>) [{1289: False, 1585: False, 1833: False, 1917: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ta-i-si', idx=2176, completeness=<Completeness.INCOMPLETE: 0>) [{1471: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ta-mi', idx=2178, completeness=<Completeness.INCOMPLETE: 0>) [{4290: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-ta-ti', idx=2180, completeness=<Completeness.INCOMPLETE: 0>) [{1246: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-we-jo', idx=2200, completeness=<Completeness.INCOMPLETE: 0>) [{1513: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ku-wo', idx=2201, completeness=<Completeness.INCOMPLETE: 0>) [{4386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='m', idx=2202, completeness=<Completeness.INCOMPLETE: 0>) [{2897: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-da-jo', idx=2205, completeness=<Completeness.INCOMPLETE: 0>) [{2718: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-di-jo', idx=2207, completeness=<Completeness.INCOMPLETE: 0>) [{5726: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ja-ro', idx=2210, completeness=<Completeness.INCOMPLETE: 0>) [{2714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ja-to', idx=2211, completeness=<Completeness.INCOMPLETE: 0>) [{1918: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-jo', idx=2212, completeness=<Completeness.INCOMPLETE: 0>) [{2695: False, 3679: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-jo-wo', idx=2213, completeness=<Completeness.INCOMPLETE: 0>) [{4766: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ke-ra-mo', idx=2217, completeness=<Completeness.INCOMPLETE: 0>) [{964: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ki-nu-wo', idx=2219, completeness=<Completeness.INCOMPLETE: 0>) [{76: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ko-ro', idx=2221, completeness=<Completeness.INCOMPLETE: 0>) [{3887: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ku', idx=2222, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-na-je-u', idx=2225, completeness=<Completeness.INCOMPLETE: 0>) [{689: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ne', idx=2228, completeness=<Completeness.INCOMPLETE: 0>) [{5150: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-no-ne', idx=2231, completeness=<Completeness.INCOMPLETE: 0>) [{698: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-no-we', idx=2233, completeness=<Completeness.INCOMPLETE: 0>) [{3601: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-pu', idx=2234, completeness=<Completeness.INCOMPLETE: 0>) [{2411: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-qa-to', idx=2235, completeness=<Completeness.INCOMPLETE: 0>) [{3971: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ra-me-na', idx=2238, completeness=<Completeness.INCOMPLETE: 0>) [{5407: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ra-ne', idx=2239, completeness=<Completeness.INCOMPLETE: 0>) [{5153: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ri-ne', idx=2257, completeness=<Completeness.INCOMPLETE: 0>) [{2402: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-si-jo', idx=2270, completeness=<Completeness.INCOMPLETE: 0>) [{3878: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-so', idx=2271, completeness=<Completeness.INCOMPLETE: 0>) [{5409: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ta-qe', idx=2278, completeness=<Completeness.INCOMPLETE: 0>) [{2382: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ti', idx=2285, completeness=<Completeness.INCOMPLETE: 0>) [{1268: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ti-ri', idx=2288, completeness=<Completeness.INCOMPLETE: 0>) [{976: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-to', idx=2289, completeness=<Completeness.INCOMPLETE: 0>) [{5467: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-u-do', idx=2293, completeness=<Completeness.INCOMPLETE: 0>) [{961: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-we', idx=2296, completeness=<Completeness.INCOMPLETE: 0>) [{1822: False, 3648: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-wo', idx=2297, completeness=<Completeness.INCOMPLETE: 0>) [{4471: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ma-ze-to', idx=2298, completeness=<Completeness.INCOMPLETE: 0>) [{1363: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-de', idx=2302, completeness=<Completeness.INCOMPLETE: 0>) [{2713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ja', idx=2304, completeness=<Completeness.INCOMPLETE: 0>) [{713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-jo', idx=2305, completeness=<Completeness.INCOMPLETE: 0>) [{5053: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ki-ta', idx=2308, completeness=<Completeness.INCOMPLETE: 0>) [{1713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ki-to-de', idx=2310, completeness=<Completeness.INCOMPLETE: 0>) [{5044: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-na-qe', idx=2313, completeness=<Completeness.INCOMPLETE: 0>) [{730: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-na-wa-te', idx=2314, completeness=<Completeness.INCOMPLETE: 0>) [{1307: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ne-u', idx=2315, completeness=<Completeness.INCOMPLETE: 0>) [{2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-no-no', idx=2319, completeness=<Completeness.INCOMPLETE: 0>) [{4230: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-qe', idx=2322, completeness=<Completeness.INCOMPLETE: 0>) [{2417: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-re-ja', idx=2326, completeness=<Completeness.INCOMPLETE: 0>) [{1071: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-re-ti-rya-o', idx=2329, completeness=<Completeness.INCOMPLETE: 0>) [{4641: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ri-du', idx=2334, completeness=<Completeness.INCOMPLETE: 0>) [{1236: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ri-te-o', idx=2338, completeness=<Completeness.INCOMPLETE: 0>) [{605: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ri-ti', idx=2340, completeness=<Completeness.INCOMPLETE: 0>) [{5454: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ri-wo', idx=2343, completeness=<Completeness.INCOMPLETE: 0>) [{2750: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-si', idx=2347, completeness=<Completeness.INCOMPLETE: 0>) [{2138: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ta', idx=2348, completeness=<Completeness.INCOMPLETE: 0>) [{4738: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ta-ra-wo', idx=2360, completeness=<Completeness.INCOMPLETE: 0>) [{2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-te', idx=2363, completeness=<Completeness.INCOMPLETE: 0>) [{1415: False, 5433: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-ti', idx=2367, completeness=<Completeness.INCOMPLETE: 0>) [{1439: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-to', idx=2370, completeness=<Completeness.INCOMPLETE: 0>) [{677: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-tu', idx=2373, completeness=<Completeness.INCOMPLETE: 0>) [{1329: False, 2728: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='me-u', idx=2377, completeness=<Completeness.INCOMPLETE: 0>) [{2555: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-dwe', idx=2395, completeness=<Completeness.INCOMPLETE: 0>) [{2686: False, 4159: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-ja', idx=2396, completeness=<Completeness.INCOMPLETE: 0>) [{1555: False, 1572: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-jo', idx=2399, completeness=<Completeness.INCOMPLETE: 0>) [{2720: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-ka', idx=2401, completeness=<Completeness.INCOMPLETE: 0>) [{4993: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-ko', idx=2406, completeness=<Completeness.INCOMPLETE: 0>) [{948: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-ni-to', idx=2409, completeness=<Completeness.INCOMPLETE: 0>) [{2995: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-ta-qe', idx=2418, completeness=<Completeness.INCOMPLETE: 0>) [{2331: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mi-to-no', idx=2421, completeness=<Completeness.INCOMPLETE: 0>) [{4704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mo-ke-re-we-i', idx=2427, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mo-ni-ja-ro', idx=2428, completeness=<Completeness.INCOMPLETE: 0>) [{3563: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mu-da', idx=2440, completeness=<Completeness.INCOMPLETE: 0>) [{3587: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mu-ki-ti', idx=2443, completeness=<Completeness.INCOMPLETE: 0>) [{1190: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mu-ti-ja', idx=2448, completeness=<Completeness.INCOMPLETE: 0>) [{1191: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='mu-to', idx=2451, completeness=<Completeness.INCOMPLETE: 0>) [{3963: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na', idx=2454, completeness=<Completeness.INCOMPLETE: 0>) [{407: False, 516: False, 1085: False, 1404: False, 1467: False, 1589: False, 1745: False, 1835: False, 1920: False, 1968: False, 2333: False, 2654: False, 2663: False, 3194: False, 4203: False, 4476: False, 5357: False, 5412: False, 5431: False, 5533: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-de', idx=2455, completeness=<Completeness.INCOMPLETE: 0>) [{5014: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-i', idx=2458, completeness=<Completeness.INCOMPLETE: 0>) [{2835: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-jo', idx=2460, completeness=<Completeness.INCOMPLETE: 0>) [{2780: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-ka-te', idx=2461, completeness=<Completeness.INCOMPLETE: 0>) [{5397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-ko-to', idx=2463, completeness=<Completeness.INCOMPLETE: 0>) [{678: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-me', idx=2465, completeness=<Completeness.INCOMPLETE: 0>) [{5119: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-ni', idx=2466, completeness=<Completeness.INCOMPLETE: 0>) [{1062: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-o', idx=2467, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-po', idx=2469, completeness=<Completeness.INCOMPLETE: 0>) [{679: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-pu', idx=2470, completeness=<Completeness.INCOMPLETE: 0>) [{921: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-ro', idx=2472, completeness=<Completeness.INCOMPLETE: 0>) [{2677: False, 3690: False, 3913: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-so', idx=2475, completeness=<Completeness.INCOMPLETE: 0>) [{1756: False, 4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-ta', idx=2478, completeness=<Completeness.INCOMPLETE: 0>) [{5745: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-to-de', idx=2480, completeness=<Completeness.INCOMPLETE: 0>) [{4115: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-u-si-ke-re', idx=2482, completeness=<Completeness.INCOMPLETE: 0>) [{977: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-wo', idx=2487, completeness=<Completeness.INCOMPLETE: 0>) [{4785: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='na-zo', idx=2488, completeness=<Completeness.INCOMPLETE: 0>) [{2836: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-a', idx=2490, completeness=<Completeness.INCOMPLETE: 0>) [{178: False, 222: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-do-u-re', idx=2494, completeness=<Completeness.INCOMPLETE: 0>) [{5706: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-ja', idx=2498, completeness=<Completeness.INCOMPLETE: 0>) [{1837: False, 2264: False, 2368: False, 4424: False, 5121: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-je-ta', idx=2499, completeness=<Completeness.INCOMPLETE: 0>) [{3901: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-jo', idx=2500, completeness=<Completeness.INCOMPLETE: 0>) [{664: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-ki', idx=2502, completeness=<Completeness.INCOMPLETE: 0>) [{783: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-ko', idx=2505, completeness=<Completeness.INCOMPLETE: 0>) [{4697: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-o', idx=2507, completeness=<Completeness.INCOMPLETE: 0>) [{683: False, 2651: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-re-we', idx=2514, completeness=<Completeness.INCOMPLETE: 0>) [{5482: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-ri-jo', idx=2515, completeness=<Completeness.INCOMPLETE: 0>) [{954: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-to', idx=2521, completeness=<Completeness.INCOMPLETE: 0>) [{862: False, 2396: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-u', idx=2522, completeness=<Completeness.INCOMPLETE: 0>) [{877: False, 1084: False, 2967: False, 5070: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-we', idx=2524, completeness=<Completeness.INCOMPLETE: 0>) [{143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ne-wi', idx=2527, completeness=<Completeness.INCOMPLETE: 0>) [{4784: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni', idx=2532, completeness=<Completeness.INCOMPLETE: 0>) [{1905: False, 2375: False, 2405: False, 5667: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ja', idx=2533, completeness=<Completeness.INCOMPLETE: 0>) [{62: False, 598: False, 689: False, 1290: False, 1799: False, 1921: False, 2043: False, 4052: False, 4168: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ja-de', idx=2534, completeness=<Completeness.INCOMPLETE: 0>) [{4955: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ja-mo', idx=2535, completeness=<Completeness.INCOMPLETE: 0>) [{4020: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ja-so', idx=2536, completeness=<Completeness.INCOMPLETE: 0>) [{3306: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-jo', idx=2537, completeness=<Completeness.INCOMPLETE: 0>) [{1832: False, 2672: False, 3914: False, 4156: False, 4159: False, 4221: False, 5264: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-jo-jo', idx=2538, completeness=<Completeness.INCOMPLETE: 0>) [{5666: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ki-jo', idx=2539, completeness=<Completeness.INCOMPLETE: 0>) [{153: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ko', idx=2540, completeness=<Completeness.INCOMPLETE: 0>) [{407: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-mo', idx=2541, completeness=<Completeness.INCOMPLETE: 0>) [{3777: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-pi', idx=2542, completeness=<Completeness.INCOMPLETE: 0>) [{4779: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-sa', idx=2543, completeness=<Completeness.INCOMPLETE: 0>) [{1254: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-ta', idx=2544, completeness=<Completeness.INCOMPLETE: 0>) [{2659: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ni-to', idx=2545, completeness=<Completeness.INCOMPLETE: 0>) [{2863: False, 2898: False, 3530: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no', idx=2546, completeness=<Completeness.INCOMPLETE: 0>) [{215: False, 259: False, 304: False, 598: False, 648: False, 670: False, 881: False, 949: False, 1096: False, 1106: False, 1131: False, 1238: False, 1332: False, 1464: False, 1517: False, 1667: False, 1683: False, 1684: False, 2028: False, 2130: False, 2218: False, 2430: False, 2459: False, 2460: False, 2461: False, 2462: False, 2463: False, 2464: False, 2473: False, 2478: False, 2529: False, 2678: False, 2681: False, 2773: False, 2811: False, 2852: False, 3410: False, 3417: False, 3668: False, 3804: False, 4012: False, 4029: False, 4122: False, 4235: False, 4407: False, 4723: False, 4730: False, 5260: False, 5395: False, 5746: False, 5753: False, 5758: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-da', idx=2547, completeness=<Completeness.INCOMPLETE: 0>) [{1096: False, 2113: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-da-ma', idx=2548, completeness=<Completeness.INCOMPLETE: 0>) [{1004: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-di-mi-zo-jo', idx=2550, completeness=<Completeness.INCOMPLETE: 0>) [{3907: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-du', idx=2553, completeness=<Completeness.INCOMPLETE: 0>) [{701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-ka-ra-o-re', idx=2555, completeness=<Completeness.INCOMPLETE: 0>) [{5241: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-mo', idx=2557, completeness=<Completeness.INCOMPLETE: 0>) [{681: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-no', idx=2558, completeness=<Completeness.INCOMPLETE: 0>) [{1422: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-qa-ta', idx=2562, completeness=<Completeness.INCOMPLETE: 0>) [{2984: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-qe', idx=2563, completeness=<Completeness.INCOMPLETE: 0>) [{2859: False, 2861: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-re', idx=2564, completeness=<Completeness.INCOMPLETE: 0>) [{686: False, 2806: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-ro', idx=2567, completeness=<Completeness.INCOMPLETE: 0>) [{40: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-so', idx=2570, completeness=<Completeness.INCOMPLETE: 0>) [{3875: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-ta-na', idx=2571, completeness=<Completeness.INCOMPLETE: 0>) [{3059: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-to', idx=2572, completeness=<Completeness.INCOMPLETE: 0>) [{3691: False, 3811: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-u-ro', idx=2573, completeness=<Completeness.INCOMPLETE: 0>) [{3727: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-wi-jo', idx=2574, completeness=<Completeness.INCOMPLETE: 0>) [{5122: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-wo', idx=2575, completeness=<Completeness.INCOMPLETE: 0>) [{4766: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='no-wo-ko', idx=2576, completeness=<Completeness.INCOMPLETE: 0>) [{4677: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-a', idx=2577, completeness=<Completeness.INCOMPLETE: 0>) [{1068: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-ka', idx=2578, completeness=<Completeness.INCOMPLETE: 0>) [{3392: False, 4325: False, 4371: False, 4450: False, 4451: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-po', idx=2579, completeness=<Completeness.INCOMPLETE: 0>) [{74: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-ra-qa', idx=2580, completeness=<Completeness.INCOMPLETE: 0>) [{2937: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-se', idx=2581, completeness=<Completeness.INCOMPLETE: 0>) [{1741: False, 2430: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-we', idx=2585, completeness=<Completeness.INCOMPLETE: 0>) [{155: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-we-jo', idx=2586, completeness=<Completeness.INCOMPLETE: 0>) [{4284: False, 5070: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-we-we', idx=2587, completeness=<Completeness.INCOMPLETE: 0>) [{5390: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-wi-jo', idx=2588, completeness=<Completeness.INCOMPLETE: 0>) [{5048: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nu-wo', idx=2589, completeness=<Completeness.INCOMPLETE: 0>) [{598: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nwa', idx=2590, completeness=<Completeness.INCOMPLETE: 0>) [{3663: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nwa-jo', idx=2591, completeness=<Completeness.INCOMPLETE: 0>) [{635: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='nwa-re', idx=2592, completeness=<Completeness.INCOMPLETE: 0>) [{876: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-*64', idx=2597, completeness=<Completeness.INCOMPLETE: 0>) [{4704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-a-pu', idx=2600, completeness=<Completeness.INCOMPLETE: 0>) [{756: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-da', idx=2601, completeness=<Completeness.INCOMPLETE: 0>) [{2701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-da-sa-to', idx=2606, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False, 5426: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-de-qa-ha', idx=2610, completeness=<Completeness.INCOMPLETE: 0>) [{5264: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ke-te', idx=2626, completeness=<Completeness.INCOMPLETE: 0>) [{953: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ki', idx=2629, completeness=<Completeness.INCOMPLETE: 0>) [{1367: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ma', idx=2641, completeness=<Completeness.INCOMPLETE: 0>) [{5577: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-me', idx=2642, completeness=<Completeness.INCOMPLETE: 0>) [{4723: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-me-no', idx=2643, completeness=<Completeness.INCOMPLETE: 0>) [{2727: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-mo', idx=2646, completeness=<Completeness.INCOMPLETE: 0>) [{485: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ne', idx=2656, completeness=<Completeness.INCOMPLETE: 0>) [{2835: False, 4198: False, 4211: False, 4514: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ne-ryo-i', idx=2657, completeness=<Completeness.INCOMPLETE: 0>) [{4520: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-no-ka-ra', idx=2661, completeness=<Completeness.INCOMPLETE: 0>) [{5155: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-no-we-wo-ro', idx=2662, completeness=<Completeness.INCOMPLETE: 0>) [{2301: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-pa-we', idx=2671, completeness=<Completeness.INCOMPLETE: 0>) [{3601: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-pe-re', idx=2679, completeness=<Completeness.INCOMPLETE: 0>) [{919: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-pe-re-ra', idx=2680, completeness=<Completeness.INCOMPLETE: 0>) [{730: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-pe-ri', idx=2682, completeness=<Completeness.INCOMPLETE: 0>) [{5722: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-pe-te-wo-qe', idx=2693, completeness=<Completeness.INCOMPLETE: 0>) [{4280: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-po-ro-u-si-jo', idx=2726, completeness=<Completeness.INCOMPLETE: 0>) [{2670: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ra', idx=2732, completeness=<Completeness.INCOMPLETE: 0>) [{3247: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ra-qe-te', idx=2733, completeness=<Completeness.INCOMPLETE: 0>) [{5394: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ru', idx=2752, completeness=<Completeness.INCOMPLETE: 0>) [{711: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-se-ko-do', idx=2757, completeness=<Completeness.INCOMPLETE: 0>) [{3964: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-si-to', idx=2758, completeness=<Completeness.INCOMPLETE: 0>) [{5432: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-ta-ke', idx=2759, completeness=<Completeness.INCOMPLETE: 0>) [{1020: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-to-ro', idx=2772, completeness=<Completeness.INCOMPLETE: 0>) [{3728: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-tya-no', idx=2781, completeness=<Completeness.INCOMPLETE: 0>) [{3617: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-u', idx=2782, completeness=<Completeness.INCOMPLETE: 0>) [{2674: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-u-ka', idx=2785, completeness=<Completeness.INCOMPLETE: 0>) [{2345: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-u-pa-ro-ke-ne', idx=2788, completeness=<Completeness.INCOMPLETE: 0>) [{4666: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-u-qe-po', idx=2790, completeness=<Completeness.INCOMPLETE: 0>) [{4456: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-wa', idx=2795, completeness=<Completeness.INCOMPLETE: 0>) [{5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-wi-de-ta', idx=2801, completeness=<Completeness.INCOMPLETE: 0>) [{5422: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-wo', idx=2806, completeness=<Completeness.INCOMPLETE: 0>) [{5710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='o-za-mi', idx=2811, completeness=<Completeness.INCOMPLETE: 0>) [{4699: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-*34-so', idx=2814, completeness=<Completeness.INCOMPLETE: 0>) [{1006: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-da', idx=2815, completeness=<Completeness.INCOMPLETE: 0>) [{237: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ja', idx=2828, completeness=<Completeness.INCOMPLETE: 0>) [{680: False, 729: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ke-te-ri', idx=2842, completeness=<Completeness.INCOMPLETE: 0>) [{2389: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ke-u', idx=2843, completeness=<Completeness.INCOMPLETE: 0>) [{5289: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ki', idx=2845, completeness=<Completeness.INCOMPLETE: 0>) [{679: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ki-ja', idx=2846, completeness=<Completeness.INCOMPLETE: 0>) [{5000: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ko-qe', idx=2857, completeness=<Completeness.INCOMPLETE: 0>) [{2856: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-mo', idx=2863, completeness=<Completeness.INCOMPLETE: 0>) [{3781: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-na', idx=2864, completeness=<Completeness.INCOMPLETE: 0>) [{1146: False, 3805: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-no', idx=2869, completeness=<Completeness.INCOMPLETE: 0>) [{993: False, 1222: False, 2708: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-pu', idx=2873, completeness=<Completeness.INCOMPLETE: 0>) [{914: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ra', idx=2878, completeness=<Completeness.INCOMPLETE: 0>) [{4174: False, 5377: False, 5513: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ra-ke-we-qe', idx=2884, completeness=<Completeness.INCOMPLETE: 0>) [{5345: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ra-u-jo', idx=2893, completeness=<Completeness.INCOMPLETE: 0>) [{4140: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ri-so', idx=2898, completeness=<Completeness.INCOMPLETE: 0>) [{5195: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-si-ja', idx=2907, completeness=<Completeness.INCOMPLETE: 0>) [{695: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ta-re', idx=2913, completeness=<Completeness.INCOMPLETE: 0>) [{935: False, 1432: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-te-o', idx=2919, completeness=<Completeness.INCOMPLETE: 0>) [{1539: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-ti-jo-ro', idx=2921, completeness=<Completeness.INCOMPLETE: 0>) [{2416: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-to', idx=2922, completeness=<Completeness.INCOMPLETE: 0>) [{4928: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-u-ro', idx=2925, completeness=<Completeness.INCOMPLETE: 0>) [{3070: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-u-ta', idx=2926, completeness=<Completeness.INCOMPLETE: 0>) [{4999: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-we', idx=2929, completeness=<Completeness.INCOMPLETE: 0>) [{4447: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pa-we-pi', idx=2933, completeness=<Completeness.INCOMPLETE: 0>) [{4248: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-di-e', idx=2949, completeness=<Completeness.INCOMPLETE: 0>) [{5445: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-do', idx=2955, completeness=<Completeness.INCOMPLETE: 0>) [{1202: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ka', idx=2957, completeness=<Completeness.INCOMPLETE: 0>) [{2403: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ki-ti', idx=2961, completeness=<Completeness.INCOMPLETE: 0>) [{2192: False, 4442: False, 4580: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-o', idx=2971, completeness=<Completeness.INCOMPLETE: 0>) [{5540: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-qe', idx=2974, completeness=<Completeness.INCOMPLETE: 0>) [{1747: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ra', idx=2980, completeness=<Completeness.INCOMPLETE: 0>) [{2374: False, 3877: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ra-te-ro', idx=2984, completeness=<Completeness.INCOMPLETE: 0>) [{1014: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-re-*82-ta', idx=2990, completeness=<Completeness.INCOMPLETE: 0>) [{2669: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ri-mo', idx=3014, completeness=<Completeness.INCOMPLETE: 0>) [{3686: False, 4737: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ru-si', idx=3029, completeness=<Completeness.INCOMPLETE: 0>) [{4214: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-rya-wo', idx=3036, completeness=<Completeness.INCOMPLETE: 0>) [{946: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-ryo', idx=3037, completeness=<Completeness.INCOMPLETE: 0>) [{3872: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-se-to', idx=3040, completeness=<Completeness.INCOMPLETE: 0>) [{4767: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-se-wi-ja', idx=3041, completeness=<Completeness.INCOMPLETE: 0>) [{5487: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pe-to-me', idx=3047, completeness=<Completeness.INCOMPLETE: 0>) [{683: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='phu', idx=3052, completeness=<Completeness.INCOMPLETE: 0>) [{2167: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='phu-te-me-no', idx=3061, completeness=<Completeness.INCOMPLETE: 0>) [{4960: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi', idx=3065, completeness=<Completeness.INCOMPLETE: 0>) [{479: False, 697: False, 918: False, 1253: False, 1269: False, 1868: False, 2002: False, 2004: False, 2408: False, 2654: False, 4261: False, 4295: False, 4325: False, 4443: False, 5200: False, 5685: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-a-ze-ra', idx=3069, completeness=<Completeness.INCOMPLETE: 0>) [{2107: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ja', idx=3072, completeness=<Completeness.INCOMPLETE: 0>) [{2527: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ja-qe', idx=3075, completeness=<Completeness.INCOMPLETE: 0>) [{4235: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ja-to', idx=3078, completeness=<Completeness.INCOMPLETE: 0>) [{2713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-jo', idx=3080, completeness=<Completeness.INCOMPLETE: 0>) [{4707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ka', idx=3081, completeness=<Completeness.INCOMPLETE: 0>) [{2412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ko', idx=3088, completeness=<Completeness.INCOMPLETE: 0>) [{4768: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ku-e-wi', idx=3089, completeness=<Completeness.INCOMPLETE: 0>) [{5085: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ma', idx=3090, completeness=<Completeness.INCOMPLETE: 0>) [{687: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-po-to', idx=3096, completeness=<Completeness.INCOMPLETE: 0>) [{1351: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ra', idx=3097, completeness=<Completeness.INCOMPLETE: 0>) [{1147: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-rai', idx=3105, completeness=<Completeness.INCOMPLETE: 0>) [{5408: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ri', idx=3107, completeness=<Completeness.INCOMPLETE: 0>) [{214: False, 1033: False, 1250: False, 4988: False, 5363: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ri-da-ke', idx=3108, completeness=<Completeness.INCOMPLETE: 0>) [{2360: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ri-ja', idx=3111, completeness=<Completeness.INCOMPLETE: 0>) [{1025: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ri-je-te-re', idx=3116, completeness=<Completeness.INCOMPLETE: 0>) [{4707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ri-jo', idx=3117, completeness=<Completeness.INCOMPLETE: 0>) [{2112: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ro-i', idx=3127, completeness=<Completeness.INCOMPLETE: 0>) [{2681: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ro-qa-wo', idx=3133, completeness=<Completeness.INCOMPLETE: 0>) [{2675: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ro-qo-ro', idx=3135, completeness=<Completeness.INCOMPLETE: 0>) [{4754: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-ru', idx=3139, completeness=<Completeness.INCOMPLETE: 0>) [{1575: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-rya-mo', idx=3141, completeness=<Completeness.INCOMPLETE: 0>) [{1462: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pi-rya-ta-je', idx=3142, completeness=<Completeness.INCOMPLETE: 0>) [{1549: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-*34', idx=3153, completeness=<Completeness.INCOMPLETE: 0>) [{315: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ki', idx=3165, completeness=<Completeness.INCOMPLETE: 0>) [{992: False, 5529: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ki-ro', idx=3166, completeness=<Completeness.INCOMPLETE: 0>) [{4272: False, 4450: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ko-me', idx=3170, completeness=<Completeness.INCOMPLETE: 0>) [{1319: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ku', idx=3172, completeness=<Completeness.INCOMPLETE: 0>) [{2780: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-mi-jo', idx=3180, completeness=<Completeness.INCOMPLETE: 0>) [{672: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ni-ki', idx=3187, completeness=<Completeness.INCOMPLETE: 0>) [{1739: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-pu-re-jo', idx=3197, completeness=<Completeness.INCOMPLETE: 0>) [{1223: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ra', idx=3203, completeness=<Completeness.INCOMPLETE: 0>) [{5475: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-re-no-tu-te', idx=3207, completeness=<Completeness.INCOMPLETE: 0>) [{5367: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ri', idx=3209, completeness=<Completeness.INCOMPLETE: 0>) [{3199: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ro-du', idx=3216, completeness=<Completeness.INCOMPLETE: 0>) [{4849: False, 4951: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ro-we', idx=3238, completeness=<Completeness.INCOMPLETE: 0>) [{1230: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ru-o', idx=3246, completeness=<Completeness.INCOMPLETE: 0>) [{1327: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ta', idx=3269, completeness=<Completeness.INCOMPLETE: 0>) [{1050: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ti', idx=3273, completeness=<Completeness.INCOMPLETE: 0>) [{682: False, 2368: False, 4239: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='po-ti-ni-ja-we-i-jo', idx=3279, completeness=<Completeness.INCOMPLETE: 0>) [{3519: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pte', idx=3292, completeness=<Completeness.INCOMPLETE: 0>) [{1495: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pte-re', idx=3295, completeness=<Completeness.INCOMPLETE: 0>) [{489: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pte-re-e', idx=3296, completeness=<Completeness.INCOMPLETE: 0>) [{557: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pte-si', idx=3298, completeness=<Completeness.INCOMPLETE: 0>) [{3998: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pte-we', idx=3299, completeness=<Completeness.INCOMPLETE: 0>) [{5079: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu', idx=3300, completeness=<Completeness.INCOMPLETE: 0>) [{632: False, 633: False, 686: False, 1405: False, 2012: False, 2244: False, 5050: False, 5752: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ka', idx=3303, completeness=<Completeness.INCOMPLETE: 0>) [{2435: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ko', idx=3310, completeness=<Completeness.INCOMPLETE: 0>) [{3803: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ma', idx=3315, completeness=<Completeness.INCOMPLETE: 0>) [{3692: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-mo-ne', idx=3317, completeness=<Completeness.INCOMPLETE: 0>) [{836: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-na', idx=3318, completeness=<Completeness.INCOMPLETE: 0>) [{1906: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-pte', idx=3324, completeness=<Completeness.INCOMPLETE: 0>) [{5735: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ra', idx=3325, completeness=<Completeness.INCOMPLETE: 0>) [{689: False, 2712: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ra-ko', idx=3326, completeness=<Completeness.INCOMPLETE: 0>) [{958: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-sa-ra', idx=3335, completeness=<Completeness.INCOMPLETE: 0>) [{1238: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='pu-ti', idx=3345, completeness=<Completeness.INCOMPLETE: 0>) [{5727: False, 5733: False, 5744: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa', idx=3354, completeness=<Completeness.INCOMPLETE: 0>) [{58: False, 241: False, 743: False, 745: False, 993: False, 1031: False, 1117: False, 1339: False, 1391: False, 1459: False, 1592: False, 1646: False, 1651: False, 1919: False, 1926: False, 3512: False, 3603: False, 4018: False, 4079: False, 4505: False, 5553: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-ka', idx=3361, completeness=<Completeness.INCOMPLETE: 0>) [{4509: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-ka-na-pi', idx=3362, completeness=<Completeness.INCOMPLETE: 0>) [{912: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-mi-ki', idx=3367, completeness=<Completeness.INCOMPLETE: 0>) [{3725: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-na', idx=3370, completeness=<Completeness.INCOMPLETE: 0>) [{704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-no', idx=3372, completeness=<Completeness.INCOMPLETE: 0>) [{1249: False, 2160: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-nu-we', idx=3374, completeness=<Completeness.INCOMPLETE: 0>) [{3982: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-ra-su-ti-jo', idx=3383, completeness=<Completeness.INCOMPLETE: 0>) [{11: False, 964: False, 3176: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-rya-ri', idx=3388, completeness=<Completeness.INCOMPLETE: 0>) [{1467: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-sa', idx=3393, completeness=<Completeness.INCOMPLETE: 0>) [{1270: False, 4038: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-ta', idx=3400, completeness=<Completeness.INCOMPLETE: 0>) [{1553: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-te', idx=3401, completeness=<Completeness.INCOMPLETE: 0>) [{5075: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-tya', idx=3404, completeness=<Completeness.INCOMPLETE: 0>) [{1380: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qa-wo', idx=3405, completeness=<Completeness.INCOMPLETE: 0>) [{2707: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe', idx=3406, completeness=<Completeness.INCOMPLETE: 0>) [{696: False, 698: False, 924: False, 1019: False, 1067: False, 1101: False, 2382: False, 2417: False, 2729: False, 2773: False, 2784: False, 2855: False, 2860: False, 2864: False, 4723: False, 5264: False, 5565: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-na', idx=3409, completeness=<Completeness.INCOMPLETE: 0>) [{884: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-ra', idx=3415, completeness=<Completeness.INCOMPLETE: 0>) [{1552: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-ra-si', idx=3419, completeness=<Completeness.INCOMPLETE: 0>) [{4021: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-re', idx=3422, completeness=<Completeness.INCOMPLETE: 0>) [{886: False, 4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-re-jo', idx=3423, completeness=<Completeness.INCOMPLETE: 0>) [{4271: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-re-me', idx=3425, completeness=<Completeness.INCOMPLETE: 0>) [{5152: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-re-te-u', idx=3431, completeness=<Completeness.INCOMPLETE: 0>) [{5566: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-ro-me-no', idx=3439, completeness=<Completeness.INCOMPLETE: 0>) [{4672: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-sa-ma-qa', idx=3441, completeness=<Completeness.INCOMPLETE: 0>) [{4109: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-u', idx=3458, completeness=<Completeness.INCOMPLETE: 0>) [{3739: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-wa', idx=3459, completeness=<Completeness.INCOMPLETE: 0>) [{813: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qe-we', idx=3461, completeness=<Completeness.INCOMPLETE: 0>) [{5394: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qi-ja', idx=3463, completeness=<Completeness.INCOMPLETE: 0>) [{1073: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qi-no', idx=3469, completeness=<Completeness.INCOMPLETE: 0>) [{2363: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qi-ta', idx=3477, completeness=<Completeness.INCOMPLETE: 0>) [{2752: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo', idx=3480, completeness=<Completeness.INCOMPLETE: 0>) [{378: False, 1148: False, 1382: False, 1869: False, 2139: False, 2738: False, 3806: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-i-na', idx=3481, completeness=<Completeness.INCOMPLETE: 0>) [{1417: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-jo', idx=3484, completeness=<Completeness.INCOMPLETE: 0>) [{249: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-no-jo', idx=3485, completeness=<Completeness.INCOMPLETE: 0>) [{3454: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-pa-ra', idx=3487, completeness=<Completeness.INCOMPLETE: 0>) [{1366: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-si-jo', idx=3492, completeness=<Completeness.INCOMPLETE: 0>) [{3526: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-te-jo', idx=3496, completeness=<Completeness.INCOMPLETE: 0>) [{3486: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-u', idx=3499, completeness=<Completeness.INCOMPLETE: 0>) [{1717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-wa', idx=3504, completeness=<Completeness.INCOMPLETE: 0>) [{5584: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-wi-ro', idx=3508, completeness=<Completeness.INCOMPLETE: 0>) [{5577: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='qo-wo', idx=3509, completeness=<Completeness.INCOMPLETE: 0>) [{5077: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-i', idx=3513, completeness=<Completeness.INCOMPLETE: 0>) [{3992: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-i-ja', idx=3514, completeness=<Completeness.INCOMPLETE: 0>) [{920: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-i-jo', idx=3515, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-i-ka', idx=3516, completeness=<Completeness.INCOMPLETE: 0>) [{4797: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-jo', idx=3522, completeness=<Completeness.INCOMPLETE: 0>) [{213: False, 1150: False, 2644: False, 2750: False, 2876: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ke-re-we', idx=3527, completeness=<Completeness.INCOMPLETE: 0>) [{989: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ki', idx=3529, completeness=<Completeness.INCOMPLETE: 0>) [{2036: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ki-si', idx=3530, completeness=<Completeness.INCOMPLETE: 0>) [{1440: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ko', idx=3531, completeness=<Completeness.INCOMPLETE: 0>) [{874: False, 1079: False, 2326: False, 3064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ma', idx=3535, completeness=<Completeness.INCOMPLETE: 0>) [{41: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ma-na', idx=3537, completeness=<Completeness.INCOMPLETE: 0>) [{2508: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-na-i', idx=3543, completeness=<Completeness.INCOMPLETE: 0>) [{5485: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ne-to', idx=3544, completeness=<Completeness.INCOMPLETE: 0>) [{3607: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-no', idx=3547, completeness=<Completeness.INCOMPLETE: 0>) [{1652: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-o', idx=3548, completeness=<Completeness.INCOMPLETE: 0>) [{5421: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-pa', idx=3550, completeness=<Completeness.INCOMPLETE: 0>) [{3807: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-pi', idx=3557, completeness=<Completeness.INCOMPLETE: 0>) [{2323: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-qa-ra-to', idx=3563, completeness=<Completeness.INCOMPLETE: 0>) [{1536: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-qe', idx=3564, completeness=<Completeness.INCOMPLETE: 0>) [{5549: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ri-jo', idx=3568, completeness=<Completeness.INCOMPLETE: 0>) [{3435: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ro', idx=3569, completeness=<Completeness.INCOMPLETE: 0>) [{52: False, 3102: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-si-jo', idx=3571, completeness=<Completeness.INCOMPLETE: 0>) [{2321: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-so', idx=3572, completeness=<Completeness.INCOMPLETE: 0>) [{1322: False, 2746: False, 3073: False, 4951: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-te-o', idx=3578, completeness=<Completeness.INCOMPLETE: 0>) [{5486: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ti', idx=3579, completeness=<Completeness.INCOMPLETE: 0>) [{2710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-to-jo', idx=3582, completeness=<Completeness.INCOMPLETE: 0>) [{3193: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-to-po-ro', idx=3583, completeness=<Completeness.INCOMPLETE: 0>) [{1281: False, 2417: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-tu', idx=3584, completeness=<Completeness.INCOMPLETE: 0>) [{720: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-u-ko', idx=3586, completeness=<Completeness.INCOMPLETE: 0>) [{2413: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-u-to', idx=3591, completeness=<Completeness.INCOMPLETE: 0>) [{4022: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wa-e-si-jo', idx=3592, completeness=<Completeness.INCOMPLETE: 0>) [{3874: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wa-ke', idx=3593, completeness=<Completeness.INCOMPLETE: 0>) [{1233: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wa-ke-si', idx=3595, completeness=<Completeness.INCOMPLETE: 0>) [{964: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wi', idx=3605, completeness=<Completeness.INCOMPLETE: 0>) [{922: False, 1034: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wo', idx=3609, completeness=<Completeness.INCOMPLETE: 0>) [{865: False, 3608: False, 4275: False, 4457: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-wo-te', idx=3616, completeness=<Completeness.INCOMPLETE: 0>) [{1008: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-ze', idx=3618, completeness=<Completeness.INCOMPLETE: 0>) [{748: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ra-zo', idx=3619, completeness=<Completeness.INCOMPLETE: 0>) [{2806: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ja', idx=3623, completeness=<Completeness.INCOMPLETE: 0>) [{2506: False, 4322: False, 4327: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ja-i', idx=3624, completeness=<Completeness.INCOMPLETE: 0>) [{4475: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-jo', idx=3625, completeness=<Completeness.INCOMPLETE: 0>) [{3393: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ka-se-ra-wo', idx=3628, completeness=<Completeness.INCOMPLETE: 0>) [{846: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ke', idx=3631, completeness=<Completeness.INCOMPLETE: 0>) [{5427: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ki-si', idx=3634, completeness=<Completeness.INCOMPLETE: 0>) [{1410: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ma-ta', idx=3637, completeness=<Completeness.INCOMPLETE: 0>) [{4950: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-mi-si-jo', idx=3639, completeness=<Completeness.INCOMPLETE: 0>) [{1007: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-mo', idx=3640, completeness=<Completeness.INCOMPLETE: 0>) [{1717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ne-o', idx=3642, completeness=<Completeness.INCOMPLETE: 0>) [{4289: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-o', idx=3643, completeness=<Completeness.INCOMPLETE: 0>) [{121: False, 305: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-pi-ru-nu-we', idx=3646, completeness=<Completeness.INCOMPLETE: 0>) [{3977: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ro', idx=3654, completeness=<Completeness.INCOMPLETE: 0>) [{4767: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-si-jo', idx=3656, completeness=<Completeness.INCOMPLETE: 0>) [{1475: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-ta', idx=3659, completeness=<Completeness.INCOMPLETE: 0>) [{2705: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-to', idx=3661, completeness=<Completeness.INCOMPLETE: 0>) [{2780: False, 3748: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-u', idx=3662, completeness=<Completeness.INCOMPLETE: 0>) [{1272: False, 1435: False, 4441: False, 4855: False, 5058: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-u-ka-ta-ra-ja', idx=3666, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-wa', idx=3676, completeness=<Completeness.INCOMPLETE: 0>) [{879: False, 1494: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-we', idx=3679, completeness=<Completeness.INCOMPLETE: 0>) [{2679: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-wi', idx=3680, completeness=<Completeness.INCOMPLETE: 0>) [{1315: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-wi-ja-qe', idx=3681, completeness=<Completeness.INCOMPLETE: 0>) [{5411: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='re-wi-jo-te', idx=3682, completeness=<Completeness.INCOMPLETE: 0>) [{4737: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-*65', idx=3689, completeness=<Completeness.INCOMPLETE: 0>) [{1103: False, 1279: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-*83-re-to', idx=3692, completeness=<Completeness.INCOMPLETE: 0>) [{3390: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-de', idx=3693, completeness=<Completeness.INCOMPLETE: 0>) [{783: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-de-ja', idx=3694, completeness=<Completeness.INCOMPLETE: 0>) [{2403: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-di-ne-to', idx=3695, completeness=<Completeness.INCOMPLETE: 0>) [{3595: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ja', idx=3696, completeness=<Completeness.INCOMPLETE: 0>) [{1546: False, 2169: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ja-ta', idx=3698, completeness=<Completeness.INCOMPLETE: 0>) [{3594: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ko', idx=3704, completeness=<Completeness.INCOMPLETE: 0>) [{580: False, 1751: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-mi-jo', idx=3709, completeness=<Completeness.INCOMPLETE: 0>) [{3883: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-mo-qe', idx=3710, completeness=<Completeness.INCOMPLETE: 0>) [{2382: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-na-jo', idx=3711, completeness=<Completeness.INCOMPLETE: 0>) [{4493: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ne', idx=3713, completeness=<Completeness.INCOMPLETE: 0>) [{1525: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ro', idx=3718, completeness=<Completeness.INCOMPLETE: 0>) [{3651: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-sa-ta', idx=3720, completeness=<Completeness.INCOMPLETE: 0>) [{2937: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-si', idx=3721, completeness=<Completeness.INCOMPLETE: 0>) [{2766: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-si-ja', idx=3722, completeness=<Completeness.INCOMPLETE: 0>) [{1197: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-si-jo', idx=3723, completeness=<Completeness.INCOMPLETE: 0>) [{4165: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-ti', idx=3728, completeness=<Completeness.INCOMPLETE: 0>) [{1925: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-to', idx=3729, completeness=<Completeness.INCOMPLETE: 0>) [{2168: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-to-wo', idx=3730, completeness=<Completeness.INCOMPLETE: 0>) [{819: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ri-wa', idx=3732, completeness=<Completeness.INCOMPLETE: 0>) [{1695: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro', idx=3736, completeness=<Completeness.INCOMPLETE: 0>) [{36: False, 79: False, 586: False, 610: False, 714: False, 882: False, 922: False, 1429: False, 1593: False, 1724: False, 1752: False, 1843: False, 2075: False, 2140: False, 2171: False, 2407: False, 2651: False, 2683: False, 2899: False, 3011: False, 3024: False, 3029: False, 3041: False, 3242: False, 3436: False, 3499: False, 3550: False, 3658: False, 3659: False, 3906: False, 3909: False, 3917: False, 4127: False, 4179: False, 4248: False, 4267: False, 4722: False, 4732: False, 5030: False, 5068: False, 5144: False, 5405: False, 5623: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-i', idx=3738, completeness=<Completeness.INCOMPLETE: 0>) [{2369: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-jo', idx=3740, completeness=<Completeness.INCOMPLETE: 0>) [{2003: False, 4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-qe', idx=3745, completeness=<Completeness.INCOMPLETE: 0>) [{2417: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-si-ja', idx=3748, completeness=<Completeness.INCOMPLETE: 0>) [{5478: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-ti-jo', idx=3749, completeness=<Completeness.INCOMPLETE: 0>) [{2673: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-to-qa', idx=3750, completeness=<Completeness.INCOMPLETE: 0>) [{3984: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-u-si', idx=3752, completeness=<Completeness.INCOMPLETE: 0>) [{5154: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ro-we', idx=3756, completeness=<Completeness.INCOMPLETE: 0>) [{706: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-e', idx=3766, completeness=<Completeness.INCOMPLETE: 0>) [{5416: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ha', idx=3767, completeness=<Completeness.INCOMPLETE: 0>) [{2714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ka', idx=3769, completeness=<Completeness.INCOMPLETE: 0>) [{1331: False, 5774: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ka-*18', idx=3770, completeness=<Completeness.INCOMPLETE: 0>) [{3074: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ke-ja', idx=3771, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ki', idx=3773, completeness=<Completeness.INCOMPLETE: 0>) [{679: False, 1922: False, 4060: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ko-si', idx=3784, completeness=<Completeness.INCOMPLETE: 0>) [{1027: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-o-wo', idx=3792, completeness=<Completeness.INCOMPLETE: 0>) [{276: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-po', idx=3793, completeness=<Completeness.INCOMPLETE: 0>) [{651: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-po-to', idx=3794, completeness=<Completeness.INCOMPLETE: 0>) [{943: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-si', idx=3797, completeness=<Completeness.INCOMPLETE: 0>) [{1538: False, 2567: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-ta', idx=3799, completeness=<Completeness.INCOMPLETE: 0>) [{2326: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-wa', idx=3802, completeness=<Completeness.INCOMPLETE: 0>) [{1036: False, 1588: False, 4306: False, 5393: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-wo', idx=3805, completeness=<Completeness.INCOMPLETE: 0>) [{1236: False, 2701: False, 3351: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-wo-i-ko', idx=3806, completeness=<Completeness.INCOMPLETE: 0>) [{3115: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ru-wo-we-ja', idx=3807, completeness=<Completeness.INCOMPLETE: 0>) [{4276: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='rya', idx=3808, completeness=<Completeness.INCOMPLETE: 0>) [{4282: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ryo', idx=3809, completeness=<Completeness.INCOMPLETE: 0>) [{1996: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-do-ro-jo', idx=3813, completeness=<Completeness.INCOMPLETE: 0>) [{2717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-i', idx=3814, completeness=<Completeness.INCOMPLETE: 0>) [{3917: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ja', idx=3815, completeness=<Completeness.INCOMPLETE: 0>) [{5711: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ka-ri-jo', idx=3817, completeness=<Completeness.INCOMPLETE: 0>) [{697: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ma', idx=3821, completeness=<Completeness.INCOMPLETE: 0>) [{1459: False, 1834: False, 1909: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ma-ru', idx=3829, completeness=<Completeness.INCOMPLETE: 0>) [{683: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-me', idx=3831, completeness=<Completeness.INCOMPLETE: 0>) [{1185: False, 2713: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-me-u', idx=3833, completeness=<Completeness.INCOMPLETE: 0>) [{4252: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-mu', idx=3835, completeness=<Completeness.INCOMPLETE: 0>) [{136: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-na-so', idx=3837, completeness=<Completeness.INCOMPLETE: 0>) [{26: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-no', idx=3840, completeness=<Completeness.INCOMPLETE: 0>) [{3522: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-nu', idx=3841, completeness=<Completeness.INCOMPLETE: 0>) [{1595: False, 5064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-nu-we-si-jo', idx=3843, completeness=<Completeness.INCOMPLETE: 0>) [{1245: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ra-pe-do', idx=3856, completeness=<Completeness.INCOMPLETE: 0>) [{4960: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-ri', idx=3857, completeness=<Completeness.INCOMPLETE: 0>) [{4697: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-so', idx=3864, completeness=<Completeness.INCOMPLETE: 0>) [{1252: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-to', idx=3866, completeness=<Completeness.INCOMPLETE: 0>) [{1417: False, 5091: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-tu', idx=3867, completeness=<Completeness.INCOMPLETE: 0>) [{2570: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-wa-ti', idx=3870, completeness=<Completeness.INCOMPLETE: 0>) [{2324: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='sa-wo', idx=3871, completeness=<Completeness.INCOMPLETE: 0>) [{2380: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-ai', idx=3876, completeness=<Completeness.INCOMPLETE: 0>) [{2086: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-ke-ru-pa-ko', idx=3878, completeness=<Completeness.INCOMPLETE: 0>) [{994: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-me-ni', idx=3879, completeness=<Completeness.INCOMPLETE: 0>) [{706: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-te-jo', idx=3889, completeness=<Completeness.INCOMPLETE: 0>) [{1526: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-to', idx=3890, completeness=<Completeness.INCOMPLETE: 0>) [{1644: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-to-i-jo', idx=3892, completeness=<Completeness.INCOMPLETE: 0>) [{1838: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-u', idx=3893, completeness=<Completeness.INCOMPLETE: 0>) [{1802: False, 3330: False, 4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-wa', idx=3894, completeness=<Completeness.INCOMPLETE: 0>) [{5481: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-we', idx=3895, completeness=<Completeness.INCOMPLETE: 0>) [{5137: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-wi-jo', idx=3898, completeness=<Completeness.INCOMPLETE: 0>) [{1525: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-wo', idx=3899, completeness=<Completeness.INCOMPLETE: 0>) [{2329: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='se-wo-te', idx=3900, completeness=<Completeness.INCOMPLETE: 0>) [{4709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si', idx=3902, completeness=<Completeness.INCOMPLETE: 0>) [{765: False, 1247: False, 1421: False, 1821: False, 2309: False, 2654: False, 2659: False, 2787: False, 3034: False, 3203: False, 3970: False, 4182: False, 4225: False, 4273: False, 4322: False, 4958: False, 5552: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-da-o-ne', idx=3904, completeness=<Completeness.INCOMPLETE: 0>) [{4207: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-du-wo', idx=3905, completeness=<Completeness.INCOMPLETE: 0>) [{80: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ja', idx=3907, completeness=<Completeness.INCOMPLETE: 0>) [{1261: False, 1535: False, 1903: False, 2101: False, 3987: False, 4280: False, 4458: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ja-phu', idx=3911, completeness=<Completeness.INCOMPLETE: 0>) [{913: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ja-qo', idx=3913, completeness=<Completeness.INCOMPLETE: 0>) [{2668: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-jo', idx=3914, completeness=<Completeness.INCOMPLETE: 0>) [{328: False, 419: False, 632: False, 1284: False, 1295: False, 1453: False, 1998: False, 2707: False, 3134: False, 3710: False, 4222: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-jo-jo', idx=3915, completeness=<Completeness.INCOMPLETE: 0>) [{1241: False, 3045: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-mi-jo', idx=3922, completeness=<Completeness.INCOMPLETE: 0>) [{2330: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-na-jo', idx=3926, completeness=<Completeness.INCOMPLETE: 0>) [{4020: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ni-ja', idx=3928, completeness=<Completeness.INCOMPLETE: 0>) [{1461: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-pe', idx=3935, completeness=<Completeness.INCOMPLETE: 0>) [{1490: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ra-ri-ja', idx=3944, completeness=<Completeness.INCOMPLETE: 0>) [{4378: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-re-we', idx=3950, completeness=<Completeness.INCOMPLETE: 0>) [{2705: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-ro', idx=3954, completeness=<Completeness.INCOMPLETE: 0>) [{1856: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-to-po', idx=3958, completeness=<Completeness.INCOMPLETE: 0>) [{2674: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-wa', idx=3960, completeness=<Completeness.INCOMPLETE: 0>) [{892: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='si-we', idx=3961, completeness=<Completeness.INCOMPLETE: 0>) [{748: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so', idx=3963, completeness=<Completeness.INCOMPLETE: 0>) [{68: False, 594: False, 652: False, 669: False, 703: False, 1321: False, 1344: False, 1719: False, 1746: False, 1941: False, 1944: False, 2649: False, 3214: False, 3395: False, 3562: False, 3570: False, 3685: False, 4303: False, 4305: False, 4337: False, 4351: False, 4993: False, 5034: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-de', idx=3964, completeness=<Completeness.INCOMPLETE: 0>) [{5012: False, 5039: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-i', idx=3965, completeness=<Completeness.INCOMPLETE: 0>) [{2663: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-i-ne', idx=3966, completeness=<Completeness.INCOMPLETE: 0>) [{2403: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-ma', idx=3967, completeness=<Completeness.INCOMPLETE: 0>) [{4411: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-mo', idx=3968, completeness=<Completeness.INCOMPLETE: 0>) [{3808: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-na-pe', idx=3969, completeness=<Completeness.INCOMPLETE: 0>) [{4709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-ni-ja', idx=3970, completeness=<Completeness.INCOMPLETE: 0>) [{5711: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='so-tu-wo-no', idx=3972, completeness=<Completeness.INCOMPLETE: 0>) [{965: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-*56-ta', idx=3978, completeness=<Completeness.INCOMPLETE: 0>) [{2692: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-di-ni-ko', idx=3979, completeness=<Completeness.INCOMPLETE: 0>) [{3223: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-ki-ri', idx=3983, completeness=<Completeness.INCOMPLETE: 0>) [{1361: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-pa-ta', idx=3993, completeness=<Completeness.INCOMPLETE: 0>) [{670: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-ra', idx=3998, completeness=<Completeness.INCOMPLETE: 0>) [{922: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='su-ti-jo', idx=4008, completeness=<Completeness.INCOMPLETE: 0>) [{3720: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-*22', idx=4012, completeness=<Completeness.INCOMPLETE: 0>) [{5765: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-jo', idx=4020, completeness=<Completeness.INCOMPLETE: 0>) [{25: False, 5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-jo-jo', idx=4021, completeness=<Completeness.INCOMPLETE: 0>) [{2592: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ki-jo', idx=4022, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ko-ro', idx=4023, completeness=<Completeness.INCOMPLETE: 0>) [{623: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ma', idx=4024, completeness=<Completeness.INCOMPLETE: 0>) [{128: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-mo', idx=4029, completeness=<Completeness.INCOMPLETE: 0>) [{666: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-na', idx=4030, completeness=<Completeness.INCOMPLETE: 0>) [{2394: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-na-du', idx=4031, completeness=<Completeness.INCOMPLETE: 0>) [{1302: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-na-qo', idx=4033, completeness=<Completeness.INCOMPLETE: 0>) [{2510: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-na-ti', idx=4034, completeness=<Completeness.INCOMPLETE: 0>) [{619: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-no', idx=4040, completeness=<Completeness.INCOMPLETE: 0>) [{1841: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-nu', idx=4041, completeness=<Completeness.INCOMPLETE: 0>) [{4013: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-o', idx=4043, completeness=<Completeness.INCOMPLETE: 0>) [{376: False, 3541: False, 3543: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-pa', idx=4044, completeness=<Completeness.INCOMPLETE: 0>) [{1276: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-qa-ra', idx=4049, completeness=<Completeness.INCOMPLETE: 0>) [{1425: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-qa-ra-te', idx=4050, completeness=<Completeness.INCOMPLETE: 0>) [{1711: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-qe', idx=4052, completeness=<Completeness.INCOMPLETE: 0>) [{5394: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ra-jo', idx=4055, completeness=<Completeness.INCOMPLETE: 0>) [{4251: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ra-ke-wi', idx=4056, completeness=<Completeness.INCOMPLETE: 0>) [{4704: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ra-pi', idx=4066, completeness=<Completeness.INCOMPLETE: 0>) [{2903: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-re-wo', idx=4074, completeness=<Completeness.INCOMPLETE: 0>) [{3510: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ri-ja', idx=4075, completeness=<Completeness.INCOMPLETE: 0>) [{5394: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ro', idx=4076, completeness=<Completeness.INCOMPLETE: 0>) [{1554: False, 2172: False, 2432: False, 2433: False, 3295: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-rya', idx=4077, completeness=<Completeness.INCOMPLETE: 0>) [{2502: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-ti-jo', idx=4086, completeness=<Completeness.INCOMPLETE: 0>) [{1472: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-u-na', idx=4092, completeness=<Completeness.INCOMPLETE: 0>) [{1244: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-wo', idx=4100, completeness=<Completeness.INCOMPLETE: 0>) [{1480: False, 3529: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-za', idx=4101, completeness=<Completeness.INCOMPLETE: 0>) [{3084: False, 3637: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ta-zo-te-ja', idx=4103, completeness=<Completeness.INCOMPLETE: 0>) [{5412: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-a', idx=4105, completeness=<Completeness.INCOMPLETE: 0>) [{1685: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-i-jo', idx=4108, completeness=<Completeness.INCOMPLETE: 0>) [{4805: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ja', idx=4109, completeness=<Completeness.INCOMPLETE: 0>) [{1692: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-je', idx=4111, completeness=<Completeness.INCOMPLETE: 0>) [{4998: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ko', idx=4115, completeness=<Completeness.INCOMPLETE: 0>) [{5077: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-me-u', idx=4121, completeness=<Completeness.INCOMPLETE: 0>) [{998: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-mi-ti-ja', idx=4128, completeness=<Completeness.INCOMPLETE: 0>) [{5264: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-mi-we-te', idx=4130, completeness=<Completeness.INCOMPLETE: 0>) [{519: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-na-ja-so', idx=4131, completeness=<Completeness.INCOMPLETE: 0>) [{2671: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-o-ne', idx=4138, completeness=<Completeness.INCOMPLETE: 0>) [{2531: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-o-po', idx=4139, completeness=<Completeness.INCOMPLETE: 0>) [{4700: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-pi-ja-qe', idx=4148, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ra', idx=4155, completeness=<Completeness.INCOMPLETE: 0>) [{689: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ra-u-re-o', idx=4161, completeness=<Completeness.INCOMPLETE: 0>) [{5295: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-re', idx=4164, completeness=<Completeness.INCOMPLETE: 0>) [{1233: False, 2780: False, 4717: False, 5142: False, 5464: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-re-u', idx=4175, completeness=<Completeness.INCOMPLETE: 0>) [{1538: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-re-wa', idx=4176, completeness=<Completeness.INCOMPLETE: 0>) [{1288: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ro', idx=4179, completeness=<Completeness.INCOMPLETE: 0>) [{4342: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ru', idx=4182, completeness=<Completeness.INCOMPLETE: 0>) [{1599: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ru-wo-te', idx=4185, completeness=<Completeness.INCOMPLETE: 0>) [{2785: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-rya', idx=4186, completeness=<Completeness.INCOMPLETE: 0>) [{1376: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-so', idx=4189, completeness=<Completeness.INCOMPLETE: 0>) [{2671: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-ta', idx=4191, completeness=<Completeness.INCOMPLETE: 0>) [{875: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-te', idx=4193, completeness=<Completeness.INCOMPLETE: 0>) [{4237: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-to', idx=4196, completeness=<Completeness.INCOMPLETE: 0>) [{3218: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-u', idx=4200, completeness=<Completeness.INCOMPLETE: 0>) [{1254: False, 1316: False, 3034: False, 3317: False, 4714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-u-po-ro', idx=4202, completeness=<Completeness.INCOMPLETE: 0>) [{4730: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-u-to-ri-*65', idx=4205, completeness=<Completeness.INCOMPLETE: 0>) [{979: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-wa', idx=4206, completeness=<Completeness.INCOMPLETE: 0>) [{662: False, 1325: False, 4710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-wa-te-u', idx=4209, completeness=<Completeness.INCOMPLETE: 0>) [{623: False, 640: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-we', idx=4210, completeness=<Completeness.INCOMPLETE: 0>) [{42: False, 721: False, 1484: False, 2386: False, 4319: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='te-wi-ja', idx=4211, completeness=<Completeness.INCOMPLETE: 0>) [{1258: False, 5418: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti', idx=4213, completeness=<Completeness.INCOMPLETE: 0>) [{727: False, 735: False, 894: False, 1080: False, 1105: False, 1419: False, 1725: False, 1959: False, 2076: False, 2114: False, 2194: False, 2317: False, 3204: False, 3280: False, 3687: False, 3830: False, 4688: False, 5083: False, 5139: False, 5389: False, 5671: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-da', idx=4214, completeness=<Completeness.INCOMPLETE: 0>) [{5749: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-e-si', idx=4215, completeness=<Completeness.INCOMPLETE: 0>) [{5255: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ja-no', idx=4217, completeness=<Completeness.INCOMPLETE: 0>) [{66: False, 5481: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-jo', idx=4218, completeness=<Completeness.INCOMPLETE: 0>) [{661: False, 2677: False, 2988: False, 3653: False, 3868: False, 4136: False, 4181: False, 4286: False, 4354: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-me-no', idx=4222, completeness=<Completeness.INCOMPLETE: 0>) [{4960: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-mi-nu-wo', idx=4223, completeness=<Completeness.INCOMPLETE: 0>) [{4129: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-na-jo', idx=4231, completeness=<Completeness.INCOMPLETE: 0>) [{5064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-no-de', idx=4234, completeness=<Completeness.INCOMPLETE: 0>) [{5024: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-nwa-ti', idx=4237, completeness=<Completeness.INCOMPLETE: 0>) [{5087: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ri', idx=4243, completeness=<Completeness.INCOMPLETE: 0>) [{1202: False, 1457: False, 1671: False, 2143: False, 4446: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ri-sa-ta', idx=4253, completeness=<Completeness.INCOMPLETE: 0>) [{2830: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ri-ti', idx=4256, completeness=<Completeness.INCOMPLETE: 0>) [{4143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ro', idx=4260, completeness=<Completeness.INCOMPLETE: 0>) [{3628: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ta-ma', idx=4262, completeness=<Completeness.INCOMPLETE: 0>) [{1222: False, 1323: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-ta-ra', idx=4264, completeness=<Completeness.INCOMPLETE: 0>) [{4732: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-tu', idx=4266, completeness=<Completeness.INCOMPLETE: 0>) [{4754: False, 5663: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-wo', idx=4269, completeness=<Completeness.INCOMPLETE: 0>) [{5554: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ti-za', idx=4270, completeness=<Completeness.INCOMPLETE: 0>) [{3422: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-de', idx=4272, completeness=<Completeness.INCOMPLETE: 0>) [{1278: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-i-je', idx=4274, completeness=<Completeness.INCOMPLETE: 0>) [{701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-ke', idx=4277, completeness=<Completeness.INCOMPLETE: 0>) [{2733: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-mo', idx=4287, completeness=<Completeness.INCOMPLETE: 0>) [{4701: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-ro', idx=4311, completeness=<Completeness.INCOMPLETE: 0>) [{1444: False, 2324: False, 3165: False, 5563: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-se', idx=4333, completeness=<Completeness.INCOMPLETE: 0>) [{5418: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-so-ro', idx=4341, completeness=<Completeness.INCOMPLETE: 0>) [{4978: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-te', idx=4342, completeness=<Completeness.INCOMPLETE: 0>) [{1460: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-wi-jo', idx=4355, completeness=<Completeness.INCOMPLETE: 0>) [{2916: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='to-wo-na', idx=4358, completeness=<Completeness.INCOMPLETE: 0>) [{2329: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ma', idx=4370, completeness=<Completeness.INCOMPLETE: 0>) [{1547: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ma-i', idx=4372, completeness=<Completeness.INCOMPLETE: 0>) [{1400: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-mi', idx=4376, completeness=<Completeness.INCOMPLETE: 0>) [{2328: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ni', idx=4378, completeness=<Completeness.INCOMPLETE: 0>) [{1280: False, 1396: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ri', idx=4386, completeness=<Completeness.INCOMPLETE: 0>) [{3981: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ro', idx=4394, completeness=<Completeness.INCOMPLETE: 0>) [{4775: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-ru-pe-te', idx=4395, completeness=<Completeness.INCOMPLETE: 0>) [{1224: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tu-si-jo', idx=4400, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='twe-te', idx=4412, completeness=<Completeness.INCOMPLETE: 0>) [{580: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tya', idx=4413, completeness=<Completeness.INCOMPLETE: 0>) [{3833: False, 4290: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tya-mo', idx=4414, completeness=<Completeness.INCOMPLETE: 0>) [{1845: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='tya-no', idx=4415, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u', idx=4416, completeness=<Completeness.INCOMPLETE: 0>) [{227: False, 399: False, 658: False, 1493: False, 1594: False, 2066: False, 2195: False, 2477: False, 2711: False, 3691: False, 3737: False, 3954: False, 4363: False, 4744: False, 5058: False, 5139: False, 5543: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-*56', idx=4417, completeness=<Completeness.INCOMPLETE: 0>) [{3258: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-a', idx=4418, completeness=<Completeness.INCOMPLETE: 0>) [{1441: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-da', idx=4419, completeness=<Completeness.INCOMPLETE: 0>) [{1788: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-de-wi-ni-jo', idx=4421, completeness=<Completeness.INCOMPLETE: 0>) [{5060: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-jo', idx=4426, completeness=<Completeness.INCOMPLETE: 0>) [{1341: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-jo-e', idx=4427, completeness=<Completeness.INCOMPLETE: 0>) [{2633: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ke', idx=4431, completeness=<Completeness.INCOMPLETE: 0>) [{1098: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ko-ro', idx=4432, completeness=<Completeness.INCOMPLETE: 0>) [{2193: False, 2687: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ma', idx=4433, completeness=<Completeness.INCOMPLETE: 0>) [{5132: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ne', idx=4435, completeness=<Completeness.INCOMPLETE: 0>) [{4006: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-no', idx=4436, completeness=<Completeness.INCOMPLETE: 0>) [{2475: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-o', idx=4437, completeness=<Completeness.INCOMPLETE: 0>) [{664: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-pa-ta', idx=4440, completeness=<Completeness.INCOMPLETE: 0>) [{2476: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ra', idx=4450, completeness=<Completeness.INCOMPLETE: 0>) [{2654: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-re', idx=4454, completeness=<Completeness.INCOMPLETE: 0>) [{5260: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ru-pi-ja', idx=4456, completeness=<Completeness.INCOMPLETE: 0>) [{3985: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-ta', idx=4462, completeness=<Completeness.INCOMPLETE: 0>) [{1251: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-te', idx=4468, completeness=<Completeness.INCOMPLETE: 0>) [{1074: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-te-ro', idx=4469, completeness=<Completeness.INCOMPLETE: 0>) [{5214: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-te-si', idx=4470, completeness=<Completeness.INCOMPLETE: 0>) [{3957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='u-to', idx=4471, completeness=<Completeness.INCOMPLETE: 0>) [{2698: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-de', idx=4481, completeness=<Completeness.INCOMPLETE: 0>) [{2403: False, 4461: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-du-ri', idx=4488, completeness=<Completeness.INCOMPLETE: 0>) [{4145: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-i-jo', idx=4494, completeness=<Completeness.INCOMPLETE: 0>) [{2709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ja', idx=4495, completeness=<Completeness.INCOMPLETE: 0>) [{1606: False, 4038: False, 4312: False, 5550: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-jo', idx=4497, completeness=<Completeness.INCOMPLETE: 0>) [{1094: False, 1124: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ko', idx=4502, completeness=<Completeness.INCOMPLETE: 0>) [{19: False, 2624: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-na', idx=4503, completeness=<Completeness.INCOMPLETE: 0>) [{5035: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-na-ka-to', idx=4508, completeness=<Completeness.INCOMPLETE: 0>) [{5078: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ne-u', idx=4516, completeness=<Completeness.INCOMPLETE: 0>) [{4710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ni-jo', idx=4517, completeness=<Completeness.INCOMPLETE: 0>) [{4717: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ni-ko', idx=4518, completeness=<Completeness.INCOMPLETE: 0>) [{5063: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-no', idx=4519, completeness=<Completeness.INCOMPLETE: 0>) [{2027: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-pa-no', idx=4524, completeness=<Completeness.INCOMPLETE: 0>) [{5064: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ra-qi-si-ro', idx=4530, completeness=<Completeness.INCOMPLETE: 0>) [{5722: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-re-u-ka-ra', idx=4534, completeness=<Completeness.INCOMPLETE: 0>) [{5217: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-si-jo', idx=4536, completeness=<Completeness.INCOMPLETE: 0>) [{863: False, 2434: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ta', idx=4538, completeness=<Completeness.INCOMPLETE: 0>) [{918: False, 5546: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-te', idx=4540, completeness=<Completeness.INCOMPLETE: 0>) [{1153: False, 1298: False, 4062: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-ti-ro', idx=4544, completeness=<Completeness.INCOMPLETE: 0>) [{5062: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-u-so', idx=4552, completeness=<Completeness.INCOMPLETE: 0>) [{4023: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-wo', idx=4555, completeness=<Completeness.INCOMPLETE: 0>) [{5683: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wa-wo-u', idx=4556, completeness=<Completeness.INCOMPLETE: 0>) [{5468: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-*19', idx=4559, completeness=<Completeness.INCOMPLETE: 0>) [{1697: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-a', idx=4560, completeness=<Completeness.INCOMPLETE: 0>) [{1256: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-e', idx=4565, completeness=<Completeness.INCOMPLETE: 0>) [{2924: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-e-ha', idx=4566, completeness=<Completeness.INCOMPLETE: 0>) [{5386: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ha-no', idx=4568, completeness=<Completeness.INCOMPLETE: 0>) [{5395: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-i', idx=4571, completeness=<Completeness.INCOMPLETE: 0>) [{1610: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-i-jo', idx=4572, completeness=<Completeness.INCOMPLETE: 0>) [{2763: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ja', idx=4574, completeness=<Completeness.INCOMPLETE: 0>) [{483: False, 1409: False, 1574: False, 2024: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-je', idx=4576, completeness=<Completeness.INCOMPLETE: 0>) [{4960: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-jo', idx=4581, completeness=<Completeness.INCOMPLETE: 0>) [{4713: False, 4955: False, 5534: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ka-sa', idx=4583, completeness=<Completeness.INCOMPLETE: 0>) [{2515: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ke-i-jo', idx=4588, completeness=<Completeness.INCOMPLETE: 0>) [{5076: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ke-se', idx=4589, completeness=<Completeness.INCOMPLETE: 0>) [{1720: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ko', idx=4590, completeness=<Completeness.INCOMPLETE: 0>) [{1656: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ko-we-ka-te', idx=4591, completeness=<Completeness.INCOMPLETE: 0>) [{2563: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ma-na', idx=4592, completeness=<Completeness.INCOMPLETE: 0>) [{3810: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ra-ti-ja', idx=4597, completeness=<Completeness.INCOMPLETE: 0>) [{2576: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ra-to', idx=4598, completeness=<Completeness.INCOMPLETE: 0>) [{3220: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-re-ki-ja', idx=4602, completeness=<Completeness.INCOMPLETE: 0>) [{2513: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ri', idx=4605, completeness=<Completeness.INCOMPLETE: 0>) [{1696: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ri-jo-jo', idx=4606, completeness=<Completeness.INCOMPLETE: 0>) [{4419: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-ro', idx=4607, completeness=<Completeness.INCOMPLETE: 0>) [{887: False, 2677: False, 2692: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-si-jo-jo', idx=4611, completeness=<Completeness.INCOMPLETE: 0>) [{1608: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-to-ro', idx=4616, completeness=<Completeness.INCOMPLETE: 0>) [{705: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-u', idx=4617, completeness=<Completeness.INCOMPLETE: 0>) [{686: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-wa', idx=4619, completeness=<Completeness.INCOMPLETE: 0>) [{1003: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='we-wa-ta', idx=4621, completeness=<Completeness.INCOMPLETE: 0>) [{878: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-du', idx=4636, completeness=<Completeness.INCOMPLETE: 0>) [{697: False, 2678: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ja', idx=4642, completeness=<Completeness.INCOMPLETE: 0>) [{596: False, 2786: False, 5624: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ja-ma', idx=4644, completeness=<Completeness.INCOMPLETE: 0>) [{1740: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ja-na-tu', idx=4646, completeness=<Completeness.INCOMPLETE: 0>) [{2656: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ja-we', idx=4650, completeness=<Completeness.INCOMPLETE: 0>) [{5153: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-jo', idx=4654, completeness=<Completeness.INCOMPLETE: 0>) [{266: False, 947: False, 2681: False, 5141: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-jo-do', idx=4655, completeness=<Completeness.INCOMPLETE: 0>) [{4473: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-jo-jo', idx=4656, completeness=<Completeness.INCOMPLETE: 0>) [{3905: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-pe-wa', idx=4664, completeness=<Completeness.INCOMPLETE: 0>) [{1063: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-pi-jo', idx=4665, completeness=<Completeness.INCOMPLETE: 0>) [{1693: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ri', idx=4670, completeness=<Completeness.INCOMPLETE: 0>) [{1187: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-ri-wo', idx=4679, completeness=<Completeness.INCOMPLETE: 0>) [{4714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-sa', idx=4683, completeness=<Completeness.INCOMPLETE: 0>) [{2397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-to', idx=4689, completeness=<Completeness.INCOMPLETE: 0>) [{672: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wi-to-te-ra', idx=4690, completeness=<Completeness.INCOMPLETE: 0>) [{5424: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo', idx=4693, completeness=<Completeness.INCOMPLETE: 0>) [{77: False, 411: False, 412: False, 598: False, 635: False, 677: False, 686: False, 690: False, 697: False, 1088: False, 1113: False, 1227: False, 1348: False, 1589: False, 1672: False, 1705: False, 1806: False, 2065: False, 2082: False, 2093: False, 2137: False, 2178: False, 2222: False, 2586: False, 2590: False, 2708: False, 2732: False, 2795: False, 2854: False, 2928: False, 3540: False, 3698: False, 3905: False, 4169: False, 4294: False, 4757: False, 4766: False, 4780: False, 4895: False, 5134: False, 5541: False, 5573: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-*65', idx=4694, completeness=<Completeness.INCOMPLETE: 0>) [{2318: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-a', idx=4697, completeness=<Completeness.INCOMPLETE: 0>) [{1460: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-do-we-qe', idx=4704, completeness=<Completeness.INCOMPLETE: 0>) [{5024: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ja', idx=4706, completeness=<Completeness.INCOMPLETE: 0>) [{5369: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ne', idx=4714, completeness=<Completeness.INCOMPLETE: 0>) [{626: False, 664: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ni', idx=4718, completeness=<Completeness.INCOMPLETE: 0>) [{4987: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ni-jo', idx=4719, completeness=<Completeness.INCOMPLETE: 0>) [{1000: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-no-da', idx=4721, completeness=<Completeness.INCOMPLETE: 0>) [{956: False, 1078: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-no-qi', idx=4724, completeness=<Completeness.INCOMPLETE: 0>) [{972: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-no-wa-ti-si', idx=4727, completeness=<Completeness.INCOMPLETE: 0>) [{5409: False, 5491: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-pi', idx=4728, completeness=<Completeness.INCOMPLETE: 0>) [{1611: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-qo', idx=4730, completeness=<Completeness.INCOMPLETE: 0>) [{2712: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-qo-jo', idx=4731, completeness=<Completeness.INCOMPLETE: 0>) [{2033: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ra', idx=4732, completeness=<Completeness.INCOMPLETE: 0>) [{583: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ra-ke-re', idx=4734, completeness=<Completeness.INCOMPLETE: 0>) [{821: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ro-qo-ta', idx=4741, completeness=<Completeness.INCOMPLETE: 0>) [{5287: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-ta', idx=4748, completeness=<Completeness.INCOMPLETE: 0>) [{2714: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-to', idx=4750, completeness=<Completeness.INCOMPLETE: 0>) [{4763: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-tu', idx=4752, completeness=<Completeness.INCOMPLETE: 0>) [{1047: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='wo-tu-ko', idx=4753, completeness=<Completeness.INCOMPLETE: 0>) [{5529: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='za-ka-wo', idx=4770, completeness=<Completeness.INCOMPLETE: 0>) [{5524: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='za-ra-ro', idx=4778, completeness=<Completeness.INCOMPLETE: 0>) [{1612: False, 3201: False, 3321: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='za-te-o', idx=4779, completeness=<Completeness.INCOMPLETE: 0>) [{4709: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='za-we', idx=4780, completeness=<Completeness.INCOMPLETE: 0>) [{1204: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ze', idx=4784, completeness=<Completeness.INCOMPLETE: 0>) [{4030: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ze-i-ja-ka-ra-na', idx=4785, completeness=<Completeness.INCOMPLETE: 0>) [{5463: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ze-me-qe', idx=4786, completeness=<Completeness.INCOMPLETE: 0>) [{4277: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ze-pu', idx=4791, completeness=<Completeness.INCOMPLETE: 0>) [{1011: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='ze-ta', idx=4795, completeness=<Completeness.INCOMPLETE: 0>) [{2402: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='zo', idx=4800, completeness=<Completeness.INCOMPLETE: 0>) [{32: False, 361: False, 702: False, 1121: False, 1197: False, 1760: False, 2101: False, 2311: False, 3136: False, 4318: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='zo-pa', idx=4803, completeness=<Completeness.INCOMPLETE: 0>) [{5731: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='zo-to', idx=4805, completeness=<Completeness.INCOMPLETE: 0>) [{5048: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*1641', idx=4826, completeness=<Completeness.INCOMPLETE: 0>) [{143: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*167', idx=4830, completeness=<Completeness.INCOMPLETE: 0>) [{119: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*167+PE', idx=4831, completeness=<Completeness.INCOMPLETE: 0>) [{122: False, 123: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*172+KE', idx=4838, completeness=<Completeness.INCOMPLETE: 0>) [{590: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*184', idx=4848, completeness=<Completeness.INCOMPLETE: 0>) [{1182: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*22', idx=4877, completeness=<Completeness.INCOMPLETE: 0>) [{3012: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*251', idx=4892, completeness=<Completeness.INCOMPLETE: 0>) [{5368: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*34', idx=4900, completeness=<Completeness.INCOMPLETE: 0>) [{3673: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*47', idx=4901, completeness=<Completeness.INCOMPLETE: 0>) [{957: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*56', idx=4902, completeness=<Completeness.INCOMPLETE: 0>) [{682: False, 760: False, 1862: False, 2084: False, 2179: False, 2481: False, 2704: False, 2707: False, 3800: False, 5717: False, 5741: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*64', idx=4903, completeness=<Completeness.INCOMPLETE: 0>) [{4710: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*65', idx=4904, completeness=<Completeness.INCOMPLETE: 0>) [{1355: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*82', idx=4905, completeness=<Completeness.INCOMPLETE: 0>) [{2382: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*83', idx=4906, completeness=<Completeness.INCOMPLETE: 0>) [{3765: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='*86', idx=4907, completeness=<Completeness.INCOMPLETE: 0>) [{5625: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='BOSx', idx=4927, completeness=<Completeness.INCOMPLETE: 0>) [{2818: False, 2825: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='EQU+QE', idx=4950, completeness=<Completeness.INCOMPLETE: 0>) [{397: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+KU', idx=5041, completeness=<Completeness.INCOMPLETE: 0>) [{4313: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='TELAI[]+PU', idx=5046, completeness=<Completeness.INCOMPLETE: 0>) [{4348: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+KU', idx=5048, completeness=<Completeness.INCOMPLETE: 0>) [{4313: False, 4346: False}, 0.0]\n",
      "WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+PA', idx=5049, completeness=<Completeness.INCOMPLETE: 0>) [{4271: False}, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for w in my_voc.get_vocabulary:\n",
    "    if w.completeness == Completeness.INCOMPLETE:\n",
    "        print(w, sequence_data[w.form])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fragment, Document and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fragment:\n",
    "    def __init__(self, sequence_list, complete, idx, vocab):\n",
    "        self.word_list = []\n",
    "        self.complete_list = []\n",
    "        self.complete = complete\n",
    "        self.idx = idx\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for seq in sequence_list:\n",
    "            form_w, complete = seq\n",
    "            word_idx = vocab.get_form_idx(form_w)\n",
    "            self.word_list.append(vocab.get_word(word_idx))\n",
    "            self.complete_list.append(complete)\n",
    "\n",
    "    @property\n",
    "    def form(self):\n",
    "        return \" \".join([w.form for w in self.word_list])\n",
    "\n",
    "    def num_words(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(s) for s in self.word_list])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Fragment idx={self.idx} form='{self.form}' complete='{self.complete}'\"\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        seq = []\n",
    "        base_logo_idx_vocab = self.vocab.log_start\n",
    "        base_logo_idx_tokens = len(get_charset(self.vocab.lang))\n",
    "\n",
    "        for i, word in enumerate(self.word_list):\n",
    "            # tokenize each word and extend the list\n",
    "            tokenized_word = word.id_seq.copy()\n",
    "\n",
    "            if tokenized_word[0] == UNK_ID:\n",
    "                tokenized_word[0] = base_logo_idx_tokens + (word.idx - base_logo_idx_vocab)\n",
    "            seq.extend(tokenized_word)\n",
    "\n",
    "\n",
    "        #adjustments to tokenized sequence:\n",
    "        # y always ends with EOS\n",
    "        seq[-1] = EOS_ID\n",
    "        return seq\n",
    "\n",
    "    def reconstruct_numerals(self, num_list):\n",
    "        num_idx = 0\n",
    "        for i, w in enumerate(self.word_list):\n",
    "            if w.form == \"NUM\":\n",
    "                self.word_list[i] = self.vocab.get_word(self.vocab.get_form_idx(num_list[num_idx]))\n",
    "                num_idx += 1\n",
    "        return num_idx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, fragments_list, idx):\n",
    "        self.fragments = fragments_list\n",
    "        self.idx = idx\n",
    "        self.idx_to_frag = {frag.idx: list_idx for list_idx, frag in enumerate(fragments_list)}\n",
    "\n",
    "    @property\n",
    "    def form(self):\n",
    "        return \" [...] \".join([f.form for f in self.fragments])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Document idx={self.idx} form='{self.form}'\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fragments)\n",
    "\n",
    "    def num_words(self):\n",
    "        return sum(f.num_words() for f in self.fragments)\n",
    "\n",
    "    @property\n",
    "    def word_list(self):\n",
    "        return sum([f.word_list for f in self.fragments], [])\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        return sum([f.tokenized_sequence for f in self.fragments], [])\n",
    "\n",
    "    def get_fragment(self, frag_idx):\n",
    "        frag_list_idx = self.idx_to_frag[frag_idx]\n",
    "        return self.fragments[frag_list_idx]\n",
    "\n",
    "    def reconstruct_numerals(self, numerals):\n",
    "        for frag in self.fragments:\n",
    "            next_idx = frag.reconstruct_numerals(numerals)\n",
    "            numerals = numerals[next_idx:]\n",
    "        assert len(numerals)==0, \"Did not consume all numerals!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, documents_data, vocab):\n",
    "        self.documents = {}\n",
    "        self.frag_to_doc = []\n",
    "\n",
    "        for doc_idx, fragments_list in documents_data.items():\n",
    "            curr_frag_list = []\n",
    "            for fragment_info in fragments_list:\n",
    "                sequence_list, complete = fragment_info\n",
    "                frag_idx = len(self.frag_to_doc)\n",
    "                curr_frag = Fragment(sequence_list, complete, frag_idx, vocab)\n",
    "                curr_frag_list.append(curr_frag)                                \n",
    "                self.frag_to_doc.append(doc_idx)\n",
    "            self.documents[doc_idx] = Document(curr_frag_list, doc_idx)\n",
    "    \n",
    "    @property\n",
    "    def num_documents(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    @property\n",
    "    def num_fragments(self):\n",
    "        return len(self.frag_to_doc)\n",
    "\n",
    "    def get_document(self, doc_idx):\n",
    "        return self.documents[doc_idx]\n",
    "\n",
    "    def get_document_no(self, list_idx):\n",
    "        doc_idx = list(self.documents.keys())[list_idx]\n",
    "        return self.get_document(doc_idx)\n",
    "\n",
    "    def get_fragment(self, frag_idx):\n",
    "        doc_idx = self.frag_to_doc[frag_idx]\n",
    "        return self.documents[doc_idx].get_fragment(frag_idx)\n",
    "\n",
    "    def restore_numerals(self, numerals):\n",
    "        for doc_idx, nums in numerals.items():\n",
    "            self.get_document(doc_idx).reconstruct_numerals(numerals[doc_idx])\n",
    "        \n",
    "corpus = Corpus(docs_data, my_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['M', True],\n",
       "   ['RI', True],\n",
       "   ['NUM', True],\n",
       "   ['O', True],\n",
       "   ['M', True],\n",
       "   ['NUM', True],\n",
       "   ['2', False]],\n",
       "  False],\n",
       " [[['KE', True], ['M', True], ['NUM', True], ['*146', True], ['NUM', True]],\n",
       "  True]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_data[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6, 49, 55, 60, 2, 256, 2, 192, 4],\n",
       " 'e-ri-sa-ta M 1',\n",
       " 4,\n",
       " 2,\n",
       " [True, True, True],\n",
       " [WordWithCompleteness(lang='transliterated_linear_b', form='e-ri-sa-ta', idx=1201, completeness=<Completeness.COMPLETE: 4>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='M', idx=4972, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Fragment(docs_data[3][0][0], docs_data[3][0][1], 0, my_voc)\n",
    "f.tokenized_sequence, f.form, EOS_ID, EOW_ID, f.complete_list, f.word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 49, 55, 60, 2, 256, 2, 192, 4], [1, 1, 1]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [f.tokenized_sequence, [1]*3]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document idx=13 form='M RI NUM O M NUM 2 [...] KE M NUM *146 NUM',\n",
       " Document idx=13 form='M RI NUM O M NUM 2 [...] KE M NUM *146 NUM',\n",
       " Fragment idx=11 form='M RI NUM O M NUM 2' complete='False',\n",
       " Fragment idx=12 form='KE M NUM *146 NUM' complete='True',\n",
       " 5240,\n",
       " 9894)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_document(13), corpus.get_document_no(8), corpus.get_fragment(11), corpus.get_fragment(12), corpus.num_documents, corpus.num_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([WordWithCompleteness(lang='transliterated_linear_b', form='M', idx=4972, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='RI', idx=5012, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='O', idx=4984, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='M', idx=4972, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       "  WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>)],\n",
       " [True, True, True, True, True, True, False],\n",
       " [256, 2, 296, 2, 267, 2, 268, 2, 256, 2, 267, 2, 193, 4],\n",
       " [249, 2, 256, 2, 267, 2, 95, 2, 267, 4],\n",
       " [256,\n",
       "  2,\n",
       "  296,\n",
       "  2,\n",
       "  267,\n",
       "  2,\n",
       "  268,\n",
       "  2,\n",
       "  256,\n",
       "  2,\n",
       "  267,\n",
       "  2,\n",
       "  193,\n",
       "  4,\n",
       "  249,\n",
       "  2,\n",
       "  256,\n",
       "  2,\n",
       "  267,\n",
       "  2,\n",
       "  95,\n",
       "  2,\n",
       "  267,\n",
       "  4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = corpus.get_fragment(11)\n",
    "f.word_list, f.complete_list, f.tokenized_sequence, corpus.get_fragment(12).tokenized_sequence, corpus.get_document_no(8).tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document idx=3 form='e-ri-sa-ta M 1', [6, 49, 55, 60, 2, 256, 2, 192, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_document_no(0), corpus.get_document_no(0).tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordWithCompleteness(lang='transliterated_linear_b', form='*132', idx=4809, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*142', idx=4810, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*146', idx=4811, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*146+PE', idx=4812, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*1462', idx=4813, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*150', idx=4814, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*152', idx=4815, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*153', idx=4816, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*154', idx=4817, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*155VAS', idx=4818, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*155VAS+DI', idx=4819, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*155VAS+NI', idx=4820, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*157', idx=4821, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*158', idx=4822, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*160', idx=4823, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*161', idx=4824, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*164', idx=4825, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*1641', idx=4826, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*165', idx=4827, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*166', idx=4828, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*166+WE', idx=4829, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*167', idx=4830, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*167+PE', idx=4831, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*168', idx=4832, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*168+SE', idx=4833, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*169', idx=4834, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*170', idx=4835, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*171', idx=4836, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*172', idx=4837, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*172+KE', idx=4838, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*172+KE+RO2', idx=4839, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*174', idx=4840, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*177', idx=4841, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*178', idx=4842, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*179', idx=4843, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*180', idx=4844, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*181', idx=4845, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*182', idx=4846, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*183', idx=4847, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*184', idx=4848, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*185', idx=4849, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*189', idx=4850, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*190', idx=4851, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*200VAS', idx=4852, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*201VAS', idx=4853, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*202VAS', idx=4854, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*202VAS+DI', idx=4855, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*203VAS', idx=4856, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*204VAS', idx=4857, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*205VAS', idx=4858, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*206VAS', idx=4859, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*207VAS', idx=4860, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*208VAS', idx=4861, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*209VAS', idx=4862, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*209VAS+A', idx=4863, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*210VAS', idx=4864, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*210VAS+KA', idx=4865, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*211VAS+PO', idx=4866, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*212VAS', idx=4867, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*212VAS+U', idx=4868, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*213VAS', idx=4869, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*214VAS', idx=4870, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*214VAS+DI', idx=4871, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*215VAS', idx=4872, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*216VAS', idx=4873, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*217VAS', idx=4874, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*218VAS', idx=4875, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*219VAS', idx=4876, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*22', idx=4877, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*220', idx=4878, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*221VAS', idx=4879, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*222VAS', idx=4880, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*226VAS', idx=4881, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*227VAS', idx=4882, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*228VAS', idx=4883, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*229VAS', idx=4884, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*232', idx=4885, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*234', idx=4886, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*245', idx=4887, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*246', idx=4888, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*248', idx=4889, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*249', idx=4890, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*250VAS', idx=4891, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*251', idx=4892, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*253', idx=4893, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*255', idx=4894, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*256', idx=4895, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*257', idx=4896, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*258', idx=4897, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*259', idx=4898, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*260', idx=4899, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*34', idx=4900, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*47', idx=4901, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*56', idx=4902, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*64', idx=4903, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*65', idx=4904, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*82', idx=4905, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*83', idx=4906, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='*86', idx=4907, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='A', idx=4910, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='A+RE+PA', idx=4911, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AES', idx=4912, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AI', idx=4913, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ALV', idx=4914, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ARB', idx=4915, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ARG', idx=4916, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ARM', idx=4917, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AROM', idx=4918, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AROM+CYP', idx=4919, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AROM+KO', idx=4920, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='AUR', idx=4921, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BIG', idx=4922, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BOS', idx=4923, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BOS+SI', idx=4924, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BOSf', idx=4925, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BOSm', idx=4926, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='BOSx', idx=4927, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAP', idx=4928, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAP+E', idx=4929, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAPS', idx=4930, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAPf', idx=4931, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAPm', idx=4932, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CAPx', idx=4933, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CERV', idx=4934, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CORN', idx=4935, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CROC', idx=4936, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CUR', idx=4937, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CYP', idx=4938, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CYP+KU', idx=4939, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CYP+O', idx=4940, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CYP+PA', idx=4941, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='CYP+QA', idx=4942, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='DA', idx=4943, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='DE', idx=4944, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='DI', idx=4945, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='DI+PTE', idx=4946, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='DU', idx=4947, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='E', idx=4948, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='EQU', idx=4949, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='EQU+QE', idx=4950, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='EQUf', idx=4951, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='EQUm', idx=4952, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='FAR', idx=4953, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='GAL', idx=4954, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='GRA', idx=4955, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='GRA+PE', idx=4956, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='GUP', idx=4957, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='HA', idx=4958, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='HAS', idx=4959, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='HORD', idx=4960, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='JAC', idx=4961, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KA', idx=4962, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KA+NA+KO', idx=4963, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KA+PO', idx=4964, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KE', idx=4965, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KI', idx=4966, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KO', idx=4967, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='KU', idx=4968, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='L', idx=4969, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='LANA', idx=4970, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='LUNA', idx=4971, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='M', idx=4972, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='MA', idx=4973, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ME', idx=4974, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ME+PO', idx=4975, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ME+RI', idx=4976, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='MI', idx=4977, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='MO', idx=4978, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='MU', idx=4979, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='MUL', idx=4980, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='N', idx=4981, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='NI', idx=4982, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='O', idx=4984, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='O-pe-ro', idx=4985, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE', idx=4986, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+A', idx=4987, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+O', idx=4988, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+PA', idx=4989, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+PO', idx=4990, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+RO', idx=4991, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLE+WE', idx=4992, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLIV', idx=4993, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLIV+A', idx=4994, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OLIV+TI', idx=4995, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OVIS', idx=4996, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OVIS+TA', idx=4997, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OVISf', idx=4998, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OVISm', idx=4999, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='OVISx', idx=5000, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='P', idx=5001, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='PA', idx=5002, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='PE', idx=5003, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='PO', idx=5004, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='PUG', idx=5005, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='Q', idx=5006, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='QE', idx=5007, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='QI', idx=5008, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='QO', idx=5009, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='RA', idx=5010, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='RE', idx=5011, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='RI', idx=5012, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='RO', idx=5013, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ROTA', idx=5014, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ROTA+TE', idx=5015, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='RU', idx=5016, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='S', idx=5017, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SA', idx=5018, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SAG', idx=5019, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUS', idx=5020, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUS+KA', idx=5021, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUS+SI', idx=5022, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUSf', idx=5023, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUSm', idx=5024, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='SUSx', idx=5025, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='T', idx=5026, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TA', idx=5027, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TE', idx=5028, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA', idx=5029, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA+KU', idx=5030, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA+PU', idx=5031, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA+TE', idx=5032, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA1', idx=5033, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA1+PA', idx=5034, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA1+PU', idx=5035, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA1+TE', idx=5036, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA4', idx=5037, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA4+PU', idx=5038, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELA4+ZO', idx=5039, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI', idx=5040, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+KU', idx=5041, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+PA', idx=5042, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+PU', idx=5043, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+TE', idx=5044, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI+ZO', idx=5045, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAI[]+PU', idx=5046, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAx', idx=5047, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+KU', idx=5048, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+PA', idx=5049, completeness=<Completeness.INCOMPLETE: 0>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+PU', idx=5050, completeness=<Completeness.MOSTLY_INCOMPLETE: 1>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELAx+TE', idx=5051, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELHA', idx=5052, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELHA+KU', idx=5053, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELHA+PA', idx=5054, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELHA+PU', idx=5055, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TELHA+TE', idx=5056, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TU+RYO', idx=5057, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TUN', idx=5058, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TUN+KI', idx=5059, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TUN+QE', idx=5060, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='TUN+RI', idx=5061, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='U', idx=5062, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='V', idx=5063, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='VIN', idx=5064, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='VINb', idx=5065, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='VIR', idx=5066, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='WA', idx=5067, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='WE', idx=5068, completeness=<Completeness.UNCERTAIN: 2>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='WI', idx=5069, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='WO', idx=5070, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='Z', idx=5071, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='ZE', idx=5072, completeness=<Completeness.MOSTLY_COMPLETE: 3>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='3', idx=5073, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='4', idx=5074, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='5', idx=5075, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='6', idx=5076, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='7', idx=5077, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='8', idx=5078, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='9', idx=5079, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='10', idx=5080, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='11', idx=5081, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='12', idx=5082, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='13', idx=5083, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='14', idx=5084, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='15', idx=5085, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='16', idx=5086, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='17', idx=5087, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='18', idx=5088, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='19', idx=5089, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='20', idx=5090, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='21', idx=5091, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='22', idx=5092, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='23', idx=5093, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='24', idx=5094, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='25', idx=5095, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='26', idx=5096, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='27', idx=5097, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='28', idx=5098, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='29', idx=5099, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='30', idx=5100, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='31', idx=5101, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='32', idx=5102, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='33', idx=5103, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='34', idx=5104, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='35', idx=5105, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='36', idx=5106, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='37', idx=5107, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='38', idx=5108, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='39', idx=5109, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='40', idx=5110, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='41', idx=5111, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='42', idx=5112, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='43', idx=5113, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='44', idx=5114, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='45', idx=5115, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='46', idx=5116, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='47', idx=5117, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='48', idx=5118, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='49', idx=5119, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='50', idx=5120, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='51', idx=5121, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='52', idx=5122, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='53', idx=5123, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='54', idx=5124, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='55', idx=5125, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='56', idx=5126, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='57', idx=5127, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='58', idx=5128, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='59', idx=5129, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='60', idx=5130, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='61', idx=5131, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='62', idx=5132, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='63', idx=5133, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='64', idx=5134, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='65', idx=5135, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='66', idx=5136, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='67', idx=5137, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='68', idx=5138, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='69', idx=5139, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='70', idx=5140, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='71', idx=5141, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='72', idx=5142, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='73', idx=5143, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='74', idx=5144, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='75', idx=5145, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='76', idx=5146, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='77', idx=5147, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='78', idx=5148, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='79', idx=5149, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='80', idx=5150, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='81', idx=5151, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='82', idx=5152, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='83', idx=5153, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='84', idx=5154, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='85', idx=5155, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='86', idx=5156, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='87', idx=5157, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='88', idx=5158, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='89', idx=5159, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='90', idx=5160, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='91', idx=5161, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='92', idx=5162, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='93', idx=5163, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='94', idx=5164, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='95', idx=5165, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='96', idx=5166, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='97', idx=5167, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='98', idx=5168, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='99', idx=5169, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='100', idx=5170, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='101', idx=5171, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='102', idx=5172, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='103', idx=5173, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='104', idx=5174, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='105', idx=5175, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='106', idx=5176, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='108', idx=5177, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='109', idx=5178, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='110', idx=5179, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='111', idx=5180, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='113', idx=5181, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='114', idx=5182, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='116', idx=5183, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='117', idx=5184, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='120', idx=5185, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='121', idx=5186, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='122', idx=5187, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='123', idx=5188, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='125', idx=5189, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='126', idx=5190, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='127', idx=5191, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='128', idx=5192, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='129', idx=5193, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='130', idx=5194, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='132', idx=5195, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='133', idx=5196, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='134', idx=5197, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='136', idx=5198, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='137', idx=5199, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='140', idx=5200, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='141', idx=5201, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='142', idx=5202, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='143', idx=5203, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='144', idx=5204, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='149', idx=5205, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='150', idx=5206, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='151', idx=5207, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='152', idx=5208, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='153', idx=5209, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='154', idx=5210, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='155', idx=5211, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='157', idx=5212, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='159', idx=5213, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='160', idx=5214, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='162', idx=5215, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='163', idx=5216, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='165', idx=5217, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='167', idx=5218, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='168', idx=5219, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='170', idx=5220, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='172', idx=5221, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='175', idx=5222, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='176', idx=5223, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='177', idx=5224, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='178', idx=5225, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='179', idx=5226, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='180', idx=5227, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='181', idx=5228, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='186', idx=5229, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='188', idx=5230, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='190', idx=5231, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='192', idx=5232, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='195', idx=5233, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='197', idx=5234, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='200', idx=5235, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='202', idx=5236, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='204', idx=5237, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='205', idx=5238, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='208', idx=5239, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='210', idx=5240, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='213', idx=5241, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='214', idx=5242, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='217', idx=5243, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='220', idx=5244, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='225', idx=5245, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='230', idx=5246, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='237', idx=5247, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='240', idx=5248, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='246', idx=5249, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='250', idx=5250, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='252', idx=5251, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='256', idx=5252, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='261', idx=5253, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='264', idx=5254, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='267', idx=5255, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='270', idx=5256, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='277', idx=5257, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='290', idx=5258, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='291', idx=5259, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='300', idx=5260, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='302', idx=5261, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='330', idx=5262, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='339', idx=5263, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='345', idx=5264, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='350', idx=5265, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='354', idx=5266, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='362', idx=5267, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='365', idx=5268, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='372', idx=5269, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='400', idx=5270, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='402', idx=5271, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='405', idx=5272, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='415', idx=5273, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='420', idx=5274, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='421', idx=5275, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='440', idx=5276, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='450', idx=5277, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='453', idx=5278, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='456', idx=5279, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='457', idx=5280, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='500', idx=5281, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='517', idx=5282, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='542', idx=5283, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='590', idx=5284, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='600', idx=5285, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='612', idx=5286, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='640', idx=5287, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='650', idx=5288, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='750', idx=5289, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='780', idx=5290, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='900', idx=5291, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='904', idx=5292, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='980', idx=5293, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1000', idx=5294, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1031', idx=5295, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1034', idx=5296, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1100', idx=5297, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1200', idx=5298, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1222', idx=5299, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1239', idx=5300, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1350', idx=5301, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1370', idx=5302, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1500', idx=5303, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1509', idx=5304, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1700', idx=5305, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='1770', idx=5306, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2000', idx=5307, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2003', idx=5308, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2040', idx=5309, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2252', idx=5310, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2290', idx=5311, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2390', idx=5312, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2440', idx=5313, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='2630', idx=5314, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='3103', idx=5315, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='3300', idx=5316, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='4000', idx=5317, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='4080', idx=5318, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='6010', idx=5319, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='10000', idx=5320, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='10300', idx=5321, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='11900', idx=5322, completeness=<Completeness.COMPLETE: 4>),\n",
       " WordWithCompleteness(lang='transliterated_linear_b', form='19200', idx=5323, completeness=<Completeness.COMPLETE: 4>)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_voc.get_logograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document idx=5373 form='pa-ki-ja-si mu-jo-me-no e-pi wa-na-ka-te a-pi-e-ke o-pi-te-ke-e-u HORD NUM T NUM CYP+PA T 1 V NUM o V NUM FAR 1 T 2 OLIV NUM T 2 *132 S 2 ME S 1 NI 1 BOS 1 OVISm NUM OVISf NUM CAPm 2 CAPf 2 SUS+SI 1 SUSf NUM VIN NUM S 1 *146 2' [36, 22, 17, 57, 2, 29, 19, 26, 33, 2, 6, 38, 2, 68, 30, 20, 61, 2, 5, 38, 6, 21, 2, 8, 38, 61, 21, 6, 9, 2, 244, 2, 267, 2, 310, 2, 267, 2, 225, 2, 310, 2, 192, 2, 347, 2, 267, 2, 8, 2, 347, 2, 267, 2, 237, 2, 192, 2, 310, 2, 193, 2, 277, 2, 267, 2, 310, 2, 193, 2, 93, 2, 301, 2, 193, 2, 258, 2, 301, 2, 192, 2, 266, 2, 192, 2, 207, 2, 192, 2, 283, 2, 267, 2, 282, 2, 267, 2, 216, 2, 193, 2, 215, 2, 193, 2, 306, 2, 192, 2, 307, 2, 267, 2, 348, 2, 267, 2, 301, 2, 192, 2, 95, 2, 193, 4] 70 [WordWithCompleteness(lang='transliterated_linear_b', form='pa-ki-ja-si', idx=2855, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='mu-jo-me-no', idx=2441, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='e-pi', idx=1075, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='wa-na-ka-te', idx=4505, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='a-pi-e-ke', idx=352, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='o-pi-te-ke-e-u', idx=2719, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='HORD', idx=4960, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='T', idx=5026, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='CYP+PA', idx=4941, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='T', idx=5026, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='V', idx=5063, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='o', idx=2593, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='V', idx=5063, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='FAR', idx=4953, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='T', idx=5026, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='OLIV', idx=4993, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='T', idx=5026, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='*132', idx=4809, completeness=<Completeness.COMPLETE: 4>), WordWithCompleteness(lang='transliterated_linear_b', form='S', idx=5017, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='ME', idx=4974, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='S', idx=5017, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NI', idx=4982, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='BOS', idx=4923, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='OVISm', idx=4999, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='OVISf', idx=4998, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='CAPm', idx=4932, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='CAPf', idx=4931, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='SUS+SI', idx=5022, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='SUSf', idx=5023, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='VIN', idx=5064, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='NUM', idx=4983, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='S', idx=5017, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='1', idx=4908, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='*146', idx=4811, completeness=<Completeness.MOSTLY_COMPLETE: 3>), WordWithCompleteness(lang='transliterated_linear_b', form='2', idx=4909, completeness=<Completeness.MOSTLY_COMPLETE: 3>)]\n"
     ]
    }
   ],
   "source": [
    "cset = get_charset(\"transliterated_linear_b\")\n",
    "len(cset), cset.id2char(92), cset.id2char(4), cset.char2id('ta'), cset.id2char(60)\n",
    "for doc in corpus.documents.values():\n",
    "    if len(cset) in doc.tokenized_sequence:\n",
    "        print(doc, doc.tokenized_sequence, doc.tokenized_sequence.index(len(cset)), doc.word_list)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*132',\n",
       " '*142',\n",
       " '*146',\n",
       " '*146+PE',\n",
       " '*1462',\n",
       " '*150',\n",
       " '*152',\n",
       " '*153',\n",
       " '*154',\n",
       " '*157',\n",
       " '*158',\n",
       " '*160',\n",
       " '*161',\n",
       " '*164',\n",
       " '*1641',\n",
       " '*165',\n",
       " '*166',\n",
       " '*166+WE',\n",
       " '*167',\n",
       " '*167+PE',\n",
       " '*168',\n",
       " '*168+SE',\n",
       " '*169',\n",
       " '*170',\n",
       " '*171',\n",
       " '*172',\n",
       " '*172+KE',\n",
       " '*172+KE+RO2',\n",
       " '*174',\n",
       " '*177',\n",
       " '*178',\n",
       " '*179',\n",
       " '*180',\n",
       " '*181',\n",
       " '*182',\n",
       " '*183',\n",
       " '*184',\n",
       " '*185',\n",
       " '*189',\n",
       " '*190',\n",
       " '*22',\n",
       " '*220',\n",
       " '*232',\n",
       " '*234',\n",
       " '*245',\n",
       " '*246',\n",
       " '*248',\n",
       " '*249',\n",
       " '*251',\n",
       " '*253',\n",
       " '*255',\n",
       " '*256',\n",
       " '*257',\n",
       " '*258',\n",
       " '*259',\n",
       " '*260',\n",
       " '*34',\n",
       " '*47',\n",
       " '*56',\n",
       " '*64',\n",
       " '*65',\n",
       " '*82',\n",
       " '*83',\n",
       " '*86',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '10300',\n",
       " '1031',\n",
       " '1034',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '108',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '111',\n",
       " '113',\n",
       " '114',\n",
       " '116',\n",
       " '117',\n",
       " '11900',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '121',\n",
       " '122',\n",
       " '1222',\n",
       " '123',\n",
       " '1239',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '1350',\n",
       " '136',\n",
       " '137',\n",
       " '1370',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '149',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1509',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '157',\n",
       " '159',\n",
       " '16',\n",
       " '160',\n",
       " '162',\n",
       " '163',\n",
       " '165',\n",
       " '167',\n",
       " '168',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '172',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '1770',\n",
       " '178',\n",
       " '179',\n",
       " '18',\n",
       " '180',\n",
       " '181',\n",
       " '186',\n",
       " '188',\n",
       " '19',\n",
       " '190',\n",
       " '192',\n",
       " '19200',\n",
       " '195',\n",
       " '197',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '202',\n",
       " '204',\n",
       " '2040',\n",
       " '205',\n",
       " '208',\n",
       " '21',\n",
       " '210',\n",
       " '213',\n",
       " '214',\n",
       " '217',\n",
       " '22',\n",
       " '220',\n",
       " '225',\n",
       " '2252',\n",
       " '2290',\n",
       " '23',\n",
       " '230',\n",
       " '237',\n",
       " '2390',\n",
       " '24',\n",
       " '240',\n",
       " '2440',\n",
       " '246',\n",
       " '25',\n",
       " '250',\n",
       " '252',\n",
       " '256',\n",
       " '26',\n",
       " '261',\n",
       " '2630',\n",
       " '264',\n",
       " '267',\n",
       " '27',\n",
       " '270',\n",
       " '277',\n",
       " '28',\n",
       " '29',\n",
       " '290',\n",
       " '291',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '302',\n",
       " '31',\n",
       " '3103',\n",
       " '32',\n",
       " '33',\n",
       " '330',\n",
       " '3300',\n",
       " '339',\n",
       " '34',\n",
       " '345',\n",
       " '35',\n",
       " '350',\n",
       " '354',\n",
       " '36',\n",
       " '362',\n",
       " '365',\n",
       " '37',\n",
       " '372',\n",
       " '38',\n",
       " '39',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '402',\n",
       " '405',\n",
       " '4080',\n",
       " '41',\n",
       " '415',\n",
       " '42',\n",
       " '420',\n",
       " '421',\n",
       " '43',\n",
       " '44',\n",
       " '440',\n",
       " '45',\n",
       " '450',\n",
       " '453',\n",
       " '456',\n",
       " '457',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '517',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '542',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '590',\n",
       " '6',\n",
       " '60',\n",
       " '600',\n",
       " '6010',\n",
       " '61',\n",
       " '612',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '640',\n",
       " '65',\n",
       " '650',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '7',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '750',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '780',\n",
       " '79',\n",
       " '8',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '9',\n",
       " '90',\n",
       " '900',\n",
       " '904',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '980',\n",
       " '99',\n",
       " 'A',\n",
       " 'AI',\n",
       " 'DA',\n",
       " 'DE',\n",
       " 'DI',\n",
       " 'DI+PTE',\n",
       " 'DU',\n",
       " 'E',\n",
       " 'HA',\n",
       " 'KA',\n",
       " 'KE',\n",
       " 'KI',\n",
       " 'L',\n",
       " 'M',\n",
       " 'ME',\n",
       " 'ME+PO',\n",
       " 'MI',\n",
       " 'MU',\n",
       " 'N',\n",
       " 'NUM',\n",
       " 'O',\n",
       " 'O-pe-ro',\n",
       " 'P',\n",
       " 'PA',\n",
       " 'PE',\n",
       " 'PO',\n",
       " 'Q',\n",
       " 'QE',\n",
       " 'QI',\n",
       " 'QO',\n",
       " 'RA',\n",
       " 'RE',\n",
       " 'RI',\n",
       " 'RO',\n",
       " 'RU',\n",
       " 'S',\n",
       " 'T',\n",
       " 'TA',\n",
       " 'TE',\n",
       " 'U',\n",
       " 'V',\n",
       " 'WA',\n",
       " 'WE',\n",
       " 'WI',\n",
       " 'WO',\n",
       " 'Z'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logos = set([v.form for v in my_voc.get_logograms])\n",
    "logos -= set(my_voc.logogram_vocab.keys())\n",
    "logos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_file = os.path.join(prefix_path, \"linb_words_translation.tsv\")\n",
    "with open(dump_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "    # header\n",
    "    writer.writerow([\"word\", \"completeness_level\", \"documents_info\"])\n",
    "\n",
    "    for w in my_voc.get_words:\n",
    "        tuple_to_save = [\n",
    "            w.form,\n",
    "            w.completeness.name,\n",
    "            [(doc_id, complete) for (doc_id, complete) in sequence_data[w.form][0].items()]\n",
    "        ]\n",
    "        writer.writerow(tuple_to_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word', 'completeness_level', 'documents_info']\n",
      "['*18-jo', 'INCOMPLETE', '[(3402, False)]']\n",
      "['*18-to-no', 'COMPLETE', '[(2654, True)]']\n",
      "['*22-ja-ro', 'COMPLETE', '[(911, True)]']\n",
      "['*22-je-mi', 'INCOMPLETE', '[(930, False)]']\n",
      "['*22-jo', 'INCOMPLETE', '[(1069, False)]']\n",
      "['*22-ri-ta-ro', 'COMPLETE', '[(3573, True)]']\n",
      "['*34-ka', 'INCOMPLETE', '[(4454, False)]']\n",
      "['*34-ka-te-re', 'COMPLETE', '[(5399, True)]']\n",
      "['*34-ke-ja', 'COMPLETE', '[(4891, True), (4986, True)]']\n"
     ]
    }
   ],
   "source": [
    "with open(dump_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 9:  # header + 9 rows = 10 total\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dU1iu5QQH-O",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### AuxiliaryClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "sHb6DLR_QscX"
   },
   "outputs": [],
   "source": [
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "DATASET_PATH = os.path.join(prefix_path, \"classifiers_dataset.csv\")\n",
    "LOST_LANG = \"transliterated_linear_b\"\n",
    "KNOWN_LANG = \"greek\"\n",
    "TASKS = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "classes = [4, 4, 3]\n",
    "CLASSES_PER_TASK = dict(zip(TASKS, classes))\n",
    "TASK = TASKS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "naO519QW_y0D"
   },
   "outputs": [],
   "source": [
    "def parse_file_simple(data_path, task):\n",
    "   with open(data_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "       reader = csv.reader(csvfile)\n",
    "       header = next(reader)\n",
    "       # Ensure the task is one of the fields in position 1, 2, or 3\n",
    "       valid_fields = header[1:4]  # Fields at index 1, 2, 3\n",
    "       assert task in valid_fields, f\"Task must be one of the fields: {valid_fields}. Got '{task}'.\"\n",
    "       task_index = header.index(task)\n",
    "       dataset = []\n",
    "       for row in reader:\n",
    "           if len(row) > task_index and row[task_index] != \"-1\":\n",
    "               form = row[0].strip()\n",
    "               dataset.append((form, int(row[task_index])))\n",
    "   return dataset\n",
    "\n",
    "\n",
    "class AuxiliaryClassifier:\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        if self.task == \"word_type\":\n",
    "            self.labels = [\"anthroponym/animal name/theonym\", \"toponym\", \"ethnonym\", \"common\"]\n",
    "        elif self.task == \"part_of_speech\":\n",
    "            self.labels = [\"noun\", \"verb\", \"adjective\", \"adverb\"]\n",
    "        elif self.task == \"inflection\":\n",
    "            self.labels = [\"thematic in -o\", \"thematic in -a\", \"athematic\"]\n",
    "        self.labels = {k: v for (k, v) in enumerate(self.labels)}\n",
    "        self.features = None\n",
    "\n",
    "    def retrieve_data(self, data_path):\n",
    "        data = parse_file_simple(data_path, self.task)\n",
    "        return data\n",
    "\n",
    "    def tokenize_data(self, data):\n",
    "        assert data is not None and len(data) > 0, \"Data is empty.\"\n",
    "        if isinstance(data[0], tuple) and len(data[0]) == 2:\n",
    "            assert self.features is None, \"Features already initialized.\"\n",
    "            self.features = FeatureUnion([\n",
    "                (\"syllables\", CountVectorizer(tokenizer=lambda x: x.split(\"-\"), token_pattern=None)),\n",
    "                (\"char_ngrams\", TfidfVectorizer(analyzer='char', ngram_range=(2,4), preprocessor=lambda x: x.replace(\"-\", \"\"))),\n",
    "            ])\n",
    "            X, y = zip(*data)\n",
    "            X_features = self.features.fit_transform(X)\n",
    "            return X_features, y\n",
    "        elif isinstance(data[0], str):\n",
    "            assert self.features is not None, \"Features not initialized, cannot run the model.\"\n",
    "            X_features = self.features.transform(data)\n",
    "            return X_features\n",
    "        else:\n",
    "            raise ValueError(\"Data must be a list of tuples (word, label) or a list of strings.\")\n",
    "\n",
    "    def split_data(self, X, y, test_size=0.2, random_state=SEED):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def initialize_model(self, model_cls, **kwargs):\n",
    "        self.model = model_cls(**kwargs)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not initialized. Call initialize_model first.\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not initialized. Call initialize_model first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"AuxiliaryClassifier({self.task})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "RhlRGUiDGk1Y"
   },
   "outputs": [],
   "source": [
    "def train_classifier(task, data_path=DATASET_PATH, model_cls=LinearSVC, seed=SEED, **kwargs):\n",
    "    aux = AuxiliaryClassifier(task)\n",
    "    data = aux.retrieve_data(data_path)\n",
    "    X, y = aux.tokenize_data(data)\n",
    "    X_train, X_test, y_train, y_test = aux.split_data(X, y)\n",
    "    aux.initialize_model(model_cls, random_state=seed, **kwargs)\n",
    "    aux.fit(X_train, y_train)\n",
    "    y_test_pred = aux.predict(X_test)\n",
    "    #print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "    #print(classification_report(y_test, y_test_pred, target_names=aux.labels.values()))\n",
    "    #print(confusion_matrix(y_test, y_test_pred))\n",
    "    return aux\n",
    "\n",
    "def use_classifier(aux, words):\n",
    "    assert aux is not None, \"Classifier not initialized. Call train_classifier first.\"\n",
    "    X = aux.tokenize_data(words)\n",
    "    y_pred = aux.predict(X)\n",
    "    labels = {}\n",
    "    for word, label in zip(words, y_pred):\n",
    "        labels[word]=aux.labels[label]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaTDgRdkQBOl"
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "K7Pv1CdiP_7Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'wa-na-ka': 'common',\n",
       "  'ko-no-so': 'anthroponym/animal name/theonym',\n",
       "  'a-mi-ni-si-jo': 'toponym'},\n",
       " {'wa-na-ka': 'noun', 'ko-no-so': 'noun', 'a-mi-ni-si-jo': 'noun'},\n",
       " {'wa-na-ka': 'thematic in -a',\n",
       "  'ko-no-so': 'thematic in -o',\n",
       "  'a-mi-ni-si-jo': 'thematic in -o'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_classifiers = [train_classifier(TASKS[i], max_iter=2000) for i in range(len(TASKS))]\n",
    "sentence = [\"wa-na-ka\", \"ko-no-so\", \"a-mi-ni-si-jo\"]\n",
    "class_results = [use_classifier(aux, sentence) for aux in aux_classifiers]\n",
    "class_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez_-5lhQkHBD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Auxiliary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "zyylabH6kKAE"
   },
   "outputs": [],
   "source": [
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "DATASET_PATH = os.path.join(prefix_path, \"classifiers_dataset.csv\")\n",
    "LOST_LANG = \"transliterated_linear_b\"\n",
    "KNOWN_LANG = \"greek\"\n",
    "TASKS = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "classes = [4, 4, 3]\n",
    "CLASSES_PER_TASK = dict(zip(TASKS, classes))\n",
    "TASK = TASKS[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gisi9B669oNQ"
   },
   "source": [
    "#### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJgQxdw-ZuJw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_file(dataset_path, task, vocab, lang=LOST_LANG):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "\n",
    "        # Ensure the task is one of the fields in position 1, 2, or 3\n",
    "        valid_fields = header[1:4]  # Fields at index 1, 2, 3\n",
    "        assert task in valid_fields, f\"Task must be one of the fields: {valid_fields}. Got '{task}'.\"\n",
    "\n",
    "        task_index = header.index(task)\n",
    "        dataset = []\n",
    "        next_idx = len(get_words(lang))\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) > task_index and row[task_index] != \"-1\":\n",
    "                form = row[0].strip()\n",
    "                try:\n",
    "                    word = vocab.get_word_from_form(form)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Word '{form}' not found in vocabulary, creating new Word object.\")\n",
    "                    #logging.error(\"Exception occurred\", exc_info=True)\n",
    "                    word = Word(lang, form, next_idx)\n",
    "                    next_idx += 1\n",
    "\n",
    "                dataset.append((word, int(row[task_index])))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "try:\n",
    "    build_vocabs(COG_PATH, LOST_LANG, KNOWN_LANG)\n",
    "except Exception as e:\n",
    "    logging.warning(\"Vocabulary already initialized\")\n",
    "luo_lb_vocab = get_vocab(LOST_LANG)\n",
    "data = parse_file(DATASET_PATH, TASK, luo_lb_vocab)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2/0.8, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk0tqyCsxdsb"
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "M6wh6yP3q6FD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def collate_aux(batch):\n",
    "    words = _get_item('word', batch)\n",
    "    xs = _get_item('x', batch)\n",
    "    ys = _get_item('y', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "\n",
    "    lengths, words, xs, ys, forms = sort_all(words, xs, ys, forms)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    xs = pad_to_dense(xs, dtype='l')\n",
    "\n",
    "    xs = get_tensor(xs[:, :max_len])\n",
    "    ys = get_tensor(ys, dtype='l')\n",
    "\n",
    "    lang = batch[0].lang\n",
    "    return Map(words=words, x=xs, y=ys, forms=forms, lengths=lengths, lang=lang, num_samples=len(words))\n",
    "\n",
    "\n",
    "class AuxiliaryClassifierDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word, y = self.data[idx]\n",
    "        x = word.id_seq\n",
    "        return Map(word=word, x=x, y=y, form=word.form, lang=word.lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        return collate_aux([self[i] for i in range(len(self))])  # real code!\n",
    "\n",
    "class AuxiliaryDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.dataset)\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_aux)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return self.dataset.entire_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJyuylWgxX58"
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "yz1-bWI5xW5A"
   },
   "outputs": [],
   "source": [
    "dataset = AuxiliaryClassifierDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "12DW2ZSp8I_j"
   },
   "outputs": [],
   "source": [
    "entire_batch = dataset.entire_batch\n",
    "xs = entire_batch.x\n",
    "forms = entire_batch.forms\n",
    "results = []\n",
    "\n",
    "for i in range(xs.shape[0]):\n",
    "    form_parts = forms[i].split('-')\n",
    "    for j in range(xs.shape[1]):\n",
    "        if xs[i, j].item() == UNK_ID:\n",
    "            # Check if j is within the parts range\n",
    "            char_at_j = form_parts[j] if j < len(form_parts) else None\n",
    "            if char_at_j == \"ge\": logging.critical(f\"PORCODIO {forms[i]}\")\n",
    "            results.append((i, j, char_at_j))\n",
    "\n",
    "# Print all occurrences\n",
    "for i, j, char in results:\n",
    "    print(f\"Row {i}, Index {j}: Token == 3, Corresponding char in form: {char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "2GPiRgTLITEo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:{'words': array([Word(lang='transliterated_linear_b', form='e-re-wi-jo-po-ti-ni-ja', idx=524),\n",
      "       Word(lang='transliterated_linear_b', form='de-we-ro-ai-ko-ra-i-ja', idx=322),\n",
      "       Word(lang='transliterated_linear_b', form='e-u-ru-po-to-re-mo-jo', idx=578),\n",
      "       ..., Word(lang='transliterated_linear_b', form='o', idx=1062),\n",
      "       Word(lang='transliterated_linear_b', form='dwo', idx=409),\n",
      "       Word(lang='transliterated_linear_b', form='pe', idx=1931)],\n",
      "      dtype=object), 'x': tensor([[ 6, 48, 70,  ..., 32, 17,  2],\n",
      "        [11, 69, 50,  ...,  7, 17,  2],\n",
      "        [ 6,  9, 51,  ..., 28, 19,  2],\n",
      "        ...,\n",
      "        [ 8,  2,  0,  ...,  0,  0,  0],\n",
      "        [16,  2,  0,  ...,  0,  0,  0],\n",
      "        [37,  2,  0,  ...,  0,  0,  0]], device='cuda:0'), 'y': tensor([0, 1, 0,  ..., 3, 3, 0], device='cuda:0'), 'forms': array(['e-re-wi-jo-po-ti-ni-ja', 'de-we-ro-ai-ko-ra-i-ja',\n",
      "       'e-u-ru-po-to-re-mo-jo', ..., 'o', 'dwo', 'pe'], dtype=object), 'lengths': tensor([9, 9, 9,  ..., 2, 2, 2], device='cuda:0'), 'lang': 'transliterated_linear_b', 'num_samples': 3162}\n"
     ]
    }
   ],
   "source": [
    "logging.critical(entire_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "xpEx8_fjI45-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:{'words': array([Word(lang='transliterated_linear_b', form='ha-ka-ha-ki-ri-jo', idx=3110),\n",
      "       Word(lang='transliterated_linear_b', form='ne-qa-sa-pi', idx=2701),\n",
      "       Word(lang='transliterated_linear_b', form='ta-si-ko-no', idx=2986),\n",
      "       Word(lang='transliterated_linear_b', form='ai-ka-na-jo', idx=3125),\n",
      "       Word(lang='transliterated_linear_b', form='a-te-mi-to', idx=226),\n",
      "       Word(lang='transliterated_linear_b', form='pe-ko-to', idx=1226),\n",
      "       Word(lang='transliterated_linear_b', form='o-ne-u', idx=1090),\n",
      "       Word(lang='transliterated_linear_b', form='a-ro-zo', idx=2113),\n",
      "       Word(lang='transliterated_linear_b', form='ke-ra-no', idx=760),\n",
      "       Word(lang='transliterated_linear_b', form='to-e', idx=1705)],\n",
      "      dtype=object), 'x': tensor([[75, 20, 75, 22, 49, 19,  2],\n",
      "        [31, 43, 55, 38,  2,  0,  0],\n",
      "        [60, 57, 23, 33,  2,  0,  0],\n",
      "        [76, 20, 30, 19,  2,  0,  0],\n",
      "        [ 5, 61, 27, 63,  2,  0,  0],\n",
      "        [37, 23, 63,  2,  0,  0,  0],\n",
      "        [ 8, 31,  9,  2,  0,  0,  0],\n",
      "        [ 5, 50, 74,  2,  0,  0,  0],\n",
      "        [21, 47, 33,  2,  0,  0,  0],\n",
      "        [63,  6,  2,  0,  0,  0,  0]], device='cuda:0'), 'y': tensor([1, 3, 0, 3, 0, 3, 0, 3, 3, 3], device='cuda:0'), 'forms': array(['ha-ka-ha-ki-ri-jo', 'ne-qa-sa-pi', 'ta-si-ko-no', 'ai-ka-na-jo',\n",
      "       'a-te-mi-to', 'pe-ko-to', 'o-ne-u', 'a-ro-zo', 'ke-ra-no', 'to-e'],\n",
      "      dtype=object), 'lengths': tensor([7, 5, 5, 5, 5, 4, 4, 4, 4, 3], device='cuda:0'), 'lang': 'transliterated_linear_b', 'num_samples': 10}\n"
     ]
    }
   ],
   "source": [
    "dataloader = AuxiliaryDataLoader(dataset, batch_size=10)\n",
    "for batch in dataloader:\n",
    "    logging.critical(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4g4oCsQPwJy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "B1hBxAYfPyEj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BRNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, num_classes, depth=1):\n",
    "        super(BRNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers,\n",
    "            bidirectional=True, batch_first=True, dropout=dropout #perche bidirectional\n",
    "        )\n",
    "\n",
    "        input_size = hidden_size * 2  # bidirectional\n",
    "        #layers = []\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        #for _ in range(depth):\n",
    "        #    layers.append(nn.Linear(input_size, input_size))\n",
    "        #    layers.append(nn.ReLU())\n",
    "        #    layers.append(nn.Dropout(dropout))\n",
    "        #    layers.append(nn.LayerNorm(input_size))\n",
    "\n",
    "        #self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(input_size, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        # Pooling over the sequence dimension (dim=1)\n",
    "        out = out.transpose(1, 2)  # shape: (batch_size, hidden_size, seq_len)\n",
    "        pooled = self.avg_pool(out).squeeze(-1)  # shape: (batch_size, hidden_size*2)\n",
    "\n",
    "        ## Pass through hidden layers and classification head\n",
    "        #pooled = self.hidden_layers(pooled)  # (batch_size, hidden_size*2)\n",
    "        logits = self.output_layer(pooled)   # (batch_size, num_classes)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        return log_probs  # shape: (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "b-1oJohYxhxU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BRNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, num_classes, depth=1):\n",
    "        super(BRNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers,\n",
    "            bidirectional=True, batch_first=True, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=16, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        rnn_out, _ = self.brnn(packed_embedded)\n",
    "        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True, total_length=batch.x.shape[1])  # [B, L, H*2]\n",
    "\n",
    "        # --- Attention: Self-attention over RNN outputs ---\n",
    "        # MultiheadAttention expects input of shape (batch, seq, embed) when batch_first=True\n",
    "        # Generate attention mask (True = ignore, False = keep)\n",
    "        max_len = batch.x.size(1)\n",
    "        attn_mask = torch.arange(max_len, device=batch.lengths.device)[None, :] >= batch.lengths[:, None]  # [B, L]\n",
    "\n",
    "        # Apply multi-head attention (query=key=value=rnn_out)\n",
    "        attended_out, _ = self.attention(rnn_out, rnn_out, rnn_out, key_padding_mask=attn_mask)  # [B, L, H*2]\n",
    "\n",
    "        # Reduce to a single vector (e.g. mean over time, ignoring pads)\n",
    "        attn_mask_float = (~attn_mask).float().unsqueeze(-1)  # [B, L, 1]\n",
    "        summed = (attended_out * attn_mask_float).sum(dim=1)  # [B, H*2]\n",
    "        lengths = batch.lengths.unsqueeze(1).clamp(min=1).to(summed.dtype)\n",
    "        pooled = summed / lengths  # [B, H*2]\n",
    "\n",
    "        logits = self.output_layer(pooled)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "qvQUM4fUakYC"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv1dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv1dLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x))\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A single fully connected block with activation, dropout, and optional normalization.\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, normalize):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = normalize\n",
    "        if self.normalize:\n",
    "            self.layer_norm = nn.LayerNorm(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ConvTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, num_layers, dropout, normalize, num_classes):\n",
    "        super(ConvTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers: each kernel size gets its own Conv1D layer\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # Fully connected layers\n",
    "        #self.fc_layers = [FCBlock(num_filters * len(kernel_sizes), num_filters * len(kernel_sizes), dropout, normalize) for _ in range(num_layers-1)]\n",
    "        #self.fc_layers.append(FCBlock(num_filters * len(kernel_sizes), num_filters, dropout, normalize))\n",
    "        #self.fc_layers = nn.ModuleList(self.fc_layers)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(batch.x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "\n",
    "        #conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        ## Fully connected layers with normalization\n",
    "        #for fc in self.fc_layers:\n",
    "        #    conv_out = fc(conv_out)\n",
    "\n",
    "        #out = conv_out.transpose(1, 2)  # shape: (B, C*k, L)\n",
    "        pooled = self.avg_pool(conv_out).squeeze(-1)  # shape: (B, C*k)\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(pooled) # shape: (B, out_dim)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW33X_JoQi-y"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "mGgC3cIpQlA0"
   },
   "outputs": [],
   "source": [
    "@has_properties('num_epochs', 'saved_path', 'learning_rate', 'log_dir', 'save_all', 'eval_interval', 'check_interval')\n",
    "class TextClassifierTrainer:\n",
    "    def __init__(self, model, train_data_loader, test_data_loader, num_epochs, saved_path, learning_rate, log_dir, save_all, eval_interval, check_interval):\n",
    "        self.tracker = Tracker('text-classifier')\n",
    "        stage = self.tracker.add_stage('round', self.num_epochs)\n",
    "        stage.add_stage('train step')\n",
    "        self.tracker.fix_schedule()\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self._init_optimizer()\n",
    "        self._init_loss()\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_loss(self):\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"none\")\n",
    "        self.nll_loss2 = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"mean\")\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'train step':\n",
    "            self._do_train_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return self.round_num\n",
    "\n",
    "    def _do_train_step(self, evaluator):\n",
    "        self._train_step_kernel()\n",
    "        self._do_post_train_step(evaluator)\n",
    "\n",
    "    def _train_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._do_train_step_batch(batch)\n",
    "\n",
    "    def _do_post_train_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        eval_scores = evaluator.evaluate(self.epoch)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            self.tb_writer.add_scalar(setting, score, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "\n",
    "        #debug\n",
    "        train_preds = self.model(self.train_data_loader.entire_batch)\n",
    "        train_acc = evaluator.compute_accuracy(train_preds, self.train_data_loader.entire_batch.y)\n",
    "        train_loss = self.nll_loss2(train_preds, self.train_data_loader.entire_batch.y)\n",
    "        logging.critical(f\"'train_acc', {train_acc.mean}, loss: {train_loss}, global_step={self.epoch}\")\n",
    "\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_train_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        model_ret = self.model(batch)\n",
    "        if update:\n",
    "            self._do_train_step_batch_update(model_ret, batch)\n",
    "        return model_ret\n",
    "\n",
    "    def _do_train_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        #metrics.loss.mean.backward()\n",
    "        lss = self.nll_loss2(model_ret, batch.y)\n",
    "        #logging.critical(f\"Loss Value: {lss}\")\n",
    "        lss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        # NOTE This means we are conditioning on one specific flow.\n",
    "        #logging.critical(f\"{model_ret.shape}, {batch.y.shape}, {batch.x.shape}\")\n",
    "        nll_loss = self.nll_loss(model_ret, batch.y)\n",
    "\n",
    "        nll_loss = Metric('nll_loss', nll_loss.sum(dim=-1), batch.num_samples)\n",
    "        loss = Metric('loss', nll_loss.mean, 1)\n",
    "\n",
    "        return Metrics(loss, nll_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3o3VulFSA6a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "qaHOhNmDSDQs"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TextClassifierEvaluator:\n",
    "    def __init__(self, model, data_loader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'accuracy'\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'accuracy'\n",
    "\n",
    "        metrics = Metrics()\n",
    "\n",
    "        for batch in self.data_loader:\n",
    "            model_ret = self.model(batch)\n",
    "            # Magic tensor to the rescue!\n",
    "            gt = batch.y\n",
    "            preds = model_ret\n",
    "\n",
    "            seq_acc = self.compute_accuracy(preds, gt)\n",
    "\n",
    "            new_metrics = Metrics(seq_acc)\n",
    "            metrics += new_metrics\n",
    "\n",
    "        values = [epoch] + [getattr(metrics, field).mean for field in table.field_names[1:]]\n",
    "        table.add_row(values)\n",
    "\n",
    "        eval_scores = {fn: val for (fn, val) in zip(table.field_names, values)}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def compute_accuracy(self, preds, gt):\n",
    "        logging.critical(preds.shape)\n",
    "        pred_indices = preds.argmax(dim=-1)\n",
    "        logging.critical(pred_indices.shape)\n",
    "        # Compare with ground truth and check where predictions are correct\n",
    "        correct = (pred_indices == gt)\n",
    "        # Mean over the batch\n",
    "        accuracy = correct.sum(dim=-1)\n",
    "        return Metric('accuracy', accuracy, len(pred_indices))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O_exCdBTx85",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "7chanGmKTz87"
   },
   "outputs": [],
   "source": [
    "\n",
    "@has_properties('data', 'lang', 'batch_size')\n",
    "class TextClassifierManager:\n",
    "\n",
    "    model_cls = BRNNTextClassifier\n",
    "    #model_cls = ConvTextClassifier\n",
    "    trainer_cls = TextClassifierTrainer\n",
    "\n",
    "    def __init__(self, data, lang, batch_size, args):\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.test_data_loader, args[\"num_epochs\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"save_all\"], args[\"eval_interval\"], args[\"check_interval\"])\n",
    "        self.evaluator = TextClassifierEvaluator(self.model, self.test_data_loader)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        train_data, test_data = self.data\n",
    "        train_dataset = AuxiliaryClassifierDataset(train_data)\n",
    "        self.train_data_loader = AuxiliaryDataLoader(train_dataset, self.batch_size)\n",
    "        test_dataset = AuxiliaryClassifierDataset(test_data)\n",
    "        self.test_data_loader = AuxiliaryDataLoader(test_dataset, self.batch_size)\n",
    "\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        self.model = type(self).model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"num_classes\"], args[\"depth\"])\n",
    "        #self.model = type(self).model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"num_filters\"], args[\"kernel_sizes\"], args[\"num_layers\"], args[\"dropout\"], args[\"normalize\"], args[\"num_classes\"])\n",
    "\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6her1ZpgLaS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0ORBW5ZeWb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'check_interval': 10,\n",
      "           'depth': 0,\n",
      "           'dropout': 0.2,\n",
      "           'embed_size': 256,\n",
      "           'eval_interval': 10,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 512,\n",
      "           'learning_rate': 0.0001,\n",
      "           'log_dir': './DMPROJECT/auxiliary_classifiers/word_type',\n",
      "           'log_level': 'INFO',\n",
      "           'num_classes': 4,\n",
      "           'num_epochs': 200,\n",
      "           'num_layers': 16,\n",
      "           'save_all': None,\n",
      "           'saved_path': None,\n",
      "           'task': 'word_type',\n",
      "           'vocab_size': 93}\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:172 - \n",
      "          BRNNTextClassifier(\n",
      "            (embedding): Embedding(93, 256, padding_idx=0)\n",
      "            (brnn): RNN(256, 512, num_layers=16, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "            (attention): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output_layer): Linear(in_features=1024, out_features=4, bias=True)\n",
      "            (log_softmax): LogSoftmax(dim=-1)\n",
      "          )\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>round  12%|                                         |  24/200 [10:35&lt;1h 17:37, 0.04 samples/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mIMP - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:143 - *STARTING* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *FINISHED* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *STARTING* _init_loss\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *FINISHED* _init_loss\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:172 - \n",
      "          Stage(name=text-classifier, num_steps=1)\n",
      "           Stage(name=round, num_steps=200)\n",
      "               Stage(name=train step, num_steps=1)\n",
      "          \n",
      "          \u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:172 - \n",
      "          +-------+----------+\n",
      "          | epoch | accuracy |\n",
      "          +-------+----------+\n",
      "          +-------+----------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([25, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([25])\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:05 - 0:04:25 at 1009358421.py:172 - \n",
      "          +-----------------------------------------+\n",
      "          |                Epoch: 10                |\n",
      "          +-------+---------------------------------+\n",
      "          | epoch | accuracy                        |\n",
      "          +-------+---------------------------------+\n",
      "          | 10    | tensor(0.3160, device='cuda:0') |\n",
      "          +-------+---------------------------------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2442567354.py:41 - torch.Size([2529, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2442567354.py:43 - torch.Size([2529])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:104 - 'train_acc', 0.3222617506980896, loss: 0.7462226152420044, global_step=10\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:42 - Saving to ./DMPROJECT/auxiliary_classifiers/word_type\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:51 - Finished saving decipher trainer\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 1009358421.py:172 - \n",
      "          +--------------------------------------+\n",
      "          |              Epoch: 10               |\n",
      "          +----------+----------+--------+-------+\n",
      "          | name     | value    | weight | mean  |\n",
      "          +----------+----------+--------+-------+\n",
      "          | loss     | 288.403  | 800    | 0.361 |\n",
      "          | nll_loss | 9035.674 | 25290  | 0.357 |\n",
      "          +----------+----------+--------+-------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([25, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([25])\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:29 - 0:08:49 at 1009358421.py:172 - \n",
      "          +-----------------------------------------+\n",
      "          |                Epoch: 20                |\n",
      "          +-------+---------------------------------+\n",
      "          | epoch | accuracy                        |\n",
      "          +-------+---------------------------------+\n",
      "          | 20    | tensor(0.2970, device='cuda:0') |\n",
      "          +-------+---------------------------------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2442567354.py:41 - torch.Size([2529, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2442567354.py:43 - torch.Size([2529])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2691311305.py:104 - 'train_acc', 0.3396599292755127, loss: 0.71871018409729, global_step=20\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:30 - 0:08:50 at 2691311305.py:42 - Saving to ./DMPROJECT/auxiliary_classifiers/word_type\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:31 - 0:08:51 at 2691311305.py:51 - Finished saving decipher trainer\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:31 - 0:08:51 at 1009358421.py:172 - \n",
      "          +--------------------------------------+\n",
      "          |              Epoch: 20               |\n",
      "          +----------+----------+--------+-------+\n",
      "          | name     | value    | weight | mean  |\n",
      "          +----------+----------+--------+-------+\n",
      "          | loss     | 268.186  | 800    | 0.335 |\n",
      "          | nll_loss | 8412.519 | 25290  | 0.333 |\n",
      "          +----------+----------+--------+-------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = [train_data, test_data]\n",
    "lang = LOST_LANG\n",
    "batch_size = 32\n",
    "log_dir = os.path.join(os.path.join(prefix_path, \"auxiliary_classifiers\"), TASK)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "brnn_args = {\n",
    "    \"vocab_size\": len(get_charset(lang)),\n",
    "    \"embed_size\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 16,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 200,\n",
    "    \"saved_path\": None,#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": log_dir,\n",
    "    \"save_all\": None,\n",
    "    \"eval_interval\": 10,\n",
    "    \"check_interval\": 10,\n",
    "    \"depth\": 0,\n",
    "    \"num_classes\": CLASSES_PER_TASK[TASK],\n",
    "    \"task\": TASK,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "'''\n",
    "hyperparams = {\n",
    "    #\"model\": \"ConvTextInfillerRNN\",\n",
    "    \"embed_size\": 256,\n",
    "    \"num_filters\": 256,\n",
    "    \"kernel_sizes\": [9, 11, 35, 57, 75],  # Example kernel sizes\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"normalize\": True\n",
    "}\n",
    "'''\n",
    "#for l,v in hyperparams.items(): brnn_args[l] = v\n",
    "\n",
    "\n",
    "if brnn_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(brnn_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = brnn_args[\"gpu\"]\n",
    "\n",
    "create_logger(filepath=brnn_args[\"log_dir\"] + '/log', log_level=brnn_args[\"log_level\"])\n",
    "log_pp(pformat(brnn_args))\n",
    "\n",
    "clear_stages()\n",
    "\n",
    "\n",
    "tcm = TextClassifierManager(data, lang, batch_size, brnn_args)\n",
    "tcm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qCvm8h1rTYh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Prova senza luo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "uFImTDF-sS8e"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"vocab_size\": len(get_charset(LOST_LANG)),\n",
    "    \"embed_size\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 16,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"saved_path\": None,#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"save_all\": None,\n",
    "    \"eval_interval\": 10,\n",
    "    \"check_interval\": 10,\n",
    "    \"depth\": 0,\n",
    "    \"num_classes\": CLASSES_PER_TASK[TASK],\n",
    "    \"task\": TASK,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    #\"model\": \"ConvTextInfillerRNN\",\n",
    "    \"embed_size\": 256,\n",
    "    \"num_filters\": 256,\n",
    "    \"kernel_sizes\": [1, 3, 7, 9, 11],  # Example kernel sizes\n",
    "    \"num_layers\": 8,\n",
    "    \"dropout\": 0.2,\n",
    "    \"normalize\": True\n",
    "}\n",
    "#for l,v in hyperparams.items(): args[l] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "5soMin9hrad6"
   },
   "outputs": [],
   "source": [
    "train_dataset = AuxiliaryClassifierDataset(train_data)\n",
    "train_data_loader = AuxiliaryDataLoader(train_dataset, 32)\n",
    "test_dataset = AuxiliaryClassifierDataset(test_data)\n",
    "test_data_loader = AuxiliaryDataLoader(test_dataset, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "opdl-uj1sq1e"
   },
   "outputs": [],
   "source": [
    "model_cls = BRNNTextClassifier#ConvTextClassifier\n",
    "model_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"mean\")\n",
    "#model = model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"num_filters\"], args[\"kernel_sizes\"], args[\"num_layers\"], args[\"dropout\"], args[\"normalize\"], args[\"num_classes\"])\n",
    "model =  model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"num_classes\"], args[\"depth\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"learning_rate\"])#, weight_decay=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdcDx_HdtzqO"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import math\n",
    "\n",
    "def train(model, train_data_loader, test_data_loader, loss_fn, optimizer, num_epochs=50, print_every=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            inputs = Map(x=batch.x.to(device), lengths=batch.lengths.to(device))\n",
    "            labels = batch.y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            if math.isnan(loss.item()):\n",
    "                print(f\"[Epoch {epoch:03d}]  NaN detected PORCODIO in loss! Skipping this batch.\")\n",
    "                continue  # Skip this batch to avoid breaking backprop\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total if total > 0 else 0\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] Train Loss: {total_loss:.4f} | Train Accuracy: {acc:.2f}%\")\n",
    "            print(total_loss)\n",
    "            # --- Evaluation on test data ---\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_data_loader:\n",
    "                    inputs = Map(x=batch.x.to(device), lengths=batch.lengths.to(device))\n",
    "                    labels = batch.y.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    test_correct += (preds == labels).sum().item()\n",
    "                    test_total += labels.size(0)\n",
    "\n",
    "            test_acc = 100.0 * test_correct / test_total if test_total > 0 else 0\n",
    "            print(f\"[Epoch {epoch:03d}] Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "            print(test_loss)\n",
    "\n",
    "train(model, train_data_loader, test_data_loader, model_loss, optimizer, num_epochs=100, print_every=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYpB3YBqVla4"
   },
   "source": [
    "#### SkLearn classifiers baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ahVoVJLcVv7O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Word(lang='transliterated_linear_b', form='*56-ra-ku-ja', idx=0), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ma-o', idx=1), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-me-we', idx=2), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ra-te-ja', idx=3), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-rya', idx=5), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-te', idx=6), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-te-re', idx=7), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-di-nwa-ta', idx=8), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-di-ri-ja-pi', idx=9), 3)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "o5Kszl-pV_fY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word objects: [Word(lang='transliterated_linear_b', form='*56-ra-ku-ja', idx=0), Word(lang='transliterated_linear_b', form='a-da-ma-o', idx=1), Word(lang='transliterated_linear_b', form='a-da-me-we', idx=2), Word(lang='transliterated_linear_b', form='a-da-ra-te-ja', idx=3), Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), Word(lang='transliterated_linear_b', form='a-de-rya', idx=5), Word(lang='transliterated_linear_b', form='a-de-te', idx=6), Word(lang='transliterated_linear_b', form='a-de-te-re', idx=7), Word(lang='transliterated_linear_b', form='a-di-nwa-ta', idx=8), Word(lang='transliterated_linear_b', form='a-di-ri-ja-pi', idx=9)]\n",
      "Labels: [1, 0, 2, 1, 0, 1, 2, 2, 1, 2]\n",
      "Original: [(Word(lang='transliterated_linear_b', form='ma-so-mo', idx=2578), 0), (Word(lang='transliterated_linear_b', form='ko-ru-ta-ta', idx=2509), 1), (Word(lang='transliterated_linear_b', form='se-ri-no-te', idx=1581), 2), (Word(lang='transliterated_linear_b', form='qo-u-ka-ra-o-i', idx=1448), 0), (Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), 0), (Word(lang='transliterated_linear_b', form='si-ja-ma-to', idx=2883), 1), (Word(lang='transliterated_linear_b', form='wi-su-ro', idx=1861), 0), (Word(lang='transliterated_linear_b', form='a-qi-ja-i', idx=2056), 1), (Word(lang='transliterated_linear_b', form='a-*35-ka', idx=1933), 1), (Word(lang='transliterated_linear_b', form='da-i-ra', idx=2297), 1)]\n",
      "Forms to be vectorized: ['*56-ra-ku-ja', 'a-da-ma-o', 'a-da-me-we', 'a-da-ra-te-ja', 'a-da-ra-ti-jo', 'a-de-rya', 'a-de-te', 'a-de-te-re', 'a-di-nwa-ta', 'a-di-ri-ja-pi']\n",
      "Shape of feature matrix: (2979, 5381)\n",
      "First row (dense): 1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 3. Extract X (Word) and y (label)\n",
    "\n",
    "X = [word for word, label in data]\n",
    "\n",
    "y = [label for word, label in data]\n",
    "\n",
    "# Log what we're feeding in\n",
    "print(\"Raw Word objects:\", X[:10])\n",
    "print(\"Labels:\", y[:10])\n",
    "print(\"Original:\", train_data[:10])\n",
    "\n",
    "# Optional: show the forms that will actually be used for feature extraction\n",
    "forms = [word.form for word in X]\n",
    "print(\"Forms to be vectorized:\", forms[:10])\n",
    "\n",
    "# Custom transformer to extract simple numeric features per word form\n",
    "def word_length(forms):\n",
    "    # returns a numpy array of shape (n_samples, 1) with word lengths (counting syllables)\n",
    "    return np.array([len(f.split(\"-\")) for f in forms]).reshape(-1, 1)\n",
    "\n",
    "def vowel_count(forms):\n",
    "    vowels = set(\"aeiou\")\n",
    "    counts = []\n",
    "    for f in forms:\n",
    "        # count vowels ignoring dashes\n",
    "        counts.append(sum(ch in vowels for ch in f.replace(\"-\", \"\")))\n",
    "    return np.array(counts).reshape(-1, 1)\n",
    "\n",
    "# 4. Feature extractor with syllables and character n-grams\n",
    "features = FeatureUnion([\n",
    "    (\"syllables\", CountVectorizer(tokenizer=lambda x: x.split(\"-\"), token_pattern=None)),\n",
    "    (\"char_ngrams\", TfidfVectorizer(analyzer='char', ngram_range=(2,4), preprocessor=lambda x: x.replace(\"-\", \"\"))),\n",
    "    #(\"word_length\", FunctionTransformer(word_length, validate=False)),\n",
    "    #(\"vowel_count\", FunctionTransformer(vowel_count, validate=False)),\n",
    "])\n",
    "\n",
    "# 5. Fit and transform\n",
    "X_features = features.fit_transform(forms)\n",
    "\n",
    "# 6. Log output\n",
    "print(\"Shape of feature matrix:\", X_features.shape)\n",
    "print(\"First row (dense):\", X_features[0].toarray().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "qKMz5CYOZru7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression\n",
      "Train Accuracy: 0.906547285954113\n",
      "Validation Accuracy: 0.8187919463087249\n",
      "Test Accuracy: 0.7835570469798657\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87       318\n",
      "           1       0.80      0.71      0.75       159\n",
      "           2       0.61      0.56      0.59       119\n",
      "\n",
      "    accuracy                           0.78       596\n",
      "   macro avg       0.75      0.73      0.73       596\n",
      "weighted avg       0.78      0.78      0.78       596\n",
      "\n",
      "\n",
      " Random Forest\n",
      "Train Accuracy: 0.9155008393956351\n",
      "Validation Accuracy: 0.7483221476510067\n",
      "Test Accuracy: 0.7248322147651006\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.81       318\n",
      "           1       0.83      0.48      0.61       159\n",
      "           2       0.66      0.47      0.55       119\n",
      "\n",
      "    accuracy                           0.72       596\n",
      "   macro avg       0.73      0.63      0.66       596\n",
      "weighted avg       0.73      0.72      0.71       596\n",
      "\n",
      "\n",
      " Linear SVM\n",
      "Train Accuracy: 0.9826524902070509\n",
      "Validation Accuracy: 0.8338926174496645\n",
      "Test Accuracy: 0.825503355704698\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       318\n",
      "           1       0.83      0.82      0.83       159\n",
      "           2       0.64      0.65      0.64       119\n",
      "\n",
      "    accuracy                           0.83       596\n",
      "   macro avg       0.79      0.79      0.79       596\n",
      "weighted avg       0.83      0.83      0.83       596\n",
      "\n",
      "\n",
      " Multinomial Naive Bayes\n",
      "Train Accuracy: 0.7839955232232793\n",
      "Validation Accuracy: 0.7315436241610739\n",
      "Test Accuracy: 0.6963087248322147\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.95      0.79       318\n",
      "           1       0.78      0.40      0.53       159\n",
      "           2       0.72      0.43      0.54       119\n",
      "\n",
      "    accuracy                           0.70       596\n",
      "   macro avg       0.72      0.59      0.62       596\n",
      "weighted avg       0.71      0.70      0.67       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HistGradientBoosting\n",
      "Train Accuracy: 0.9188584219362059\n",
      "Validation Accuracy: 0.802013422818792\n",
      "Test Accuracy: 0.7432885906040269\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       318\n",
      "           1       0.77      0.67      0.72       159\n",
      "           2       0.52      0.47      0.49       119\n",
      "\n",
      "    accuracy                           0.74       596\n",
      "   macro avg       0.70      0.67      0.68       596\n",
      "weighted avg       0.74      0.74      0.74       596\n",
      "\n",
      "\n",
      " Neural Network (MLP)\n",
      "Train Accuracy: 0.8393956351426972\n",
      "Validation Accuracy: 0.7466442953020134\n",
      "Test Accuracy: 0.7248322147651006\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.82       318\n",
      "           1       0.76      0.49      0.60       159\n",
      "           2       0.70      0.46      0.56       119\n",
      "\n",
      "    accuracy                           0.72       596\n",
      "   macro avg       0.73      0.63      0.66       596\n",
      "weighted avg       0.73      0.72      0.71       596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "SEED = 17\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, \n",
    "                                              random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200,\n",
    "                                            max_depth=30,\n",
    "                                            random_state=SEED),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000, random_state=SEED),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_iter=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4\n",
    "    ),\n",
    "    \"Neural Network (MLP)\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),  # default is one hidden layer with 100 neurons\n",
    "        max_iter=300,\n",
    "        alpha=1e-4,\n",
    "        solver='adam',\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10\n",
    "    )\n",
    "}\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2/0.8, random_state=SEED)\n",
    "\n",
    "split_strings = [\"Train\", \"Validation\", \"Test\"]\n",
    "X_list = [X_train, X_val, X_test]\n",
    "y_list = [y_train, y_val, y_test]\n",
    "# Train & Evaluate\n",
    "for name, model in models.items():\n",
    "    for split_s, X_vec, y_vec in zip(split_strings, X_list, y_list):\n",
    "\n",
    "        if name in [\"HistGradientBoosting\", \"Neural Network (MLP)\"]:\n",
    "            # These models expect dense input\n",
    "            X_vec = X_vec.toarray()\n",
    "        if split_s.lower() == \"train\":\n",
    "            model.fit(X_vec, y_vec)\n",
    "            print(f\"\\n {name}\")\n",
    "\n",
    "        y_pred = model.predict(X_vec)\n",
    "        print(f\"{split_s} Accuracy:\", accuracy_score(y_vec, y_pred))\n",
    "        if split_s.lower() == \"test\":\n",
    "            print(\"Classification Report (Test):\")\n",
    "            print(classification_report(y_vec, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression\n",
      "Fold accuracies:      [0.7936 0.8087 0.8272 0.8003 0.8185]\n",
      "Fold macro-F1:        [0.7551 0.774  0.7988 0.7676 0.7818]\n",
      "Mean acc  std:      0.8097  0.0121\n",
      "Mean f1_macro  std: 0.7755  0.0146\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      1590\n",
      "           1       0.81      0.77      0.79       735\n",
      "           2       0.72      0.62      0.66       654\n",
      "\n",
      "    accuracy                           0.81      2979\n",
      "   macro avg       0.79      0.77      0.78      2979\n",
      "weighted avg       0.81      0.81      0.81      2979\n",
      "\n",
      "\n",
      " Random Forest\n",
      "Fold accuracies:      [0.7349 0.7483 0.7483 0.7265 0.7445]\n",
      "Fold macro-F1:        [0.6808 0.6914 0.6908 0.6664 0.6886]\n",
      "Mean acc  std:      0.7405  0.0086\n",
      "Mean f1_macro  std: 0.6836  0.0094\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81      1590\n",
      "           1       0.81      0.59      0.68       735\n",
      "           2       0.75      0.44      0.56       654\n",
      "\n",
      "    accuracy                           0.74      2979\n",
      "   macro avg       0.76      0.65      0.68      2979\n",
      "weighted avg       0.75      0.74      0.72      2979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Linear SVM\n",
      "Fold accuracies:      [0.8372 0.8305 0.8523 0.8255 0.8403]\n",
      "Fold macro-F1:        [0.8057 0.7948 0.8255 0.7989 0.8109]\n",
      "Mean acc  std:      0.8372  0.0092\n",
      "Mean f1_macro  std: 0.8072  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      1590\n",
      "           1       0.83      0.86      0.84       735\n",
      "           2       0.73      0.65      0.69       654\n",
      "\n",
      "    accuracy                           0.84      2979\n",
      "   macro avg       0.81      0.80      0.81      2979\n",
      "weighted avg       0.83      0.84      0.83      2979\n",
      "\n",
      "\n",
      " Multinomial Naive Bayes\n",
      "Fold accuracies:      [0.7282 0.7198 0.7332 0.7248 0.7429]\n",
      "Fold macro-F1:        [0.6678 0.6542 0.676  0.6613 0.6856]\n",
      "Mean acc  std:      0.7298  0.0079\n",
      "Mean f1_macro  std: 0.6690  0.0110\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81      1590\n",
      "           1       0.79      0.53      0.63       735\n",
      "           2       0.74      0.46      0.57       654\n",
      "\n",
      "    accuracy                           0.73      2979\n",
      "   macro avg       0.75      0.64      0.67      2979\n",
      "weighted avg       0.74      0.73      0.71      2979\n",
      "\n",
      "\n",
      " HistGradientBoosting\n",
      "Fold accuracies:      [0.7836 0.7752 0.8138 0.7785 0.7765]\n",
      "Fold macro-F1:        [0.7404 0.7371 0.7763 0.7375 0.7302]\n",
      "Mean acc  std:      0.7855  0.0144\n",
      "Mean f1_macro  std: 0.7443  0.0163\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1590\n",
      "           1       0.76      0.74      0.75       735\n",
      "           2       0.68      0.58      0.62       654\n",
      "\n",
      "    accuracy                           0.79      2979\n",
      "   macro avg       0.76      0.74      0.74      2979\n",
      "weighted avg       0.78      0.79      0.78      2979\n",
      "\n",
      "\n",
      " Neural Network (MLP)\n",
      "Fold accuracies:      [0.7735 0.8037 0.8037 0.7869 0.8286]\n",
      "Fold macro-F1:        [0.7286 0.7662 0.7648 0.7496 0.7962]\n",
      "Mean acc  std:      0.7993  0.0185\n",
      "Mean f1_macro  std: 0.7611  0.0222\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87      1590\n",
      "           1       0.80      0.77      0.78       735\n",
      "           2       0.69      0.58      0.63       654\n",
      "\n",
      "    accuracy                           0.80      2979\n",
      "   macro avg       0.77      0.75      0.76      2979\n",
      "weighted avg       0.79      0.80      0.79      2979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Densifier for models that don't accept sparse input\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        return X.toarray()\n",
    "\n",
    "dense_models = {\"HistGradientBoosting\", \"Neural Network (MLP)\"}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scoring = {\"acc\": \"accuracy\", \"f1_macro\": \"f1_macro\"}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Wrap with densifier only if needed\n",
    "    est = make_pipeline(DenseTransformer(), model) if name in dense_models else model\n",
    "\n",
    "    scores = cross_validate(\n",
    "        est, X_features, y, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n {name}\")\n",
    "    print(\"Fold accuracies:     \", np.round(scores[\"test_acc\"], 4))\n",
    "    print(\"Fold macro-F1:       \", np.round(scores[\"test_f1_macro\"], 4))\n",
    "    print(\"Mean acc  std:      {:.4f}  {:.4f}\".format(scores[\"test_acc\"].mean(),\n",
    "                                                       scores[\"test_acc\"].std()))\n",
    "    print(\"Mean f1_macro  std: {:.4f}  {:.4f}\".format(scores[\"test_f1_macro\"].mean(),\n",
    "                                                       scores[\"test_f1_macro\"].std()))\n",
    "\n",
    "    # Out-of-fold predictions for an overall classification report\n",
    "    y_pred_oof = cross_val_predict(est, X_features, y, cv=cv, n_jobs=-1)\n",
    "    print(\"Classification Report (OOF):\")\n",
    "    print(classification_report(y, y_pred_oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "t3VfTuT9BcBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Loss: 0.9436\n",
      "Epoch 10/50 - Loss: 0.8314\n",
      "Epoch 15/50 - Loss: 0.6973\n",
      "Epoch 20/50 - Loss: 0.5510\n",
      "Epoch 25/50 - Loss: 0.4169\n",
      "Epoch 30/50 - Loss: 0.3098\n",
      "Epoch 35/50 - Loss: 0.2281\n",
      "Epoch 40/50 - Loss: 0.1676\n",
      "Epoch 45/50 - Loss: 0.1251\n",
      "Epoch 50/50 - Loss: 0.0955\n",
      "\n",
      " Train Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.9871\n",
      "\n",
      " Validation Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.7584\n",
      "\n",
      " Test Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.7450\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 2. Define the model\n",
    "class DumbMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DumbMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.LogSoftmax(dim=1)  # final log-probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "n_classes = len(set(y))  # assumes classes are 0, 1, ..., n-1\n",
    "\n",
    "model = DumbMLP(input_dim=X_train.shape[1], output_dim=n_classes)\n",
    "\n",
    "# 3. Loss and Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Training\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. Evaluation\n",
    "model.eval()\n",
    "split_strings = [\"Train\", \"Validation\", \"Test\"]\n",
    "in_vecs = [X_train_tensor, X_val_tensor, X_test_tensor]\n",
    "out_vecs = [y_train_tensor, y_val_tensor, y_test_tensor]\n",
    "for split_s, x_vec, y_vec in zip(split_strings, in_vecs, out_vecs):\n",
    "    with torch.no_grad():\n",
    "        test_output = model(x_vec)\n",
    "        predicted = torch.argmax(test_output, dim=1)\n",
    "        acc = accuracy_score(y_vec.numpy(), predicted.numpy())\n",
    "        print(f\"\\n {split_s} Accuracy (PyTorch Dumb MLP + LogSoftmax): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "YazxyTY2gsRm"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=30, random_state=SEED),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000, random_state=SEED),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_iter=100,       # fewer boosting rounds for speed\n",
    "        max_depth=10,       # shallower trees\n",
    "        learning_rate=0.1,  # standard learning rate\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,  # stops training if no improvement\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n9sP8gIgNKw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define weights for models (you decide these)\n",
    "weights = {\n",
    "    \"Logistic Regression\": 5,\n",
    "    \"Linear SVM\": 5,\n",
    "    \"Random Forest\": 1,\n",
    "    \"Multinomial NB\": 1,\n",
    "    \"HistGradientBoosting\": 3\n",
    "}\n",
    "\n",
    "# Fit all models (same as you have)\n",
    "for name, model in models.items():\n",
    "    if name == \"HistGradientBoosting\":\n",
    "        model.fit(X_train.toarray(), y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "# Get class labels (assuming all models use the same label order)\n",
    "classes = models[\"Logistic Regression\"].classes_\n",
    "\n",
    "# Collect weighted probabilities\n",
    "weighted_probs = np.zeros((X_test.shape[0], len(classes)))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"Linear SVM\":\n",
    "        # Use decision_function + softmax\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        if len(decision_scores.shape) == 1:\n",
    "            # Binary classification, convert to 2-class probs\n",
    "            decision_scores = np.vstack([-decision_scores, decision_scores]).T\n",
    "        probs = softmax(decision_scores, axis=1)\n",
    "    elif name == \"HistGradientBoosting\":\n",
    "        probs = model.predict_proba(X_test.toarray())\n",
    "    else:\n",
    "        probs = model.predict_proba(X_test)\n",
    "\n",
    "    weighted_probs += weights[name] * probs\n",
    "\n",
    "# Normalize weighted_probs (optional, but sum of weights used)\n",
    "weighted_probs /= sum(weights.values())\n",
    "\n",
    "# Final ensemble prediction\n",
    "y_pred_ensemble = classes[np.argmax(weighted_probs, axis=1)]\n",
    "\n",
    "# Evaluate ensemble\n",
    "print(\"\\n Ensemble weighted voting results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
    "\n",
    "print(classification_report(y_test, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "IXFUR9M3Jo28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in ./.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from fasttext) (1.23.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.local/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.local/lib/python3.10/site-packages (from fasttext) (80.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: numpy 1.23.5\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2.0 in ./.local/lib/python3.10/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "!pip uninstall -y thinc\n",
    "!pip uninstall -y spacy\n",
    "!pip uninstall -y fastai\n",
    "!pip uninstall -y numpy\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7jXgc6kRBwW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgxsiJlcJTv6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LinearBWordClassifier:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext\"), \"linearb_fasttext_classifier.bin\"),\n",
    "                 train_data_path=\"linearb_supervised_train.txt\",\n",
    "                 dim=1000,\n",
    "                 epoch=100,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=6,\n",
    "                 word_ngrams=1,\n",
    "                 bucket=2000000,\n",
    "                 loss='softmax',\n",
    "                 verbose=2\n",
    "                 ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'dim': dim,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'bucket': bucket,\n",
    "            'loss': loss,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "    def preprocess_pairs(self, word_label_pairs):\n",
    "        # Return new list with hyphens removed from word.form\n",
    "        return [((word.form.replace(\"-\", \"\")), label) for word, label in word_label_pairs]\n",
    "\n",
    "    def _write_supervised_data(self, word_label_pairs):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for word, label in word_label_pairs:\n",
    "                f.write(f\"__label__{label} {word}\\n\")\n",
    "\n",
    "    def train(self, word_label_pairs):\n",
    "        clean_data = self.preprocess_pairs(word_label_pairs)\n",
    "        self._write_supervised_data(clean_data)\n",
    "        self.model = fasttext.train_supervised(\n",
    "            input=self.train_data_path,\n",
    "            **self.params\n",
    "        )\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def predict(self, word, k=1):\n",
    "        if self.model:\n",
    "            label, prob = self.model.predict(str(word), k=k)  # ensure string input\n",
    "            return label[0].replace(\"__label__\", \"\"), prob[0]\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    def evaluate_accuracy(self, word_label_pairs):\n",
    "        clean_data = self.preprocess_pairs(word_label_pairs)\n",
    "        y_true = [str(label) for _, label in clean_data]\n",
    "        y_pred = [self.predict(word)[0] for word, _ in clean_data]\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace this with your real word-label list\n",
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Train\n",
    "classifier = LinearBWordClassifier()\n",
    "classifier.train(train_data)\n",
    "classifier.save_model()\n",
    "\n",
    "# Evaluate\n",
    "train_acc = classifier.evaluate_accuracy(train_data)\n",
    "test_acc = classifier.evaluate_accuracy(test_data)\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4xvLfAjoWNQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text Infilling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-PzMfDGWWA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### First part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "z85VlGAuobKU"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, brnn_model, loaders, brnn_args, vocabulary, topk_values):\n",
    "        self.brnn_model = brnn_model\n",
    "        self.loaders = loaders\n",
    "        self.brnn_args = brnn_args\n",
    "        self.vocabulary = vocabulary\n",
    "        self.topks = topk_values\n",
    "\n",
    "    def create_words_dataset(self):\n",
    "        for topk in self.topks:\n",
    "            word_dict, word2seq = self.brute_force_infillings(topk)\n",
    "            self.write_to_file(word_dict, word2seq, topk)\n",
    "\n",
    "    def write_to_file(self, word_dict, word2seq, topk):\n",
    "        file_path = os.path.join(prefix_path, f\"linb_words_top_{topk}.tsv\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word, valid in sorted(word_dict.items()):\n",
    "                #logging.critical(f\"{word}\\t{word2seq[word]}\")\n",
    "                f.write(f\"{word}\\t{int(valid)}\\t{sorted([(seq.idx, val) for (seq, val) in word2seq[word]])}\\n\")  # Each line: key TAB value\n",
    "\n",
    "\n",
    "    def brute_force_infillings(self, topk):\n",
    "        forms = [w.form for w in self.vocabulary.get_words]\n",
    "        word_dict = {form: True if \"*\" not in form else False for form in forms}\n",
    "        word2seq = defaultdict(set)\n",
    "        for loader in self.loaders:\n",
    "            for batch in loader:\n",
    "                for seq in batch.sequences:\n",
    "                    if seq.idx == 127:\n",
    "                        logging.critical(f\"{seq.form}, {seq.missing_form}\")\n",
    "                    for word in seq.word_list:\n",
    "                        word2seq[word.form].add((seq, 1))\n",
    "\n",
    "                word_bf = self.brute_force_infillings_batch(batch, topk)\n",
    "                for w in word_bf:\n",
    "                    if w not in word_dict:\n",
    "                        word_dict[w] = False\n",
    "                    word2seq[w].update(word_bf[w])\n",
    "\n",
    "        return word_dict, word2seq\n",
    "\n",
    "    def brute_force_infillings_batch(self, batch, topk):\n",
    "        word_bf = defaultdict(set)\n",
    "\n",
    "        brnn_ret = self.brnn_model(batch)\n",
    "        # Get indices of UNK tokens\n",
    "        batch_indices, token_indices = torch.where(batch.x == UNK_ID)\n",
    "\n",
    "        # Get the predictions just before each missing token\n",
    "        # Shape: [num_missings, vocab_size]\n",
    "        predictions_before_missing = brnn_ret.predictions[batch_indices, token_indices - 1]\n",
    "\n",
    "        # Get top 10 predicted token IDs for each missing position\n",
    "        # Shape: [num_missings, 10]\n",
    "        topk_values, topk_indices = torch.topk(predictions_before_missing, k=topk, dim=-1)\n",
    "        predicted_sequence = torch.argmax(brnn_ret.predictions, dim=-1)\n",
    "        incomplete = self.extract_incomplete_words(batch.x)\n",
    "\n",
    "        cset = get_charset(batch.lang)\n",
    "        for i in range(batch.x.shape[0]):\n",
    "            word = incomplete[i][0]\n",
    "            missing_idx = token_indices[i] - incomplete[i][1]\n",
    "            for j, infilling in enumerate(topk_indices[i]):\n",
    "                word[missing_idx] = infilling\n",
    "                word_str = word.cpu().numpy()\n",
    "                word_str = cset.id2char(word_str)\n",
    "                word_str = \"-\".join(word_str)\n",
    "\n",
    "                if \"<\" not in word_str:\n",
    "                    word_bf[word_str].add((batch.sequences[i], topk_values[i][j].item()))\n",
    "                #DEBUG\n",
    "                #else:\n",
    "                #    logging.critical(f\"{word_str}, {topk_values[i][j].item()}\")\n",
    "        return word_bf\n",
    "\n",
    "\n",
    "\n",
    "    def extract_incomplete_words(self, sequence_batch):\n",
    "        result = []\n",
    "        for i in range(sequence_batch.shape[0]):\n",
    "            seq = sequence_batch[i]\n",
    "            pos = (seq == UNK_ID).nonzero(as_tuple=True)[0][0].item()\n",
    "\n",
    "            init = (seq[:pos] == EOW_ID).nonzero(as_tuple=True)[0]\n",
    "            end = (seq[pos + 1:] == EOW_ID).nonzero(as_tuple=True)[0]\n",
    "            if len(init) > 0:\n",
    "                init = init[-1].item() + 1\n",
    "            else:\n",
    "                init = 1\n",
    "\n",
    "            if len(end) > 0:\n",
    "                end = end[0].item() + pos + 1\n",
    "            else:\n",
    "                end = (seq[pos + 1:] == PAD_ID).nonzero(as_tuple=True)[0]\n",
    "                if len(end) > 0:\n",
    "                    end = end[0].item() + pos + 1\n",
    "                else:\n",
    "                    end = seq.shape[0]\n",
    "            word = seq[init:end]\n",
    "            result.append((word, init))\n",
    "        return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "-B0c4ITBqlNr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p \u001b[38;5;241m=\u001b[39m Pipeline(\u001b[43mtim\u001b[49m\u001b[38;5;241m.\u001b[39m_get_trained_model(brnn_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]), [tim\u001b[38;5;241m.\u001b[39mtrain_data_loader, tim\u001b[38;5;241m.\u001b[39mtest_data_loader], brnn_args, our_vocabulary, topk_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mcreate_words_dataset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tim' is not defined"
     ]
    }
   ],
   "source": [
    "p = Pipeline(tim._get_trained_model(brnn_args[\"saved_path\"]), [tim.train_data_loader, tim.test_data_loader], brnn_args, our_vocabulary, topk_values=[5,10,15,20])\n",
    "p.create_words_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfd_sJpKGbqD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Various Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSLjWESoJLM5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size):#, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        #if not cognate_only:\n",
    "        self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        #else:\n",
    "        #    lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "        #    self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "class LoadTrainedModel:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "        self.dataset = BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"])\n",
    "        trie = Trie(args[\"known_lang\"])\n",
    "        self.model = DecipherModelWithFlow(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "\n",
    "        self.model = self._get_trained_model(args['saved_path'])\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "        trie_weight_device = self.model.trie._weight.device\n",
    "        logging.critical(f\"model.trie.weight is on device: {trie_weight_device}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    @property\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    @property\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "EXP_NAME = \"full_data_residual_old\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "luo_args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 128, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if luo_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(luo_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = luo_args[\"gpu\"]\n",
    "\n",
    "# we already used a seed\n",
    "#if not args[\"random\"]:\n",
    "#    random.seed(luo_args[\"seed\"])\n",
    "#    np.random.seed(luo_args[\"seed\"])\n",
    "#    torch.manual_seed(luo_args[\"seed\"])\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "#CAMBIAMENTI FATTI IN: MAGIC_TENSOR\n",
    "luo_args[\"saved_path\"] = SAVED_PATH\n",
    "ltm = LoadTrainedModel(luo_args)\n",
    "model = ltm.get_model\n",
    "dataset = ltm.get_dataset\n",
    "\n",
    "logging.critical(model)\n",
    "\n",
    "translations = {}\n",
    "for batch in dataset:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    # Magic tensor to the rescue!\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds = almt.get_best()\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        translations[k.form] = v.form\n",
    "        #logging.critical(f\"{k.form}\\t{v.form}\")\n",
    "'''\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXdqO-mMEyg6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear_vocabs()\n",
    "#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\n",
    "#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "dataset = man.train_data_loader.entire_batch#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\n",
    "translations = {}\n",
    "\n",
    "model_ret = man.model(dataset, mode=\"mle\", num_cognates=1911, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "#logging.critical(preds)\n",
    "for k, v in preds.items():\n",
    "    translations[k.form] = v.form\n",
    "    #logging.critical(f\"{k.form}\\t{v.form}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = man.train_data_loader.entire_batch\n",
    "forms = dataset.lost.forms\n",
    "fasttext_embs = [get_tensor(embedding_model.get_vector(form.replace(\"-\", \"\")), dtype=\"f\") for form in forms]\n",
    "fasttext_embs = torch.stack(fasttext_embs)\n",
    "logging.critical(fasttext_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk2jzQZWQ0Fl"
   },
   "outputs": [],
   "source": [
    "args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "\n",
    "# Step 3: Force garbage collection and free GPU memory\n",
    "gc.collect()                      # Collect CPU memory\n",
    "torch.cuda.empty_cache()         # Release unused GPU memory\n",
    "torch.cuda.ipc_collect()         # Collect interprocess memory (if needed)\n",
    "\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "translations = {}\n",
    "\n",
    "#logging.critical(batch)\n",
    "model_ret = model(data_loader.entire_batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "preds = model_ret.valid_log_probs.get_best()\n",
    "for k, v in preds.items():\n",
    "    translations[k.form] = v.form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F3WOy0-3qrQ",
    "outputId": "468f72b0-741e-45b5-acbf-4eb1688ea45a"
   },
   "outputs": [],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for word in check_words:\n",
    "    logging.critical(f\"{word}\\t{translations[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASPO_Cd--C5k"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_with_fasttext_original\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0'#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnTW5y4p_B3-"
   },
   "outputs": [],
   "source": [
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTcWGPYF-Fmt",
    "outputId": "4b104308-3be3-4edf-d794-a71cc8d5b692"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "7YX6aGwVfG6x",
    "outputId": "07e34326-033a-4168-fc6e-bb1e3363391a"
   },
   "outputs": [],
   "source": [
    "translations = {}\n",
    "\n",
    "for batch in data_loader:\n",
    "    if len(batch.lost.forms) != args[\"batch_size\"]:\n",
    "        break\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None, fasttext_embs=fasttext_embs)\n",
    "    # Magic tensor to the rescue!\n",
    "    preds = model_ret.valid_log_probs.get_best()\n",
    "    for k, v in preds.items():\n",
    "        translations[k.form] = v.form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy0d5OtYBWCM",
    "outputId": "661d75e4-b637-4eac-b9d7-ca586aaad8f0"
   },
   "outputs": [],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for word in check_words:\n",
    "    logging.critical(f\"{word}\\t{translations[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab5jJORncSi5"
   },
   "outputs": [],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "\n",
    "for batch in data_loader:\n",
    "    logging.critical(len(batch.lost.forms))\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    logging.critical(log_probs.shape)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    logging.critical(predicted_ids.max())\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    reconstructed_dict = {}\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(reconstructed_dict.items()):\n",
    "    logging.critical(f\"{k}, {v}\")\n",
    "    if i > 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiF0WQ1UYbxS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Dataset translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Gk8OC8hrNV9",
    "outputId": "d45a2e90-f1c5-4d6a-93ac-021108104020"
   },
   "outputs": [],
   "source": [
    "#translations directly from the dataset\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def extract_dict_from_tsv(tsv_path):\n",
    "    result = {}\n",
    "    with open(tsv_path, newline='', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "        reader.fieldnames = [field.strip() for field in reader.fieldnames]  # Clean headers\n",
    "        for row in reader:\n",
    "            key = row.get(\"transliterated_linear_b\")\n",
    "            value = row.get(\"greek\")\n",
    "            if key and value:\n",
    "                result[key.strip()] = value.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tsv_file_path = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "translations = extract_dict_from_tsv(tsv_file_path)\n",
    "\n",
    "# Optional: print a preview\n",
    "for k, v in list(translations.items())[:5]:\n",
    "    logging.debug(f\"{k} -> {v}\")\n",
    "\n",
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for key in check_words:\n",
    "    logging.debug(f\"{key} -> {translations[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Model translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_80_rounds\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : COG_PATH,\n",
    "    \"char_emb_dim\" : 400, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0'#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"finetune\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 90, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#clear_vocabs()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_data_loader\u001b[38;5;241m.\u001b[39mentire_batch\u001b[38;5;66;03m#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m reconstructed_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "#clear_vocabs()\n",
    "#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\n",
    "#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "dataset = man.train_data_loader.entire_batch#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\n",
    "translations = {}\n",
    "reconstructed_dict = {}\n",
    "model_ret = man.model(dataset, mode=\"mle\", num_cognates=1911, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "#logging.critical(preds)\n",
    "for k, v in preds.items():\n",
    "    if k.form not in translations:\n",
    "        if type(v) == list:\n",
    "            translations[k.form] = \"|\".join([word.form for word in v])\n",
    "        else:\n",
    "            translations[k.form] = v.form    #logging.critical(f\"{k.form}\\t{v.form}\")\n",
    "\n",
    "log_probs = model_ret.log_probs\n",
    "log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "\n",
    "# Build dictionary mapping from original form to predicted sequence\n",
    "for i, seq in enumerate(predicted_ids):\n",
    "    decoded = \"\".join([charset[idx] for idx in seq])\n",
    "    key = dataset.lost.forms[i]\n",
    "    reconstructed_dict[key] = decoded\n",
    "\n",
    "logging.debug(f\"Having {len(translations)} translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import math\n",
    "\n",
    "reconstructed_dict = {}\n",
    "reconstruction_confidences = {}\n",
    "\n",
    "log_probs = model_ret.log_probs\n",
    "log_probs = log_probs.permute(2, 0, 1)  # shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "\n",
    "# Compute exp(max log_prob) per character in the prediction\n",
    "max_log_probs = log_probs.gather(-1, predicted_ids.unsqueeze(-1)).squeeze(-1)  # shape: (batch_size, seq_len)\n",
    "exp_max_log_probs = max_log_probs.exp()  # shape: (batch_size, seq_len)\n",
    "\n",
    "for i, (seq, prob_seq) in enumerate(zip(predicted_ids, exp_max_log_probs)):\n",
    "    decoded = \"\".join([charset[idx] for idx in seq])\n",
    "    key = dataset.lost.forms[i]\n",
    "\n",
    "    reconstructed_dict[key] = decoded\n",
    "    reconstruction_confidences[key] = prob_seq.tolist()  # list of float probabilities for each char\n",
    "    reconstruction_confidences[key] = [str(i)[:5] for i in reconstruction_confidences[key]]\n",
    "logging.debug(f\"Having {len(reconstructed_dict)} reconstructions with confidences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in list(translations.keys())[:100]:\n",
    "    logging.debug(f\"{k}, {translations[k]}, {reconstructed_dict[k]}, {reconstruction_confidences[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_ret\u001b[49m\u001b[38;5;241m.\u001b[39malmt_distr[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ret' is not defined"
     ]
    }
   ],
   "source": [
    "model_ret.almt_distr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage value of top-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prediction across B and L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topk_vals, topk_idxs  \u001b[38;5;66;03m# Optional: return for further use\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(analyze_topk(\u001b[43mmodel_ret\u001b[49m\u001b[38;5;241m.\u001b[39mlog_probs, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ret' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "\n",
    "def analyze_topk(log_probs: torch.Tensor, top_k: int = 2):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape (L, C, B), reshape to (B, L, C), then:\n",
    "    - compute top-k values for each (B, L) along C\n",
    "    - print the average score for the last k classes across B and L\n",
    "    \"\"\"\n",
    "    # Step 1: Reshape (L, C, B) -> (B, L, C)\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # new shape: (B, L, C)\n",
    "\n",
    "    # Step 2: Get top-k values and indices along C\n",
    "    topk_vals, topk_idxs = torch.topk(log_probs, top_k, dim=2)\n",
    "\n",
    "    # Step 3: Get average values for each of the top-k ranks\n",
    "    for i in range(top_k):\n",
    "        avg_val = math.exp(topk_vals[:, :, i].mean().item())\n",
    "        logging.error(f\"Average value of top-{i+1} prediction across B and L: {avg_val:.4f}\")\n",
    "\n",
    "    return topk_vals, topk_idxs  # Optional: return for further use\n",
    "\n",
    "logging.debug(analyze_topk(model_ret.log_probs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save only what you want\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Save model before cleanup\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 2: Clear vocabs and rebuild\u001b[39;00m\n\u001b[1;32m      7\u001b[0m clear_vocabs()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "translations = {}\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "#model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "'''\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    #analyze_topk(log_probs, 5)\n",
    "    # PyTorch style\n",
    "    ret = trie.analyze(log_probs, model_ret.almt_distr, batch.known.words, batch.lost.lengths)\n",
    "    ret.log_probs = log_probs\n",
    "    ret.valid_log_probs = MagicTensor(ret.valid_log_probs, batch.lost.words, batch.known.words)\n",
    "\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    preds = ret.valid_log_probs.get_best()\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "        translations[key] = preds[batch.lost.words[i]].form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a-mi-ni-so'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreconstructed_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma-mi-ni-so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'a-mi-ni-so'"
     ]
    }
   ],
   "source": [
    "reconstructed_dict[\"a-mi-ni-so\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save only what you want\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Save model before cleanup\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Clear vocabs and rebuild\u001b[39;00m\n\u001b[1;32m      8\u001b[0m clear_vocabs()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "translations = {}\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "'''\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "all_log_probs = []\n",
    "all_valid_log_probs = []\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    all_log_probs.append(log_probs.cpu())\n",
    "    all_valid_log_probs.append(model_ret.valid_log_probs.tensor.cpu())\n",
    "    # PyTorch style\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "\n",
    "known = batch.known.lang\n",
    "known_forms = batch.known.forms\n",
    "known_charset = get_charset(known)\n",
    "log_probs = torch.cat(all_log_probs, dim=2).cpu()  # now [tl  nc  sum(bs_i)]\n",
    "valid_log_probs = torch.cat(all_valid_log_probs, dim=0).cpu()\n",
    "\n",
    "expected_edits = compute_expected_edits(known_charset, log_probs, known_forms, valid_log_probs, edit=True) # also edit=False should be ok\n",
    "flow, cost = min_cost_flow(expected_edits.detach().cpu().numpy(), 13308, capacity=3, n_similar=10)\n",
    "flow = MagicTensor(get_tensor(flow), data_loader.entire_batch.lost.words, data_loader.entire_batch.known.words)\n",
    "preds = flow.get_best()\n",
    "for k, v in preds.items():\n",
    "    if k.form not in translations:\n",
    "        if type(v) == list:\n",
    "            translations[k.form] = \"|\".join([word.form for word in v])\n",
    "        else:\n",
    "            translations[k.form] = v.form\n",
    "        translations[k.form] += f\"|{reconstructed_dict[k.form].replace('<EOW>', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3805948/783888086.py:9: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Initialized with lang={lang}, total words={len(words)}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:36 - 0:06:56 at 783888086.py:9 - [Trie] Initialized with lang=greek, total words=12070\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:40: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Weight built: shape={self._weight.shape}, nnz={self._weight._nnz()}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:37 - 0:06:57 at 783888086.py:40 - [Trie] Weight built: shape=torch.Size([12070, 627]), nnz=97507\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:74: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Sampled submatrix shape={weight.shape}, nnz={weight._nnz()}, max_len={self._eff_max_length}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:38 - 0:06:58 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:87: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Analyzing with log_probs shape={log_probs.shape}, eff_weight shape={self._eff_weight.shape}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:38 - 0:06:58 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:39 - 0:06:59 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:39 - 0:06:59 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:40 - 0:07:00 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:40 - 0:07:00 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:41 - 0:07:01 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:41 - 0:07:01 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:42 - 0:07:02 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:42 - 0:07:02 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:43 - 0:07:03 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:43 - 0:07:03 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:44 - 0:07:04 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:44 - 0:07:04 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1842]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[37mDEBUG - 06/29/25 19:27:44 - 0:07:04 at 207726662.py:19 - Having 13308 translations\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = man.model\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "model.trie = trie\n",
    "\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds = almt.get_best()\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        if k.form not in translations:\n",
    "            if type(v) == list:\n",
    "                translations[k.form] = \"|\".join([word.form for word in v])\n",
    "            else:\n",
    "                translations[k.form] = v.form\n",
    "            translations[k.form] += f\"|{reconstructed_dict[k.form].replace('<EOW>', '')}\"\n",
    "    \n",
    "logging.debug(f\"Having {len(translations)} translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[\"ke-se-da-o-ne\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntranslations = {}\\nscores = {}\\nfor batch in data_loader:\\n    model_ret = man.model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\\n\\n    almt = model_ret.valid_log_probs\\n    preds,score = almt.get_best(topk=100, return_scores=True)\\n    #logging.critical(preds)\\n    for k, v in preds.items():\\n        if k.form not in translations:\\n                translations[k.form] = \"|\".join([word.form for word in v])\\n                scores[k.form] = score[k]\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translations = {}\n",
    "scores = {}\n",
    "for batch in data_loader:\n",
    "    model_ret = man.model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds,score = almt.get_best(topk=100, return_scores=True)\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        if k.form not in translations:\n",
    "                translations[k.form] = \"|\".join([word.form for word in v])\n",
    "                scores[k.form] = score[k]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>', '')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"ke-se-da-o-ne\"\n",
    "reconstructed_dict[w], translations[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trie._word2rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - a-mi-ni-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - wa-na-ka\tf\tf<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ko-no-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - to-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ke-se-da-o-ne\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ka-ra-ja\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - di-pi-zo\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - o-wi-to-wo\t\tf<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - pa-ka-ma\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\", \"ke-se-da-o-ne\", \"ka-ra-ja\", \"di-pi-zo\", \"o-wi-to-wo\", \"pa-ka-ma\"] # last is invalid\n",
    "for word in check_words:\n",
    "    logging.debug(f\"{word}\\t{translations[word]}\\t{reconstructed_dict[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>',\n",
       " '')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_dict[\"ka-ma-to\"], translations[\"ka-ma-to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2mD8AD1X89w",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Auxiliary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBEOQMEWYCom",
    "outputId": "e259eec7-72b7-40d9-fe89-1051108e3556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m TASKS:\n\u001b[1;32m      3\u001b[0m   classifier \u001b[38;5;241m=\u001b[39m train_classifier(task, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m   words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtranslations\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      5\u001b[0m   labels \u001b[38;5;241m=\u001b[39m use_classifier(classifier, words)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translations' is not defined"
     ]
    }
   ],
   "source": [
    "classifications =  defaultdict(list)\n",
    "for task in TASKS:\n",
    "  classifier = train_classifier(task, max_iter=2000)\n",
    "  words = list(translations.keys())\n",
    "  labels = use_classifier(classifier, words)\n",
    "  for word in labels.keys():\n",
    "    classifications[word].append(labels[word])\n",
    "\n",
    "logging.debug(classifications[\"wa-na-ka-te\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYEuAhe5ahUG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Sequence Transliterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "NOE2ZVniK6RH"
   },
   "outputs": [],
   "source": [
    "@has_properties('dataset', 'translations', 'vocabulary')\n",
    "class SequenceTransliterator:\n",
    "    def __init__(self, dataset, translations, vocabulary, classifications):\n",
    "        self.linb_versions = defaultdict(dict)\n",
    "\n",
    "        for s in dataset:\n",
    "            seq = s.missing.split(\" \")\n",
    "            unk = s.sequence.unknown\n",
    "\n",
    "            for word_idx in unk:\n",
    "                if len(unk[word_idx]) > 0:\n",
    "                    seq[word_idx] = \"BRUTE_ME\"\n",
    "            #logging.critical(f\"{s}, {s.sequence.idx}\")\n",
    "            self.linb_versions[s.sequence.idx][\"word_list\"] = seq\n",
    "            self.linb_versions[s.sequence.idx][\"classifications\"] = []\n",
    "            self.linb_versions[s.sequence.idx][\"brute_classifications\"] = []\n",
    "            for word in self.linb_versions[s.sequence.idx][\"word_list\"]:\n",
    "                if word == \"BRUTE_ME\":\n",
    "                    self.linb_versions[s.sequence.idx][\"classifications\"].append([])\n",
    "                else:\n",
    "                    self.linb_versions[s.sequence.idx][\"classifications\"].append(classifications[word])\n",
    "\n",
    "\n",
    "        input_file = os.path.join(prefix_path, \"linb_words.tsv\")\n",
    "        inverted = {}\n",
    "        with open(input_file, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    logging.debug(f\"Error with word: {parts[0]}\")\n",
    "                    continue\n",
    "                word = parts[0]\n",
    "                id_list = ast.literal_eval(parts[2])  # Convert string \"[1633]\" to list\n",
    "                id_list = list(set([int(seq_id) for (seq_id, prob) in id_list if prob != 1]))\n",
    "                #logging.critical(id_list)\n",
    "                for id_num in id_list:\n",
    "                    if word not in set(self.linb_versions[id_num][\"word_list\"]):\n",
    "                        if \"brute\" in self.linb_versions[id_num]:\n",
    "                            self.linb_versions[id_num][\"brute\"].append(word)\n",
    "                        else:\n",
    "                            self.linb_versions[id_num][\"brute\"] = [word]\n",
    "                        self.linb_versions[id_num][\"brute_classifications\"].append(classifications[word])\n",
    "                    #else:\n",
    "                    #    print(word, \"PORCODIO\" ,dataset[id_num].sequence.missing_form) VERIFY THIS\n",
    "        self.produce_greek_versions()\n",
    "\n",
    "    def produce_greek_versions(self):\n",
    "        self.greek_versions = defaultdict(dict)\n",
    "        for seq_id, dic in self.linb_versions.items():\n",
    "            self.greek_versions[seq_id][\"word_list\"] = []\n",
    "            self.greek_versions[seq_id][\"full_word_list\"] = []\n",
    "            self.greek_versions[seq_id][\"brute\"] = []\n",
    "            self.greek_versions[seq_id][\"solutions\"] = []\n",
    "            sequence = self.dataset[seq_id].sequence\n",
    "            unknown = sequence.unknown\n",
    "            for word_idx in unknown:\n",
    "                if len(unknown[word_idx]) > 0:\n",
    "                    unknown_word = sequence.form.split(\" \")[word_idx]\n",
    "                    self.greek_versions[seq_id][\"solutions\"].append(self.lb2greek(unknown_word))\n",
    "            count = 0\n",
    "            for word in dic[\"word_list\"]:\n",
    "                if word == \"BRUTE_ME\":\n",
    "                    self.greek_versions[seq_id][\"word_list\"].append(word)\n",
    "                    self.greek_versions[seq_id][\"full_word_list\"].append(self.greek_versions[seq_id][\"solutions\"][count])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    self.greek_versions[seq_id][\"word_list\"].append(self.lb2greek(word))\n",
    "                    self.greek_versions[seq_id][\"full_word_list\"].append(self.lb2greek(word))\n",
    "            if \"brute\" in dic:\n",
    "                for word in dic[\"brute\"]:\n",
    "                        self.greek_versions[seq_id][\"brute\"].append(self.lb2greek(word))\n",
    "            #logging.critical(f\"{dic}, {self.greek_versions[seq_id]}\" )\n",
    "\n",
    "    def lb2greek(self, word):\n",
    "        if word in self.translations:\n",
    "            return self.translations[word]\n",
    "        else:\n",
    "            return self.vocabulary.logogram_vocab.get(word, word)\n",
    "            #return self.vocabulary.logogram_vocab[word]\n",
    "\n",
    "#HAve to import Cognate matching imports and phonetic embeddings\n",
    "phonetic_embeddings = recover_phonetic_embeddings(loaded_embeddings, missing)\n",
    "seq_dataset = SequenceDataset(sequences, missing, unknown, phonetic_embeddings, idxs, our_vocabulary)\n",
    "st = SequenceTransliterator(seq_dataset, translations, our_vocabulary, classifications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBR9NEuhv3Jw",
    "outputId": "559cb38d-a2d8-478c-ffdb-86ae5ffc6e18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'word_list': ['BRUTE_ME', 'CROC', 'N', '1'],\n",
       "  'classifications': [[], [], [], []],\n",
       "  'brute_classifications': [[], [], [], [], []],\n",
       "  'brute': ['e-a-na-wo',\n",
       "   'e-ko-na-wo',\n",
       "   'e-po-na-wo',\n",
       "   'e-re-na-wo',\n",
       "   'e-ri-na-wo']},\n",
       " {'word_list': ['BRUTE_ME', '', 'N', '1'],\n",
       "  'full_word_list': ['f', '', 'N', '1'],\n",
       "  'brute': ['e-a-na-wo',\n",
       "   'e-ko-na-wo',\n",
       "   'e-po-na-wo',\n",
       "   'e-re-na-wo',\n",
       "   'e-ri-na-wo'],\n",
       "  'solutions': ['f']},\n",
       " {'sequence': Sequence idx=13 form='e-u-na-wo CROC N 1' missing_form='e-?-na-wo CROC N 1',\n",
       "  'x': [1, 6, 3, 30, 71, 2, 194, 2, 233, 2, 167],\n",
       "  'y': [6, 9, 30, 71, 2, 194, 2, 233, 2, 167, 4],\n",
       "  'lang': 'transliterated_linear_b',\n",
       "  'form': 'e-u-na-wo CROC N 1',\n",
       "  'missing': 'e-?-na-wo CROC N 1',\n",
       "  'phonetic': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [-0.52298063,  0.56957954, -0.63964498, ...,  0.41888806,\n",
       "          -0.5710628 , -0.21507236],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]])},\n",
       " 'e-u-na-wo')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 13\n",
    "for missing_word_idx in seq_dataset[n].sequence.unknown:\n",
    "    if len(seq_dataset[n].sequence.unknown[missing_word_idx]) > 0:\n",
    "        break\n",
    "st.linb_versions[n], st.greek_versions[n], seq_dataset[n], seq_dataset[n].sequence.form.split(\" \")[missing_word_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pfmoWZU1xSB4",
    "outputId": "82ece903-d2fc-460a-ad63-0be3a87c8e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRUTE_ME  N 1'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(st.greek_versions[n][\"word_list\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW1q-mW1dp7V",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Gemini initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Djc5XL_Wea6N",
    "outputId": "7b84f2d6-6bfb-4259-fb5c-61626cb44e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dotenv in ./.local/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in ./.local/lib/python3.10/site-packages (from dotenv) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google.generativeai in ./.local/lib/python3.10/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in ./.local/lib/python3.10/site-packages (from google.generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.172.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.10/site-packages (from google.generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in ./.local/lib/python3.10/site-packages (from google.generativeai) (1.10.22)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.local/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./.local/lib/python3.10/site-packages (from google-api-core->google.generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2020.6.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./.local/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google.generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client->google.generativeai) (0.20.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.local/lib/python3.10/site-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.local/lib/python3.10/site-packages (from google-api-python-client->google.generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (2.4.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv\n",
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "alb_8Nriduu9"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "1g9HFh3yd9o0"
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(prefix_path,\".env\"))\n",
    "n_keys = 13\n",
    "# Retrieve the API key\n",
    "api_keys = []\n",
    "for i in range(1, n_keys+1):\n",
    "    api_keys.append(os.getenv(f\"GOOGLE_API_KEY_{i}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Gk7GsjCPYq16",
    "outputId": "82bd42b5-951d-45e0-c701-d97dc6a8d28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat session started. Type 'exit' or 'quit' to end the conversation.\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Exiting chat session. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "# Corrected import for GenerationConfig\n",
    "from google.generativeai.types import GenerationConfig\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace 'YOUR_API_KEY' with your actual Gemini API key.\n",
    "# For security, consider loading this from an environment variable.\n",
    "# Example: api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY_10\") # Placeholder: Please replace with your actual API key\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "try:\n",
    "    genai.configure(api_key=api_key)\n",
    "    # The 'client' object is no longer directly used for chat creation in this way.\n",
    "    # We will directly initialize the model.\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Generative AI: {e}\")\n",
    "    print(\"Please ensure your API key is correct and you have network access.\")\n",
    "    exit()\n",
    "\n",
    "# --- Chat Initialization ---\n",
    "try:\n",
    "    # Initialize the GenerativeModel directly\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\", # Specify the model to use\n",
    "        generation_config=GenerationConfig(temperature=0.6) # Configure generation parameters\n",
    "    )\n",
    "\n",
    "    # Start a chat session with initial history\n",
    "    # The history format is a list of dictionaries with 'role' and 'parts'\n",
    "    chat = model.start_chat(\n",
    "        history=[{'role': 'user', 'parts': [{'text': 'Learn these Linear B to ancient greek correspondances from my json object: '}]},\n",
    "                 {'role': 'model', 'parts': [{'text': 'Okay, I got it.'}]} # Add a model response to establish the system instruction\n",
    "                ]\n",
    "    )\n",
    "    print(\"Chat session started. Type 'exit' or 'quit' to end the conversation.\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating chat session: {e}\")\n",
    "    print(\"Please check your model name and configuration.\")\n",
    "    exit()\n",
    "\n",
    "# --- Interactive Loop ---\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \") # Prompt the user for input\n",
    "\n",
    "        # Check for exit commands\n",
    "        if user_input.lower().strip() in {\"exit\", \"quit\"}:\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            print(\"Exiting chat session. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Send the user's message to the Gemini model\n",
    "        # The SDK automatically appends this message to the chat history\n",
    "        # and sends the full history with each subsequent call to maintain state.\n",
    "        response = chat.send_message(user_input)\n",
    "        #logging.critical(chat.history)\n",
    "        # Print the model's response\n",
    "        if response.text:\n",
    "            print(\"Gemini:\", response.text)\n",
    "        else:\n",
    "            print(\"Gemini: (No response text received)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the chat: {e}\")\n",
    "        print(\"Please try again or type 'exit' to quit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "V4vms_JcfFAu",
    "outputId": "81a071dc-f258-4303-a7ef-36b289043837",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;66;03m# Make sure json is imported if it's not already\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m linear_b_correspondances \u001b[38;5;241m=\u001b[39m \u001b[43mtranslations\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m linear_b_correspondances\u001b[38;5;241m.\u001b[39mupdate(our_vocabulary\u001b[38;5;241m.\u001b[39mlogogram_vocab)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fix: Use ensure_ascii=False to prevent Unicode escaping\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translations' is not defined"
     ]
    }
   ],
   "source": [
    "import json # Make sure json is imported if it's not already\n",
    "\n",
    "linear_b_correspondances = translations.copy()\n",
    "linear_b_correspondances.update(our_vocabulary.logogram_vocab)\n",
    "# Fix: Use ensure_ascii=False to prevent Unicode escaping\n",
    "str_linear_b_correspondances = json.dumps(linear_b_correspondances, ensure_ascii=False)\n",
    "\n",
    "str_linear_b_correspondances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0WyWPwanHDu",
    "outputId": "9cd64d42-a42d-450b-ca81-5c6836d7d975"
   },
   "outputs": [],
   "source": [
    "\n",
    "syllabograms_matching = {\n",
    "  \"a\": [\"\"],\n",
    "  \"e\": [\"\", \"\"],\n",
    "  \"i\": [\"\"],\n",
    "  \"o\": [\"\", \"\"],\n",
    "  \"u\": [\"\"],\n",
    "  \"da\": [\"\", \"\"],\n",
    "  \"de\": [\"\", \"\", \"\"],\n",
    "  \"di\": [\"\", \"\"],\n",
    "  \"do\": [\"\", \"\", \"\"],\n",
    "  \"du\": [\"\", \"\"],\n",
    "  \"dwe\": [\"\", \"f\", \"\"],\n",
    "  \"dwo\": [\"\", \"f\", \"\"],\n",
    "  \"ja\": [\"\", \"\"],\n",
    "  \"je\": [\"\", \"\", \"\"],\n",
    "  \"jo\": [\"\", \"\", \"\"],\n",
    "  \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ma\": [\"\", \"\", \"\"],\n",
    "  \"me\": [\"\", \"\", \"\"],\n",
    "  \"mi\": [\"\", \"\"],\n",
    "  \"mo\": [\"\", \"\"],\n",
    "  \"mu\": [\"\", \"\"],\n",
    "  \"na\": [\"\", \"\"],\n",
    "  \"ne\": [\"\", \"\", \"\"],\n",
    "  \"ni\": [\"\", \"\"],\n",
    "  \"no\": [\"\", \"\", \"\"],\n",
    "  \"nu\": [\"\", \"\"],\n",
    "  \"nwa\": [\"f\", \"\", \"\"],\n",
    "  \"pa\": [\"\", \"\", \"\"],\n",
    "  \"pe\": [\"\", \"\", \"\"],\n",
    "  \"pi\": [\"\", \"\", \"\"],\n",
    "  \"po\": [\"\", \"\", \"\"],\n",
    "  \"pu\": [\"\", \"\", \"\"],\n",
    "  \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "  \"phu\": [\"\", \"\"],\n",
    "  \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "  \"re\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ri\": [\"\", \"\", \"\"],\n",
    "  \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ru\": [\"\", \"\", \"\"],\n",
    "  \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "  \"rai\": [\"\", \"\", \"\"],\n",
    "  \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "  \"sa\": [\"\", \"\"],\n",
    "  \"se\": [\"\", \"\", \"\"],\n",
    "  \"si\": [\"\", \"\"],\n",
    "  \"so\": [\"\", \"\", \"\"],\n",
    "  \"su\": [\"\", \"\"],\n",
    "  \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "  \"te\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ti\": [\"\", \"\", \"\"],\n",
    "  \"to\": [\"\", \"\", \"\", \"\"],\n",
    "  \"tu\": [\"\", \"\", \"\"],\n",
    "  \"tya\": [\"\", \"\", \"\"],\n",
    "  \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "  \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "  \"wa\": [\"\", \"f\", \"\"],\n",
    "  \"we\": [\"\", \"f\", \"\"],\n",
    "  \"wi\": [\"\", \"f\"],\n",
    "  \"wo\": [\"\", \"f\", \"\"],\n",
    "  \"za\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ze\": [\"\", \"\", \"\"],\n",
    "  \"zo\": [\"\", \"\", \"\"],\n",
    "  \"ha\": [\"\", \"h\"],\n",
    "  \"ai\": [\"\", \"\"],\n",
    "  \"au\": [\"\", \"\"],\n",
    "  \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "  \"*64\": [\"\", \"\", \"f\"],\n",
    "  \"*65\": [\"\", \"\"],\n",
    "  \"*79\": [\"\", \"\"],\n",
    "  \"*82\": [\"\", \"\", \"f\"]\n",
    "}\n",
    "str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "len(str_syllabogram_matching), str_syllabogram_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvq9k3Rj1Z0U",
    "outputId": "d751c339-34c6-436d-ed5b-d5106af499a7"
   },
   "outputs": [],
   "source": [
    "suffixes = 'The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -) -> AMBIGUOUS\\n\\\n",
    "3. -de: can be either:\\n\\\n",
    "   - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "   - Allative/demonstrative suffix (equivalent to Greek -) -> AMBIGUOUS\\n\\\n",
    "4. -pi: instrumental/locative suffix'\n",
    "#Prefixes\n",
    "prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "3. we- : f- numerical prefix meaning \"six\" -> AMBIGUOUS\\n\\\n",
    "4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "6. jo- :  - relative/comparative prefix meaning \"as, like, how\"  -> AMBIGUOUS\\n'\n",
    "prefixes_and_suffixes = prefixes + suffixes\n",
    "print(prefixes_and_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1U_i9xj2WDv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Prompt for infilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Your problematic JSON string\n",
    "json_string = '''{\n",
    "    \"chosen_sequence\": \"Option 4\",\n",
    "    \"reasoning\": \"The Linear B sequence describes various items, including a 'rose-colored table' (`wo-de-wi-jo to-pe-za`) and several instances of `o-u-ki-te-mi` and `o-u-te-mi` (likely proper nouns or specific items). The final part of the sequence is `BRUTE_ME i-ku-wo-i-pi`. The word `i-ku-wo-i-pi` (Ancient Greek: ff) is classified as a common noun, thematic in -o, and notably ends with the instrumental/locative suffix `-pi`, indicating 'by means of horses' or 'among horses'.\n",
    "\n",
    "Let's evaluate the options:\n",
    "*   **Option 1 (`e-ki`):** Classified as an anthroponym/animal name/theonym noun. While grammatically possible as another item in a list, its Ancient Greek translation `` (echi) doesn't form a clear semantic connection with 'horses' in an inventory context.\n",
    "*   **Option 2 (`e-pa`):** Classified as a common noun, thematic in -a. Its Ancient Greek translation `` (ephas, 'speech' or 'daybreak') does not semantically fit with 'horses' in a practical inventory.\n",
    "*   **Option 3 (`e-pe`):** Classified as a common noun, thematic in -o. However, its Ancient Greek translation `` (epo) is a verb ('to say', 'to follow'), which directly contradicts its classification as a noun. This makes it grammatically inconsistent with the provided information.\n",
    "*   **Option 4 (`e-pi`):** Classified as a common adverb. Its Ancient Greek translation `` (epi) is a highly common preposition/adverb meaning 'on', 'at', 'upon', 'for', or 'in addition to'. This word perfectly complements the instrumental/locative `i-ku-wo-i-pi`. The phrase ` ff` (epi ikkuwoihi) translates to 'on/for horses' or 'with horses', which makes excellent semantic and grammatical sense in an administrative or inventory context, describing items associated with horses.\n",
    "*   **Option 5 (`e-ra`):** Classified as an anthroponym/animal name/theonym noun, thematic in -a. Its Ancient Greek translation `` (era, 'earth' or 'Hera') does not form a coherent semantic unit with 'horses' in this context.\n",
    "\n",
    "Therefore, `e-pi` () is the most semantically coherent and grammatically consistent choice. It functions as an adverb/preposition that logically connects the preceding items or the general context to the 'horses', which is highly plausible for Linear B administrative texts.\"\n",
    "}'''\n",
    "\n",
    "# Solution 1: Clean the JSON by escaping backticks\n",
    "def clean_json_backticks(json_str):\n",
    "    \"\"\"Escape backticks in JSON string values\"\"\"\n",
    "    # This is a simple approach - replace backticks with escaped backticks\n",
    "    return json_str.replace('`', '\\\\`')\n",
    "\n",
    "# Solution 2: Aggressive JSON cleaning\n",
    "def clean_json_thoroughly(json_str):\n",
    "    \"\"\"Thoroughly clean JSON string for parsing\"\"\"\n",
    "    # Remove or replace control characters\n",
    "    cleaned = ''.join(char for char in json_str if unicodedata.category(char) != 'Cc' or char in '\\n\\r\\t')\n",
    "    \n",
    "    # Remove backticks (code formatting)\n",
    "    cleaned = re.sub(r'`([^`]*)`', r'\\1', cleaned)\n",
    "    \n",
    "    # Remove bold formatting\n",
    "    cleaned = re.sub(r'\\*\\*([^*]*)\\*\\*', r'\\1', cleaned)\n",
    "    \n",
    "    # Remove bullet points\n",
    "    cleaned = re.sub(r'^\\s*\\*\\s+', '', cleaned, flags=re.MULTILINE)\n",
    "    \n",
    "    # Fix common escape sequence issues\n",
    "    cleaned = cleaned.replace('\\\\', '\\\\\\\\')  # Escape backslashes\n",
    "    cleaned = cleaned.replace('\\\\\"', '\"')    # Fix escaped quotes\n",
    "    cleaned = cleaned.replace('\\\\\\\\\"', '\\\\\"') # Fix double-escaped quotes\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Solution 3: Extract JSON from markdown-wrapped content\n",
    "def extract_json_from_markdown(text):\n",
    "    \"\"\"Extract JSON from markdown code blocks\"\"\"\n",
    "    # Look for JSON in markdown code blocks\n",
    "    json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    match = re.search(json_pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return text\n",
    "\n",
    "# Solution 4: Ultra-robust JSON parsing\n",
    "def parse_gemini_json_robust(json_str):\n",
    "    \"\"\"Parse JSON with extensive preprocessing for Gemini responses\"\"\"\n",
    "    \n",
    "    # Strategy 1: Try direct parsing first\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Extract from markdown if present\n",
    "    if '```json' in json_str:\n",
    "        json_str = extract_json_from_markdown(json_str)\n",
    "    \n",
    "    # Strategy 3: Clean thoroughly\n",
    "    try:\n",
    "        cleaned = clean_json_thoroughly(json_str)\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: Character-by-character reconstruction\n",
    "    try:\n",
    "        # Find JSON boundaries\n",
    "        start = json_str.find('{')\n",
    "        end = json_str.rfind('}') + 1\n",
    "        if start >= 0 and end > start:\n",
    "            json_portion = json_str[start:end]\n",
    "            cleaned = clean_json_thoroughly(json_portion)\n",
    "            return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 5: Manual field extraction as fallback\n",
    "    try:\n",
    "        return extract_fields_manually(json_str)\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"All parsing strategies failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_fields_manually(text):\n",
    "    \"\"\"Extract fields manually when JSON parsing fails\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Extract chosen_sequence\n",
    "    match = re.search(r'\"chosen_sequence\":\\s*\"([^\"]*)\"', text)\n",
    "    if match:\n",
    "        result['chosen_sequence'] = match.group(1)\n",
    "    \n",
    "    # Extract reasoning (more complex due to multiline)\n",
    "    match = re.search(r'\"reasoning\":\\s*\"(.*?)\"(?=\\s*})', text, re.DOTALL)\n",
    "    if match:\n",
    "        reasoning = match.group(1)\n",
    "        # Clean up the reasoning text\n",
    "        reasoning = reasoning.replace('\\\\\"', '\"')\n",
    "        reasoning = re.sub(r'`([^`]*)`', r'\\1', reasoning)  # Remove backticks\n",
    "        reasoning = re.sub(r'\\*\\*([^*]*)\\*\\*', r'\\1', reasoning)  # Remove bold\n",
    "        reasoning = re.sub(r'^\\s*\\*\\s+', '', reasoning, flags=re.MULTILINE)  # Remove bullets\n",
    "        result['reasoning'] = reasoning.strip()\n",
    "    \n",
    "    return result if result else None\n",
    "\n",
    "# Test the solutions\n",
    "logging.debug(\"=== Testing Robust Solutions ===\")\n",
    "\n",
    "\n",
    "# Test robust parsing\n",
    "try:\n",
    "    parsed = parse_gemini_json_robust(json_string)\n",
    "    if parsed:\n",
    "        logging.debug(\" Robust parsing worked!\")\n",
    "        logging.debug(f\"Chosen sequence: {parsed['chosen_sequence']}\")\n",
    "        logging.debug(f\"Reasoning preview: {parsed['reasoning']}\")\n",
    "    else:\n",
    "        logging.debug(\" Robust parsing failed\")\n",
    "except Exception as e:\n",
    "    logging.debug(f\" Exception in robust parsing: {e}\")\n",
    "\n",
    "# Simple utility function for your use case\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Quick parsing function for Gemini JSON responses\"\"\"\n",
    "    \n",
    "    # Handle markdown-wrapped JSON\n",
    "    if '```json' in response_text:\n",
    "        response_text = extract_json_from_markdown(response_text)\n",
    "    \n",
    "    # Try robust parsing\n",
    "    result = parse_gemini_json_robust(response_text)\n",
    "    \n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        logging.debug(\"Failed to parse JSON response\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def make_infill_selection_prompt(linear_b_data, greek_data, api_key):\n",
    "    \"\"\"\n",
    "    This is a prompt for selecting the most semantically coherent Greek translation\n",
    "    of a Linear B sequence with an unknown syllable to infill.\n",
    "\n",
    "    Args:\n",
    "        linear_b_sequence: str - Linear B sequence with unknown syllable marked\n",
    "        greek_translations: list - 5 possible Greek translations\n",
    "        word_classifiers: list - Logic role classifiers for each word in Linear B script\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with chosen sequence, reasoning and index of sequence within the list of translations\n",
    "    \"\"\"\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"## HISTORICAL CONTEXT \\n Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\\n\\\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\"\n",
    "\n",
    "    syll_matching_context = '## LINEAR B SYLLABOGRAMS TO GREEK CHARACTERS MAPPING \\n Consider this mapping between Linear B syllabograms and Ancient Greek\\'s characters.' + str_syllabogram_matching + \"\\n\"\n",
    "    syll_matching_context += 'Be careful with the following aspect: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> '\n",
    "\n",
    "    suffixes = '## SUFFIXES AND PREFIXES INFORMATION \\n The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "    CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "    Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "    1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "    2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); **AMBIGUOUS** \\n\\\n",
    "    3. -de: can be either:\\n\\\n",
    "       - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); **AMBIGUOUS** \\n\\\n",
    "    4. -pi: instrumental/locative suffix'\n",
    "\n",
    "    #Prefixes\n",
    "    prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "    They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "    1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "    2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "    3. we- : f- numerical prefix meaning \"six\"; **AMBIGUOUS** \\n\\\n",
    "    4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "    5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "    6. jo- :  - relative/comparative prefix meaning \"as, like, how\"; **AMBIGUOUS** \\n'\n",
    "    prefixes_and_suffixes = prefixes + suffixes\n",
    "\n",
    "    prompt = \"\"\n",
    "    # Task overview\n",
    "    task_overview = \"## TASK OVERVIEW \\n You are presented with a Linear B sequence containing an unknown word that needs to be infilled.\\n\\\n",
    "    We have generated 5 possible infillings, with corresponding Ancient Greek translations of this sequence, where only the word with the unknown syllable varies between translations.\\n\\\n",
    "    For logograms, rely on their ancient greek translations.\\n\\\n",
    "    BRUTE_ME is the placeholder for the word that you need to fill in.\\n\\\n",
    "    Your task is to select the single translation that makes the most semantic and contextual sense, considering the grammatical roles and meaning coherence of the entire sequence.\\n\\\n",
    "    The input is encoded using markdown.\\n\\n\"\n",
    "    #CRITICAL CONSIDERATION: Linear B is a very old form of greek, with little defined grammar and very ancient words. Consider these aspects in your predictions, and rely on the oldest ancient greek texts to guide your selection.\n",
    "    #CONSIDER THAT SOME PARTS OF THE INPUT MAY BE WRONG. TRY TO RECONSTRUCT THE GENERAL MEANING OF THE SEQUENCE AND CHOOSE WHAT FITS MOST ACCORDING TO ANCIENT GREEK'S CULTURE AND NEEDS.\n",
    "    prompt += task_overview\n",
    "    \n",
    "    # Input description\n",
    "    input_desc = \"## INPUT DESCRIPTION \\n\"\n",
    "\n",
    "    input_1 = \"### INPUT SEQUENCE \\n Linear B sequence with unknown syllable: A sequence of Linear B syllabograms where one syllable is unknown and marked for infilling. Also a tentative Ancient greek correspondance is provided.\\n\"\n",
    "\n",
    "    input_2 = \"### INPUT BRUTEFORCED PREDICTION \\n Five Greek translations: 5 possible infillings of the complete sequence, differing only in the word that corresponds to the unknown syllable. Also tentative Ancient greek correspondances are provided.     CRITICAL: TAKE INTO ACCOUNT THAT ANCIENT GREEK CORRESPONDANCES DERIVE FROM AN AUTOREGRESSIVE MODEL, WHICH TENDS TO REPEAT THE SAME CHARACTER MULTIPLE TIMES. WHEN THIS HAPPENS, TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD!\\n\"\n",
    "\n",
    "    input_3 = \"### INPUT CLASSIFICATION \\n Word classifiers: Word Type: 1. anthroponym/animalname/theonym, 2. toponym, 3. ethnonym, 4. common; \\n\\\n",
    "    Part of speech: 1. noun, 2. verb, 3. adjective, 4. adverb; \\n\\\n",
    "    Inflection: 1. thematic in -o, 2. thematic in -a, 3. athematic\\n\\n\"\n",
    "\n",
    "    input_desc += (input_1 + input_2 + input_3)\n",
    "    prompt += input_desc \n",
    "    \n",
    "    # Selection criteria\n",
    "    criteria = \"## SELECTION CRITERIA \\n Base your selection on the following linguistic and semantic factors:\\n\"\n",
    "\n",
    "    criterion_1 = \"### SEMANTIC COHERENCE \\n The chosen translation should form a meaningful,\\\n",
    "    logically coherent statement. Consider whether the proposed word fits naturally within\\\n",
    "    the semantic field of the entire sequence.\\n\"\n",
    "\n",
    "    criterion_2 = \"### GRAMMATICAL CONSINSTENCY \\n Verify that the proposed word aligns with\\\n",
    "    the grammatical role specified by its classifier. Check for proper case agreement,\\\n",
    "    gender agreement, and syntactic compatibility with surrounding words.\\n\"\n",
    "\n",
    "    criterion_3 = \"### CONTEXTUAL PLAUSIBILITY \\n Consider the historical and cultural context\\\n",
    "    of Mycenaean Greek. The chosen word should be appropriate for the time period and\\\n",
    "    cultural setting represented by Linear B texts.\\n\"\n",
    "\n",
    "    criterion_4 = \"### LINGUISTIC AUTHENTICITY: Prefer words that are consistent with\\\n",
    "    known Mycenaean Greek vocabulary and morphological patterns. Consider whether the\\\n",
    "    proposed word represents plausible Mycenaean forms.\\n\"\n",
    "\n",
    "    criterion_5 = \"### OVERALL MEANING: Evaluate which translation produces the most\\\n",
    "    meaningful and interpretable complete sentence or phrase, considering the practical\\\n",
    "    and administrative nature of most Linear B texts.\\n\\n\"\n",
    "    \n",
    "    criteria += (criterion_1 + criterion_2 + criterion_3 + criterion_4 + criterion_5)\n",
    "    prompt += criteria\n",
    "\n",
    "    # Output format specification\n",
    "    output_format = \"\"\"## OUTPUT FORMAT \\n CRITICAL: Your response must be ONLY a valid JSON object with exactly these two fields:\n",
    "    {\n",
    "        \"chosen_sequence\": \"Option NUMBER (the number corresponds to the selected infilling option)\",\n",
    "        \"reasoning\": \"detailed_explanation_of_why_this_translation_was_chosen\"\n",
    "    }\n",
    "    The option should be formatted like this: \"Option 2\", for example.\n",
    "    Do not include any other text, commentary, or formatting outside of this JSON structure.\\n\\n\"\"\"\n",
    "\n",
    "    prompt += output_format\n",
    "    \n",
    "    # Quality control\n",
    "    quality_control = \"\"\"## QUALITY CONTROL \\n Before finalizing your selection:\n",
    "    1. Verify that the chosen sequence forms a grammatically correct Ancient Greek phrase/sentence\n",
    "    2. Ensure the semantic coherence of the complete sequence\n",
    "    3. Double-check that your reasoning addresses the key selection criteria\n",
    "    4. Validate that your output is properly formatted JSON.\n",
    "    5. Check that the output is not empty: ALWAYS PROVIDE A RESPONSE.\"\"\"\n",
    "\n",
    "    prompt += quality_control\n",
    "\n",
    "    # Example prompt in markdown format\n",
    "    example_prompt = \"\"\"# INPUT DATA\n",
    "\n",
    "## INPUT SEQUENCE\n",
    "**Linear B:** a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si\n",
    "\n",
    "## INPUT BRUTEFORCED PREDICTION\n",
    "### Option 1\n",
    "**Linear B:** a-u-qe\n",
    "\n",
    "### Option 2  \n",
    "**Linear B:** ma-u-qe\n",
    "\n",
    "### Option 3\n",
    "**Linear B:** o-u-qe\n",
    "\n",
    "### Option 4\n",
    "**Linear B:** te-u-qe\n",
    "\n",
    "### Option 5\n",
    "**Linear B:** to-u-qe\n",
    "\n",
    "## INPUT CLASSIFICATIONS\n",
    "\n",
    "### Classifications for a-ro-mo-te-me-na\n",
    "- **Word:** a-ro-mo-te-me-na\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adjective\n",
    "- **Inflection:** thematic in -a\n",
    "\n",
    "### Classifications for a-ni-ja\n",
    "- **Word:** a-ni-ja\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** thematic in -a\n",
    "\n",
    "### Classifications for po-si\n",
    "- **Word:** po-si\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adverb\n",
    "\n",
    "### Classifications for e-e-si\n",
    "- **Word:** e-e-si\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** verb\n",
    "\n",
    "### Classifications for a-u-qe\n",
    "- **Word:** a-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for ma-u-qe\n",
    "- **Word:** ma-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for o-u-qe\n",
    "- **Word:** o-u-qe\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adverb\n",
    "\n",
    "### Classifications for te-u-qe\n",
    "- **Word:** te-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for to-u-qe\n",
    "- **Word:** to-u-qe\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** thematic in -o\"\"\"\n",
    "\n",
    "    example_response = \"\"\"{\n",
    "    \"chosen_sequence\": \"Option 3\",\n",
    "    \"reasoning\": \"o-u-qe (in Ancient Greek: ) (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts. Furthermore, the syllabogram to character mapping is completely respected.\"\n",
    "}\"\"\"\n",
    "\n",
    "    # Grammatical information in markdown\n",
    "    gramm = \"\"\"# GRAMMATICAL INFORMATION\n",
    "\n",
    "## DECLENSION TABLE\n",
    "This is an exhaustive table with Mycenaean Linear B declension suffixes attested in the documents. This is useful context to provide you on the declensions.\n",
    "\n",
    "| Number | Case | Thematic -o (M/F) | Thematic -o (N) | Thematic -a (M) | Thematic -a (F) | Athematic (M/F) | Athematic (N) |\n",
    "|--------|------|-------------------|-----------------|-----------------|-----------------|-----------------|---------------|\n",
    "| Singular | Nominative | -o | -o | -a | -a | variable | variable |\n",
    "| Singular | Genitive | -ojo | -ojo | -ao | -a | -o | -o |\n",
    "| Singular | Dative | -o | -o | -a | -a | -e/-i | -e/-i |\n",
    "| Singular | Accusative | -o | -o | -a | -a | -a | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi | -a | -a | -a | -e | -a |\n",
    "| Plural | Genitive | -o | -o | -ao | -ao | -o | -o |\n",
    "| Plural | Dative | -oi | -oi | -ai | -ai | -si/-ti | -si/-ti |\n",
    "| Plural | Accusative | -o | -a | -a | -a | -a/-e | -a |\n",
    "\n",
    "## VERBS\n",
    "- **3rd singular:** last syllabogram ending with vowel -e\n",
    "- **3rd plural:** last syllabogram is -si\n",
    "- **Infinite:** last syllabogram ending with vowel e (optionally another syllabogram -e appears at the end)\n",
    "- **Participle active:** terminates with -o (singular - ancient greek suffix, other suffixes follow athematic nouns, e.g. -o-te like i-jo-te -> , from )\n",
    "- **Participle medium/passive:** terminates with -me-no/-me-na suffixes (ancient greek -/-/- suffixes)\n",
    "\n",
    "## ADJECTIVES\n",
    "- **Thematic adjectives:** thematic adjectives have same behavior as thematic nouns\n",
    "- **Athematic adjectives:** athematic adjectives have same behavior as athematic nouns (variable nominative and same declensions)\"\"\"\n",
    "\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, please, explain me what task you need.'}]}, # Add a model response to establish the system instruction\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will learn the mapping and pay attention to double consonants!'}]},\n",
    "        {'role': 'user', 'parts': [{'text': 'Extract grammatical information from the following markdown-encoded object!\\n' + gramm}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will use this informations to validate inflection prediction and to better understand linear b sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prefixes_and_suffixes}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will consider prefixes and suffixes carefully.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Perfect. I fully understood the task and will follow your instructions.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': example_prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': example_response}]}\n",
    "    ]\n",
    "\n",
    "    # Create input data in markdown format\n",
    "    input_data = f\"\"\"# INPUT DATA\n",
    "\n",
    "## INPUT SEQUENCE\n",
    "**Linear B:** {\" \".join(linear_b_data[\"word_list\"])}\n",
    "**Ancient Greek:** {\" \".join(greek_data[\"word_list\"])}\n",
    "\n",
    "## INPUT BRUTEFORCED PREDICTION\n",
    "\"\"\"\n",
    "\n",
    "    # Add bruteforced options\n",
    "    for i, translation in enumerate(linear_b_data[\"brute\"]):\n",
    "        input_data += f\"\"\"### Option {i+1}\n",
    "**Linear B:** {linear_b_data[\"brute\"][i]}\n",
    "**Ancient Greek:** {greek_data[\"brute\"][i]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    input_data += \"## INPUT CLASSIFICATIONS\\n\\n\"\n",
    "\n",
    "    # Add classifications for original words\n",
    "    for i, word in enumerate(linear_b_data[\"word_list\"]):\n",
    "        if len(linear_b_data[\"classifications\"][i]) > 0:\n",
    "            input_data += f\"\"\"### Classifications for {word}\n",
    "- **Word:** {word}\n",
    "- **Word Type:** {linear_b_data[\"classifications\"][i][0]}\n",
    "- **Part of Speech:** {linear_b_data[\"classifications\"][i][1]}\n",
    "\"\"\"\n",
    "            if not (linear_b_data[\"classifications\"][i][1] == \"adverb\" or linear_b_data[\"classifications\"][i][1] == \"verb\"):\n",
    "                input_data += f\"- **Inflection:** {linear_b_data['classifications'][i][2]}\\n\"\n",
    "            input_data += \"\\n\"\n",
    "\n",
    "    # Add classifications for brute force options\n",
    "    for i, word in enumerate(linear_b_data[\"brute\"]):\n",
    "        input_data += f\"\"\"### Classifications for {word}\n",
    "- **Word:** {word}\n",
    "- **Word Type:** {linear_b_data[\"brute_classifications\"][i][0]}\n",
    "- **Part of Speech:** {linear_b_data[\"brute_classifications\"][i][1]}\n",
    "\"\"\"\n",
    "        if not (linear_b_data[\"brute_classifications\"][i][1] == \"adverb\" or linear_b_data[\"brute_classifications\"][i][1] == \"verb\"):\n",
    "            input_data += f\"- **Inflection:** {linear_b_data['brute_classifications'][i][2]}\\n\"\n",
    "        input_data += \"\\n\"\n",
    "\n",
    "    # Using Gemini model to generate response\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.0,     # Minimal creativity\n",
    "            top_p=1,             # Consider all probabilities (no cutting)\n",
    "            top_k=1,             # Choose the most probable word\n",
    "        )\n",
    "    )\n",
    "    #logging.debug(prompt)\n",
    "    #logging.debug(\"#\"*50)\n",
    "    #logging.debug(gramm)\n",
    "    #logging.debug(\"#\"*50)\n",
    "    #logging.debug(input_data)\n",
    "    #return\n",
    "    \n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(input_data)\n",
    "    #Extract and parse the JSON array\n",
    "    pred = response.text.strip()\n",
    "\n",
    "    json_start = pred.find('{')\n",
    "    json_end = pred.rfind('}') + 1\n",
    "\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = pred[json_start:json_end]\n",
    "        try:\n",
    "            return json.loads(json_str)  # Returns a single dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error parsing JSON response\")\n",
    "            return {}\n",
    "    else:\n",
    "        print(\"No valid JSON object found in response\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "z2kmNCe82d7x"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def make_infill_selection_prompt(linear_b_data, greek_data, api_key):\n",
    "    \"\"\"\n",
    "    This is a prompt for selecting the most semantically coherent Greek translation\n",
    "    of a Linear B sequence with an unknown syllable to infill.\n",
    "\n",
    "    Args:\n",
    "        linear_b_sequence: str - Linear B sequence with unknown syllable marked\n",
    "        greek_translations: list - 5 possible Greek translations\n",
    "        word_classifiers: list - Logic role classifiers for each word in Linear B script\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with chosen sequence, reasoning and index of sequence within the list of translations\n",
    "    \"\"\"\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\\n\\\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\"\n",
    "\n",
    "    syll_matching_context = 'Consider this mapping between Linear B syllabograms and Ancient Greek\\'s characters.' + str_syllabogram_matching + \"\\n\"\n",
    "    syll_matching_context += 'Be careful with the following aspect: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> '\n",
    "\n",
    "    suffixes = 'The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "    CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "    Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "    1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "    2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\\n\\\n",
    "    3. -de: can be either:\\n\\\n",
    "       - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\\n\\\n",
    "    4. -pi: instrumental/locative suffix'\n",
    "\n",
    "    #Prefixes\n",
    "    prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "    They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "    1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "    2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "    3. we- : f- numerical prefix meaning \"six\"; AMBIGUOUS\\n\\\n",
    "    4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "    5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "    6. jo- :  - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\\n'\n",
    "    prefixes_and_suffixes = prefixes + suffixes\n",
    "\n",
    "\n",
    "    # Create the root XML element\n",
    "    prompt = ET.Element(\"root\")\n",
    "\n",
    "    # Task overview\n",
    "    task_overview = ET.SubElement(prompt, \"task_overview\")\n",
    "    task_overview.text = \"You are presented with a Linear B sequence containing an unknown word that needs to be infilled.\\n\\\n",
    "    We have generated 5 possible infillings, with corresponding Ancient Greek translations of this sequence, where only the word with the unknown syllable varies between translations.\\n\\\n",
    "    For logograms, rely on their ancient greek translations.\\n\\\n",
    "    BRUTE_ME is the placeholder for the word that you need to fill in.\\n\\\n",
    "    Your task is to select the single translation that makes the most semantic and contextual sense, considering the grammatical roles and meaning coherence of the entire sequence.\\n\\\n",
    "    The input is encoded using XML.\"\n",
    "    #CRITICAL CONSIDERATION: Linear B is a very old form of greek, with little defined grammar and very ancient words. Consider these aspects in your predictions, and rely on the oldest ancient greek texts to guide your selection.\n",
    "    #CONSIDER THAT SOME PARTS OF THE INPUT MAY BE WRONG. TRY TO RECONSTRUCT THE GENERAL MEANING OF THE SEQUENCE AND CHOOSE WHAT FITS MOST ACCORDING TO ANCIENT GREEK'S CULTURE AND NEEDS.\n",
    "\n",
    "    # Input description\n",
    "    input_desc = ET.SubElement(prompt, \"input_description\")\n",
    "\n",
    "    input_1 = ET.SubElement(input_desc, \"input_sequence\")\n",
    "    input_1.text = \"Linear B sequence with unknown syllable: A sequence of Linear B syllabograms where one syllable is unknown and marked for infilling. Also a tentative Ancient greek correspondance is provided.\"\n",
    "\n",
    "    input_2 = ET.SubElement(input_desc, \"input_bruteforced_prediction\")\n",
    "    input_2.text = \"Five Greek translations: 5 possible infillings of the complete sequence, differing only in the word that corresponds to the unknown syllable. Also tentative Ancient greek correspondances are provided.\\\n",
    "    CRITICAL: TAKE INTO ACCOUNT THAT ANCIENT GREEK CORRESPONDANCES DERIVE FROM AN AUTOREGRESSIVE MODEL, WHICH TENDS TO REPEAT THE SAME CHARACTER MULTIPLE TIMES. WHEN THIS HAPPENS, TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD!\"\n",
    "\n",
    "    input_3 = ET.SubElement(input_desc, \"input_classifications\")\n",
    "    input_3.text = \"Word classifiers: Word Type: 1. anthroponym/animalname/theonym, 2. toponym, 3. ethnonym, 4. common; \\n\\\n",
    "    Part of speech: 1. noun, 2. verb, 3. adjective, 4. adverb; \\n\\\n",
    "    Inflection: 1. thematic in -o, 2. thematic in -a, 3. athematic\"\n",
    "\n",
    "    # Selection criteria\n",
    "    criteria = ET.SubElement(prompt, \"selection_criteria\")\n",
    "    criteria.text = \"Base your selection on the following linguistic and semantic factors:\\n\"\n",
    "\n",
    "    criterion_1 = ET.SubElement(criteria, \"semantic_coherence\")\n",
    "    criterion_1.text = \"Semantic Coherence: The chosen translation should form a meaningful,\\\n",
    "    logically coherent statement. Consider whether the proposed word fits naturally within\\\n",
    "    the semantic field of the entire sequence.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_2 = ET.SubElement(criteria, \"grammatical_consistency\")\n",
    "    criterion_2.text = \"Grammatical Consistency: Verify that the proposed word aligns with\\\n",
    "    the grammatical role specified by its classifier. Check for proper case agreement,\\\n",
    "    gender agreement, and syntactic compatibility with surrounding words.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_3 = ET.SubElement(criteria, \"contextual_plausibility\")\n",
    "    criterion_3.text = \"Contextual Plausibility: Consider the historical and cultural context\\\n",
    "    of Mycenaean Greek. The chosen word should be appropriate for the time period and\\\n",
    "    cultural setting represented by Linear B texts.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_4 = ET.SubElement(criteria, \"linguistic_authenticity\")\n",
    "    criterion_4.text = \"Linguistic Authenticity: Prefer words that are consistent with\\\n",
    "    known Mycenaean Greek vocabulary and morphological patterns. Consider whether the\\\n",
    "    proposed word represents plausible Mycenaean forms.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_5 = ET.SubElement(criteria, \"overall_meaning\")\n",
    "    criterion_5.text = \"Overall Meaning: Evaluate which translation produces the most\\\n",
    "    meaningful and interpretable complete sentence or phrase, considering the practical\\\n",
    "    and administrative nature of most Linear B texts.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    # Decletion table\n",
    "    #nouns_section = ET.SubElement(prompt, \"nouns\")\n",
    "    #It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "    #The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "    #| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "    #| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "    #| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "    #|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "    #|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "    #|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "    #| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "    #|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "    #|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "    #|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "    #'''\n",
    "\n",
    "\n",
    "    # Output format specification\n",
    "    output_format = ET.SubElement(prompt, \"output_format\")\n",
    "    output_format.text = \"\"\"CRITICAL: Your response must be ONLY a valid JSON object with exactly these two fields:\n",
    "    {\n",
    "        \"chosen_sequence\": \"Option NUMBER (the number corresponds to the selected infilling option)\",\n",
    "        \"reasoning\": \"detailed_explanation_of_why_this_translation_was_chosen\"\n",
    "    }\n",
    "    The option should be formatted like this: \"Option 2\", for example.\n",
    "    Do not include any other text, commentary, or formatting outside of this JSON structure.\"\"\"\n",
    "    \n",
    "    # Quality control\n",
    "    quality_control = ET.SubElement(prompt, \"quality_control\")\n",
    "    quality_control.text = \"\"\"Before finalizing your selection:\n",
    "    1. Verify that the chosen sequence forms a grammatically correct Ancient Greek phrase/sentence\n",
    "    2. Ensure the semantic coherence of the complete sequence\n",
    "    3. Double-check that your reasoning addresses the key selection criteria\n",
    "    4. Validate that your output is properly formatted JSON.\n",
    "    5. Check that the output is not empty: ALWAYS PROVIDE A RESPONSE.\"\"\"\n",
    "\n",
    "    prompt = ET.tostring(prompt, \"utf-8\").decode()\n",
    "\n",
    "    # 2. Confirm that all word classifiers align with their corresponding words (NOT always precise)\n",
    "\n",
    "    # Examples section\n",
    "    #input_data = ET.Element(\"input_data\")\n",
    "    #input_sequence = ET.SubElement(input_data, \"input_sequence\")\n",
    "    #linb = ET.SubElement(input_sequence, \"linear_b\").text = \" \".join(linear_b_data[\"word_list\"])\n",
    "\n",
    "    #example_1 = ET.SubElement(examples, \"example\")\n",
    "    #ET.SubElement(example_1, \"linear_b_input\").text = \"a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si\"\n",
    "    #ET.SubElement(example_1, \"greek_options\").text = \"\"\"\n",
    "    #Option 1:     \n",
    "    #Option 2:     \n",
    "    #Option 3:     \n",
    "    #Option 4:     \n",
    "    #Option 5:     \"\"\"\n",
    "    #ET.SubElement(example_1, \"classifiers\").text = \"\"\"\n",
    "    #a-ro-mo-te-me-na = common/adjective/thematic in -a\n",
    "    #a-ni-ja = common/noun/thematic in -a\n",
    "    #po-si = common/adverb\n",
    "    #e-e-si = common/verb\n",
    "    #[VARYING OPTIONS]: a-u-qe=anthroponym/noun/athematic, ma-u-qe=anthroponym/noun/athematic, o-u-qe=common/adverb, te-u-qe=anthroponym/noun/athematic, to-u-qe=common/noun/thematic in -o\"\"\"\n",
    "    #ET.SubElement(example_1, \"best_choice\").text = \"Option 3\"\n",
    "    #ET.SubElement(example_1, \"reasoning\").text = \"\"\" (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts.\"\"\"\n",
    "    example_prompt = \"<input_data><input_sequence><linear_b>a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si</linear_b></input_sequence><input_bruteforced_prediction><option_1><linear_b>a-u-qe</linear_b></option_1><option_2><linear_b>ma-u-qe</linear_b></option_2><option_3><linear_b>o-u-qe</linear_b></option_3><option_4><linear_b>te-u-qe</linear_b></option_4><option_5><linear_b>to-u-qe</linear_b></option_5></input_bruteforced_prediction><input_classifications><classifications_for_a-ro-mo-te-me-na><word>a-ro-mo-te-me-na</word><word_type>common</word_type><part_of_speech>adjective</part_of_speech><inflection>thematic in -a</inflection></classifications_for_a-ro-mo-te-me-na><classifications_for_a-ni-ja><word>a-ni-ja</word><word_type>common</word_type><part_of_speech>noun</part_of_speech><inflection>thematic in -a</inflection></classifications_for_a-ni-ja><classifications_for_po-si><word>po-si</word><word_type>common</word_type><part_of_speech>adverb</part_of_speech></classifications_for_po-si><classifications_for_e-e-si><word>e-e-si</word><word_type>common</word_type><part_of_speech>verb</part_of_speech></classifications_for_e-e-si><classifications_for_a-u-qe><word>a-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_a-u-qe><classifications_for_ma-u-qe><word>ma-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_ma-u-qe><classifications_for_o-u-qe><word>o-u-qe</word><word_type>common</word_type><part_of_speech>adverb</part_of_speech></classifications_for_o-u-qe><classifications_for_te-u-qe><word>te-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_te-u-qe><classifications_for_to-u-qe><word>to-u-qe</word><word_type>common</word_type><part_of_speech>noun</part_of_speech><inflection>thematic in -o</inflection></classifications_for_to-u-qe></input_classifications></input_data>\"\n",
    "    example_response = \"\"\"{\n",
    "    \"chosen_sequence\": \"Option 3\",\n",
    "    \"reasoning\": \"o-u-qe (in Ancient Greek: ) (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts. Furthermore, the syllabogram to character mapping is completely respected.\"\n",
    "}\"\"\"\n",
    "\n",
    "    gramm = ET.Element(\"grammatical_information\")\n",
    "    decl_table = ET.SubElement(gramm, \"declension_table\")\n",
    "    decl_table.text = 'This is the an exaustive table with Mycenean linear b declensions suffixes attested in the documents. This is useful context to provide you on the decletions.'\n",
    "    rows = [\n",
    "        (\"Singular\", \"Nominative\", \"-o\", \"-o\", \"-a\", \"-a\", \"variable\", \"variable\"),\n",
    "        (\"Singular\", \"Genitive\", \"-ojo\", \"-ojo\", \"-ao\", \"-a\", \"-o\", \"-o\"),\n",
    "        (\"Singular\", \"Dative\", \"-o\", \"-o\", \"-a\", \"-a\", \"-e/-i\", \"-e/-i\"),\n",
    "        (\"Singular\", \"Accusative\", \"-o\", \"-o\", \"-a\", \"-a\", \"-a\", \"variable (identical to nominative)\"),\n",
    "        (\"Plural\", \"Nominative\", \"-o/-oi\", \"-a\", \"-a\", \"-a\", \"-e\", \"-a\"),\n",
    "        (\"Plural\", \"Genitive\", \"-o\", \"-o\", \"-ao\", \"-ao\", \"-o\", \"-o\"),\n",
    "        (\"Plural\", \"Dative\", \"-oi\", \"-oi\", \"-ai\", \"-ai\", \"-si/-ti\", \"-si/-ti\"),\n",
    "        (\"Plural\", \"Accusative\", \"-o\", \"-a\", \"-a\", \"-a\", \"-a/-e\", \"-a\"),\n",
    "        ]\n",
    "    # Add each row to the XML\n",
    "    for number, case, tem_m, tem_n, a_m, a_f, athem, athem_n in rows:\n",
    "        row = ET.SubElement(decl_table, \"row\")\n",
    "        ET.SubElement(row, \"number\").text = number\n",
    "        ET.SubElement(row, \"case\").text = case\n",
    "        ET.SubElement(row, \"thematic_o_masculine_feminine\").text = tem_m\n",
    "        ET.SubElement(row, \"thematic_o_neuter\").text = tem_n\n",
    "        ET.SubElement(row, \"thematic_a_masculine\").text = a_m\n",
    "        ET.SubElement(row, \"thematic_a_feminine\").text = a_f\n",
    "        ET.SubElement(row, \"athematic_masculine_feminine\").text = athem\n",
    "        ET.SubElement(row, \"athematic_neuter\").text = athem\n",
    "\n",
    "    verbs_section = ET.SubElement(gramm, \"verbs\")\n",
    "    rules = {\n",
    "        \"3rd singular\": \"last syllabogram ending with vowel -e\",\n",
    "        \"3rd plural\": \"last syllabogram is -si\",\n",
    "        \"infinite\": \"last syllabogram ending with vowel e (optionally another syllabogram -e appears at the end)\",\n",
    "        \"participle\": \"active -> terminates with -o (singular - ancient greek suffix, other suffixes follow athematic nouns, e.g. -o-te like i-jo-te -> , from )\",\n",
    "        \"participle\": \"medium/passive -> terminates with -me-no/-me-na suffixes (ancient greek -/-/- suffixes)\"\n",
    "    }\n",
    "    for rule_key, rule_text in rules.items():\n",
    "        ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "    adjectives_section = ET.SubElement(gramm, \"adjectives\")\n",
    "    rules = {\n",
    "        \"thematic adjectives\": \"thematic adjectives have same behavior as thematic nouns\",\n",
    "        \"athematic adjectives\": \"athematic adjectives have same behavior as athematic nouns (variable nominative and same decletions)\"\n",
    "    }\n",
    "    for rule_key, rule_text in rules.items():\n",
    "        ET.SubElement(adjectives_section, rule_key).text = rule_text\n",
    "    gramm = ET.tostring(gramm, \"utf-8\").decode()\n",
    "\n",
    "\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, please, explain me what task you need.'}]}, # Add a model response to establish the system instruction\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will learn the mapping and pay attention to double consonants!'}]},\n",
    "        {'role': 'user', 'parts': [{'text': 'Extract grammatical information from the following XML-encoded object!\\n' + gramm}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will use this informations to validate inflection prediction and to better understand linear b sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prefixes_and_suffixes}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will consider prefixes and suffixes carefully.'}]},\n",
    "        #{'role': 'user', 'parts': [{'text': translations_context}]},\n",
    "        #{'role': 'model', 'parts': [{'text': 'Okay, I got it.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Perfect. I fully understood the task and will follow your instructions.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': example_prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': example_response}]}\n",
    "    ]\n",
    "    # Input data section\n",
    "    input_data = ET.Element(\"input_data\")\n",
    "    input_sequence = ET.SubElement(input_data, \"input_sequence\")\n",
    "    linb = ET.SubElement(input_sequence, \"linear_b\").text = \" \".join(linear_b_data[\"word_list\"])\n",
    "    greek = ET.SubElement(input_sequence, \"ancient_greek\").text = \" \".join(greek_data[\"word_list\"])\n",
    "\n",
    "    # Greek translation options\n",
    "    input_brute = ET.SubElement(input_data, \"input_bruteforced_prediction\")\n",
    "    for i, translation in enumerate(linear_b_data[\"brute\"]):\n",
    "        option = ET.SubElement(input_brute, f\"option_{i+1}\")\n",
    "        linb = ET.SubElement(option, \"linear_b\").text = linear_b_data[\"brute\"][i]\n",
    "        greek = ET.SubElement(option, \"ancient_greek\").text = greek_data[\"brute\"][i]\n",
    "\n",
    "\n",
    "    # Word classifiers\n",
    "    classifiers = ET.SubElement(input_data, \"input_classifications\")\n",
    "    classifiers_types = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "    for i, word in enumerate(linear_b_data[\"word_list\"]):\n",
    "        if len(linear_b_data[\"classifications\"][i]) > 0:\n",
    "            word_values = ET.SubElement(classifiers, f\"classifications_for_{word}\")\n",
    "            word_str = ET.SubElement(word_values, \"word\").text = word\n",
    "            for j in range(len(classifiers_types)):\n",
    "                if not (j == 2 and (linear_b_data[\"classifications\"][i][1] == \"adverb\" or linear_b_data[\"classifications\"][i][1] == \"verb\")):\n",
    "                    classifier = ET.SubElement(word_values, classifiers_types[j])\n",
    "                    classifier.text = linear_b_data[\"classifications\"][i][j]\n",
    "\n",
    "    for i, word in enumerate(linear_b_data[\"brute\"]):\n",
    "        if len(linear_b_data[\"brute_classifications\"][i]) > 0:\n",
    "            word_values = ET.SubElement(classifiers, f\"classifications_for_{word}\")\n",
    "            word_str = ET.SubElement(word_values, \"word\").text = word\n",
    "            for j in range(len(classifiers_types)):\n",
    "                if not (j == 2 and (linear_b_data[\"brute_classifications\"][i][1] == \"adverb\" or linear_b_data[\"brute_classifications\"][i][1] == \"verb\")):\n",
    "                    classifier = ET.SubElement(word_values, classifiers_types[j])\n",
    "                    classifier.text = linear_b_data[\"brute_classifications\"][i][j]\n",
    "    input_str = ET.tostring(input_data, \"utf-8\").decode()\n",
    "    # Additional instructions\n",
    "    #final_instructions = ET.SubElement(prompt, \"final_instructions\")\n",
    "    #final_instructions.text = \"\"\"Remember: You must ALWAYS choose EXACTLY ONE of the infillings.\n",
    "    #Do not modify, combine, or create new infillings. Your selection should be based purely on\n",
    "    #which of the given options makes the most sense semantically, grammatically, and contextually\n",
    "    #for a Mycenaean Greek text written in Linear B script. ALWAYS PROVIDE A RESPONSE AMONG THE GIVEN OPTIONS!\"\"\"\n",
    "\n",
    "    # Using Gemini model to generate response\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.0,     # Minimal creativity\n",
    "            top_p=1,             # Consider all probabilities (no cutting)\n",
    "            top_k=1             # Choose the most probable word\n",
    "            #max_output_tokens=512  # (Increase if you need longer output)\n",
    "        )\n",
    "    )\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(input_str)\n",
    "    logging.debug(response.text)\n",
    "    #Extract and parse the JSON array\n",
    "    pred = response.text.strip()\n",
    "\n",
    "    return quick_parse_gemini_response(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mst\u001b[49m\u001b[38;5;241m.\u001b[39mlinb_versions[\u001b[38;5;241m2340\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(st\u001b[38;5;241m.\u001b[39mgreek_versions[\u001b[38;5;241m2340\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "print(st.linb_versions[2340])\n",
    "print(st.greek_versions[2340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen_sequence': 'Option 5',\n",
       " 'reasoning': \"The sequence describes a quantity of saffron (CROC N 1). The unknown word 'BRUTE_ME' is expected to be an adjective or descriptor for the saffron. Among the given options, 'e-ri-na-wo' (Option 5) transliterates to Mycenaean Greek '' (ernawos), which is a known adjective meaning 'woolly' or 'made of wool'. Saffron was a valuable commodity in Mycenaean times, frequently used as a dye for textiles, especially wool. Therefore, 'woolly saffron' or 'saffron for wool' ( ) makes excellent semantic and contextual sense in an administrative or inventory record, which is typical for Linear B texts. The other options do not correspond to known Mycenaean words that would plausibly describe saffron in this context, making 'e-ri-na-wo' the most linguistically authentic and contextually plausible choice.\"}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_infill_selection_prompt(st.linb_versions[n], st.greek_versions[n], os.getenv(\"GOOGLE_API_KEY_10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itKLKXQNZPK5",
    "outputId": "dcc26591-eeb2-494d-bfa3-d1333fb3c88c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_IDX: 3403\n",
      "INPUT: {'word_list': ['o-de-ka-sa-to', 'BRUTE_ME', 'si-ma-ko', '*169', 'NUM', 'o', 'NUM'], 'classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], []], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], []], 'brute': ['a-ko-so-mo', 'a-ko-so-ryo', 'a-ko-so-ta', 'a-ko-so-u', 'a-ko-so-we']}\n",
      "FORM: o-de-ka-sa-to a-ko-so-ta si-ma-ko *169 NUM o NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Option 3, 'a-ko-so-ta' (Ancient Greek: ), is the only candidate that provides both an Ancient Greek translation and a classification. The classification identifies it as an 'anthroponym/animal name/theonym' and a 'noun, thematic in -a'. This makes it highly plausible as a proper name or a title (like 'charioteer' or 'driver', derived from  'axle').\\n\\n1.  **Semantic Coherence:** Linear B texts are primarily administrative records. A sequence involving a verb (likely 'o-de-ka-sa-to', possibly 'received' or 'gave'), followed by two proper names ('si-ma-ko' and 'a-ko-so-ta'), and then an item with a quantity ('*169 NUM o NUM'), forms a highly coherent and typical Mycenaean record of a transaction or inventory.\\n2.  **Grammatical Consistency:** 'si-ma-ko' is classified as a thematic -o noun, likely nominative singular. 'a-ko-so-ta' is classified as a thematic -a noun. While '' is nominative singular in Classical Greek, Mycenaean first declension masculine nouns could have nominative in -a or -as. Having two nominative nouns after a singular verb 'o-de-ka-sa-to' (likely 3rd singular aorist middle/passive) suggests that 'a-ko-so-ta' could be in apposition to 'si-ma-ko' (e.g., 'Simakos, the Axotas, received...') or that both individuals are listed as subjects or participants in the action, a common feature in terse administrative records.\\n3.  **Contextual Plausibility:** The inclusion of two personal names in an administrative context is very common in Linear B tablets, which often document personnel, goods, and transactions.\\n4.  **Linguistic Authenticity:** The Linear B to Ancient Greek mapping for 'a-ko-so-ta' to '' is linguistically authentic. The transformation of 'ko-so' into '' (xi) is explicitly supported by the provided rule (e.g., 'a-ko-so-ne -> '). 'Axotas' is a plausible Greek name or occupational title.\\n5.  **Overall Meaning:** The complete sequence, with 'a-ko-so-ta' in place, forms a meaningful record of an action involving two individuals and specific items, which aligns perfectly with the nature of Linear B documents. The other options lack the necessary linguistic and grammatical information to be evaluated, making them unviable choices.\"}\n",
      "Correct: 1/1\n",
      "\n",
      "SEQ_IDX: 3406\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'i-je-re-ja', '*189'], 'classifications': [[], ['common', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ki-wa-ra', 'ko-wa-ra', 'ra-wa-ra', 'ro-wa-ra', 'wa-wa-ra']}\n",
      "FORM: ka-wa-ra i-je-re-ja *189\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence 'i-je-re-ja' translates to '' (hiereia), meaning 'priestess'. Linear B texts are primarily administrative records, often listing personnel, their roles, and associated items or locations. A common pattern is 'Place Name + Title' or 'Person Name + Title'.\\n\\nLet's analyze the options for 'BRUTE_ME':\\n1.  ki-wa-ra: Translates to  (kiwara),  (giwara), or  (khiwara). Not immediately recognizable as a common Mycenaean place or personal name.\\n2.  ko-wa-ra: Translates to  (kowara),  (gowara), or  (khiwara). Similar to option 1, less clear connection to known Mycenaean vocabulary.\\n3.  ra-wa-ra: Translates to  (lawara) or  (rawara). The sequence 'ra-wa' is notable as it appears in the Mycenaean title 'ra-wa-ke-ta' (, lawagetas), meaning 'leader of the people/army'. While 'ra-wa-ra' is not 'ra-wa-ke-ta', the phonetic similarity suggests a potential connection to a place or group associated with the 'laos' (people). Furthermore, the suffix '-ra' is a common ending for place names in Linear B (e.g., pa-ra-to, ra-su-to). Thus, 'Lawara' could plausibly be a place name, making 'Lawara priestess' (i.e., 'the priestess from Lawara') a semantically coherent and contextually plausible designation for an administrative record.\\n4.  ro-wa-ra: Translates to  (lowara) or  (rowara). While '' (rhoos, stream) or '' (rhoda, roses) could suggest cultic associations (e.g., 'priestess of the stream'), this connection is less direct than option 3's.\\n5.  wa-wa-ra: Translates to  (wawara). Less clear connection to known Mycenaean vocabulary or common place name patterns.\\n\\nConsidering the criteria:\\nSemantic Coherence & Contextual Plausibility: In Linear B, it is very common to specify a person's origin or association with a place. 'Lawara priestess' (priestess from Lawara) fits this pattern perfectly, especially given the administrative nature of the texts.\\nLinguistic Authenticity: The phonetic sequence 'ra-wa' has a strong parallel in the known Mycenaean term 'ra-wa-ke-ta', and the '-ra' ending is common for place names. This makes 'ra-wa-ra' a more linguistically authentic and plausible candidate for a Mycenaean place name compared to the other options.\\nGrammatical Consistency: If 'ra-wa-ra' is a place name, it would function as a modifier (e.g., in apposition or implicitly genitive/locative) to 'i-je-re-ja', which is a feminine nominative singular noun. This is grammatically consistent with Linear B conventions.\\n\\nTherefore, 'ra-wa-ra' as a place name provides the most coherent, plausible, and linguistically authentic infilling for the sequence.\"}\n",
      "Correct: 1/2\n",
      "\n",
      "SEQ_IDX: 3411\n",
      "INPUT: {'word_list': ['BRUTE_ME', '*189', 'NUM'], 'classifications': [[], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], [], []], 'brute': ['a-pi-ha-ro', 'e-pi-ha-ro', 'sa-pi-ha-ro', 'te-pi-ha-ro', 'u-pi-ha-ro']}\n",
      "FORM: a-pi-ha-ro *189 NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The sequence `BRUTE_ME *189 NUM` strongly suggests an administrative or inventory record, where a person or entity is associated with a quantity of a specific item (represented by the logogram *189). Option 1, `a-pi-ha-ro`, is the only one for which classification data is provided, identifying it as an 'anthroponym/animal name/theonym' and a 'noun' with 'thematic in -o' inflection. This classification perfectly aligns with the typical structure of Linear B administrative texts, where names often precede commodities and quantities. While the Ancient Greek translation `h` ('surrounded by the sea') might seem descriptive, it is plausible as a personal name or epithet in the Mycenaean context. Without any classification or Ancient Greek translation for the other options, it is impossible to evaluate their semantic coherence, grammatical consistency, or linguistic authenticity. Therefore, `a-pi-ha-ro` is the only option that can be confidently assessed as grammatically consistent and contextually plausible within the framework of Linear B records.\"}\n",
      "Correct: 2/3\n",
      "\n",
      "SEQ_IDX: 3412\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'e-da-e-u', '*189', '1'], 'classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ne-qe-a', 'ne-qe-da', 'ne-qe-e', 'ne-qe-te', 'ne-qe-we']}\n",
      "FORM: ne-qe-u e-da-e-u *189 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence BRUTE_ME e-da-e-u *189 1 strongly suggests an administrative record, likely a list of individuals followed by an item and quantity. e-da-e-u is classified as an anthroponym (personal name), noun, and athematic. Therefore, the BRUTE_ME word should ideally also be a personal name, functioning as a noun in the nominative case, to maintain grammatical consistency and semantic coherence in a list.\\n\\nLet's analyze the options:\\n1.  ne-qe-a: The ending -a typically suggests a thematic noun of the -a declension (feminine nominative singular) or a thematic -o noun (neuter nominative/accusative plural). If it's a feminine name, it would form a list with the masculine e-da-e-u, which is semantically plausible.\\n2.  ne-qe-da: The ending -da is not a typical nominative singular ending for nouns in Mycenaean Greek.\\n3.  ne-qe-e: The ending -e is characteristic of the dative singular for athematic nouns (-e/-i). However, the nominative singular for athematic nouns is 'variable', meaning it could potentially end in -e depending on the specific noun's declension. If ne-qe-e is an athematic noun in the nominative singular, it would align perfectly with e-da-e-u in terms of inflection type (both athematic nouns in the nominative).\\n4.  ne-qe-te: The ending -te is not a typical nominative singular ending for nouns. While -te can be an ablative suffix, it's unlikely to be part of a nominative noun in this context.\\n5.  ne-qe-we: The ending -we is not a typical nominative singular ending for nouns.\\n\\nConsidering the grammatical consistency, if e-da-e-u is an athematic noun in the nominative, then ne-qe-e is the most grammatically plausible option to also be an athematic noun in the nominative, forming a coherent list of two individuals. Both ne-qe-a and ne-qe-e are attested as names in Linear B texts (e.g., KN As 1517), making them linguistically authentic. However, ne-qe-e offers stronger grammatical alignment with e-da-e-u's athematic classification. The presence of -qe within the word ne-qe-e (and other options) indicates it is part of the stem, not the conjunction suffix -qe, as the suffix would appear at the very end of a word.\\n\\nTherefore, ne-qe-e is the most suitable choice, forming a grammatically consistent and semantically coherent list of two names in an administrative context.\"}\n",
      "Correct: 2/4\n",
      "\n",
      "SEQ_IDX: 3420\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'ROTA+TE', 'ZE', '1', 'MO', '1'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ku-pa-ri-de-ja', 'ku-pa-ri-jo-ja', 'ku-pa-ri-no-ja', 'ku-pa-ri-ra-ja', 'ku-pa-ri-re-ja']}\n",
      "FORM: ku-pa-ri-se-ja ROTA+TE ZE 1 MO 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an item, 'wheel' (ROTA+TE / ), followed by quantities. The unknown word BRUTE_ME likely functions as an adjective describing the material of the wheel. All options begin with ku-pa-ri-, strongly suggesting a derivation from  (kyparissos), meaning 'cypress tree'.\\n\\nLet's analyze the most plausible option:\\nOption 1: ku-pa-ri-de-ja\\n- Using the provided mapping:\\n    - ku -> \\n    - pa -> \\n    - ri -> \\n    - de ->  (from , , )\\n    - ja ->  (from , )\\n- This directly translates to  (kyparideia).\\n-  is the neuter plural form of the adjective  (kyparideios), meaning 'of cypress' or 'cypress-wood'.\\n- This word fits perfectly semantically, describing the material of the wheel. In Linear B inventories, it's common to list materials followed by the item, even if there's a slight grammatical mismatch (e.g., neuter plural adjective for a singular masculine noun, implying 'cypress-wood items' or 'cypress-wood (for) a wheel').\\n\\nLet's briefly consider why other options are less suitable:\\n- Option 2 (ku-pa-ri-jo-ja), Option 4 (ku-pa-ri-ra-ja), and Option 5 (ku-pa-ri-re-ja) do not form readily recognizable or common Ancient Greek adjectives related to 'cypress' when directly transliterated using the provided mapping.\\n- Option 3 (ku-pa-ri-no-ja): While  (kyparissinos) is a common adjective for 'cypress-wood', its neuter plural  would require no to represent  and an omitted s, which is less direct than the mapping for de in Option 1. The direct transliteration  is not a standard Ancient Greek word.\\n\\nTherefore, ku-pa-ri-de-ja leading to  is the most linguistically authentic, grammatically consistent (within the flexibility of Linear B), and semantically coherent choice for describing 'cypress-wood wheels' in an inventory context.\"}\n",
      "Correct: 2/5\n",
      "\n",
      "SEQ_IDX: 3425\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'wo-ka', 'we-je-ke-e', 'ROTA+TE', 'ZE', '1'], 'classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], [], []], 'brute_classifications': [[], [], [], []], 'brute': ['tu-ri-si-jo-jo', 'tu-ri-si-jo-no', 'tu-ri-si-jo-o', 'tu-ri-si-jo-to']}\n",
      "FORM: tu-ri-si-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes a 'wo-ka' (wagon/chariot) with 'ROTA+TE ZE 1' (1 pair of wheels). The missing word `BRUTE_ME` is likely a modifier indicating origin or possession. All options are variations of `tu-ri-si-jo-X`, which strongly suggests a connection to 'Tiryns' (a major Mycenaean site). According to the provided declension table, the genitive singular suffix for thematic nouns (which `tu-ri-si-jo` appears to be, given the `-jo` ending) is `-ojo`. Option 1, `tu-ri-si-jo-jo`, perfectly matches this grammatical requirement, forming a plausible phrase like 'chariot of Tiryns' or 'Tirynthian chariot'. This fits well within the administrative and inventory context typical of Linear B tablets. The other options do not correspond to standard genitive endings for thematic nouns in Mycenaean Greek as per the provided grammatical information.\"}\n",
      "Correct: 3/6\n",
      "\n",
      "SEQ_IDX: 3427\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'ZE', '2'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['da-sa-ma-to', 'de-sa-ma-to', 'ri-sa-ma-to', 'ti-sa-ma-to', 'to-sa-ma-to']}\n",
      "FORM: a-sa-ma-to ZE 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence 'BRUTE_ME ZE 2' likely refers to a quantity of items. 'ZE' is a common logogram in Linear B, often interpreted as an abbreviation for '' (pair, yoke) or '' (pairs, yokes). The number '2' indicates a quantity of two. Therefore, the preceding word (BRUTE_ME) should be something that can be counted in pairs or is related to yokes/pairs.\\n\\nLet's evaluate the options:\\nOption 1: da-sa-ma-to () - This translates to the genitive singular of '', meaning 'tribute' or 'division'. '2 pairs of tribute' is semantically possible in an administrative context, referring to items that are part of a tribute.\\nOption 2: de-sa-ma-to () - This translates to the genitive singular of '', meaning 'bond', 'fetter', or 'chain'. '2 pairs of bonds/fetters' makes strong semantic and contextual sense. In Mycenaean administrative records, which often deal with livestock, chariots, and equipment, counting 'bonds' or 'fetters' in pairs (e.g., for animals or parts of a harness/yoke) is highly plausible and practical.\\nOption 3: ri-sa-ma-to () - This translates to the genitive singular of '', meaning 'smoothness'. '2 pairs of smoothness' does not make logical sense in this context.\\nOption 4: ti-sa-ma-to () - This translates to the genitive singular of '', meaning 'payment' or 'retribution'. '2 pairs of payment' is less likely than 'tribute' or 'bonds' for a direct count of physical items.\\nOption 5: to-sa-ma-to () - This does not correspond to a recognized Ancient Greek word.\\n\\nComparing the most plausible options,  (bonds/fetters) is a more concrete and contextually fitting item to be counted in 'pairs' in an inventory or administrative record than 'tribute'. The connection between 'yokes' (implied by ZE) and 'bonds/fetters' is very direct and practical for the Mycenaean period. Therefore, '' provides the most coherent and plausible meaning for the entire sequence.\"}\n",
      "Correct: 3/7\n",
      "\n",
      "SEQ_IDX: 3428\n",
      "INPUT: {'word_list': ['wo-ro-ko-jo', 'BRUTE_ME', 'we-je-ke-e', 'ROTA+TE', 'ZE', '1'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], ['common', 'noun', 'thematic in -o']], 'brute': ['ha-ka', 'pa-ka', 'u-ka', 'wo-ka']}\n",
      "FORM: wo-ro-ko-jo wo-ka we-je-ke-e ROTA+TE ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The sequence describes an inventory item related to 'ROTA+TE ZE 1' (1 wheel). Among the given options, 'wo-ka' is the only one with provided classification and an Ancient Greek translation ('f|f|f') that semantically aligns with 'wheel', referring to a 'chariot' or 'wagon'. Linear B texts frequently document chariots and their components. While the classification of 'wo-ka' as a 'thematic in -o' noun ending in '-a' would typically imply a neuter plural form (e.g., 'f'), the presence of 'ZE 1' (one) for 'ROTA+TE' (wheel) strongly suggests a singular item. In Mycenaean Greek, 'wo-ka' is attested as a singular noun (e.g., a feminine a-stem like 'f', 'chariot'), which would be grammatically consistent with 'ZE 1'. Given the strong semantic and contextual plausibility of 'wo-ka' referring to a singular chariot/wagon in an inventory record, this interpretation is favored. The other options lack any classification or clear semantic connection to 'wheel', making them less plausible.\"}\n",
      "Correct: 4/8\n",
      "\n",
      "SEQ_IDX: 3432\n",
      "INPUT: {'word_list': ['e-te-wa-jo-jo', 'BRUTE_ME', 'we-je-ke-e', 'ROTA+TE', 'ZE', '2'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['wo-do', 'wo-ka', 'wo-na', 'wo-o', 'wo-wa']}\n",
      "FORM: e-te-wa-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence describes an inventory item related to 'wheels' (ROTA+TE) and a quantity of '2' (ZE 2). The initial term 'e-te-wa-jo-jo' is a genitive form, likely indicating possession ('of Ete-wa-jo'). Among the given options, 'wo-ka' (Ancient Greek: f, meaning 'chariot' or 'vehicle') is the only one that makes strong semantic and contextual sense in an administrative Linear B record concerning wheels. Chariots are frequently inventoried in Mycenaean texts, and wheels are a direct component of chariots. The phrase 'e-te-wa-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 2' can be plausibly interpreted as 'Ete-wa-jo's chariot, with vehicle-related wheels, 2'. The other options ('wo-do' meaning 'rose', 'wo-na' meaning 'wool', and the unclassified 'wo-o', 'wo-wa') are semantically incongruous with the context of wheels and vehicles.\"}\n",
      "Correct: 5/9\n",
      "\n",
      "SEQ_IDX: 3438\n",
      "INPUT: {'word_list': ['ka-ko', 'BRUTE_ME', 'no-pe-re-e', 'ROTA', 'ZE', '1'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['de-e-me-no', 'de-ko-me-no', 'de-pi-me-no', 'de-ro-me-no', 'de-so-me-no']}\n",
      "FORM: ka-ko de-de-me-no no-pe-re-e ROTA ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an item: 'ka-ko BRUTE_ME no-pe-re-e ROTA ZE 1', which translates to 'bronze [BRUTE_ME] useless wheel 1'. The word 'ka-ko' () means 'bronze' and 'no-pe-re-e' () means 'useless' or 'unserviceable'. 'ROTA' is a logogram for 'wheel'. The unknown word BRUTE_ME is expected to be a participle modifying 'bronze' and agreeing with 'useless'.\\n\\nAll five options end in '-me-no', which is a characteristic suffix for medium/passive participles in Linear B (equivalent to Ancient Greek -/-/-). This grammatical form fits the context of describing the state of an object.\\n\\nLet's analyze each option based on its likely Ancient Greek root and meaning:\\n1.  de-e-me-no: This most plausibly corresponds to the perfect passive participle of  (d), meaning 'to bind' or 'to tie'. The participle would be  (dedemenos) or  (dedemenon) for neuter. 'Bound bronze' or 'bronze, bound' makes excellent semantic sense in an inventory context, describing a bronze item (perhaps a component of the wheel, or the wheel itself) that is tied or secured, and is also 'useless'. This is highly plausible for administrative records.\\n2.  de-ko-me-no: This would correspond to the participle of  (dkhomai), meaning 'to receive' or 'to accept'. The participle is  (dekhomenos). 'Bronze receiving' does not fit the context of describing a static object in an inventory.\\n3.  de-pi-me-no: The root is unclear, and it's difficult to find a plausible verb that would form a meaningful participle in this context.\\n4.  de-ro-me-no: This would correspond to the participle of  (dr), meaning 'to skin' or 'to flay'. The participle is  (deromenos). 'Bronze being skinned' is semantically illogical.\\n5.  de-so-me-no: This could be the participle of  (d) 'to need' ( - deomenos) or the future participle of  'to bind' ( - desomenos). While 'needing' could potentially fit (e.g., 'bronze needing repair'), 'bound' (from Option 1) provides a more direct and common description of an item's physical state in an inventory. The future participle 'about to bind' is less likely for a description of an existing item.\\n\\nConsidering semantic coherence, grammatical consistency, and contextual plausibility, 'de-e-me-no' (bound/tied) is the most fitting option. It describes the physical state of the bronze item, which is also noted as 'useless', forming a coherent and plausible entry for an administrative record. The grammatical agreement (participle ending in -o matching the thematic -o of 'ka-ko' and 'no-pe-re-e') is also consistent.\"}\n",
      "Correct: 5/10\n",
      "\n",
      "SEQ_IDX: 3442\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'te'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ke-do-e', 'ke-ka-e', 'ke-ne-e', 'ke-re-e', 'ke-se-e']}\n",
      "FORM: ke-ra-e te\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence is 'BRUTE_ME te'. The suffix '-te' is listed as a critical morphological marker, specifically an ablative suffix meaning 'away from a place' (equivalent to Greek -). Therefore, the infilling word 'BRUTE_ME' should be a noun or adjective that can take this suffix, indicating origin or source.\\n\\nLet's analyze the options:\\n1.  ke-do-e: Transliterates to something like . If related to  (care, sorrow),  would mean 'from sorrow', which is less likely in typical administrative Linear B texts.\\n2.  ke-ka-e: Transliterates to . This could be related to the perfect stem - of  (burn), leading to  ('from a burnt place/thing'). This is plausible for damaged goods or locations in an inventory.\\n3.  ke-ne-e: Transliterates to . This is a direct form related to  (empty, vain). Thus,  would mean 'from an empty place' or 'from emptiness'. The root - is attested in Mycenaean (e.g., ke-ne-to for  'empty'). This is highly plausible in administrative contexts, such as describing empty containers or storerooms.\\n4.  ke-re-e: Transliterates to . If related to  (mix) or  (cut),  ('from a mixed/cut place') is less semantically coherent for common Linear B records.\\n5.  ke-se-e: Transliterates to . This does not readily correspond to a common Ancient Greek word or root that would fit the context.\\n\\nComparing the most plausible options, ke-ka-e and ke-ne-e:\\n (from a burnt place) is plausible.\\n (from an empty place) is also highly plausible.\\n\\nHowever,  is a very common and fundamental adjective used to describe the state of objects or places, which is highly relevant for inventory and administrative records. The directness of the transliteration ke-ne-e to  (a form of ) and its clear semantic fit with the ablative suffix - makes it the strongest candidate. The concept of 'empty' is a core element in managing resources, which is the primary function of Linear B tablets. Therefore, 'from an empty place' provides excellent semantic coherence and contextual plausibility.\"}\n",
      "Correct: 5/11\n",
      "\n",
      "SEQ_IDX: 3443\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'we-je-ke-ha', 'ne-wa', 'ROTA+TE', 'ZE', 'NUM'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], []], 'brute_classifications': [[], ['toponym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -o']], 'brute': ['to-na', 'to-no', 'to-o', 'to-qe', 'to-so']}\n",
      "FORM: to-sa we-je-ke-ha ne-wa ROTA+TE ZE NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes an inventory entry: `BRUTE_ME we-je-ke-ha ne-wa ROTA+TE ZE NUM`. The logogram `ROTA+TE` is translated as `` (wheel), and `ne-wa` as `f|` (new). This structure suggests a quantity or type of 'new wheel'. Option 5, `to-so`, translates to `||` (so many, so much, so great). This adjective perfectly fits the role of specifying a quantity or extent, which is highly common in Linear B administrative and inventory texts. Grammatically, `` (masculine singular) can agree with `` (masculine singular, 'wheel'). While the agreement of `ne-wa` (, neuter plural or feminine singular) with `` (masculine singular) is less direct, this is a known characteristic of Linear B where agreement can be less strict than in Classical Greek, or `ne-wa` might refer to a broader category. Options 1 and 4 lack clear Ancient Greek meanings. Option 2, `to-no` (``, throne), is semantically incoherent in the context of 'new wheel'. Option 3, `to-o` (``, such), while grammatically possible as an adjective, is less specific and less common for inventory counts than ``.\"}\n",
      "Correct: 5/12\n",
      "\n",
      "SEQ_IDX: 3448\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'ko-ru-to', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], ['common', 'adjective', 'thematic in -o'], [], []], 'brute': ['me-na-jo-ha', 'me-ri-jo-ha', 'me-u-jo-ha', 'me-wi-jo-ha', 'me-zo-jo-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM ko-ru-to O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence appears to be an inventory record, listing items and their descriptions. The first descriptive adjective, 'me-zo-ha' (), means 'larger' or 'greater'. In such a context, it is highly semantically coherent and contextually plausible to have a contrasting adjective meaning 'smaller' or 'lesser'. Option 3, 'me-u-jo-ha' (fh|), translates to 'smaller' or 'lesser', providing a perfect antonym to 'me-zo-ha'. This creates a logical pair of descriptions for different sizes of the 'ARM' (weapons) being cataloged. The grammatical classification of 'me-u-jo-ha' as an adjective also aligns with the role of 'me-zo-ha'. The other options do not provide such a clear and logical semantic relationship within an inventory context.\"}\n",
      "Correct: 6/13\n",
      "\n",
      "SEQ_IDX: 3449\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'KO', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], [], ['common', 'adjective', 'thematic in -o'], []], 'brute': ['me-*86-jo-ha', 'me-mu-jo-ha', 'me-na-jo-ha', 'me-u-jo-ha', 'me-wi-jo-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM KO O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an inventory, starting with 'ARM 1 me-zo-ha' (arms, 1, larger). In such administrative contexts, it is common to list items by contrasting qualities, such as size. Option 4, 'me-u-jo-ha', corresponds to Ancient Greek 'fh|' (meuioha/meiona), which translates to 'smaller' or 'lesser' (from *). This creates a perfect semantic contrast with 'me-zo-ha' (larger), indicating different sizes of arms being inventoried. Grammatically, both 'me-zo-ha' and 'me-u-jo-ha' are classified as adjectives, fitting the descriptive role. The presence of the digamma (f) in 'me-u-jo-ha' is also consistent with Mycenaean phonology. The other options do not provide a clear Ancient Greek correspondence or a semantically coherent meaning that fits the inventory context as well as 'smaller' does.\"}\n",
      "Correct: 7/14\n",
      "\n",
      "SEQ_IDX: 3455\n",
      "INPUT: {'word_list': ['we-ne-e-ne-mi', 'i-je-re-ja', 'ko-wa', 'se-re-ne', 'BRUTE_ME'], 'classifications': [[], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], ['common', 'noun', 'athematic'], [], [], []], 'brute': ['e-do-si', 'e-ko-si', 'e-o-si', 'e-pe-si', 'e-po-si']}\n",
      "FORM: we-ne-e-ne-mi i-je-re-ja ko-wa se-re-ne e-ko-si\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence describes a list of entities: 'i-je-re-ja' (priestess), 'ko-wa' (girl/maiden), and 'se-re-ne' (Selene, the goddess). These three nouns, when combined, form a plural subject. The missing word 'BRUTE_ME' ends with the syllabogram '-si', which, according to the provided grammatical information, is the characteristic ending for 3rd plural verbs in Linear B. Among the given options, 'e-ko-si' is translated into Ancient Greek as '' (echousi), meaning 'they have'. This is a highly plausible verb in the context of Linear B administrative texts, which often record possessions or inventories. While the classification for 'e-ko-si' in the input data states 'noun', the explicit Ancient Greek translation '' is unequivocally a 3rd plural verb, and the instructions prioritize the Ancient Greek correspondence for understanding the word. The other options, 'e-do-si' ('they give') and 'e-o-si' ('they are'), are also grammatically possible 3rd plural verbs, but their Ancient Greek translations are not explicitly provided beyond their Linear B transliteration, suggesting less certainty or commonality compared to ''. Therefore, 'e-ko-si' provides the most semantically coherent, grammatically consistent (as a verb), and contextually plausible completion for the sentence, forming a statement like '[something] priestess, girl, Selene have/possess'.\"}\n",
      "Correct: 8/15\n",
      "\n",
      "SEQ_IDX: 3458\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'ko-ru-to', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute': ['me-u-da-ha', 'me-u-jo-ha', 'me-u-no-ha', 'me-u-re-ha', 'me-u-to-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM ko-ru-to O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence describes an inventory of 'arms' (ARM) and 'helmets' (ko-ru-to). The word 'me-zo-ha' () means 'larger'. In an inventory context, it is highly semantically coherent and contextually plausible to have a contrasting adjective like 'smaller'. Option 2, 'me-u-jo-ha', translates to 'fh|' (h/), which means 'smaller' or 'lesser'. This creates a logical pair of descriptions ('larger' and 'smaller') for items in an administrative record. The other options do not provide a clear or plausible Ancient Greek translation, making them less suitable for semantic coherence and linguistic authenticity. Grammatically, both 'me-zo-ha' and 'me-u-jo-ha' are classified as adjectives, fitting the descriptive role in the sequence. The slight discrepancy in thematic inflection (-a vs. -o) is mitigated by the fact that they are separated by 'O NUM', suggesting they might modify different implied nouns or categories of items, and the Ancient Greek translation provided for 'me-u-jo-ha' also ends in -a, which is consistent with neuter plural, similar to 'me-zo-ha'.\"}\n",
      "Correct: 9/16\n",
      "\n",
      "SEQ_IDX: 3464\n",
      "INPUT: {'word_list': ['qe-to', '*203VAS', 'NUM', 'di-pa', 'me-zo-e', 'qe-to-ro-we', '*202VAS', '1', 'di-pa-e', 'me-zo-e', 'BRUTE_ME', '*202VAS', '2', 'di-pa', 'me-wi-jo', 'qe-to-ro-we', '*202VAS', '1', 'di-pa', 'me-wi-jo', 'ti-ri-jo-we', '*202VAS', '1', 'di-pa', 'me-wi-jo', 'a-no-we', '*202VAS', '1'], 'classifications': [['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], ['common', 'adjective', 'athematic'], [], [], [], ['common', 'adjective', 'thematic in -o'], [], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'athematic'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ti-*56-o-we-e', 'ti-da-o-we-e', 'ti-de-o-we-e', 'ti-ja-o-we-e', 'ti-te-o-we-e']}\n",
      "FORM: qe-to *203VAS NUM di-pa me-zo-e qe-to-ro-we *202VAS 1 di-pa-e me-zo-e ti-ri-o-we-e *202VAS 2 di-pa me-wi-jo qe-to-ro-we *202VAS 1 di-pa me-wi-jo ti-ri-jo-we *202VAS 1 di-pa me-wi-jo a-no-we *202VAS 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence is an inventory list detailing various types of di-pa (cups/goblets) by their size and number of handles. The pattern observed is di-pa [size] [handle_type] [quantity]. We have qe-to-ro-we (four-handled), ti-ri-jo-we (three-handled), and a-no-we (handle-less) cups listed with their respective quantities. The BRUTE_ME placeholder appears in the context of di-pa-e me-zo-e BRUTE_ME *202VAS 2, meaning 'larger cups, BRUTE_ME, 2 units'. Given that the quantity is '2', the most semantically and contextually plausible infilling for BRUTE_ME is an adjective meaning 'two-handled'.\\n\\nWhile the standard Mycenaean form for 'two-handled' would typically involve the di- or du-wo- syllabograms (e.g., di-wo-we), none of the provided options contain these. All options begin with ti-. However, among the given choices, ti-de-o-we-e is the most plausible candidate, assuming a less common or dialectal phonetic representation where ti-de might correspond to the prefix for 'two' (/-). Despite the phonetic mapping challenges (as ti typically maps to , ,  and de to , , , making a direct - mapping difficult), the strong contextual and numerical evidence points to 'two-handled' as the intended meaning. The suffix -o-we-e aligns with the pattern of other handle descriptors (-o-we). The final -e is consistent with the plural form di-pa-e and me-zo-e.\\n\\nTherefore, prioritizing semantic coherence, grammatical consistency within the established pattern of the inventory list, and contextual plausibility over a strict one-to-one phonetic mapping for this specific problematic segment, Option 3 provides the most logical completion of the sequence.\"}\n",
      "Correct: 9/17\n",
      "\n",
      "SEQ_IDX: 3466\n",
      "INPUT: {'word_list': ['to-pe-za', 'ra-e-ja', 'me-no-e-ja', 'e-re-pa-te', 'a-ja-me-na', 'qe-qi-no-to', 'au-de-pi', 'ko-ru-pi-qe', '1', 'e-ne-wo', 'pe-za', 'to-pe-za', 'ra-e-ja', 'a-pi-qo-to', 'e-re-pa-te-jo', 'BRUTE_ME', 'e-ka-ma-te-qe', 'qe-qi-no-to-qe', 'to-qi-de'], 'classifications': [['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -o'], [], [], [], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'athematic'], [], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['po-ke', 'po-ku', 'po-pi', 'po-po', 'po-ro']}\n",
      "FORM: to-pe-za ra-e-ja me-no-e-ja e-re-pa-te a-ja-me-na qe-qi-no-to au-de-pi ko-ru-pi-qe 1 e-ne-wo pe-za to-pe-za ra-e-ja a-pi-qo-to e-re-pa-te-jo po-pi e-ka-ma-te-qe qe-qi-no-to-qe to-qi-de\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes various types of tables and their features. The phrase in question is e-re-pa-te-jo BRUTE_ME e-ka-ma-te-qe qe-qi-no-to-qe to-qi-de. Based on the provided classifications:\\n    - e-re-pa-te-jo is classified as a common noun, thematic in -o, meaning 'ivory' or 'an ivory thing'.\\n    - e-ka-ma-te-qe is classified as a common adjective, athematic, with '-qe' meaning 'and'.\\n    - qe-qi-no-to-qe is classified as a common adjective, thematic in -o, with '-qe' meaning 'and'. Its Ancient Greek translation  means 'decorated'.\\n    - to-qi-de is classified as a common noun, thematic in -o, meaning 'keel/base'.\\n\\n    The structure suggests that e-re-pa-te-jo BRUTE_ME forms a noun phrase, followed by conjoined adjectives (e-ka-ma-te and qe-qi-no-to) modifying the subsequent noun to-qi-de.\\n\\n    Let's evaluate the options for BRUTE_ME:\\n    - Option 3: po-pi (): Classified as an anthroponym/animal name/theonym, noun, athematic. While it's a noun, its classification as a proper name or animal/theonym makes it semantically implausible to be in apposition to 'an ivory thing' in a description of table parts. The -pi suffix indicates instrumental/locative, which would make it function adverbially ('by means of po' or 'at po'), disrupting the noun phrase structure.\\n    - Option 5: po-ro (): Classified as a common noun, thematic in -o. The Ancient Greek translation  means 'callus, passage, means'. This is highly plausible in the context of describing a table's materials or features. e-re-pa-te-jo po-ro could be interpreted as 'an ivory thing, a callus/passage' or 'an ivory knob/protuberance/part'. This fits perfectly as a descriptive noun in apposition to e-re-pa-te-jo.\\n\\n    The other options (po-ke, po-ku, po-po) lack classification and Ancient Greek translations, making them impossible to evaluate against the criteria.\\n\\n    Therefore, po-ro is the most semantically coherent and grammatically consistent choice, forming a plausible description of a feature of an ivory table within an inventory-like text.\"}\n",
      "Correct: 9/18\n",
      "\n",
      "SEQ_IDX: 3472\n",
      "INPUT: {'word_list': ['o-wi-de', 'phu-ke-qi-ri', 'o-te', 'BRUTE_ME', 'te-ke', 'au-ke-wa', 'da-mo-ko-ro', 'qe-ra-na', 'wa-na-se-wi-ja', 'qo-u-ka-ra', 'ko-ki-re-ja', '*204VAS', '1', 'qe-ra-na', 'a-mo-te-wi-ja', 'ko-ro-no-we-sa', 'qe-ra-na', 'wa-na-se-wi-ja', 'ku-na-ja', 'qo-u-ka-ra', '1', 'to-qi-de-we-sa', '*204VAS', '1'], 'classifications': [['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], ['common', 'adverb', 'athematic'], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -a'], [], []], 'brute_classifications': [[], [], [], ['toponym', 'noun', 'thematic in -o'], []], 'brute': ['wa-na-de', 'wa-na-jo', 'wa-na-ko', 'wa-na-so', 'wa-na-to']}\n",
      "FORM: o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro qe-ra-na wa-na-se-wi-ja qo-u-ka-ra ko-ki-re-ja *204VAS 1 qe-ra-na a-mo-te-wi-ja ko-ro-no-we-sa qe-ra-na wa-na-se-wi-ja ku-na-ja qo-u-ka-ra 1 to-qi-de-we-sa *204VAS 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence describes an event where 'Phugeqiris saw when BRUTE_ME placed Augeuas, the damokoros', followed by a detailed inventory of vessels. The missing word (BRUTE_ME) must function as the subject of the verb 'te-ke' (placed). Among the given options, 'wa-na-ko' is the most plausible candidate. While 'wa-na-ka' is the more common nominative form for 'wanax' (king) in Linear B, 'wa-na-ko' is a known variant or case form (e.g., genitive). However, in the context of an administrative record where an official ('damokoros') is being 'placed' or appointed, the 'wanax' (king) is the most logical and contextually appropriate agent. Options 1, 2, and 5 ('wa-na-de', 'wa-na-jo', 'wa-na-to') do not readily correspond to a suitable noun or agent in Mycenaean Greek that could perform the action of 'placing' an official. Option 4, 'wa-na-so', is classified as a toponym (place name), which cannot grammatically or semantically be the subject of 'te-ke'. Therefore, 'wa-na-ko' as the 'king' provides the most semantically coherent, grammatically plausible, and contextually authentic interpretation for this administrative record.\"}\n",
      "Correct: 9/19\n",
      "\n",
      "SEQ_IDX: 3476\n",
      "INPUT: {'word_list': ['pa-sa-ro', 'BRUTE_ME', 'a-pi', 'to-ni-jo', '2', 'wa-o', '*232', '2', 'qi-si-pe-e', '*234', '2'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'athematic'], [], [], [], [], [], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ku-ru-e', 'ku-ru-no', 'ku-ru-ta', 'ku-ru-te', 'ku-ru-wa']}\n",
      "FORM: pa-sa-ro ku-ru-so a-pi to-ni-jo 2 wa-o *232 2 qi-si-pe-e *234 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence appears to be an inventory list, a common type of text found in Mycenaean archives. The initial word pa-sa-ro is translated as  (psalion), meaning 'bridle' or 'bit'. Although pa-sa-ro is classified as an 'anthroponym/animal name/theonym', the provided Ancient Greek translation  strongly suggests it functions as a common noun referring to an item. In Linear B inventory contexts, it is common to list an item followed by the name of its owner or a descriptor.\\n\\nAmong the given options for BRUTE_ME, ku-ru-no is the most plausible. It can be transliterated as  (Krounos), a known personal name in Ancient Greek. The construction   (bridle Krounos) is a common Mycenaean way to indicate possession, where the owner's name (Krounos) is in the nominative case, implying 'Krounos's bridle'. This fits perfectly within the administrative and inventory nature of Linear B texts.\\n\\nGrammatically, pa-sa-ro is classified as 'thematic in -o'. If ku-ru-no is also interpreted as a thematic -o noun (a common declension for personal names), it maintains grammatical consistency with the preceding word, even though its specific classification is not provided. The other options (ku-ru-e, ku-ru-ta, ku-ru-te, ku-ru-wa) are less readily identifiable as plausible personal names or common nouns that would fit this specific inventory context and grammatical pattern. For instance, ku-ru-e could be a dative form, but it would imply 'for Krounos', which is also plausible but less direct for indicating ownership in such lists compared to the nominative form. However, the dative of a thematic -o noun is typically -o, not -e, suggesting ku-ru-e would need to be an athematic noun, which introduces more assumptions.\\n\\nTherefore, ku-ru-no provides the best semantic coherence, grammatical consistency (as a thematic -o noun in apposition or implying possession), and contextual plausibility for an inventory record.\"}\n",
      "Correct: 9/20\n",
      "\n",
      "SEQ_IDX: 3477\n",
      "INPUT: {'word_list': ['ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'BRUTE_ME', 'ka-ru-we-qe', '*220', '1', 'ta-ra-nu-we', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', 'to-qi-de-qe', '*220', 'NUM', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', '*220', '1', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', '*220', '1', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', '*220', '1'], 'classifications': [['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'athematic'], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-qi-de-qe', 'i-qi-de-qe', 'o-qi-de-qe', 'ro-qi-de-qe', 'wo-qi-de-qe']}\n",
      "FORM: ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi to-qi-de-qe ka-ru-we-qe *220 1 ta-ra-nu-we a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe to-qi-de-qe *220 NUM ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe *220 1 ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe *220 1 ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi *220 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes 'thrones (ta-ra-nu) inlaid (a-ja-me-no) with ivory (e-re-pa-te-jo) and other materials'. The placeholder BRUTE_ME is followed by 'ka-ru-we-qe' (, 'nut/kernel' + 'and'), indicating that BRUTE_ME should also be a material or decorative element, likely in the dative/instrumental case, similar to 'au-de-pi' (, 'with ornaments/handles'). The subsequent entries in the text list other inlay materials like 'so-we-no-qe' (, 'wedges/pins') and 'to-qi-de-qe' (, 'small panels/inlays').\\n\\nAmong the given options, 'a-qi-de-qe' and 'ro-qi-de-qe' are the most semantically plausible.\\n- 'a-qi-de-qe' could correspond to Ancient Greek '' (akides), meaning 'points, splinters, needles'.\\n- 'ro-qi-de-qe' could correspond to Ancient Greek '' (rokides), meaning 'splinters, chips, fragments'.\\n\\nBoth '' and '' are plausible as small decorative elements or inlay materials for furniture. However, '' (chips/fragments) is slightly more specific and fitting for the concept of 'inlay' which typically involves small pieces of material. Both terms are attested in Linear B in contexts related to furniture.\\n\\nWhile the exact dative plural ending (-si) is not explicitly present in 'qi-de' for athematic nouns, Linear B often simplifies or omits final consonants, and the context strongly implies a list of materials used for inlay, which would be in the dative/instrumental case. The classification of 'ro-qi-de-qe' as an athematic noun is consistent with ''.\\n\\nTherefore, 'ro-qi-de-qe' () provides the most coherent semantic fit within the context of describing a throne inlaid with various materials, aligning with the administrative nature of Linear B texts.\"}\n",
      "Correct: 9/21\n",
      "\n",
      "SEQ_IDX: 3478\n",
      "INPUT: {'word_list': ['ta-ra-nu', 'BRUTE_ME', 'e-re-pa-te-jo', 'a-to-ro-qo', 'i-qo-qe', 'po-ru-po-de-qe', 'po-ni-ke-qe', '*220', '1', 'ta-ra-nu'], 'classifications': [['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], ['common', 'noun', 'athematic'], [], [], ['common', 'noun', 'athematic']], 'brute_classifications': [['common', 'adjective', 'thematic in -o'], [], [], [], []], 'brute': ['a-ja-me-no', 'a-ke-me-no', 'a-nu-me-no', 'a-re-me-no', 'a-u-me-no']}\n",
      "FORM: ta-ra-nu a-ja-me-no e-re-pa-te-jo a-to-ro-qo i-qo-qe po-ru-po-de-qe po-ni-ke-qe *220 1 ta-ra-nu\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence describes a 'ta-ra-nu' (footstool/throne) that is 'e-re-pa-te-jo' (ivory) and decorated with 'a-to-ro-qo' (human), 'i-qo-qe' (horse-and), 'po-ru-po-de-qe' (octopus/many-footed-and), and 'po-ni-ke-qe' (phoenix/palm-tree-and) figures. The unknown word 'BRUTE_ME' is classified as a common adjective, thematic in -o, suggesting it describes the footstool's condition or state.\\n\\n1.  Elimination of Options:\\nOption 1: a-ja-me-no ( - lamented, grieved). This meaning is semantically incoherent for describing a footstool.\\nOption 4: a-re-me-no (likely related to  - to please, satisfied). This meaning is semantically incoherent for a footstool.\\nOption 5: a-u-me-no (likely related to  - to increase, grown). This meaning is semantically incoherent for a footstool.\\n\\n2.  Comparison of Plausible Options: This leaves Option 2 (a-ke-me-no) and Option 3 (a-nu-me-no). Both are grammatically consistent as perfect middle/passive participles (ending in -me-no) and fit the thematic -o adjective classification.\\nOption 2: a-ke-me-no (). This participle can derive from two main verbs:\\n (to break, shatter), yielding 'broken'.\\n (to lead, bring), yielding 'led' or 'brought'.\\nOption 3: a-nu-me-no (). This participle derives from  (to accomplish, finish, complete), yielding 'finished' or 'completed'.\\n\\n3.  Contextual and Semantic Analysis:\\nLinear B texts are primarily administrative records, often detailing inventories of goods, their materials, and their condition.\\nThe item described is a valuable 'ivory footstool' with intricate 'human, horse, octopus, and phoenix' decorations.\\nNoting the condition of such a valuable item is crucial for inventory management.\\nBoth 'broken' and 'finished' are plausible descriptors for an item in an inventory.\\nHowever, a-ke-me-no () meaning 'broken' is a very common and significant descriptor in Linear B for damaged items (e.g., chariots, furniture). Explicitly stating that a valuable item is 'broken' provides critical information about its usability, value, and potential need for repair or replacement. While 'finished' is also relevant, 'broken' denotes a specific state of defect or damage that is highly pertinent to administrative records.\\n\\nTherefore, a-ke-me-no (broken) provides the most specific and contextually relevant information for an inventory record of a valuable, decorated ivory footstool, making it the most semantically coherent and plausible choice.\"}\n",
      "Correct: 9/22\n",
      "\n",
      "SEQ_IDX: 3481\n",
      "INPUT: {'word_list': ['po-ro-wi-to-jo', 'i-je-to-qe', 'pa-ki-ja-si', 'do-ra-qe', 'pe-re', 'po-re-na-qe', 'pu-ro', 'BRUTE_ME', 'po-ti-ni-ja', 'AUR', '*215VAS', '1', 'MUL', '1', 'ma-na-sa', 'AUR', '*213VAS', '1', 'MUL', '1', 'po-si-da-e-ja', 'ARG', '*213VAS', '1', 'MUL', '1', 'ti-ri-se-ro-e', 'AUR', '*216VAS', '1', 'do-po-ta', 'ARG', '*215VAS', '1', 'pu-ro'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'verb', 'athematic'], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], []], 'brute': ['a-me', 'a-ra', 'a-re', 'a-sa', 'a-ta']}\n",
      "FORM: po-ro-wi-to-jo i-je-to-qe pa-ki-ja-si do-ra-qe pe-re po-re-na-qe pu-ro a-ke po-ti-ni-ja AUR *215VAS 1 MUL 1 ma-na-sa AUR *213VAS 1 MUL 1 po-si-da-e-ja ARG *213VAS 1 MUL 1 ti-ri-se-ro-e AUR *216VAS 1 do-po-ta ARG *215VAS 1 pu-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence describes a list of offerings to various deities and figures, including Potnia, Manasa, Poseidaia, Trisheros, and Despota. The placeholder BRUTE_ME appears within this list, specifically after 'pu-ro' (Pylos or Puros) and before 'po-ti-ni-ja' (Potnia). This strongly suggests that the missing word is another recipient of offerings, likely a deity or a significant figure. Option 3, 'a-re', is classified as an 'anthroponym/animal name/theonym' and maps directly to the Ancient Greek '' (Ares), a prominent Greek deity. Including Ares in a list of divine recipients is highly semantically coherent and contextually plausible for a Mycenaean administrative record of religious offerings. The grammatical classification as an athematic noun also fits perfectly within such a list. The other options lack clear Ancient Greek equivalents that fit the context or are not classified, making them less suitable.\"}\n",
      "Correct: 9/23\n",
      "\n",
      "SEQ_IDX: 3483\n",
      "INPUT: {'word_list': ['ko-ma-we-te-ja', 'i-je-to-qe', 'pe-re-*82-jo', 'i-pe-me-de-ja-qe', 'di-u-ja-jo-qe', 'do-ra-qe', 'pe-re-po-re-na-qe', 'a', 'pe-re-*82', 'AUR', '*213VAS', '1', 'MUL', '1', 'i-pe-me-de-ja', 'ARG', '*213VAS', '1', 'di-u-ja', 'AUR', '*213VAS', '1', 'MUL', '1', 'pu-ro', 'e-ma-ha', 'a-re-ja', 'AUR', '*216VAS', '1', 'VIR', '1', 'i-je-to-qe', 'di-u-jo', 'do-ra-qe', 'BRUTE_ME', 'po-re-na-qe', 'a-ke', 'di-we', 'ARG', '*213VAS', '1', 'VIR', '1', 'e-ra', 'ARG', '*213VAS', '1', 'MUL', '1', 'di-ri-mi-jo', 'di-wo', 'i-je-we', 'ARG', '*213VAS', '1', 'pu-ro', 'pu-ro'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'verb', 'athematic'], ['toponym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], ['toponym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], [], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['e-re', 'o-re', 'pte-re', 'te-re']}\n",
      "FORM: ko-ma-we-te-ja i-je-to-qe pe-re-*82-jo i-pe-me-de-ja-qe di-u-ja-jo-qe do-ra-qe pe-re-po-re-na-qe a pe-re-*82 AUR *213VAS 1 MUL 1 i-pe-me-de-ja ARG *213VAS 1 di-u-ja AUR *213VAS 1 MUL 1 pu-ro e-ma-ha a-re-ja AUR *216VAS 1 VIR 1 i-je-to-qe di-u-jo do-ra-qe pe-re po-re-na-qe a-ke di-we ARG *213VAS 1 VIR 1 e-ra ARG *213VAS 1 MUL 1 di-ri-mi-jo di-wo i-je-we ARG *213VAS 1 pu-ro pu-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 4'}\n",
      "Correct: 9/24\n",
      "\n",
      "SEQ_IDX: 3490\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'e-te-wa-jo', 'O'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-si-ja-du-ja', 'a-si-ja-ni-ja', 'a-si-ja-qi-ja', 'a-si-ja-si-ja', 'a-si-ja-wi-ja']}\n",
      "FORM: a-si-ja-ti-ja e-te-wa-jo O\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence 'BRUTE_ME e-te-wa-jo O' likely represents an administrative record, a common type of Linear B text. 'e-te-wa-jo' (Etewajo) is a known masculine personal name in Mycenaean Greek, and 'O' is a logogram commonly interpreted as 'oil' (). The most semantically and contextually plausible interpretation of the sequence is 'Place, Person, Commodity' or 'Commodity from Place for Person'. All options for BRUTE_ME begin with 'a-si-ja-', strongly suggesting a connection to 'Asia' (). Among the given options, 'a-si-ja-ni-ja' () is the most linguistically authentic and plausible form for a place name or a feminine ethnonym derived from 'Asia' in Mycenaean Greek. This fits the common administrative pattern of specifying the origin of goods or the association of a person with a place. For example, 'Asiania, Etewajo, Oil' would mean 'Oil from Asiania for Etewajo' or 'Etewajo from Asiania's oil'. The other options are less common or less plausible as place names or ethnonyms in Mycenaean Greek (e.g., 'a-si-ja-si-ja' with its reduplication is less likely for a proper noun).\"}\n",
      "Correct: 9/25\n",
      "\n",
      "SEQ_IDX: 3498\n",
      "INPUT: {'word_list': ['au-ke-i-ja-te-we', 'o-pi-de-so-mo', 'ka-tu-ryo', 'di-pte-ra', 'NUM', 'ka-ne-ja', 'wo-ro-ma-ta', 'NUM', 'me-ti-ja-no', 'to-pa', 'ru-de-ha', 'di-pte-ra', '1', 'a-re-se-si', 'e-ru-ta-ra', 'di-pte-ra', 'NUM', 'BRUTE_ME', 'pe-di-ra', '2', 'we-e-wi-ja', 'di-pte-ra', 'NUM', 'wi-ri-no', 'we-ru-ma-ta', 'ti-ri-si', 'ze-u-ke-si', '1', 'wi-ri-no', 'pe-di-ro', 'e-ma-ta', 'NUM', 'e-ra-pe-ja', 'e-pi-u-ru-te-we', 'E', '2', 'a-pe-i-ja', 'u-po', 'ka-ro', 'we-e-wi-ja', '1', 'u-po', 'we-e-wi-ja', 'e-ra-pe-ja', 'E', '1', 'mu-te-we', 'we-re-ne-ja', 'ku'], 'classifications': [[], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], [], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -o'], [], ['common', 'noun', 'athematic'], ['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -a'], [], [], [], [], ['common', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], [], [], ['common', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['wo-di-je-ja', 'wo-di-je-re', 'wo-di-je-ro', 'wo-di-je-u', 'wo-di-je-we']}\n",
      "FORM: au-ke-i-ja-te-we o-pi-de-so-mo ka-tu-ryo di-pte-ra NUM ka-ne-ja wo-ro-ma-ta NUM me-ti-ja-no to-pa ru-de-ha di-pte-ra 1 a-re-se-si e-ru-ta-ra di-pte-ra NUM wo-di-je-ja pe-di-ra 2 we-e-wi-ja di-pte-ra NUM wi-ri-no we-ru-ma-ta ti-ri-si ze-u-ke-si 1 wi-ri-no pe-di-ro e-ma-ta NUM e-ra-pe-ja e-pi-u-ru-te-we E 2 a-pe-i-ja u-po ka-ro we-e-wi-ja 1 u-po we-e-wi-ja e-ra-pe-ja E 1 mu-te-we we-re-ne-ja ku\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an inventory of items, specifically mentioning 'di-pte-ra' (hides/leather) and 'pe-di-ra' (sandals). The phrase 'e-ru-ta-ra di-pte-ra NUM BRUTE_ME pe-di-ra 2' suggests that 'BRUTE_ME' is an adjective or noun describing the 'pe-di-ra' (sandals). The word 'pe-di-ra' is a neuter plural noun (thematic in -a). Among the given options, 'wo-di-je-ja' (Option 1) ends in '-ja', which can correspond to the Ancient Greek suffix '-' or '-'. This suffix is commonly used to form neuter plural adjectives (e.g., ), which would grammatically agree with 'pe-di-ra'. This interpretation ('sandal-like' or 'for sandals') makes the most semantic sense in the context of an inventory listing types of sandals. The other options (wo-di-je-re, wo-di-je-ro, wo-di-je-u, wo-di-je-we) do not form grammatically consistent adjectives or nouns that would agree with 'pe-di-ra' in number and gender, or fit the descriptive context as well.\"}\n",
      "Correct: 10/26\n",
      "\n",
      "SEQ_IDX: 3499\n",
      "INPUT: {'word_list': ['1', 'BRUTE_ME', 'di-pte-ra', 'ai-za', 'pe-di-ro-i', '1'], 'classifications': [[], [], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['mu-te-jo', 'mu-te-o', 'mu-te-pi', 'mu-te-re', 'mu-te-wo']}\n",
      "FORM: 1 mu-te-we di-pte-ra ai-za pe-di-ro-i 1\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes an inventory item: '1 BRUTE_ME leather goat sandals 1'. The known words are di-pte-ra (leather/hide, feminine singular), ai-za (goat, feminine singular), and pe-di-ro-i (sandals, dative plural). The numbers '1' at the beginning and end indicate a count.\\n\\nOption 5, mu-te-wo, is classified as a 'common noun' and 'thematic in -o'. The suffix -wo in Linear B frequently represents the genitive singular ending for thematic nouns (e.g., corresponding to Ancient Greek - or -, often with a preserved digamma). If mu-te-wo is a genitive singular, it would translate to 'of Mutewos' (a person, place, or specific type/material).\\n\\nThis interpretation provides the most semantically coherent and grammatically consistent meaning: '1 (pair of) sandals of Mutewos (person/place/material) made of goat leather'. This structure is highly plausible for an administrative or inventory record in Mycenaean Greek, indicating ownership, origin, or a specific type of item.\\n\\nLet's briefly consider why other options are less suitable:\\n- Option 1 (mu-te-jo): While jo can form nouns or adjectives, its use as a genitive is less common than -wo in this context, and if it's a neuter nominative/accusative, it's less specific in meaning.\\n- Option 2 (mu-te-o): Could be nominative/accusative singular or genitive plural. If nominative/accusative, it's less clear how it modifies the phrase. If genitive plural, 'of Mutewos (plural)', it's possible but less likely than a singular possessor/origin for a single item.\\n- Option 3 (mu-te-pi): The suffix -pi indicates instrumental or locative case. Placing an instrumental/locative noun directly after the count '1' and before the descriptive nouns (di-pte-ra ai-za) makes the grammatical structure awkward and semantically less plausible for an inventory entry.\\n- Option 4 (mu-te-re): Classified as an 'anthroponym/animal name/theonym' and 'athematic noun'. While it could indicate ownership, the genitive case (as in Option 5) is a more direct and common way to express possession in Linear B inventory contexts. The ending -re for an athematic nominative singular anthroponym is less typical for indicating ownership compared to a genitive.\\n\\nTherefore, mu-te-wo as a genitive singular noun best fits the context of an inventory record, providing a clear and plausible relationship (e.g., ownership or origin) for the described item.\"}\n",
      "Correct: 10/27\n",
      "\n",
      "SEQ_IDX: 3503\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'TELA1+TE'], 'classifications': [[], []], 'brute_classifications': [[], [], ['common', 'noun', 'thematic in -o'], [], []], 'brute': ['ka-di-wi-po-ro', 'ka-je-wi-po-ro', 'ka-ra-wi-po-ro', 'ka-si-wi-po-ro', 'ka-te-wi-po-ro']}\n",
      "FORM: ka-ra-wi-po-ro TELA1+TE\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence is 'BRUTE_ME TELA1+TE', where TELA1+TE translates to '' (fabric/cloth). We need to find the most semantically and grammatically coherent infilling for BRUTE_ME.\\n\\nLet's analyze the options by transliterating them using the provided mapping, paying close attention to the rule for double consonants (, ):\\n\\n1.  Option 1: ka-di-wi-po-ro\\nTransliteration: f or f. This does not correspond to a readily recognizable Ancient Greek word.\\n\\n2.  Option 2: ka-je-wi-po-ro\\nTransliteration: f or f. This also does not correspond to a readily recognizable Ancient Greek word.\\n\\n3.  Option 3: ka-ra-wi-po-ro\\nProvided Ancient Greek: f||f. The most common and plausible interpretation is  (kleidophoros), meaning 'key-bearer'. If this is the case, 'key-bearer fabric' ( ) is semantically possible (e.g., fabric for a key-bearer's garment), but perhaps less direct than other possibilities. The classification provided for this option is 'common noun, thematic in -o', which fits .\\n\\n4.  Option 4: ka-si-wi-po-ro\\nTransliteration: Applying the rule for double consonants, ka-si ( + ) strongly suggests  (xi). Thus, ka-si-wi-po-ro transliterates to f (xiphophoros), meaning 'sword-bearer'. This is a well-attested Ancient Greek word.\\nSemantic Coherence: 'Sword-bearer fabric' (f ) makes excellent semantic sense in the context of Mycenaean administrative records, which frequently list items related to military personnel, equipment, or specific roles. This could refer to fabric for a sword-bearer's uniform or a fabric with a sword motif.\\nGrammatical Consistency: f is a common noun, thematic in -o, which aligns with the grammatical patterns expected for such a word.\\n\\n5.  Option 5: ka-te-wi-po-ro\\nTransliteration: f or f. This does not correspond to a readily recognizable Ancient Greek word.\\n\\nConclusion:\\nOption 4, ka-si-wi-po-ro, leads to the Ancient Greek word f (sword-bearer), which forms a highly coherent and plausible phrase (f  - sword-bearer fabric) within the typical administrative and military context of Linear B tablets. The transliteration is robust, adhering to the specific rule for double consonants. While Option 3 provides a plausible word (), its semantic connection to 'fabric' is less direct and compelling than 'sword-bearer'. The instruction to be critical of autoregressive model translations and rely on the mapping reinforces the choice of Option 4, as its derivation is clear and the resulting word is highly relevant.\"}\n",
      "Correct: 10/28\n",
      "\n",
      "SEQ_IDX: 3508\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'O', '1', 'pa-de-we', 'O', '1', 'ka-ru-ke', 'PE', '2', 'KA', '1', 'O', 'NUM', 'te-qi-jo-ne', 'O', '1', 'a-ke-ti-ri-ja-i', 'KA', '1', 'a-ti-mi-te', 'O', '1', 'da-ko-ro-i', 'E', '1', 'di-pte-ra-po-ro', 'RA', '1', 'O', 'NUM', 'ko-ro'], 'classifications': [[], [], [], [], [], [], ['common', 'noun', 'athematic'], [], [], [], [], [], [], [], [], [], ['common', 'noun', 'thematic in -a'], [], [], ['toponym', 'noun', 'athematic'], [], [], [], [], [], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['da-de-we', 'di-de-we', 'me-de-we', 'ta-de-we']}\n",
      "FORM: pa-de-we O 1 pa-de-we O 1 ka-ru-ke PE 2 KA 1 O NUM te-qi-jo-ne O 1 a-ke-ti-ri-ja-i KA 1 a-ti-mi-te O 1 da-ko-ro-i E 1 di-pte-ra-po-ro RA 1 O NUM ko-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence is an administrative list, typical of Mycenaean texts, enumerating various individuals or items with associated quantities. The placeholder BRUTE_ME appears at the beginning of this list, followed by a logogram and a number (O 1). This structure suggests that BRUTE_ME is likely a noun, specifically an anthroponym (personal name) or a common noun representing an item.\\n\\nAll provided options (da-de-we, di-de-we, me-de-we, ta-de-we) share the common ending -de-we. In Mycenaean Greek, the suffix -we is attested in personal names (e.g., e-we-de-we, a-ka-sa-no-we), often indicating a masculine nominative singular form. This makes all options grammatically plausible as names.\\n\\nHowever, when considering linguistic authenticity and plausibility, 'me-de-we' stands out. The root 'med-' (from which names like Medon, , derive) is well-established in Ancient Greek, meaning 'to rule' or 'to protect'. A name like Medon would be a very plausible anthroponym to appear in an administrative record. While the other options could also be names, 'me-de-we' has a stronger etymological connection to known Greek naming conventions, making it the most contextually and linguistically authentic choice for a personal name in this type of document. The sequence then reads as a list starting with 'Medon' (or a similar name) followed by other entries like 'heralds', 'female practitioners', 'Artemis' (or a place name), and 'leather-carriers', all consistent with an inventory.\"}\n",
      "Correct: 10/29\n",
      "\n",
      "SEQ_IDX: 3512\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'MA', '1', 'KO', '1', 'ME'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], []], 'brute': ['ai-wa-ke-ta', 'jo-wa-ke-ta', 'ko-wa-ke-ta', 'ra-wa-ke-ta', 'wo-wa-ke-ta']}\n",
      "FORM: ra-wa-ke-ta MA 1 KO 1 ME\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence `BRUTE_ME MA 1 KO 1 ME` strongly suggests an administrative or inventory record, common in Mycenaean texts. The placeholder `BRUTE_ME` is followed by what appear to be logograms or abbreviations (`MA`, `KO`, `ME`) and numbers (`1`). Among the given options, `ra-wa-ke-ta` is the most plausible infilling because it directly corresponds to the well-attested Mycenaean title `lawagetas` (f), a high-ranking official or military leader. This title fits perfectly within the context of administrative records. The provided Ancient Greek translation `f|` for `ra-wa-ke-ta` further confirms this identification, as the mapping of `ra` to `` and `wa` to `f` (digamma) is consistent with the Linear B syllabary. The classification of `ra-wa-ke-ta` as an 'anthroponym/animal name/theonym' and 'noun' with 'thematic in -a' inflection aligns with the known grammatical behavior of `lawagetas`. The other options do not correspond to known Mycenaean titles or words that would fit as coherently into this administrative context.\"}\n",
      "Correct: 11/30\n",
      "\n",
      "SEQ_IDX: 3516\n",
      "INPUT: {'word_list': ['ka-pa-ti-ja', 'HORD', '1', 'BRUTE_ME', 'GRA', '1', 'LANA', 'NUM'], 'classifications': [['toponym', 'noun', 'thematic in -a'], [], [], [], [], [], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], []], 'brute': ['te-i-ja', 'te-mi-ja', 'te-se-ja', 'te-ti-ja', 'te-wi-ja']}\n",
      "FORM: ka-pa-ti-ja HORD 1 te-ri-ja GRA 1 LANA NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence appears to be an administrative record or inventory, listing commodities associated with 'ka-pa-ti-ja' (Karpathos or a related entity). The structure is 'ka-pa-ti-ja HORD 1 BRUTE_ME GRA 1 LANA NUM', which translates to 'Karpathos: Barley 1, [BRUTE_ME], Wheat 1, Wool NUM'. This suggests that the unknown word BRUTE_ME should either be another commodity or a descriptor/recipient related to the listed items.\\n\\nOption 1, te-i-ja, is translated as  (theia) in Ancient Greek. This word means 'divine', 'goddess', or 'aunt'. The classification provided for te-i-ja is 'anthroponym/animal name/theonym' and 'noun', 'thematic in -a'. This classification strongly supports the interpretation of te-i-ja as a proper noun, likely referring to a deity or a person.\\n\\nIn the context of Mycenaean administrative texts, it is common to find records of offerings to deities or allocations for specific individuals. Therefore, 'Karpathos: Barley 1, (for) Theia, Wheat 1, Wool NUM' makes perfect semantic and contextual sense. 'Theia' could be the recipient of the listed goods, or the goods could be designated 'for Theia' (e.g., an offering). Grammatically, if 'Theia' is a recipient, it would be in the dative case. A thematic -a noun in the dative singular ends in -a, which is consistent with te-i-ja.\\n\\nThe other options (ma-u-qe, o-u-qe, te-u-qe, to-u-qe) are not provided with clear Ancient Greek translations by the model (they are just transliterated), which, according to the critical instruction, suggests they are less likely to be the intended words compared to an option with a recognized Ancient Greek equivalent. This makes Option 1 the most linguistically authentic choice.\\n\\nTherefore, Option 1 provides the most coherent and plausible interpretation for the complete sequence, fitting the typical content and structure of Linear B administrative tablets.\"}\n",
      "Correct: 11/31\n",
      "\n",
      "SEQ_IDX: 3519\n",
      "INPUT: {'word_list': ['ta-we-si-jo-jo', 'BRUTE_ME', 'VIR', 'NUM'], 'classifications': [[], [], [], []], 'brute_classifications': [[], [], [], ['common', 'noun', 'thematic in -a'], []], 'brute': ['ke-ro-i-ja', 'ke-ro-ne-ja', 'ke-ro-pi-ja', 'ke-ro-si-ja', 'ke-ro-wi-ja']}\n",
      "FORM: ta-we-si-jo-jo ke-ro-si-ja VIR NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence `ta-we-si-jo-jo BRUTE_ME VIR NUM` strongly suggests an administrative record counting men. The term `ta-we-si-jo-jo` ends in the genitive singular suffix `-ojo`, indicating possession or association. Option 4, `ke-ro-si-ja`, is classified as a common noun, thematic in -a, and is translated as `|` (gerousia), meaning 'council of elders' or 'senate'. This fits perfectly within the administrative context of Linear B tablets, where records of personnel and groups were common. The phrase can be interpreted as 'X men of the gerousia of Ta-we-si-jo' or 'Ta-we-si-jo's gerousia: X men'. This interpretation is grammatically consistent, semantically coherent, and contextually plausible for Mycenaean Greek. The other options lack a clear Ancient Greek translation or a plausible semantic fit within this specific context of counting 'men'.\"}\n",
      "Correct: 12/32\n",
      "\n",
      "SEQ_IDX: 3531\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'pa-ma-ko', 'jo-qi'], 'classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['ai-wa-ra-ka-na-o', 'ko-wa-ra-ka-na-o', 'ro-wa-ra-ka-na-o', 'wa-wa-ra-ka-na-o']}\n",
      "FORM: a-wa-ra-ka-na-o pa-ma-ko jo-qi\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence is BRUTE_ME pa-ma-ko jo-qi.\\n    1.  Interpretation of pa-ma-ko and jo-qi: The prompt provides conflicting information for pa-ma-ko and jo-qi. Their classifications state they are 'anthroponym/animal name/theonym, noun, thematic in -o'. However, the provided Ancient Greek translations are  (pharmakon, 'drug/poison') for pa-ma-ko and | (hoti/hostis, 'that/which/whoever') for jo-qi. Crucially, the jo- prefix is explicitly listed as a 'CRITICAL' relative/comparative prefix (-). The prompt also warns that Ancient Greek correspondences are from an autoregressive model and may repeat characters, implying that the *meaning* or *function* derived from the Linear B and other critical information (like prefixes) should be prioritized over potentially misleading classifications or literal character-by-character translations. Therefore, I interpret pa-ma-ko as a common noun (pharmakon, likely neuter nominative/accusative singular) and jo-qi as a relative pronoun/conjunction.\\n\\n    2.  Grammatical Role of BRUTE_ME: Given pa-ma-ko as 'drug' (a noun), BRUTE_ME would most logically function as a possessive modifier (genitive case) or an adjective. The options for BRUTE_ME all end in -o. According to the provided declension table, an athematic noun in the singular genitive case ends in -o. This fits the structure of the BRUTE_ME options, suggesting they are proper nouns (names) in the genitive case.\\n\\n    3.  Semantic Coherence: The resulting structure [Name in Genitive] [Drug] [Relative Pronoun] (e.g., 'X's drug, which...') forms a semantically coherent phrase, highly plausible for an administrative record in Linear B, which often detail ownership or origin of goods.\\n\\n    4.  Linguistic Authenticity and Selection: All four options for BRUTE_ME (ai-wa-ra-ka-na-o, ko-wa-ra-ka-na-o, ro-wa-ra-ka-na-o, wa-wa-ra-ka-na-o) are plausible as Mycenaean names in the genitive case. Without further specific contextual clues or frequency data for Mycenaean names, distinguishing between them is challenging. However, ko- is a very common initial syllable in attested Mycenaean names (e.g., Ko-ma-we, Ko-to-no). While this is a subtle preference, it provides a slight edge in terms of linguistic authenticity compared to the other options, which are equally plausible but less demonstrably common in their initial syllable. Therefore, Option 2 (ko-wa-ra-ka-na-o) is chosen as the most fitting option.\"}\n",
      "Correct: 12/33\n",
      "\n",
      "SEQ_IDX: 3533\n",
      "INPUT: {'word_list': ['PA', 'NUM', 'BRUTE_ME', 'GRA', 'HA', '1', 'WO', '2', 're-u-ko-to', 'GRA', 'HA', '2', 'PA', '1', 'E', 'NUM', 'a-ro-ka', 'E', 'NUM'], 'classifications': [[], [], [], [], [], [], [], [], ['common', 'adjective', 'thematic in -o'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [['common', 'noun', 'athematic'], [], [], [], []], 'brute': ['e-ri-ka-we-e', 'pa-ri-ka-we-e', 'po-ri-ka-we-e', 'we-ri-ka-we-e', 'wi-ri-ka-we-e']}\n",
      "FORM: PA NUM e-ri-ka-we-e GRA HA 1 WO 2 re-u-ko-to GRA HA 2 PA 1 E NUM a-ro-ka E NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence appears to be an administrative record or inventory, listing quantities of various commodities, specifically 'GRA' (grain/wheat) and 're-u-ko-to GRA' (white grain). The unknown word 'BRUTE_ME' precedes the first instance of 'GRA', suggesting it functions as a descriptor or type of grain. Option 1, 'e-ri-ka-we-e', is the only candidate for which an Ancient Greek translation, '|' (helices, spirals, or a type of plant/tree), is provided. This translation, referring to a type of plant or a specific form/quality, makes semantic sense in the context of an inventory where different varieties or preparations of grain would be listed. For example, 'helical grain' or 'grain from a helical plant' is a plausible entry. The other options lack Ancient Greek translations, making it impossible to assess their semantic coherence or contextual plausibility. Therefore, 'e-ri-ka-we-e' is the only option that provides a meaningful and grammatically consistent fit within the overall inventory context.\"}\n",
      "Correct: 13/34\n",
      "\n",
      "SEQ_IDX: 3536\n",
      "INPUT: {'word_list': ['A', 'NUM', 'pa-ro', 'BRUTE_ME', 'A', '2', 'pa-ro', 'ru-na', 'A', '1'], 'classifications': [[], [], ['common', 'noun', 'thematic in -o'], [], [], [], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], []], 'brute': ['e-u-ka-jo', 'e-u-ka-mo', 'e-u-ka-ra', 'e-u-ka-ro', 'e-u-ka-ta']}\n",
      "FORM: A NUM pa-ro e-u-ka-no A 2 pa-ro ru-na A 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence 'A NUM pa-ro BRUTE_ME A 2 pa-ro ru-na A 1' strongly suggests an administrative or inventory record, a common type of Linear B text. The phrase 'pa-ro X' typically indicates 'from/by X', where X is often a person's name or a place. The presence of 'pa-ro ru-na' (likely 'from/by Runas', a personal name) further supports this interpretation. Among the given options, 'e-u-ka-ro' is explicitly classified as an 'anthroponym/animal name/theonym' and a 'noun' with 'thematic in -o' inflection. This classification perfectly aligns with the grammatical and semantic requirements for a personal name in this context. While the Ancient Greek correspondence '' provided for 'e-u-ka-ro' by the autoregressive model is problematic (as it's an adjective and doesn't map perfectly syllabically), the *classification* of 'e-u-ka-ro' as an anthroponym is the critical piece of information. A plausible Ancient Greek name for 'e-u-ka-ro' consistent with the Linear B mapping rules and thematic -o inflection is '' (Eukharos), meaning 'gracious' or 'charming'. This makes Option 4 the most semantically coherent, grammatically consistent, and contextually plausible choice for infilling the unknown word as a personal name in an administrative record.\"}\n",
      "Correct: 13/35\n",
      "\n",
      "SEQ_IDX: 3542\n",
      "INPUT: {'word_list': ['BRUTE_ME', '*146', 'GRA', 'NUM', 'we'], 'classifications': [[], [], [], [], []], 'brute_classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['re-ke-to', 're-po-to', 're-ra-to', 're-sa-to', 're-u-to']}\n",
      "FORM: re-po-to *146 GRA NUM we\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence involves the logogram 'GRA' (grain/wheat) followed by 'NUM' (number), indicating an administrative record of quantities of grain. Option 2, 're-po-to', translates to '' or '' in Ancient Greek, meaning 'thin', 'fine', or 'light'. When combined with '' (grain), ' ' (fine grain) makes perfect semantic sense in the context of an inventory or record distinguishing different qualities of grain. This is highly plausible for Linear B administrative texts. The classification of 're-po-to' as a 'common noun' (thematic in -o) is consistent with '' functioning as an adjective modifying 'grain' or as a substantivized adjective referring to 'fine grain'. The other options do not provide clear Ancient Greek translations or classifications, making them less semantically coherent and less likely to be linguistically authentic in this context.\"}\n",
      "Correct: 14/36\n",
      "\n",
      "SEQ_IDX: 3548\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'o'], 'classifications': [[], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['pu-de', 'pu-no', 'pu-ro', 'pu-so', 'pu-ta']}\n",
      "FORM: pu-ro o\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The input sequence BRUTE_ME o with the Ancient Greek hint BRUTE_ME  suggests that the word to be infilled, BRUTE_ME, should be a Linear B word whose Ancient Greek translation ends in '-'. The o in the Linear B sequence and  in the Ancient Greek hint likely refer to the final syllabogram and its corresponding Ancient Greek ending, respectively.\\n\\nLet's evaluate the options based on this:\\n1.  Option 1: pu-de\\nLinear B: pu-de. Ancient Greek: pu-de. This does not end in '-'.\\n2.  Option 2: pu-no\\nLinear B: pu-no. Ancient Greek: pu-no. If this corresponds to  (a type of wood/tree), it ends in '-'. No classification is provided for pu-no.\\n3.  Option 3: pu-ro\\nLinear B: pu-ro. Ancient Greek: |. Both  (Pylos) and  (wheat/fire) end in '-'.\\nClassification for pu-ro: 'anthroponym/animal name/theonym', 'noun', 'thematic in -o'. This classification perfectly matches  (Pylos), which is a proper noun, masculine, and thematic in -o (e.g., Nominative Singular ). Pylos was a major Mycenaean center, making this word highly contextually plausible in Linear B texts.\\n4.  Option 4: pu-so\\nLinear B: pu-so. Ancient Greek: pu-so. If this corresponds to  (a type of plant), it ends in '-'. No classification is provided for pu-so.\\n5.  Option 5: pu-ta\\nLinear B: pu-ta. Ancient Greek: |. Neither  (plant, neuter, ends in -) nor  (plants, neuter plural, ends in -) ends in '-'.\\nClassification for pu-ta: 'common', 'noun', 'thematic in -o'. This is contradictory, as pu-ta ends in 'a', suggesting a thematic in -a declension, not -o.\\n\\nComparing the plausible options (2, 3, and 4), Option 3 (pu-ro) is the strongest. It has a clear and highly relevant Ancient Greek translation ( or ) that fits the '-' ending. Furthermore, the provided classification for pu-ro (thematic in -o) is perfectly consistent with  (Pylos), a common and significant noun in Mycenaean contexts. The other options lack specific Ancient Greek translations or consistent classifications, or are less contextually prominent.\"}\n",
      "Correct: 15/37\n",
      "\n",
      "SEQ_IDX: 3557\n",
      "INPUT: {'word_list': ['e-ke-i-ja', 'NUM', 'pe-di-je-wi-ja', 'NUM', 'BRUTE_ME', '2'], 'classifications': [['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], []], 'brute_classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], []], 'brute': ['a-ka-so-ne', 'a-ko-so-ne', 'a-na-so-ne', 'a-ri-so-ne', 'a-wi-so-ne']}\n",
      "FORM: e-ke-i-ja NUM pe-di-je-wi-ja NUM a-ko-so-ne 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': 'The Linear B sequence appears to be an inventory list, typical of Mycenaean texts. The known words e-ke-i-ja (spear) and pe-di-je-wi-ja (foot-strap) are common items, followed by a numerical quantity. Therefore, the missing word BRUTE_ME should logically be another countable item.\\n\\nOption 2, a-ko-so-ne, translates to  (axones), meaning \\'axles\\'. This translation is highly plausible for several reasons:\\n1.  Linguistic Authenticity: The conversion from a-ko-so-ne to  perfectly matches the critical rule provided: \\'Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> \\'. Here, ko-so forms  (ks), leading to . This is a strong indicator of its correctness.\\n2.  Semantic Coherence: \\'Axles\\' are countable items that fit naturally within an inventory alongside \\'spears\\' and \\'foot-straps\\'. The quantity \\'2\\' for axles is also very common, as axles typically come in pairs for vehicles.\\n3.  Contextual Plausibility: Linear B tablets are predominantly administrative records, including inventories of goods and equipment. Listing items like spears, foot-straps, and axles with their quantities is entirely consistent with this historical context.\\n4.  Grammatical Consistency: While the provided classification for a-ko-so-ne lists it as an \\'anthroponym/animal name/theonym\\', the Ancient Greek translation  is clearly a common noun (plural of ). Given the instruction to \\'TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD\\' when the autoregressive model might repeat characters or provide potentially misleading classifications, the strong linguistic derivation and semantic fit of  as a common noun override the classification discrepancy. It functions grammatically as a plural noun, which is consistent with being counted.\\n\\nThe other options (a-ka-so-ne, a-na-so-ne, a-ri-so-ne, a-wi-so-ne) do not provide clear Ancient Greek translations that fit the context as well, nor do they demonstrate the specific linguistic transformation rule for  as clearly as a-ko-so-ne does.'}\n",
      "Correct: 16/38\n",
      "\n",
      "SEQ_IDX: 3567\n",
      "INPUT: {'word_list': ['1', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [['common', 'noun', 'thematic in -o'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['ne-o', 'no-o', 'nu-o', 'ta-o', 'te-o']}\n",
      "FORM: 1 wa-o\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The sequence '1 BRUTE_ME' indicates a numeral followed by a noun, likely an item or entity being counted or listed. Option 5, 'te-o', translates to Ancient Greek '' (god), '' (god, acc.), or '' (to gods, dat. pl.). In Linear B tablets, 'te-o' (god) is a very common term, frequently appearing in administrative records related to offerings or allocations to deities. The presence of the numeral '1' before 'te-o' strongly suggests 'one god' or 'one (item) for a god', which is highly consistent with the content and purpose of Mycenaean Linear B texts.\\n\\nIn contrast, Option 1, 'ne-o', translates to '' (temple/ship, dative singular). While '1 temple' or '1 ship' could be plausible in an inventory, the dative case '' ('to a temple/ship') makes it grammatically less direct for a simple count following '1' compared to the nominative/accusative forms of ''/''. Options 2, 3, and 4 lack Ancient Greek translations and classifications, making them impossible to evaluate for semantic and grammatical consistency. Therefore, 'te-o' provides the most semantically coherent, grammatically consistent, and contextually plausible interpretation for this Linear B sequence.\"}\n",
      "Correct: 16/39\n",
      "\n",
      "SEQ_IDX: 3577\n",
      "INPUT: {'word_list': ['o-ze-to', 'ke-sa-do-ro', '*34-to-pi', 'pa-ro', 'a-ke-ha', 'me-ta-pa', 'pe-ri-te', '1', 'a-pi-no-e-wi-jo', 'pa-ro', 'e-ru-si-jo', '1', 'a-pi-no-e-wi-jo', 'pa-ro', 'ai-ki-e-we', 'NUM', 'e-na-po-ro', 'pa-ro', 'wa-do-me-no', 'NUM', 'sa-ri-no-te', 'pa-ro', 'o-wo-to', 'NUM', 'pa-ki-ja-si', 'pa-ro', 'a-ta-no-re', 'NUM', 'ka-ra-do-ro', 'pa-ro', 'to-ro-wo', '1', 'pa-ki-ja-si', 'pa-ro', 'e-ri-we-ro', 'NUM', 'e-wi-te-wi-jo', 'pa-ro', 'BRUTE_ME', '1', 'me-te-to'], 'classifications': [['common', 'verb', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o'], [], ['toponym', 'noun', 'thematic in -a'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['toponym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -o'], [], [], ['ethnonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], ['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], [], []], 'brute': ['i-sa-to', 'ka-sa-to', 'ko-sa-to', 'ri-sa-to', 'ti-sa-to']}\n",
      "FORM: o-ze-to ke-sa-do-ro *34-to-pi pa-ro a-ke-ha me-ta-pa pe-ri-te 1 a-pi-no-e-wi-jo pa-ro e-ru-si-jo 1 a-pi-no-e-wi-jo pa-ro ai-ki-e-we NUM e-na-po-ro pa-ro wa-do-me-no NUM sa-ri-no-te pa-ro o-wo-to NUM pa-ki-ja-si pa-ro a-ta-no-re NUM ka-ra-do-ro pa-ro to-ro-wo 1 pa-ki-ja-si pa-ro e-ri-we-ro NUM e-wi-te-wi-jo pa-ro wi-sa-to 1 me-te-to\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence is an administrative record, likely an inventory or list of contributions/allocations. The recurring pattern is 'pa-ro [NAME/PLACE] NUM [ITEM/DESCRIPTION]'. The BRUTE_ME placeholder appears in the context 'pa-ro BRUTE_ME 1 me-te-to'. This strongly suggests that BRUTE_ME should be an anthroponym (person's name) or a toponym (place name), fitting the pattern of the preceding entries like 'pa-ro ai-ki-e-we' (Aigeus) or 'pa-ro a-ta-no-re' (Atanor).\\n\\nOption 2, ka-sa-to, is classified as an 'anthroponym/animal name/theonym' (word type) and 'verb' (part of speech). While the 'verb' classification is contradictory to its likely role as a name, the 'anthroponym' classification aligns perfectly with the contextual pattern. The Ancient Greek translation provided for ka-sa-to is  (Xanthos), which is a well-attested Greek proper name. The Linear B to Ancient Greek mapping ka-sa-to ->  is linguistically plausible, as ka followed by sa can form  ( +  + ).\\n\\nThe other options (i-sa-to, ko-sa-to, ri-sa-to, ti-sa-to) do not have corresponding Ancient Greek translations provided, making it impossible to verify their linguistic authenticity or semantic coherence as names or relevant terms in this context. Without this crucial information, they cannot be confidently selected.\\n\\nTherefore, ka-sa-to (Xanthos) is the most semantically coherent, grammatically consistent (as an anthroponym in this list structure), contextually plausible, and linguistically authentic choice, despite the minor inconsistency in its provided part-of-speech classification.\"}\n",
      "Correct: 16/40\n",
      "\n",
      "SEQ_IDX: 3578\n",
      "INPUT: {'word_list': ['pa-ro', 'BRUTE_ME', 'NUM', 'ro-u-so', 'NUM', 'me-te-to'], 'classifications': [['common', 'noun', 'thematic in -o'], [], [], ['toponym', 'noun', 'thematic in -o'], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], [], [], []], 'brute': ['di-do', 'e-do', 'pu-do', 'se-do', 'we-do']}\n",
      "FORM: pa-ro ko-do NUM ro-u-so NUM me-te-to\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an administrative transaction, which is typical for Mycenaean texts. The presence of 'NUM' (numbers) and 'ro-u-so' (a toponym) strongly suggests a record of quantities being moved or assigned to a specific location. Option 1, 'di-do', is classified as a verb and translated as forms of 'to give' (, , ). This fits perfectly into an administrative context. If 'pa-ro' is interpreted as a personal name (as its classification as a 'noun, thematic in -o' suggests, e.g., 'Paros'), then the phrase 'Paros gives [quantity] to Lousos' makes excellent semantic and grammatical sense. The other options (e-do, pu-do, se-do, we-do) lack any provided Ancient Greek translation or classification, making it impossible to assess their semantic coherence or grammatical fit within the sentence. Therefore, 'di-do' is the only option that provides a plausible and coherent meaning for this administrative record.\"}\n",
      "Correct: 16/41\n",
      "\n",
      "SEQ_IDX: 3588\n",
      "INPUT: {'word_list': ['1', 'wa-na-si-ja-ke', '1', 'sa-nu-we', '1', 'wi-ri-ke-ja', '1', 'BRUTE_ME', '1', 'e-ti-ri-ja', '1', 'te-do-ne-ja', '1', 'e-ti-je-ja', '1', 'ne-ka-ta-ta', '1', 'ta-zo-te-ja'], 'classifications': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ko-to-wo-we-i', 'me-to-wo-we-i', 'po-to-wo-we-i', 'ri-to-wo-we-i', 'wi-to-wo-we-i']}\n",
      "FORM: 1 wa-na-si-ja-ke 1 sa-nu-we 1 wi-ri-ke-ja 1 o-to-wo-we-i 1 e-ti-ri-ja 1 te-do-ne-ja 1 e-ti-je-ja 1 ne-ka-ta-ta 1 ta-zo-te-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence presents a list of items, each preceded by the numeral '1', which is characteristic of administrative or inventory records. The surrounding words like `wi-ri-ke-ja` (possibly related to 'weaver') and `wa-na-si-ja-ke` (related to 'queen' or 'lady') appear to be designations of individuals or types of people. Among the given options for `BRUTE_ME`, `ko-to-wo-we-i` (Option 1) is the most semantically and contextually plausible. The Linear B term `ko-to-wo-we` () is well-attested and refers to 'land-holders' or 'possessors of koto-land'. The form `ko-to-wo-we-i` would be a plausible dative singular inflection, meaning 'to the land-holder' or 'for the land-holder'. Including '1 land-holder' in a list of individuals or their roles is highly consistent with the administrative nature of Linear B texts. The other options (`me-to-wo-we-i`, `po-to-wo-we-i`, `ri-to-wo-we-i`, `wi-to-wo-we-i`) do not have clear connections to known Mycenaean vocabulary that would make them suitable designations in such a list, nor do they offer the same level of contextual fit as 'land-holder'.\"}\n",
      "Correct: 16/42\n",
      "\n",
      "SEQ_IDX: 3593\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'NUM', 'ko-ni-ti-ja-ja', 'pe-*65-ka', 'NUM', 'e-to-ki-ja', 'qa-ra-de-ro', 'NUM', 'pa-ke-te-re', 'qa-ra-de-ro', 'NUM'], 'classifications': [[], [], [], [], [], ['common', 'noun', 'thematic in -a'], [], [], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['pe-ke-ka', 'pe-ko-ka', 'pe-nu-ka', 'pe-re-ka', 'pe-ri-ka']}\n",
      "FORM: pe-*65-ka NUM ko-ni-ti-ja-ja pe-*65-ka NUM e-to-ki-ja qa-ra-de-ro NUM pa-ke-te-re qa-ra-de-ro NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an inventory of items, indicated by the presence of 'NUM' (numerical logogram) and common nouns like 'e-to-ki-ja' (, 'fixtures') and 'pa-ke-te-re' (, 'fasteners' or 'pegs'). In this context, the unknown word 'BRUTE_ME' is expected to be a noun, likely referring to a tool or material.\\n\\nOption 4, 'pe-re-ka', can be plausibly transliterated into Ancient Greek as '' (pelekas), the accusative plural of '' (pelekys), meaning 'axe'. Axes were common tools and frequently listed in Mycenaean administrative records. This interpretation fits perfectly within the semantic field of an inventory of tools and fittings.\\n\\nLet's analyze the full sequence with this choice:\\n' NUM ko-ni-ti-ja-ja  NUM  qa-ra-de-ro NUM  qa-ra-de-ro NUM'\\nThis translates to something like: 'Axes [quantity], ko-ni-ti-ja-ja (possibly 'stone' or 'lime' related) pine wood [quantity], fixtures of type qa-ra-de-ro [quantity], fasteners of type qa-ra-de-ro [quantity].'\\n\\nThis interpretation provides strong semantic coherence and contextual plausibility. The other options ('pe-ke-ka', 'pe-ko-ka', 'pe-nu-ka', 'pe-ri-ka') do not readily correspond to known Ancient Greek words that fit the inventory context as well as 'axe' does. 'pe-ri-ka' could potentially be related to '' (pelix, 'bowl'), but 'axes' aligns more strongly with 'fixtures' and 'fasteners' as practical equipment. The linguistic authenticity is high, as 'pe-re-ka' is a recognized Linear B form for 'axe'.\"}\n",
      "Correct: 16/43\n",
      "\n",
      "SEQ_IDX: 3596\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'jo-pi-do-ja'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ki-wa-ra-e-ke', 'ki-wa-ra-e-ra', 'ki-wa-ra-e-ri', 'ki-wa-ra-e-ta', 'ki-wa-ra-e-we']}\n",
      "FORM: ki-wa-ra-e-ru jo-pi-do-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence is 'BRUTE_ME jo-pi-do-ja'. Based on research, 'jo-pi-do-ja' is a known toponym (place name) in Linear B, likely referring to 'Opidoia' or 'Ophidoia'. Therefore, the preceding word 'BRUTE_ME' is most likely an item or commodity associated with this place. Among the given options, 'ki-wa-ra-e-ta' can be transliterated as '' (kibarrata). '' (kibarra) is a well-attested Mycenaean Greek word referring to a type of garment or textile. The suffix '-ata' is a common neuter plural ending in Ancient Greek, making '' a grammatically plausible form for 'garments' or 'textiles'. This interpretation ('garments/textiles from/at Opidoia') fits perfectly within the administrative and inventory-keeping context typical of Linear B tablets, demonstrating strong semantic coherence, grammatical consistency, and linguistic authenticity. The other options do not form recognizable or grammatically plausible Mycenaean words that would fit this context as well.\"}\n",
      "Correct: 16/44\n",
      "\n",
      "SEQ_IDX: 3598\n",
      "INPUT: {'word_list': ['NUM', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['a-ta-jo', 'ku-ta-jo', 'me-ta-jo', 'mu-ta-jo', 'u-ta-jo']}\n",
      "FORM: NUM pa-ta-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The sequence `NUM BRUTE_ME` strongly suggests that `BRUTE_ME` should be a noun, likely a proper noun (an anthroponym) or a common noun representing an item being counted. Among the given options, only 'u-ta-jo' (Option 5) is explicitly associated with plausible Ancient Greek names: '' or ''. Linear B tablets are primarily administrative records, and the structure 'NUMBER + NAME' (e.g., 'X units for person Y') is extremely common and contextually plausible. The other options (a-ta-jo, ku-ta-jo, me-ta-jo, mu-ta-jo) are presented as direct transliterations without corresponding known Ancient Greek words, making them less semantically coherent or linguistically authentic in this context. Therefore, 'u-ta-jo' provides the most meaningful and interpretable complete phrase, fitting perfectly within the typical content of Mycenaean texts.\"}\n",
      "Correct: 16/45\n",
      "\n",
      "SEQ_IDX: 3610\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['de-mi-na-jo', 'de-mi-ni-jo', 'de-mi-nu-jo', 'de-mi-ri-jo', 'de-mi-ti-jo']}\n",
      "FORM: de-mi-ni-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"Option 2, 'de-mi-ni-jo', is the only choice that provides a clear and well-attested Ancient Greek translation, '' (demnion), meaning 'bed' or 'couch'. This word is a common noun and fits the 'thematic in -o' inflection, consistent with the provided classification. Linear B texts are primarily administrative records, and items like beds or couches are frequently inventoried, making '' contextually plausible and linguistically authentic for Mycenaean Greek. The other options lack a specific Ancient Greek translation or classification, making it impossible to verify their semantic coherence, grammatical consistency, or linguistic authenticity.\"}\n",
      "Correct: 17/46\n",
      "\n",
      "SEQ_IDX: 3614\n",
      "INPUT: {'word_list': ['VIN', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['e-pa-wa-i', 'e-te-wa-i', 'e-wa-wa-i', 'e-we-wa-i', 'e-wo-wa-i']}\n",
      "FORM: VIN e-ti-wa-i\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence 'VIN BRUTE_ME' translates to 'wine [something]'. In Linear B administrative texts, wine entries are frequently followed by a descriptor indicating its origin, type, or destination. Among the given options, 'e-te-wa-i' is a known and attested Linear B word that appears in contexts related to wine (e.g., in the Knossos tablets, specifically KN Fp 1.1). It is generally interpreted as a place name or an adjective derived from a place name, likely in the dative or locative case, meaning 'at Etewa' or 'for Etewa'. This interpretation provides excellent semantic coherence ('wine for/from Etewa') and contextual plausibility for an administrative record. The other options ('e-pa-wa-i', 'e-wa-wa-i', 'e-we-wa-i', 'e-wo-wa-i') do not correspond to known Mycenaean wine descriptors or place names in this specific context, making them less linguistically authentic and contextually plausible. The grammatical function of 'e-te-wa-i' as an adverbial modifier (locative/dative) to 'VIN' is consistent with Mycenaean syntax.\"}\n",
      "Correct: 17/47\n",
      "\n",
      "SEQ_IDX: 3623\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], ['common', 'noun', 'thematic in -o']], 'brute': ['o-ko', 'pa-ko', 'pu-ko', 'ti-ko', 'to-ko']}\n",
      "FORM: to-ko\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The task requires selecting the most semantically and contextually sensible translation. Among the given options, only 'to-ko' (Option 5) is provided with a classification (common noun, thematic in -o) and a plausible Ancient Greek translation (''). '' (tokos) means 'birth, offspring, interest' and is a well-attested word in Ancient Greek, fitting the common administrative and economic contexts of Linear B texts. Without any surrounding Linear B sequence, the choice must rely on the inherent plausibility and provided linguistic information of the infilling options. The other options lack any such information, making it impossible to assess their semantic or grammatical coherence. Therefore, 'to-ko' is the only option that can be evaluated as a linguistically authentic and plausible Mycenaean word.\"}\n",
      "Correct: 18/48\n",
      "\n",
      "SEQ_IDX: 3625\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ke-re-te-u-jo-no', 'ke-re-te-u-ko-no', 'ke-re-te-u-ne-no', 'ke-re-te-u-ta-no', 'ke-re-te-u-wo-no']}\n",
      "FORM: ke-re-te-u-ti-no\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"Without any contextual information, grammatical classifications, or surrounding words, it is impossible to apply the criteria of semantic coherence, grammatical consistency, contextual plausibility, or overall meaning. All five options represent syntactically valid Linear B sequences. Therefore, the selection must rely solely on the general linguistic authenticity and plausibility of the Linear B sequence itself. Option 1, 'ke-re-te-u-jo-no', is chosen as the most plausible due to the common occurrence of the 'jo' syllabogram in Mycenaean Greek, often forming patronymics or derivatives (e.g., -). While this is a weak justification without further context, it represents the most reasonable choice given the severe lack of information.\"}\n",
      "Correct: 18/49\n",
      "\n",
      "SEQ_IDX: 3630\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'o'], 'classifications': [[], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [['toponym', 'noun', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -o']], 'brute': ['to-no', 'to-qe', 'to-ro', 'to-sa', 'to-so']}\n",
      "FORM: to-sa o\n",
      "ANSWER: {'chosen_sequence': 'Option 5'}\n",
      "Correct: 18/50\n",
      "\n",
      "SEQ_IDX: 3634\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'da-phu-ra-zo', 'wa'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['da-*22-ja', 'da-*22-ma', 'da-*22-ro', 'da-*22-to', 'da-*22-we']}\n",
      "FORM: da-*22-to da-phu-ra-zo wa\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': 'Due to the absence of Ancient Greek translations and word classifications for the infilling options, it is impossible to apply the primary selection criteria of semantic coherence, grammatical consistency, contextual plausibility, and linguistic authenticity as instructed. The syllabogram *22 is also not present in the provided mapping, preventing even a basic transliteration.\\n\\nTherefore, the selection is based on a very limited morphological observation: the ending -ro (as in da-*22-ro) is a common final syllabogram in Linear B words, often found in nouns or adjectives (e.g., forming agent nouns or descriptive adjectives). While this is a weak justification without further context, it represents a plausible morphological structure within the known patterns of Linear B. Without the crucial Ancient Greek translations and classifications, a definitive and well-reasoned choice according to the specified criteria cannot be made.'}\n",
      "Correct: 18/51\n",
      "\n",
      "SEQ_IDX: 3640\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], []], 'brute': ['ku-da-ni', 'ku-do-ni', 'ku-mi-ni', 'ku-pu-ni']}\n",
      "FORM: ku-ja-ni\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Given that the input sequence consists solely of the unknown word 'BRUTE_ME' and no surrounding context, the selection must be based on the linguistic plausibility and known vocabulary of Mycenaean Greek. Among the provided options, 'ku-mi-ni' (Option 3) is the most compelling. When transliterated into Ancient Greek using the provided mapping, 'ku-mi-ni' strongly corresponds to '' (kymni), which is the dative singular form of '' (kyminon), meaning 'cumin'. Cumin was a widely used spice and commodity in the ancient world, and its presence in administrative Linear B texts is well-attested. Similarly, 'ku-do-ni' (Option 2) could correspond to '' (kydni), the dative singular of '' (kydnion, 'quince'), or relate to 'Cydonia' (a place name). While 'quince' and 'Cydonia' are also plausible, 'cumin' is a very direct and common commodity found in such records. Options 1 ('ku-da-ni') and 4 ('ku-pu-ni') do not readily map to well-known Mycenaean Greek words or commodities with the same degree of certainty. Therefore, 'ku-mi-ni' offers the strongest linguistic authenticity and contextual plausibility as a commodity name in a Linear B record.\"}\n",
      "Correct: 18/52\n",
      "\n",
      "SEQ_IDX: 3643\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-ja-ni', 'i-ja-ni', 'po-ja-ni', 'ra-ja-ni', 'ri-ja-ni']}\n",
      "FORM: ku-ja-ni\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The task requires selecting the most semantically and contextually plausible word for the unknown Linear B sequence, especially in the absence of surrounding context or grammatical classifications for the options. Linear B texts are predominantly administrative records, frequently listing proper names (anthroponyms or toponyms).\\n\\n1.  Linguistic Authenticity and Plausibility:\\na-ja-ni is a highly plausible Linear B sequence. When transliterated into Ancient Greek, a-ja-ni can correspond to  (Aiani). The syllabogram ja often represents the sound ia in Mycenaean Greek (e.g., a-ja-ko for ).  is the dative singular form of the well-known Greek hero's name  (Ajax). The dative case is common in administrative records to indicate recipients or beneficiaries.\\nThe other options (i-ja-ni, po-ja-ni, ra-ja-ni, ri-ja-ni) do not readily correspond to commonly attested Mycenaean words or prominent Ancient Greek proper nouns or common nouns that would stand alone meaningfully in an administrative context. While some might be theoretically possible,  is a much stronger and more recognizable candidate.\\n\\n2.  Contextual Plausibility: In the absence of any other words, a proper noun (like a personal name) is one of the most common types of entries found as standalone words or initial entries in Linear B tablets. This makes a-ja-ni a very strong candidate.\\n\\n3.  Semantic Coherence: As a standalone word,  (to/for Ajax) is semantically coherent as a record entry. The other options do not offer such clear or common interpretations as standalone words in this context.\\n\\nTherefore, a-ja-ni is the most linguistically authentic and contextually plausible infilling for the unknown word.\"}\n",
      "Correct: 18/53\n",
      "\n",
      "SEQ_IDX: 3655\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute': ['a-ka-to', 'a-na-to', 'a-qi-to', 'a-ra-to', 'a-sa-to']}\n",
      "FORM: a-nu-to\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"Option 2, 'a-na-to' corresponding to Ancient Greek '' (blameless), is the most suitable choice. This option perfectly aligns with its provided classification as a 'common' 'adjective' with 'thematic in -o' inflection. '' is indeed a thematic adjective (neuter singular of ). Options 3, 4, and 5 lack Ancient Greek translations and classifications, making them impossible to evaluate against the criteria. Option 1, 'a-ka-to' (), while a plausible word, is classified as an 'anthroponym/animal name/theonym' and 'noun', which is less direct for '' (primarily an adjective) compared to the precise fit of '' as an adjective. Therefore, Option 2 demonstrates the strongest grammatical consistency and linguistic authenticity based on the provided data.\"}\n",
      "Correct: 18/54\n",
      "\n",
      "SEQ_IDX: 3657\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], ['common', 'noun', 'athematic'], []], 'brute': ['a-ru', 'e-ru', 'i-ru', 'ko-ru', 'ro-ru']}\n",
      "FORM: i-ru\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"Option 4, 'ko-ru', is the only choice that provides a clear and meaningful Ancient Greek translation: '' (helmet) or '' (with helmets, instrumental/locative plural). This word is a common noun, and its athematic inflection aligns with the classification provided. The potential form '' also demonstrates the application of the '-pi' instrumental/locative suffix, which is explicitly mentioned as a critical morphological marker in the provided context. Given that Linear B texts often deal with inventories and military equipment, 'helmet' is a highly plausible and contextually appropriate word. The other options ('a-ru', 'e-ru', 'i-ru', 'ro-ru') lack specific Ancient Greek translations or classifications, making them semantically ambiguous and grammatically unverified in this context. Therefore, 'ko-ru' is the most linguistically authentic, semantically coherent, and contextually plausible choice.\"}\n",
      "Correct: 18/55\n",
      "\n",
      "SEQ_IDX: 3658\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ru-ki', 'ru-ro', 'ru-to', 'ru-wo', 'ru-zo']}\n",
      "FORM: ru-i\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The input sequence consists of a single unknown word. To select the best option, I must evaluate which of the proposed Linear B syllabograms forms the most plausible and meaningful Ancient Greek word when transliterated.\\n\\n    1.  Option 1: ru-ki can be transliterated to '' or ''. This could correspond to proper nouns like '' (Lyke) or '' (Lykios), or related to '' (wolf). This is a plausible option.\\n    2.  Option 2: ru-ro can be transliterated to '' or ''. This combination does not immediately suggest a common or well-attested Ancient Greek word.\\n    3.  Option 3: ru-to can be transliterated to '' or '' (or with theta). The most prominent Ancient Greek word derived from '' is '' (lytos), meaning 'loosed, released, soluble'. This is a very common and grammatically sound adjective/noun in Ancient Greek.\\n    4.  Option 4: ru-wo can be transliterated to 'f' or 'f'. The 'f' (digamma) is present in Mycenaean Greek. This could correspond to verbal forms related to '' (lyo, 'I loose') or '' (rheo, 'I flow'), or nouns derived from them. This is also a plausible option.\\n    5.  Option 5: ru-zo can be transliterated to '' or ''. This combination does not immediately suggest a common or well-attested Ancient Greek word.\\n\\n    Comparing the plausible options (1, 3, 4), '' (from ru-to) stands out as a particularly clear, common, and complete word that can function meaningfully on its own as an adjective or a nominalized adjective. Without further context, a descriptive adjective like 'loosed' or 'released' is a very strong candidate for a standalone word in administrative or descriptive texts, which are common in Linear B. The other plausible options are either proper nouns (which are context-dependent) or verbal forms (which often require more context to fully interpret their tense/mood/voice). Therefore, 'ru-to' forming '' offers the most robust semantic and linguistic authenticity as a standalone word.\"}\n",
      "Correct: 18/56\n",
      "\n",
      "SEQ_IDX: 3660\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ta-*22-de-du', 'ta-*22-de-pi', 'ta-*22-de-qa', 'ta-*22-de-qe', 'ta-*22-de-so']}\n",
      "FORM: ta-*22-de-so\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The input sequence consists solely of the unknown word. Therefore, the selection must be based on the internal structure of the proposed infillings, particularly the presence of known and critical suffixes. Option 2, 'ta-*22-de-pi', ends with the suffix '-pi'. According to the provided grammatical information, '-pi' is an instrumental/locative suffix, which is explicitly marked as a 'CRITICAL' morphological marker. This suffix provides a clear grammatical role (instrumental or locative case) to the word, making it a complete and grammatically functional unit on its own. In contrast, options 1, 3, and 5 end with suffixes ('-du', '-qa', '-so') that are not listed as critical morphological markers, making their grammatical function less clear in isolation. Option 4, 'ta-*22-de-qe', ends with '-qe', which is also a critical suffix meaning 'and'. While grammatically valid, a word ending in a conjunction typically implies a connection to another element, making it less of a standalone, self-contained grammatical unit compared to a word with a case ending like '-pi'. Given the lack of external context, the option that provides the most specific and complete grammatical function to the word itself is preferred for its linguistic authenticity and potential for semantic reconstruction.\"}\n",
      "Correct: 18/57\n",
      "\n",
      "SEQ_IDX: 3661\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ta-*22-de-me', 'ta-*22-de-qe', 'ta-*22-de-ro', 'ta-*22-de-so', 'ta-*22-de-to']}\n",
      "FORM: ta-*22-de-so\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': 'The input sequence consists solely of the placeholder \\'BRUTE_ME\\', and no surrounding context or grammatical classifications for any of the options are provided. This makes it impossible to apply the criteria of semantic coherence, grammatical consistency, contextual plausibility, or linguistic authenticity in a meaningful way. However, the task requires a choice and a reasoning. Among the given options, \\'Option 2: ta-*22-de-qe\\' is the only one that ends with the suffix \\'-qe\\'. The instructions explicitly state that \\'-qe\\' is a \\'CRITICAL\\' suffix, a \\'key morphological marker\\' that is a \\'conjunction suffix meaning \"and\"\\'. While the meaning of the root \\'ta-*22-de-\\' remains unknown, the presence of this critical suffix suggests a clear grammatical function (conjunction) that is plausible within the structure of Linear B texts, which often involve lists or coordinated elements. Without any other distinguishing information, the presence of a recognized and critical morphological marker provides the most defensible, albeit limited, basis for selection.'}\n",
      "Correct: 18/58\n",
      "\n",
      "SEQ_IDX: 3669\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['a-ka-to', 'a-ko-to', 'a-na-to', 'a-ne-to', 'a-nu-to']}\n",
      "FORM: a-nu-to\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The task requires selecting the most semantically and contextually plausible single word. Let's evaluate each option based on the provided mapping rules and classifications:\\n\\n1.  Option 1: a-ka-to ->  (agathos)\\nMapping: a -> , ka ->  (from ka's possible characters , , ), to ->  (from to's possible characters , , , ). The final  is typically omitted in Linear B. This mapping is excellent and phonetically very close.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'good' and can function as a proper name (Agathos) or a substantive ('the good one'). This is highly plausible for a Linear B record, which often lists names or describes items/people.\\n\\n2.  Option 2: a-ko-to -> | (aktor/achthon)\\nMapping: a -> , ko ->  (from ko's possible characters , , , , ), to ->  (from to's possible characters , , , ). The final  or  is omitted. This mapping is also excellent.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'leader' or 'commander', a very plausible name or title in an administrative context.  ('burden') is less likely as a name.\\n\\n3.  Option 3: a-na-to ->  (anaiton)\\nMapping: a -> , na -> , , to -> , . The  in  is not directly represented by any syllabogram in a-na-to. While Linear B can be ambiguous, a missing vowel like this makes the mapping less direct compared to other options. a-na-to would more directly map to .\\nClassification: Common, adjective, thematic in -o.  means 'blameless'. While a common adjective is plausible, the mapping is weaker.\\n\\n4.  Option 4: a-ne-to -> a-ne-to\\nMapping: No Ancient Greek translation or classification is provided, making it impossible to evaluate against the criteria. This option is immediately disadvantaged.\\n\\n5.  Option 5: a-nu-to ->  (anytos)\\nMapping: a -> , nu -> , , to -> , . The final  is omitted. This mapping is excellent.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'unaccomplished' or 'useless'. While technically possible as a name, its negative connotation makes it less likely to be a common anthroponym compared to 'good' or 'leader'.\\n\\nConclusion:\\nOptions 3 and 4 are less plausible due to mapping issues (Option 3) or lack of information (Option 4). This leaves Options 1, 2, and 5. Among these, Option 5 ( - 'useless') is less likely as a common name given its meaning.\\n\\nBoth Option 1 ( - 'good') and Option 2 ( - 'leader') are highly plausible as names or titles in Linear B administrative contexts, and both have excellent phonetic mappings. However,  is a very fundamental and broadly applicable concept ('good', 'noble') and a common root for names, making it a highly versatile and contextually plausible single word entry in an administrative record.  is also strong, but  feels slightly more general and universally applicable in the absence of further context. Therefore, a-ka-to corresponding to  is chosen as the most semantically and contextually plausible option.\"}\n",
      "Correct: 18/59\n",
      "\n",
      "SEQ_IDX: 3670\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], ['common', 'noun', 'thematic in -a'], [], []], 'brute': ['e-ke-ja', 'e-ni-ja', 'e-ra-ja', 'e-ta-ja', 'e-wi-ja']}\n",
      "FORM: e-ko-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Option 3, 'e-ra-ja' (Ancient Greek: ), is the only option that provides a recognized and meaningful Ancient Greek word. The other options ('e-ke-ja', 'e-ni-ja', 'e-ta-ja', 'e-wi-ja') are presented with Ancient Greek translations that are merely transliterations of their Linear B forms, suggesting they do not correspond to commonly recognized Ancient Greek words in this context. '' (eraia) means 'lovely', 'desirable', or 'beloved', which is a plausible word to appear in Mycenaean texts, often dealing with descriptions of items or individuals. Although classified as a noun, '' is an adjective that can function substantively (e.g., 'the beloved one'), and its thematic -a inflection is consistent with the provided grammatical information. This makes it the most linguistically authentic and semantically coherent choice among the given options, as it is the only one that forms a meaningful unit in Ancient Greek.\"}\n",
      "Correct: 18/60\n",
      "\n",
      "SEQ_IDX: 3671\n",
      "INPUT: {'word_list': ['ra-tu-ko', '1', 'BRUTE_ME'], 'classifications': [[], [], []], 'brute_classifications': [['common', 'noun', 'thematic in -a'], ['common', 'adverb', 'athematic'], [], [], []], 'brute': ['o-pa', 'o-pi', 'o-ru', 'o-ti', 'o-tu']}\n",
      "FORM: ra-tu-ko 1 o-wo\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence 'ra-tu-ko 1 BRUTE_ME' strongly suggests an inventory or list, where 'ra-tu-ko' is an item or person, followed by a quantity '1'. In such contexts, the 'BRUTE_ME' position is most likely filled by another noun representing an item or a descriptor. Option 1, 'o-pa', is classified as a common noun and its Ancient Greek translation '|' (meaning 'tools, weapons, equipment' or 'oil-flask') fits perfectly into an administrative inventory context. The structure 'Item A 1, Item B' is common in Linear B tablets. Options 3, 4, and 5 lack Ancient Greek translations and grammatical classifications, making them impossible to evaluate for semantic and grammatical coherence. Option 2, 'o-pi', is classified as an adverb ('' - 'on, upon'), which does not fit grammatically or semantically after a numeral and a noun in an inventory list. Therefore, 'o-pa' is the only option that provides a plausible noun to complete the inventory entry, maintaining semantic coherence and grammatical consistency within the typical context of Linear B texts.\"}\n",
      "Correct: 18/61\n",
      "\n",
      "SEQ_IDX: 3672\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], ['toponym', 'noun', 'thematic in -o']], 'brute': ['wi-na-jo', 'wi-na-o', 'wi-na-qe', 'wi-na-so', 'wi-na-to']}\n",
      "FORM: wi-na-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"Option 5, 'wi-na-to' (f), is the only choice that provides a recognized and grammatically classified Ancient Greek word. The classification provided for 'wi-na-to' as a 'toponym' (place name), 'noun', and 'thematic in -o' aligns perfectly with the form 'f', which is a plausible Mycenaean place name. Linear B texts frequently consist of lists of toponyms, making this a highly contextually plausible option. The initial 'f' (digamma) in 'f' is consistent with the 'wi' syllabogram in Linear B, reflecting the preservation of this sound in Mycenaean Greek. The other options provide only direct transliterations of the Linear B syllabograms into Ancient Greek characters, without forming a recognized or classified word, which suggests they are less likely to be correct infillings based on the provided instructions about the autoregressive model's output.\"}\n",
      "Correct: 18/62\n",
      "\n",
      "SEQ_IDX: 3676\n",
      "INPUT: {'word_list': ['di-wi-jo-de', 'di-we', 'ME+RI', '*209VAS+A', '1', 'BRUTE_ME', 'ME+RI', '2'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['di-ja-nu-so', 'di-ni-nu-so', 'di-su-nu-so', 'di-u-nu-so', 'di-wi-nu-so']}\n",
      "FORM: di-wi-jo-de di-we ME+RI *209VAS+A 1 di-wo-nu-so ME+RI 2\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an offering, likely to deities, given the presence of 'di-wi-jo-de' (to the divine/Zeus) and 'di-we' (to Zeus) followed by quantities of 'ME+RI' (honey) in '*209VAS+A' (vessels). The structure 'X 1 BRUTE_ME Y 2' strongly suggests that 'BRUTE_ME' is another recipient of an offering, parallel to Zeus. Among the given options, 'di-u-nu-so' is a well-attested Linear B spelling for the deity Dionysus (). This makes the entire sequence semantically coherent as an offering to Zeus and Dionysus. While the provided classification for 'di-u-nu-so' as 'athematic' might seem to conflict with Dionysus being a thematic noun in Classical Greek, Linear B spellings can sometimes simplify or vary, and the form 'di-u-nu-so' is a known Mycenaean transcription. The strong contextual fit and linguistic authenticity of 'di-u-nu-so' as Dionysus outweigh this minor discrepancy in classification, especially considering the administrative nature of Linear B texts where names in lists might not always strictly adhere to expected case endings if the context is clear. Options 1, 2, 3, and 5 are not standard or attested spellings for Dionysus in Linear B, making them less plausible.\"}\n",
      "Correct: 18/63\n",
      "\n",
      "SEQ_IDX: 3681\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], ['common', 'adjective', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['i-no', 'me-no', 'o-no', 'wa-no', 'wo-no']}\n",
      "FORM: ko-no\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The task requires selecting the most semantically and grammatically coherent option, while also verifying the Ancient Greek correspondences against the provided Linear B mapping.\\n\\n1.  Elimination of Options 1 and 4: Options 1 (i-no) and 4 (wa-no) lack Ancient Greek translations and classifications, making it impossible to assess their semantic or grammatical fit. They are therefore the weakest candidates.\\n\\n2.  Evaluation of Option 5 (wo-no): The proposed Ancient Greek translation is f (wine). However, when checking this against the Linear B mapping:\\nwo maps to , f, .\\nno maps to , , .\\nThe Ancient Greek word f contains the character . This character  is not present in the possible mappings for either wo or no. This significant discrepancy makes f an inaccurate translation for wo-no based on the provided mapping, despite wine being a plausible commodity in Linear B texts.\\n\\n3.  Evaluation of Option 2 (me-no): The proposed Ancient Greek translations are ||.\\nme maps to , , .\\nno maps to , , .\\nFor  (month/moon):  from me,  from me,  from no. This is a perfect character match.\\nHowever, the classification for me-no is adjective.  (month/moon) is primarily a noun. While  (might/spirit) is also a noun, it would require a final  which is not directly mapped by no. This creates a grammatical inconsistency between the proposed word and its classification.\\n\\n4.  Evaluation of Option 3 (o-no): The proposed Ancient Greek translations are |.\\no maps to , .\\nno maps to , , .\\nFor  (donkey):  from o,  from no,  from no. The final  is a common omission in Linear B transcriptions, as Linear B is a syllabic script that typically does not write final consonants. This is a well-attested Mycenaean word.\\nThe classification for o-no is anthroponym/animal name/theonym, noun, thematic in -o.  (donkey) perfectly fits the 'animal name' and 'noun' classifications, and it is a thematic -o noun. This option demonstrates strong grammatical consistency and linguistic authenticity.\\n\\nConclusion: Option 3 (o-no) is the most consistent choice. Its Ancient Greek translation  (donkey) aligns perfectly with its classification as an 'animal name' and 'noun', and the character mapping is accurate (accounting for the typical omission of final consonants in Linear B). This makes it the most plausible and grammatically sound infilling for a single word in a Mycenaean context, which often deals with inventories of animals or goods.\"}\n",
      "Correct: 18/64\n",
      "\n",
      "Correct: 18/64\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "output_file_path = os.path.join(prefix_path, \"infilling_answers.csv\")\n",
    "correct = 0\n",
    "total = 0\n",
    "prec_idx = -1\n",
    "fail_count = 0\n",
    "switch_key_interval = 10\n",
    "TEST_ONLY = True\n",
    "\n",
    "already_done = set()\n",
    "file_exists = os.path.isfile(output_file_path)\n",
    "\n",
    "# Read existing rows to populate already_done\n",
    "if file_exists:\n",
    "    with open(output_file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            already_done.add(int(row[\"idx\"]))\n",
    "\n",
    "# Prepare test index filtering\n",
    "if TEST_ONLY:\n",
    "    sorted_test_idxs = sorted(test_idxs)\n",
    "    test_idx_set = set(sorted_test_idxs)\n",
    "\n",
    "with open(output_file_path, mode='a', newline='', encoding='utf-8') as output_file:\n",
    "    fieldnames = [\"idx\", \"original_form\", \"correct_word\", \"predicted_word\", \"reasoning\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if file is new\n",
    "    if not file_exists or os.stat(output_file_path).st_size == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i, seq in enumerate(seq_dataset):\n",
    "        idx = seq.sequence.idx\n",
    "\n",
    "        # Skip if already processed\n",
    "        if idx in already_done:\n",
    "            continue\n",
    "\n",
    "        # Skip if TEST_ONLY and not in test set\n",
    "        if TEST_ONLY and idx not in test_idx_set:\n",
    "            continue\n",
    "\n",
    "        lin_b = st.linb_versions[i]\n",
    "        greek = st.greek_versions[i]\n",
    "\n",
    "        # Determine which API key to use\n",
    "        if i % 300 == 0 and i != 0:\n",
    "            time.sleep(60)\n",
    "        key_idx = (i % (switch_key_interval * len(api_keys))) // switch_key_interval\n",
    "        api_key = api_keys[key_idx]\n",
    "\n",
    "        # Delay if the API key changes\n",
    "        if key_idx != prec_idx:\n",
    "            time.sleep(5)\n",
    "        prec_idx = key_idx\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                gemini_answer = make_infill_selection_prompt(lin_b, greek, api_key)\n",
    "                logging.debug(f\"SEQ_IDX: {seq.sequence.idx}\")\n",
    "                logging.debug(f\"INPUT: {lin_b}\")\n",
    "                logging.debug(f\"FORM: {seq.sequence.form}\")\n",
    "                logging.debug(f\"ANSWER: {gemini_answer}\")\n",
    "                print(f\"SEQ_IDX: {seq.sequence.idx}\")\n",
    "                print(f\"INPUT: {lin_b}\")\n",
    "                print(f\"FORM: {seq.sequence.form}\")\n",
    "                print(f\"ANSWER: {gemini_answer}\")\n",
    "\n",
    "                if not gemini_answer or \"chosen_sequence\" not in gemini_answer or not gemini_answer[\"chosen_sequence\"].startswith(\"Option\"):\n",
    "                    continue\n",
    "\n",
    "                chosen_option = int(gemini_answer[\"chosen_sequence\"].split()[1]) - 1\n",
    "                if chosen_option < 0 or chosen_option >= len(lin_b[\"brute\"]):\n",
    "                    continue\n",
    "\n",
    "                for missing_word_idx in seq.sequence.unknown:\n",
    "                    if len(seq.sequence.unknown[missing_word_idx]) > 0:\n",
    "                        break\n",
    "\n",
    "                correct_form = seq.sequence.form.split(\" \")[missing_word_idx]\n",
    "                predicted_form = lin_b[\"brute\"][chosen_option]\n",
    "\n",
    "                if correct_form == predicted_form:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                logging.debug(f\"Correct: {correct}/{total}\\n\")\n",
    "                print(f\"Correct: {correct}/{total}\\n\")\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"idx\": idx,\n",
    "                    \"original_form\": seq.sequence.form,\n",
    "                    \"correct_word\": correct_form,\n",
    "                    \"predicted_word\": predicted_form,\n",
    "                    \"reasoning\": gemini_answer.get(\"reasoning\", \"\")\n",
    "                })\n",
    "\n",
    "                if i % 20 == 0:\n",
    "                    output_file.flush()\n",
    "                fail_count = 0\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if fail_count >= switch_key_interval:\n",
    "                    break\n",
    "                fail_count += 1\n",
    "                logging.debug(\"DEBUG:\", seq.form)\n",
    "                logging.debug(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                print(\"DEBUG: \", seq.form)\n",
    "                print(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                time.sleep(65)\n",
    "\n",
    "logging.debug(f\"Correct: {correct}/{total}\")\n",
    "print(f\"Correct: {correct}/{total}\")\n",
    "if total > 0:\n",
    "    logging.debug(f\"Accuracy: {correct / total:.2%}\")\n",
    "    with open(os.path.join(prefix_path, \"final_accuracy_textInfillingPipeline.txt\"), 'a') as f:\n",
    "        f.write(f\"Correct: {correct}/{total}\")\n",
    "        f.write(f\"Accuracy: {correct / total:.2%}\")\n",
    "else:\n",
    "    logging.debug(\"No sequences processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 20,\n",
       " 22,\n",
       " 30,\n",
       " 32,\n",
       " 43,\n",
       " 50,\n",
       " 60,\n",
       " 61,\n",
       " 63,\n",
       " 72,\n",
       " 74,\n",
       " 80,\n",
       " 84,\n",
       " 107,\n",
       " 113,\n",
       " 115,\n",
       " 124,\n",
       " 125,\n",
       " 127,\n",
       " 132,\n",
       " 134,\n",
       " 135,\n",
       " 149,\n",
       " 150,\n",
       " 154,\n",
       " 160,\n",
       " 164,\n",
       " 169,\n",
       " 176,\n",
       " 180,\n",
       " 202,\n",
       " 211,\n",
       " 212,\n",
       " 217,\n",
       " 223,\n",
       " 227,\n",
       " 229,\n",
       " 238,\n",
       " 240,\n",
       " 247,\n",
       " 253,\n",
       " 255,\n",
       " 257,\n",
       " 265,\n",
       " 269,\n",
       " 275,\n",
       " 290,\n",
       " 292,\n",
       " 296,\n",
       " 313,\n",
       " 316,\n",
       " 318,\n",
       " 319,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 326,\n",
       " 333,\n",
       " 336,\n",
       " 338,\n",
       " 342,\n",
       " 350,\n",
       " 352,\n",
       " 355,\n",
       " 356,\n",
       " 359,\n",
       " 368,\n",
       " 375,\n",
       " 379,\n",
       " 382,\n",
       " 385,\n",
       " 405,\n",
       " 439,\n",
       " 445,\n",
       " 453,\n",
       " 455,\n",
       " 459,\n",
       " 463,\n",
       " 467,\n",
       " 468,\n",
       " 475,\n",
       " 485,\n",
       " 500,\n",
       " 509,\n",
       " 510,\n",
       " 516,\n",
       " 518,\n",
       " 519,\n",
       " 526,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 538,\n",
       " 539,\n",
       " 551,\n",
       " 552,\n",
       " 556,\n",
       " 563,\n",
       " 565,\n",
       " 568,\n",
       " 575,\n",
       " 576,\n",
       " 580,\n",
       " 584,\n",
       " 585,\n",
       " 588,\n",
       " 599,\n",
       " 600,\n",
       " 609,\n",
       " 610,\n",
       " 612,\n",
       " 617,\n",
       " 622,\n",
       " 623,\n",
       " 628,\n",
       " 633,\n",
       " 635,\n",
       " 645,\n",
       " 649,\n",
       " 657,\n",
       " 658,\n",
       " 662,\n",
       " 675,\n",
       " 676,\n",
       " 680,\n",
       " 683,\n",
       " 684,\n",
       " 688,\n",
       " 692,\n",
       " 696,\n",
       " 705,\n",
       " 724,\n",
       " 725,\n",
       " 739,\n",
       " 744,\n",
       " 746,\n",
       " 748,\n",
       " 751,\n",
       " 753,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 766,\n",
       " 771,\n",
       " 773,\n",
       " 774,\n",
       " 782,\n",
       " 788,\n",
       " 790,\n",
       " 793,\n",
       " 808,\n",
       " 810,\n",
       " 811,\n",
       " 817,\n",
       " 834,\n",
       " 836,\n",
       " 838,\n",
       " 840,\n",
       " 846,\n",
       " 861,\n",
       " 870,\n",
       " 874,\n",
       " 878,\n",
       " 879,\n",
       " 881,\n",
       " 885,\n",
       " 892,\n",
       " 894,\n",
       " 897,\n",
       " 900,\n",
       " 901,\n",
       " 905,\n",
       " 910,\n",
       " 915,\n",
       " 916,\n",
       " 920,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 949,\n",
       " 960,\n",
       " 962,\n",
       " 975,\n",
       " 978,\n",
       " 980,\n",
       " 989,\n",
       " 993,\n",
       " 997,\n",
       " 1000,\n",
       " 1003,\n",
       " 1019,\n",
       " 1022,\n",
       " 1023,\n",
       " 1024,\n",
       " 1026,\n",
       " 1036,\n",
       " 1040,\n",
       " 1052,\n",
       " 1055,\n",
       " 1056,\n",
       " 1071,\n",
       " 1078,\n",
       " 1081,\n",
       " 1082,\n",
       " 1084,\n",
       " 1095,\n",
       " 1099,\n",
       " 1104,\n",
       " 1109,\n",
       " 1110,\n",
       " 1114,\n",
       " 1117,\n",
       " 1118,\n",
       " 1122,\n",
       " 1124,\n",
       " 1130,\n",
       " 1137,\n",
       " 1152,\n",
       " 1173,\n",
       " 1174,\n",
       " 1176,\n",
       " 1180,\n",
       " 1183,\n",
       " 1186,\n",
       " 1188,\n",
       " 1193,\n",
       " 1195,\n",
       " 1200,\n",
       " 1201,\n",
       " 1203,\n",
       " 1204,\n",
       " 1205,\n",
       " 1210,\n",
       " 1218,\n",
       " 1223,\n",
       " 1224,\n",
       " 1229,\n",
       " 1231,\n",
       " 1232,\n",
       " 1236,\n",
       " 1246,\n",
       " 1248,\n",
       " 1251,\n",
       " 1253,\n",
       " 1257,\n",
       " 1262,\n",
       " 1269,\n",
       " 1275,\n",
       " 1280,\n",
       " 1297,\n",
       " 1299,\n",
       " 1303,\n",
       " 1304,\n",
       " 1315,\n",
       " 1316,\n",
       " 1327,\n",
       " 1328,\n",
       " 1334,\n",
       " 1336,\n",
       " 1338,\n",
       " 1351,\n",
       " 1352,\n",
       " 1360,\n",
       " 1370,\n",
       " 1380,\n",
       " 1383,\n",
       " 1388,\n",
       " 1389,\n",
       " 1390,\n",
       " 1393,\n",
       " 1395,\n",
       " 1399,\n",
       " 1401,\n",
       " 1402,\n",
       " 1413,\n",
       " 1415,\n",
       " 1416,\n",
       " 1419,\n",
       " 1427,\n",
       " 1428,\n",
       " 1438,\n",
       " 1439,\n",
       " 1447,\n",
       " 1449,\n",
       " 1452,\n",
       " 1455,\n",
       " 1457,\n",
       " 1459,\n",
       " 1468,\n",
       " 1474,\n",
       " 1475,\n",
       " 1480,\n",
       " 1489,\n",
       " 1494,\n",
       " 1497,\n",
       " 1502,\n",
       " 1509,\n",
       " 1513,\n",
       " 1528,\n",
       " 1530,\n",
       " 1535,\n",
       " 1536,\n",
       " 1542,\n",
       " 1544,\n",
       " 1552,\n",
       " 1554,\n",
       " 1561,\n",
       " 1562,\n",
       " 1565,\n",
       " 1572,\n",
       " 1575,\n",
       " 1581,\n",
       " 1588,\n",
       " 1589,\n",
       " 1597,\n",
       " 1598,\n",
       " 1599,\n",
       " 1605,\n",
       " 1606,\n",
       " 1611,\n",
       " 1623,\n",
       " 1627,\n",
       " 1638,\n",
       " 1641,\n",
       " 1643,\n",
       " 1650,\n",
       " 1651,\n",
       " 1652,\n",
       " 1662,\n",
       " 1664,\n",
       " 1667,\n",
       " 1673,\n",
       " 1675,\n",
       " 1678,\n",
       " 1681,\n",
       " 1682,\n",
       " 1687,\n",
       " 1695,\n",
       " 1699,\n",
       " 1701,\n",
       " 1726,\n",
       " 1727,\n",
       " 1730,\n",
       " 1735,\n",
       " 1739,\n",
       " 1743,\n",
       " 1747,\n",
       " 1749,\n",
       " 1751,\n",
       " 1757,\n",
       " 1760,\n",
       " 1761,\n",
       " 1766,\n",
       " 1768,\n",
       " 1776,\n",
       " 1791,\n",
       " 1793,\n",
       " 1802,\n",
       " 1803,\n",
       " 1811,\n",
       " 1814,\n",
       " 1817,\n",
       " 1821,\n",
       " 1828,\n",
       " 1829,\n",
       " 1834,\n",
       " 1838,\n",
       " 1841,\n",
       " 1856,\n",
       " 1859,\n",
       " 1860,\n",
       " 1863,\n",
       " 1865,\n",
       " 1870,\n",
       " 1871,\n",
       " 1874,\n",
       " 1878,\n",
       " 1886,\n",
       " 1888,\n",
       " 1894,\n",
       " 1900,\n",
       " 1901,\n",
       " 1910,\n",
       " 1934,\n",
       " 1939,\n",
       " 1941,\n",
       " 1942,\n",
       " 1945,\n",
       " 1949,\n",
       " 1950,\n",
       " 1953,\n",
       " 1964,\n",
       " 1965,\n",
       " 1968,\n",
       " 1970,\n",
       " 1982,\n",
       " 1994,\n",
       " 1996,\n",
       " 2006,\n",
       " 2010,\n",
       " 2016,\n",
       " 2028,\n",
       " 2036,\n",
       " 2038,\n",
       " 2046,\n",
       " 2047,\n",
       " 2048,\n",
       " 2068,\n",
       " 2069,\n",
       " 2072,\n",
       " 2076,\n",
       " 2078,\n",
       " 2080,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 2094,\n",
       " 2095,\n",
       " 2096,\n",
       " 2100,\n",
       " 2104,\n",
       " 2105,\n",
       " 2109,\n",
       " 2114,\n",
       " 2119,\n",
       " 2129,\n",
       " 2133,\n",
       " 2136,\n",
       " 2139,\n",
       " 2140,\n",
       " 2150,\n",
       " 2158,\n",
       " 2166,\n",
       " 2168,\n",
       " 2169,\n",
       " 2178,\n",
       " 2185,\n",
       " 2190,\n",
       " 2193,\n",
       " 2200,\n",
       " 2206,\n",
       " 2217,\n",
       " 2218,\n",
       " 2228,\n",
       " 2234,\n",
       " 2238,\n",
       " 2242,\n",
       " 2247,\n",
       " 2249,\n",
       " 2265,\n",
       " 2268,\n",
       " 2269,\n",
       " 2280,\n",
       " 2282,\n",
       " 2284,\n",
       " 2293,\n",
       " 2294,\n",
       " 2295,\n",
       " 2297,\n",
       " 2316,\n",
       " 2319,\n",
       " 2322,\n",
       " 2323,\n",
       " 2331,\n",
       " 2332,\n",
       " 2333,\n",
       " 2340,\n",
       " 2342,\n",
       " 2346,\n",
       " 2352,\n",
       " 2354,\n",
       " 2357,\n",
       " 2359,\n",
       " 2369,\n",
       " 2370,\n",
       " 2374,\n",
       " 2375,\n",
       " 2379,\n",
       " 2399,\n",
       " 2401,\n",
       " 2402,\n",
       " 2404,\n",
       " 2418,\n",
       " 2425,\n",
       " 2435,\n",
       " 2446,\n",
       " 2448,\n",
       " 2450,\n",
       " 2459,\n",
       " 2469,\n",
       " 2473,\n",
       " 2477,\n",
       " 2478,\n",
       " 2483,\n",
       " 2484,\n",
       " 2494,\n",
       " 2502,\n",
       " 2505,\n",
       " 2514,\n",
       " 2520,\n",
       " 2521,\n",
       " 2523,\n",
       " 2527,\n",
       " 2531,\n",
       " 2541,\n",
       " 2552,\n",
       " 2561,\n",
       " 2565,\n",
       " 2572,\n",
       " 2573,\n",
       " 2575,\n",
       " 2588,\n",
       " 2596,\n",
       " 2597,\n",
       " 2626,\n",
       " 2628,\n",
       " 2629,\n",
       " 2631,\n",
       " 2634,\n",
       " 2636,\n",
       " 2641,\n",
       " 2642,\n",
       " 2644,\n",
       " 2655,\n",
       " 2658,\n",
       " 2661,\n",
       " 2664,\n",
       " 2668,\n",
       " 2676,\n",
       " 2704,\n",
       " 2706,\n",
       " 2720,\n",
       " 2721,\n",
       " 2733,\n",
       " 2736,\n",
       " 2742,\n",
       " 2743,\n",
       " 2744,\n",
       " 2747,\n",
       " 2760,\n",
       " 2762,\n",
       " 2786,\n",
       " 2792,\n",
       " 2796,\n",
       " 2797,\n",
       " 2805,\n",
       " 2817,\n",
       " 2831,\n",
       " 2832,\n",
       " 2836,\n",
       " 2841,\n",
       " 2846,\n",
       " 2854,\n",
       " 2856,\n",
       " 2857,\n",
       " 2859,\n",
       " 2860,\n",
       " 2862,\n",
       " 2864,\n",
       " 2875,\n",
       " 2876,\n",
       " 2884,\n",
       " 2888,\n",
       " 2902,\n",
       " 2906,\n",
       " 2916,\n",
       " 2918,\n",
       " 2921,\n",
       " 2931,\n",
       " 2941,\n",
       " 2945,\n",
       " 2956,\n",
       " 2960,\n",
       " 2962,\n",
       " 2964,\n",
       " 2968,\n",
       " 2970,\n",
       " 2974,\n",
       " 2979,\n",
       " 2983,\n",
       " 2991,\n",
       " 2993,\n",
       " 3008,\n",
       " 3013,\n",
       " 3022,\n",
       " 3024,\n",
       " 3026,\n",
       " 3030,\n",
       " 3033,\n",
       " 3034,\n",
       " 3039,\n",
       " 3044,\n",
       " 3047,\n",
       " 3065,\n",
       " 3068,\n",
       " 3072,\n",
       " 3082,\n",
       " 3085,\n",
       " 3086,\n",
       " 3088,\n",
       " 3090,\n",
       " 3098,\n",
       " 3101,\n",
       " 3102,\n",
       " 3103,\n",
       " 3105,\n",
       " 3106,\n",
       " 3109,\n",
       " 3112,\n",
       " 3114,\n",
       " 3115,\n",
       " 3118,\n",
       " 3132,\n",
       " 3137,\n",
       " 3141,\n",
       " 3153,\n",
       " 3158,\n",
       " 3161,\n",
       " 3182,\n",
       " 3184,\n",
       " 3185,\n",
       " 3186,\n",
       " 3190,\n",
       " 3191,\n",
       " 3199,\n",
       " 3213,\n",
       " 3219,\n",
       " 3223,\n",
       " 3226,\n",
       " 3237,\n",
       " 3242,\n",
       " 3243,\n",
       " 3245,\n",
       " 3247,\n",
       " 3253,\n",
       " 3255,\n",
       " 3260,\n",
       " 3261,\n",
       " 3264,\n",
       " 3265,\n",
       " 3267,\n",
       " 3270,\n",
       " 3271,\n",
       " 3279,\n",
       " 3284,\n",
       " 3286,\n",
       " 3292,\n",
       " 3294,\n",
       " 3300,\n",
       " 3305,\n",
       " 3315,\n",
       " 3320,\n",
       " 3323,\n",
       " 3324,\n",
       " 3330,\n",
       " 3350,\n",
       " 3356,\n",
       " 3357,\n",
       " 3369,\n",
       " 3370,\n",
       " 3371,\n",
       " 3375,\n",
       " 3379,\n",
       " 3380,\n",
       " 3385,\n",
       " 3386,\n",
       " 3388,\n",
       " 3389,\n",
       " 3395,\n",
       " 3399,\n",
       " 3403,\n",
       " 3406,\n",
       " 3411,\n",
       " 3412,\n",
       " 3420,\n",
       " 3425,\n",
       " 3427,\n",
       " 3428,\n",
       " 3432,\n",
       " 3438,\n",
       " 3442,\n",
       " 3443,\n",
       " 3448,\n",
       " 3449,\n",
       " 3455,\n",
       " 3458,\n",
       " 3464,\n",
       " 3466,\n",
       " 3472,\n",
       " 3476,\n",
       " 3477,\n",
       " 3478,\n",
       " 3481,\n",
       " 3483,\n",
       " 3490,\n",
       " 3498,\n",
       " 3499,\n",
       " 3503,\n",
       " 3508,\n",
       " 3512,\n",
       " 3516,\n",
       " 3519,\n",
       " 3531,\n",
       " 3533,\n",
       " 3536,\n",
       " 3542,\n",
       " 3548,\n",
       " 3557,\n",
       " 3567,\n",
       " 3577,\n",
       " 3578,\n",
       " 3588,\n",
       " 3593,\n",
       " 3596,\n",
       " 3598,\n",
       " 3610,\n",
       " 3614,\n",
       " 3623,\n",
       " 3625,\n",
       " 3630,\n",
       " 3634,\n",
       " 3640,\n",
       " 3643,\n",
       " 3655,\n",
       " 3657,\n",
       " 3658,\n",
       " 3660,\n",
       " 3661,\n",
       " 3669,\n",
       " 3670,\n",
       " 3671,\n",
       " 3672,\n",
       " 3676,\n",
       " 3681]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Auxiliary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['common', 'noun', 'athematic'], ['toponym', 'noun', 'thematic in -o'], 4809)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_classifications(vocab, tasks, model_cls=LinearSVC, max_iter=2000):\n",
    "    classifications =  defaultdict(list)\n",
    "    for task in tasks:\n",
    "        classifier = train_classifier(task, model_cls=model_cls, max_iter=max_iter)\n",
    "        words = [w.form for w in vocab.get_words]\n",
    "        labels = use_classifier(classifier, words)\n",
    "        for word in labels.keys():\n",
    "            classifications[word].append(labels[word])\n",
    "    return classifications\n",
    "    \n",
    "classifications = get_words_classifications(my_voc, TASKS)\n",
    "classifications[\"wa-na-ka-te\"], classifications[\"a-mi-ni-so\"], len(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationsGatherer:\n",
    "    def __init__(self, data_path=os.path.join(prefix_path, \"cognates_final_translation.cog\")):\n",
    "        self.translations = defaultdict(dict)\n",
    "        self.extract_dict_from_tsv(data_path)\n",
    "        \n",
    "        transfer_exp_name = \"final_results_zero_long_term\"\n",
    "        transfer_args = self.luo_model_args(transfer_exp_name, data_path)\n",
    "        loader = self.luo_model_initialize_data(transfer_args)\n",
    "        self.luo_model_run(transfer_args, loader, transfer=True, test=False)\n",
    "\n",
    "        newdata_exp_name = \"final_results_translation_zero_long_term\"\n",
    "        newdata_args = self.luo_model_args(newdata_exp_name, data_path)\n",
    "        self.luo_model_run(transfer_args, loader, transfer=False, test=False)\n",
    "        \n",
    "    def extract_dict_from_tsv(self, tsv_path):\n",
    "        assert os.path.exists(tsv_path), \"The file does not exist\"\n",
    "        with open(tsv_path, newline='', encoding='utf-8') as tsvfile:\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "            reader.fieldnames = [field.strip() for field in reader.fieldnames]  # Clean headers\n",
    "            assert \"transliterated_linear_b\" in reader.fieldnames, \"The file does not contain the Linear B field\"\n",
    "            assert \"greek\" in reader.fieldnames, \"The file does not contain the Ancient Greek field\"\n",
    "            for row in reader:\n",
    "                key = row[\"transliterated_linear_b\"]\n",
    "                value = row[\"greek\"]\n",
    "                if key and value:\n",
    "                    self.translations[key.strip()][\"dataset\"] = value.strip()\n",
    "\n",
    "    def luo_model_args(self, exp_name, data_path):\n",
    "        log_dir = os.path.join(prefix_path, \"repo_cinese\")\n",
    "        log_dir = os.path.join(log_dir, exp_name)\n",
    "        saved_path = os.path.join(log_dir, \"saved.latest\")\n",
    "        num_lost_words = len(self.translations)\n",
    "        num_known_words = len(set(sum([self.translations[w]['dataset'].split(\"|\") for w in self.translations], [])))\n",
    "        \n",
    "        args = {\n",
    "            \"num_rounds\" : 80, # 25\n",
    "            \"num_epochs_per_M_step\" : 100, # 100\n",
    "            \"saved_path\" : saved_path, #SAVED_PATH,\n",
    "            \"learning_rate\" : 5e-3,\n",
    "            \"num_cognates\" : num_lost_words,\n",
    "            \"inc\" : 75, # changed\n",
    "            \"warm_up_steps\" : 5,\n",
    "            \"capacity\" : (3, ),\n",
    "            \"save_all\" : False,\n",
    "            \"eval_interval\" : 10, # 10\n",
    "            \"check_interval\" : 10,\n",
    "            \"cog_path\" : data_path,\n",
    "            \"char_emb_dim\" : 400, # changed\n",
    "            \"hidden_size\" : 200, # changed\n",
    "            \"num_layers\" : 16, # changed\n",
    "            \"dropout\" : 0.2, # changed\n",
    "            \"universal_charset_size\" : 400,#200,\n",
    "            \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "            \"known_lang\" : \"greek\",#greek\n",
    "            \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "            \"control_mode\" : \"relative\",\n",
    "            \"residual\" : True,\n",
    "            \"reg_hyper\" : 0.5,\n",
    "            \"batch_size\" : num_known_words, # changed\n",
    "            \"momentum\" : 0.9,\n",
    "            \"random\" : False,\n",
    "            \"seed\" : 17,\n",
    "            \"log_level\" : \"DEBUG\",\n",
    "            \"n_similar\" : 10, # changed\n",
    "            \"log_dir\": log_dir,\n",
    "            \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "            \"evaluation\": False\n",
    "        }\n",
    "\n",
    "        if args[\"gpu\"] is not None:\n",
    "            torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "        \n",
    "        if not args[\"random\"]:\n",
    "            random.seed(args[\"seed\"])\n",
    "            np.random.seed(args[\"seed\"])\n",
    "            torch.manual_seed(args[\"seed\"])\n",
    "        \n",
    "        return args\n",
    "\n",
    "    def luo_model_initialize_data(self, args):\n",
    "        clear_stages()\n",
    "        clear_vocabs()\n",
    "    \n",
    "        build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])    \n",
    "        loader = LostKnownDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"])\n",
    "        return loader        \n",
    "    \n",
    "    def luo_model_run(self, args, loader, transfer, test=False):\n",
    "        trie = Trie(args[\"known_lang\"])\n",
    "        model = DecipherModelWithFlow(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        ckpt = torch.load(args[\"saved_path\"], weights_only=False)\n",
    "        try:\n",
    "            model.load_state_dict(ckpt['model'])\n",
    "            logging.info(\"Loaded model's checkpoint\")\n",
    "        except RuntimeError as e:\n",
    "            logging.error(e)\n",
    "        \n",
    "        modes = [[\"mle\", None], [\"flow\", False], [\"flow\", True]]\n",
    "        charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS \n",
    "        data = loader.entire_batch\n",
    "        dict_key = \"luo_model\"\n",
    "        if transfer:\n",
    "            dict_key += \"_transfer\"\n",
    "            \n",
    "        for i, (mode, edit) in enumerate(modes):\n",
    "            capacity = args[\"capacity\"][0] if mode != \"mle\" else None\n",
    "            model_ret = model(data, mode=mode, num_cognates=args[\"num_cognates\"], edit=edit, capacity=capacity)\n",
    "            \n",
    "            if i == 0:\n",
    "                log_probs = model_ret.log_probs\n",
    "                log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "                predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "                reconstructed_dict = {}\n",
    "                for i, seq in enumerate(predicted_ids):\n",
    "                    decoded = \"\".join([charset[idx] for idx in seq if idx != EOW_ID])\n",
    "                    key = data.lost.forms[i]\n",
    "                    self.translations[key][f\"{dict_key}_raw\"] = decoded\n",
    "\n",
    "                    \n",
    "                \n",
    "            output = model_ret.valid_log_probs if mode == 'mle' else model_ret.flow\n",
    "            preds = output.get_best()\n",
    "            for lb_word, gr_word in preds.items():\n",
    "                self.translations[lb_word.form][f\"{dict_key}_{mode}_{edit}\"] = gr_word.form\n",
    "\n",
    "            if test:\n",
    "                test_words = [\"wa-na-ka\", \"wa-na-ka-te\", \"a-mi-ni-so\", \"ko-no-so\"]\n",
    "                for test in test_words:\n",
    "                    print(self.translations[test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tg = TranslationsGatherer()\n",
    "translations = tg.translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for k, v in translations.items():\n",
    "    words = set()\n",
    "    for src, out in v.items():\n",
    "        out = out.split(\"|\")\n",
    "        for w in out:\n",
    "            words.add(w)\n",
    "    res[k] = sorted(list(words))\n",
    "translations = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in my_voc.get_logograms:\n",
    "    log = log.form\n",
    "    translations[log] = [my_voc.logogram_vocab.get(log, log)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.restore_numerals(nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = [250, 5057, 5072, 568, 454, 4901, 5350, 4726, 5344, 4959, 4088, 4611, 4684, 5385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0nPynQ3avWF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Translation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(prefix_path,\".env\"))\n",
    "n_keys = 13\n",
    "# Retrieve the API key\n",
    "api_keys = []\n",
    "for i in range(1, n_keys+1):\n",
    "    api_keys.append(os.getenv(f\"GOOGLE_API_KEY_{i}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 250\n",
    "doc = corpus.get_document(doc_idx)\n",
    "\n",
    "def document_input(document, translations=translations, classifications=classifications):\n",
    "    seq = document.word_list\n",
    "    trans_doc = \" \".join([\"|\".join(translations[s.form]) for s in seq])\n",
    "    class_doc = []\n",
    "    for w in seq:\n",
    "        word_obj = {\"word\": w.form, \"completeness level\": w.completeness.name}\n",
    "        if w.form in classifications:\n",
    "            classes = classifications[w.form]\n",
    "            pos = classes[1]\n",
    "            word_obj[\"part of speech\"] = pos\n",
    "            if pos in {\"noun\", \"adjective\"}:\n",
    "                word_obj[\"noun type\"] = classes[0]\n",
    "                word_obj[\"inflection\"] = classes[2]\n",
    "        class_doc.append(word_obj)\n",
    "    return document.form, trans_doc, class_doc\n",
    "\n",
    "def sequence_input(sequence, translations=translations, classifications=classifications, vocab=my_voc):\n",
    "    seq = sequence.split(\" \")\n",
    "    trans_seq = \" \".join([\"|\".join(translations[s]) for s in seq])\n",
    "    class_seq = []\n",
    "    for w in seq:\n",
    "        w = vocab.get_word(vocab.get_form_idx(w))\n",
    "        word_obj = {\"word\": w.form, \"completeness level\": w.completeness.name}\n",
    "        if w.form in classifications:\n",
    "            classes = classifications[w.form]\n",
    "            pos = classes[1]\n",
    "            word_obj[\"part of speech\"] = pos\n",
    "            if pos in {\"noun\", \"adjective\"}:\n",
    "                word_obj[\"noun type\"] = classes[0]\n",
    "                word_obj[\"inflection\"] = classes[2]\n",
    "        class_seq.append(word_obj)\n",
    "    return trans_seq, class_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation_prompt(linear_b_document, translations, out_classifiers, api_key):\n",
    "    \"\"\"\n",
    "    Prompt for translating complete Linear B documents into Ancient Greek and English.\n",
    "\n",
    "    Args:\n",
    "        linear_b_document: str - Complete Linear B document text\n",
    "        translations: str - Pipe-separated translation options for each word\n",
    "        out_classifiers: list - List of dicts with word classifications and completeness\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with Ancient Greek translation, English translation, and reasoning\n",
    "    \"\"\"\n",
    "\n",
    "    # You'll need to define this mapping between Linear B syllabograms and Greek characters\n",
    "    syllabograms_matching = {\n",
    "      \"a\": [\"\"],\n",
    "      \"e\": [\"\", \"\"],\n",
    "      \"i\": [\"\"],\n",
    "      \"o\": [\"\", \"\"],\n",
    "      \"u\": [\"\"],\n",
    "      \"da\": [\"\", \"\"],\n",
    "      \"de\": [\"\", \"\", \"\"],\n",
    "      \"di\": [\"\", \"\"],\n",
    "      \"do\": [\"\", \"\", \"\"],\n",
    "      \"du\": [\"\", \"\"],\n",
    "      \"dwe\": [\"\", \"f\", \"\"],\n",
    "      \"dwo\": [\"\", \"f\", \"\"],\n",
    "      \"ja\": [\"\", \"\"],\n",
    "      \"je\": [\"\", \"\", \"\"],\n",
    "      \"jo\": [\"\", \"\", \"\"],\n",
    "      \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ma\": [\"\", \"\", \"\"],\n",
    "      \"me\": [\"\", \"\", \"\"],\n",
    "      \"mi\": [\"\", \"\"],\n",
    "      \"mo\": [\"\", \"\"],\n",
    "      \"mu\": [\"\", \"\"],\n",
    "      \"na\": [\"\", \"\"],\n",
    "      \"ne\": [\"\", \"\", \"\"],\n",
    "      \"ni\": [\"\", \"\"],\n",
    "      \"no\": [\"\", \"\", \"\"],\n",
    "      \"nu\": [\"\", \"\"],\n",
    "      \"nwa\": [\"f\", \"\", \"\"],\n",
    "      \"pa\": [\"\", \"\", \"\"],\n",
    "      \"pe\": [\"\", \"\", \"\"],\n",
    "      \"pi\": [\"\", \"\", \"\"],\n",
    "      \"po\": [\"\", \"\", \"\"],\n",
    "      \"pu\": [\"\", \"\", \"\"],\n",
    "      \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "      \"phu\": [\"\", \"\"],\n",
    "      \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "      \"re\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ri\": [\"\", \"\", \"\"],\n",
    "      \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ru\": [\"\", \"\", \"\"],\n",
    "      \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "      \"rai\": [\"\", \"\", \"\"],\n",
    "      \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "      \"sa\": [\"\", \"\"],\n",
    "      \"se\": [\"\", \"\", \"\"],\n",
    "      \"si\": [\"\", \"\"],\n",
    "      \"so\": [\"\", \"\", \"\"],\n",
    "      \"su\": [\"\", \"\"],\n",
    "      \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "      \"te\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ti\": [\"\", \"\", \"\"],\n",
    "      \"to\": [\"\", \"\", \"\", \"\"],\n",
    "      \"tu\": [\"\", \"\", \"\"],\n",
    "      \"tya\": [\"\", \"\", \"\"],\n",
    "      \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"wa\": [\"\", \"f\", \"\"],\n",
    "      \"we\": [\"\", \"f\", \"\"],\n",
    "      \"wi\": [\"\", \"f\"],\n",
    "      \"wo\": [\"\", \"f\", \"\"],\n",
    "      \"za\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ze\": [\"\", \"\", \"\"],\n",
    "      \"zo\": [\"\", \"\", \"\"],\n",
    "      \"ha\": [\"\", \"h\"],\n",
    "      \"ai\": [\"\", \"\"],\n",
    "      \"au\": [\"\", \"\"],\n",
    "      \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "      \"*64\": [\"\", \"\", \"f\"],\n",
    "      \"*65\": [\"\", \"\"],\n",
    "      \"*79\": [\"\", \"\"],\n",
    "      \"*82\": [\"\", \"\", \"f\"]\n",
    "    }\n",
    "    str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"\"\"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\n",
    "    Linear B texts are primarily administrative records found at palatial sites like Knossos, Pylos, and Thebes.\"\"\"\n",
    "\n",
    "    # Syllabogram matching context\n",
    "    syll_matching_context = f\"\"\"Consider this mapping between Linear B syllabograms and Ancient Greek characters: {str_syllabogram_matching}\n",
    "\n",
    "    CRITICAL: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! \n",
    "    Example: a-ko-so-ne   (axes)\"\"\"\n",
    "\n",
    "    # Morphological markers\n",
    "    morphological_markers = \"\"\"CRITICAL MORPHOLOGICAL MARKERS:\n",
    "    \n",
    "    SUFFIXES - Key grammatical markers that preserve relationships:\n",
    "     -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\n",
    "     -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\n",
    "     -de: can be either:\n",
    "       - Negative prefix meaning \"not, on the other side\"\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\n",
    "     -pi: instrumental/locative suffix\n",
    "\n",
    "    PREFIXES - Establish semantic domains and derivational patterns:\n",
    "     po-ro-: - prefix meaning \"before, forward, in front of\"\n",
    "     qe-to-ro-: - numerical prefix meaning \"four\"\n",
    "     we-: - numerical prefix meaning \"six\"; AMBIGUOUS\n",
    "     e-ne-wo-: - numerical prefix meaning \"nine\"\n",
    "     a-pu-: - separative prefix meaning \"from, away from, off\"\n",
    "     jo-: - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\"\"\"\n",
    "\n",
    "    # Create comprehensive grammatical information\n",
    "    def create_grammatical_info():\n",
    "        gramm = ET.Element(\"grammatical_information\")\n",
    "        \n",
    "        # Enhanced declension table\n",
    "        decl_section = ET.SubElement(gramm, \"declension_system\")\n",
    "        decl_intro = ET.SubElement(decl_section, \"introduction\")\n",
    "        decl_intro.text = \"\"\"Mycenaean Linear B declension system based on Ancient Greek patterns. \n",
    "        Columns 1-2: Ancient Greek second declension (thematic -o stems)\n",
    "        Columns 3-4: Ancient Greek first declension (thematic -a stems) \n",
    "        Columns 5-6: Ancient Greek third declension (athematic stems)\n",
    "        Same patterns apply to adjectives: first four columns for first-class adjectives, last two for second-class.\"\"\"\n",
    "        \n",
    "        decl_table = ET.SubElement(decl_section, \"declension_table\")\n",
    "        decl_table.text = '''It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "'''\n",
    "        # Verb system\n",
    "        verbs_section = ET.SubElement(gramm, \"verb_system\")\n",
    "        verb_rules = {\n",
    "            \"third_person_singular\": \"Final syllabogram ending with vowel -e\",\n",
    "            \"third_person_plural\": \"Final syllabogram is -si\",\n",
    "            \"infinitive\": \"Final syllabogram ending with vowel -e (optionally additional -e)\",\n",
    "            \"active_participle\": \"Terminates with -o (singular - suffix in Greek, plural follows athematic pattern, e.g., -o-te   from )\",\n",
    "            \"middle_passive_participle\": \"Terminates with -me-no/-me-na suffixes (Greek -/-/-)\"\n",
    "        }\n",
    "        \n",
    "        for rule_key, rule_text in verb_rules.items():\n",
    "            ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "        # Adjective system\n",
    "        adj_section = ET.SubElement(gramm, \"adjective_system\")\n",
    "        ET.SubElement(adj_section, \"thematic_adjectives\").text = \"Follow same declension patterns as thematic nouns\"\n",
    "        ET.SubElement(adj_section, \"athematic_adjectives\").text = \"Follow athematic noun patterns with variable nominative forms\"\n",
    "        \n",
    "        return ET.tostring(gramm, encoding=\"unicode\")\n",
    "\n",
    "    # Create the main prompt structure\n",
    "    def create_main_prompt():\n",
    "        prompt = ET.Element(\"translation_task\")\n",
    "\n",
    "        # Task description\n",
    "        task_desc = ET.SubElement(prompt, \"task_description\")\n",
    "        task_desc.text = \"\"\"You are translating complete Linear B documents composed of words and logographic signs.\n",
    "Each Linear B word has possible Ancient Greek translation options (separated by |) and grammatical classifications:\n",
    " part_of_speech: grammatical function in {noun, adjective, verb, adverb} (participles labeled as adjectives)\n",
    " noun_type: logical function in {proper, toponym, ethnonym, common}\n",
    " inflection: morphological pattern in {thematic in -o, thematic in -a, athematic}\n",
    " completeness_level: preservation state in {COMPLETE, MOSTLY_COMPLETE, UNCERTAIN, MOSTLY_INCOMPLETE, INCOMPLETE}\n",
    "Your task is to produce complete translations into both Ancient Greek and modern English with detailed linguistic analysis.\n",
    "Please note that the Linear B document can be fragmentary. Fragments will be indicated by a '[...]' separator. Nearby words can be incomplete.\"\"\"\n",
    "\n",
    "        # Selection methodology\n",
    "        methodology = ET.SubElement(prompt, \"selection_methodology\")\n",
    "        \n",
    "        ET.SubElement(methodology, \"grammatical_analysis\").text = \"\"\"Analyze each word's grammatical function as nouns, adjectives, verbs, adverbs, prepositions, etc. Consider morphological patterns and declensional endings.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"syntactic_analysis\").text = \"\"\"Identify sentence structure: subject, predicate, complements. Perform logical analysis of each word's role in the sentence.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"discourse_analysis\").text = \"\"\"Analyze document structure, distinguish different sentences, examine subordinate clauses, and establish logical connections for accurate translation.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"linguistic_authenticity\").text = \"\"\"Prioritize words consistent with attested Mycenaean Greek vocabulary and morphological patterns. Verify plausibility of proposed Mycenaean forms.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"semantic_coherence\").text = \"\"\"Evaluate which translation produces the most meaningful complete sentence, considering the administrative and practical nature of most Linear B texts.\"\"\"\n",
    "\n",
    "        # Output requirements\n",
    "        output_req = ET.SubElement(prompt, \"output_requirements\")\n",
    "        output_req.text = \"\"\"MANDATORY: Respond ONLY with valid JSON containing exactly these fields:\n",
    "        {\n",
    "            \"Ancient Greek translation\": \"complete document translated into Ancient Greek with proper diacritics and error recovery\",\n",
    "            \"English translation\": \"complete document translated into modern English\",\n",
    "            \"reasoning\": \"detailed explanation covering grammatical analysis, logical/syntactic analysis, and subordinate clause analysis that justifies the translation decisions\"\n",
    "        }\n",
    "        No additional text, commentary, or formatting permitted.\"\"\"\n",
    "\n",
    "        # Quality assurance\n",
    "        qa_section = ET.SubElement(prompt, \"quality_assurance\")\n",
    "        qa_section.text = \"\"\"Before finalizing translation:\n",
    "        1. Verify grammatical correctness of Ancient Greek translation with proper case agreements\n",
    "        2. Ensure semantic coherence and contextual appropriateness\n",
    "        3. Confirm reasoning addresses grammatical, logical, and syntactic criteria\n",
    "        4. Validate proper JSON formatting with all required fields\n",
    "        5. Check that both translations capture the administrative/documentary nature of Linear B texts\n",
    "        6. ALWAYS provide complete translations - never leave empty or partial responses\"\"\"\n",
    "\n",
    "        return ET.tostring(prompt, encoding=\"unicode\")\n",
    "\n",
    "    # Training examples section with complete translations\n",
    "    def create_examples_section():\n",
    "        examples = ET.Element(\"examples_section\")\n",
    "        \n",
    "        # Example 1: PY Eb 895+906\n",
    "        example_1 = ET.SubElement(examples, \"example_1\")\n",
    "        sequence = \"a-i-qe-u e-ke-qe ke-ke-me-na ko-to-na ko-to-no-o-ko to-so-de pe-mo GRA T 6\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_1, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_1, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_1, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_1, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"          6\",\n",
    "    \"English translation\": \"Aigeus, the plot owner, who owns a communal plot; so much grain: 6 'T' units of wheat.\",\n",
    "    \"reasoning\": \"Grammatical analysis: a-i-qe-u () is a proper noun in nominative case, anthroponym with thematic inflection. e-ke-qe ( ) contains the verb 'to have' in 3rd person singular plus conjunction 'and'. ke-ke-me-na () is a perfect passive participle agreeing with ko-to-na () in accusative plural. ko-to-no-o-ko () is a compound noun meaning 'plot-holder'. Logical analysis: Subject = Aigeus, Verb = has, Objects = plots (communal) and grain. Syntactic structure follows standard administrative record format typical of Linear B land tenure documents.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 2: PY Ta 711\n",
    "        example_2 = ET.SubElement(examples, \"example_2\")\n",
    "        sequence = \"o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_2, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_2, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_2, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_2, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"-      \",\n",
    "    \"English translation\": \"Phygekles witnessed when the king appointed Augeus as damoklos.\",\n",
    "    \"reasoning\": \"Grammatical analysis: o-wi-de (-) is 3rd person singular aorist of  meaning 'witnessed/saw'. phu-ke-qi-ri () is anthroponym in nominative. o-te () is temporal conjunction 'when'. wa-na-ka () is title 'king' in nominative. te-ke () is 3rd person singular aorist of  'placed/appointed'. au-ke-wa () is anthroponym in accusative. da-mo-ko-ro () is title/office in accusative. Logical analysis: Main clause has Phygekles as subject witnessing; subordinate temporal clause shows king appointing Augeus to office. Subordinate analysis reveals administrative appointment record with witness attestation.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 3: PY An 657\n",
    "        example_3 = ET.SubElement(examples, \"example_3\")\n",
    "        sequence = \"me-ta-qe pe-i e-qe-ta ke-ki-jo\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_3, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_3, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_3, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_3, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"    \",\n",
    "    \"English translation\": \"And with them Kerkios, the epetas (follower/companion).\",\n",
    "    \"reasoning\": \"Grammatical analysis: me-ta-qe ( ) contains preposition 'with' plus conjunction 'and'. pe-i () is 3rd person plural pronoun in dative 'them'. e-qe-ta () is military/administrative title in accusative, derived from  'to follow'. ke-ki-jo () is anthroponym in nominative. Logical analysis: Prepositional phrase with pronoun reference, followed by personal name and title in apposition. Syntactic structure indicates military or administrative personnel listing, common in Linear B records. The conjunction -qe links this entry to previous entries in the document series.\"\n",
    "}\"\"\"\n",
    "        \n",
    "        return ET.tostring(examples, encoding=\"unicode\")\n",
    "\n",
    "    examples_xml = create_examples_section()\n",
    "\n",
    "    # Build conversation history with comprehensive examples\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the historical context of Linear B and Mycenaean Greek.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will apply the syllabogram-to-character mapping carefully, especially for double consonants.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Extract grammatical information from this XML:\\n{create_grammatical_info()}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will use this declension and grammatical information to validate predictions and analyze Linear B sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': morphological_markers}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will carefully consider these prefixes and suffixes as morphological markers.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': create_main_prompt()}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the translation task methodology and output requirements.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Study these translation examples:\\n{examples_xml}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I have studied the examples and understand the expected format for Ancient Greek translation, English translation, and detailed reasoning that includes grammatical, logical, and syntactic analysis. I will do the same on your input.'}]}\n",
    "    ]\n",
    "\n",
    "    # Generate input data XML for your actual data structure\n",
    "    def create_input_data():\n",
    "        input_data = ET.Element(\"input_data\")\n",
    "        \n",
    "        # Document text\n",
    "        document = ET.SubElement(input_data, \"document\")\n",
    "        ET.SubElement(document, \"linear b\").text = linear_b_document\n",
    "        ET.SubElement(document, \"ancient greek translations\").text = translations\n",
    "        ET.SubElement(document, \"classifiers info\").text = json.dumps(out_classifiers, ensure_ascii=False)\n",
    "        \n",
    "        return ET.tostring(input_data, encoding=\"unicode\")\n",
    "\n",
    "    # Configure and use Gemini\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.1,     # Slightly increased for better reasoning\n",
    "            top_p=0.95,          # Slightly more focused\n",
    "            top_k=40,             # Allow more vocabulary options\n",
    "            response_mime_type=\"application/json\"  # <- enforce JSON\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(create_input_data())\n",
    "    logging.debug(f\"Gemini response: {response.text}\")\n",
    "    \n",
    "    return quick_parse_gemini_response(response.text.strip())\n",
    "\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Helper function to parse Gemini JSON response\"\"\"\n",
    "    # Clean up the response if needed\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:-3]\n",
    "    elif response_text.startswith('```'):\n",
    "        response_text = response_text[3:-3]\n",
    "        \n",
    "    return json.loads(response_text)\n",
    "\n",
    "# Call the function:\n",
    "# s, t, c = document_input(corpus.get_document_no(0))\n",
    "# make_translation_prompt(s, t, c, api_keys[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation_prompt(linear_b_document, translations, out_classifiers, api_key):\n",
    "    \"\"\"\n",
    "    Prompt for translating complete Linear B documents into Ancient Greek and English.\n",
    "\n",
    "    Args:\n",
    "        linear_b_document: str - Complete Linear B document text\n",
    "        translations: str - Pipe-separated translation options for each word\n",
    "        out_classifiers: list - List of dicts with word classifications and completeness\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with Ancient Greek translation, English translation, and reasoning\n",
    "    \"\"\"\n",
    "\n",
    "    # You'll need to define this mapping between Linear B syllabograms and Greek characters\n",
    "    syllabograms_matching = {\n",
    "      \"a\": [\"\"],\n",
    "      \"e\": [\"\", \"\"],\n",
    "      \"i\": [\"\"],\n",
    "      \"o\": [\"\", \"\"],\n",
    "      \"u\": [\"\"],\n",
    "      \"da\": [\"\", \"\"],\n",
    "      \"de\": [\"\", \"\", \"\"],\n",
    "      \"di\": [\"\", \"\"],\n",
    "      \"do\": [\"\", \"\", \"\"],\n",
    "      \"du\": [\"\", \"\"],\n",
    "      \"dwe\": [\"\", \"f\", \"\"],\n",
    "      \"dwo\": [\"\", \"f\", \"\"],\n",
    "      \"ja\": [\"\", \"\"],\n",
    "      \"je\": [\"\", \"\", \"\"],\n",
    "      \"jo\": [\"\", \"\", \"\"],\n",
    "      \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ma\": [\"\", \"\", \"\"],\n",
    "      \"me\": [\"\", \"\", \"\"],\n",
    "      \"mi\": [\"\", \"\"],\n",
    "      \"mo\": [\"\", \"\"],\n",
    "      \"mu\": [\"\", \"\"],\n",
    "      \"na\": [\"\", \"\"],\n",
    "      \"ne\": [\"\", \"\", \"\"],\n",
    "      \"ni\": [\"\", \"\"],\n",
    "      \"no\": [\"\", \"\", \"\"],\n",
    "      \"nu\": [\"\", \"\"],\n",
    "      \"nwa\": [\"f\", \"\", \"\"],\n",
    "      \"pa\": [\"\", \"\", \"\"],\n",
    "      \"pe\": [\"\", \"\", \"\"],\n",
    "      \"pi\": [\"\", \"\", \"\"],\n",
    "      \"po\": [\"\", \"\", \"\"],\n",
    "      \"pu\": [\"\", \"\", \"\"],\n",
    "      \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "      \"phu\": [\"\", \"\"],\n",
    "      \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "      \"re\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ri\": [\"\", \"\", \"\"],\n",
    "      \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ru\": [\"\", \"\", \"\"],\n",
    "      \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "      \"rai\": [\"\", \"\", \"\"],\n",
    "      \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "      \"sa\": [\"\", \"\"],\n",
    "      \"se\": [\"\", \"\", \"\"],\n",
    "      \"si\": [\"\", \"\"],\n",
    "      \"so\": [\"\", \"\", \"\"],\n",
    "      \"su\": [\"\", \"\"],\n",
    "      \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "      \"te\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ti\": [\"\", \"\", \"\"],\n",
    "      \"to\": [\"\", \"\", \"\", \"\"],\n",
    "      \"tu\": [\"\", \"\", \"\"],\n",
    "      \"tya\": [\"\", \"\", \"\"],\n",
    "      \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"wa\": [\"\", \"f\", \"\"],\n",
    "      \"we\": [\"\", \"f\", \"\"],\n",
    "      \"wi\": [\"\", \"f\"],\n",
    "      \"wo\": [\"\", \"f\", \"\"],\n",
    "      \"za\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ze\": [\"\", \"\", \"\"],\n",
    "      \"zo\": [\"\", \"\", \"\"],\n",
    "      \"ha\": [\"\", \"h\"],\n",
    "      \"ai\": [\"\", \"\"],\n",
    "      \"au\": [\"\", \"\"],\n",
    "      \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "      \"*64\": [\"\", \"\", \"f\"],\n",
    "      \"*65\": [\"\", \"\"],\n",
    "      \"*79\": [\"\", \"\"],\n",
    "      \"*82\": [\"\", \"\", \"f\"]\n",
    "    }\n",
    "    str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"\"\"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\n",
    "    Linear B texts are primarily administrative records found at palatial sites like Knossos, Pylos, and Thebes.\"\"\"\n",
    "\n",
    "    # Syllabogram matching context\n",
    "    syll_matching_context = f\"\"\"Consider this mapping between Linear B syllabograms and Ancient Greek characters: {str_syllabogram_matching}\n",
    "\n",
    "    CRITICAL: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! \n",
    "    Example: a-ko-so-ne   (axes)\"\"\"\n",
    "\n",
    "    # Morphological markers\n",
    "    morphological_markers = \"\"\"CRITICAL MORPHOLOGICAL MARKERS:\n",
    "    \n",
    "    SUFFIXES - Key grammatical markers that preserve relationships:\n",
    "     -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\n",
    "     -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\n",
    "     -de: can be either:\n",
    "       - Negative prefix meaning \"not, on the other side\"\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\n",
    "     -pi: instrumental/locative suffix\n",
    "\n",
    "    PREFIXES - Establish semantic domains and derivational patterns:\n",
    "     po-ro-: - prefix meaning \"before, forward, in front of\"\n",
    "     qe-to-ro-: - numerical prefix meaning \"four\"\n",
    "     we-: - numerical prefix meaning \"six\"; AMBIGUOUS\n",
    "     e-ne-wo-: - numerical prefix meaning \"nine\"\n",
    "     a-pu-: - separative prefix meaning \"from, away from, off\"\n",
    "     jo-: - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\"\"\"\n",
    "\n",
    "    # Create comprehensive grammatical information\n",
    "    def create_grammatical_info():\n",
    "        gramm = ET.Element(\"grammatical_information\")\n",
    "        \n",
    "        # Enhanced declension table\n",
    "        decl_section = ET.SubElement(gramm, \"declension_system\")\n",
    "        decl_intro = ET.SubElement(decl_section, \"introduction\")\n",
    "        decl_intro.text = \"\"\"Mycenaean Linear B declension system based on Ancient Greek patterns. \n",
    "        Columns 1-2: Ancient Greek second declension (thematic -o stems)\n",
    "        Columns 3-4: Ancient Greek first declension (thematic -a stems) \n",
    "        Columns 5-6: Ancient Greek third declension (athematic stems)\n",
    "        Same patterns apply to adjectives: first four columns for first-class adjectives, last two for second-class.\"\"\"\n",
    "        \n",
    "        decl_table = ET.SubElement(decl_section, \"declension_table\")\n",
    "        decl_table.text = '''It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "'''\n",
    "        # Verb system\n",
    "        verbs_section = ET.SubElement(gramm, \"verb_system\")\n",
    "        verb_rules = {\n",
    "            \"third_person_singular\": \"Final syllabogram ending with vowel -e\",\n",
    "            \"third_person_plural\": \"Final syllabogram is -si\",\n",
    "            \"infinitive\": \"Final syllabogram ending with vowel -e (optionally additional -e)\",\n",
    "            \"active_participle\": \"Terminates with -o (singular - suffix in Greek, plural follows athematic pattern, e.g., -o-te   from )\",\n",
    "            \"middle_passive_participle\": \"Terminates with -me-no/-me-na suffixes (Greek -/-/-)\"\n",
    "        }\n",
    "        \n",
    "        for rule_key, rule_text in verb_rules.items():\n",
    "            ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "        # Adjective system\n",
    "        adj_section = ET.SubElement(gramm, \"adjective_system\")\n",
    "        ET.SubElement(adj_section, \"thematic_adjectives\").text = \"Follow same declension patterns as thematic nouns\"\n",
    "        ET.SubElement(adj_section, \"athematic_adjectives\").text = \"Follow athematic noun patterns with variable nominative forms\"\n",
    "        \n",
    "        return ET.tostring(gramm, encoding=\"unicode\")\n",
    "\n",
    "    # Create the main prompt structure\n",
    "    def create_main_prompt():\n",
    "        prompt = ET.Element(\"translation_task\")\n",
    "\n",
    "        # Task description\n",
    "        task_desc = ET.SubElement(prompt, \"task_description\")\n",
    "        task_desc.text = \"\"\"You are translating complete Linear B documents composed of words and logographic signs.\n",
    "Each Linear B word has possible Ancient Greek translation options (separated by |) and grammatical classifications:\n",
    " part_of_speech: grammatical function in {noun, adjective, verb, adverb} (participles labeled as adjectives)\n",
    " noun_type: logical function in {proper, toponym, ethnonym, common}\n",
    " inflection: morphological pattern in {thematic in -o, thematic in -a, athematic}\n",
    " completeness_level: preservation state in {COMPLETE, MOSTLY_COMPLETE, UNCERTAIN, MOSTLY_INCOMPLETE, INCOMPLETE}\n",
    "Your task is to produce complete translations into both Ancient Greek and modern English with detailed linguistic analysis.\n",
    "Please note that the Linear B document can be fragmentary. Fragments are divided by a '[...]' separator. Nearby words can be incomplete.\n",
    "Notice that within each document, similar patterns occur. If you are sure about how a sentence is built, you can try to extend its structure to nearby sentences.\"\"\"\n",
    "\n",
    "        # Selection methodology\n",
    "        methodology = ET.SubElement(prompt, \"selection_methodology\")\n",
    "        \n",
    "        ET.SubElement(methodology, \"grammatical_analysis\").text = \"\"\"Analyze each word's grammatical function as nouns, adjectives, verbs, adverbs, prepositions, etc. Consider morphological patterns and declensional endings.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"syntactic_analysis\").text = \"\"\"Identify sentence structure: subject, predicate, complements. Perform logical analysis of each word's role in the sentence.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"discourse_analysis\").text = \"\"\"Analyze document structure, distinguish different sentences, examine subordinate clauses, and establish logical connections for accurate translation.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"linguistic_authenticity\").text = \"\"\"Prioritize words consistent with attested Mycenaean Greek vocabulary and morphological patterns. Verify plausibility of proposed Mycenaean forms.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"semantic_coherence\").text = \"\"\"Evaluate which translation produces the most meaningful complete sentence, considering the administrative and practical nature of most Linear B texts.\"\"\"\n",
    "\n",
    "        # Output requirements\n",
    "        output_req = ET.SubElement(prompt, \"output_requirements\")\n",
    "        output_req.text = \"\"\"MANDATORY: Respond ONLY with valid JSON containing exactly these fields:\n",
    "        {\n",
    "            \"Ancient Greek translation\": \"complete document translated into Ancient Greek with proper diacritics and error recovery\",\n",
    "            \"English translation\": \"complete document translated into modern English\",\n",
    "            \"reasoning\": \"detailed explanation covering grammatical analysis, logical/syntactic analysis, and subordinate clause analysis that justifies the translation decisions\"\n",
    "        }\n",
    "        No additional text, commentary, or formatting permitted.\"\"\"\n",
    "\n",
    "        # Quality assurance\n",
    "        qa_section = ET.SubElement(prompt, \"quality_assurance\")\n",
    "        qa_section.text = \"\"\"Before finalizing translation:\n",
    "1. Verify grammatical correctness of Ancient Greek translation with proper case agreements\n",
    "2. Ensure semantic coherence and contextual appropriateness\n",
    "3. Confirm reasoning addresses grammatical, logical, and syntactic criteria\n",
    "4. Validate proper JSON formatting with all required fields\n",
    "5. Check that both translations capture the administrative/documentary nature of Linear B texts\n",
    "6. Use the Chain Of Thoughts to capture all logical patterns inside the document. Reiterate translations grammatical, logical, syntactical, and translation steps until a clear and correct result is obtained. Do not try to translate units of measures, indicated by isolated uppercase letters.\n",
    "7. ALWAYS provide complete translations - never leave empty or partial responses\"\"\"\n",
    "\n",
    "        return ET.tostring(prompt, encoding=\"unicode\")\n",
    "\n",
    "    # Training examples section with complete translations\n",
    "    def create_examples_section():\n",
    "        examples = ET.Element(\"examples_section\")\n",
    "        \n",
    "        # Example 1: PY Eb 895+906\n",
    "        example_1 = ET.SubElement(examples, \"example_1\")\n",
    "        sequence = \"a-i-qe-u e-ke-qe ke-ke-me-na ko-to-na ko-to-no-o-ko to-so-de pe-mo GRA T 6\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_1, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_1, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_1, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_1, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"          6\",\n",
    "    \"English translation\": \"Aigeus, the plot owner, who owns a communal plot; so much grain: 6 'T' units of wheat.\",\n",
    "    \"reasoning\": \"Grammatical analysis: a-i-qe-u () is a proper noun in nominative case, anthroponym with thematic inflection. e-ke-qe ( ) contains the verb 'to have' in 3rd person singular plus conjunction 'and'. ke-ke-me-na () is a perfect passive participle agreeing with ko-to-na () in accusative plural. ko-to-no-o-ko () is a compound noun meaning 'plot-holder'. Logical analysis: Subject = Aigeus, Verb = has, Objects = plots (communal) and grain. Syntactic structure follows standard administrative record format typical of Linear B land tenure documents.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 2: PY Ta 711\n",
    "        example_2 = ET.SubElement(examples, \"example_2\")\n",
    "        sequence = \"o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_2, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_2, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_2, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_2, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"-      \",\n",
    "    \"English translation\": \"Phygekles witnessed when the king appointed Augeus as damoklos.\",\n",
    "    \"reasoning\": \"Grammatical analysis: o-wi-de (-) is 3rd person singular aorist of  meaning 'witnessed/saw'. phu-ke-qi-ri () is anthroponym in nominative. o-te () is temporal conjunction 'when'. wa-na-ka () is title 'king' in nominative. te-ke () is 3rd person singular aorist of  'placed/appointed'. au-ke-wa () is anthroponym in accusative. da-mo-ko-ro () is title/office in accusative. Logical analysis: Main clause has Phygekles as subject witnessing; subordinate temporal clause shows king appointing Augeus to office. Subordinate analysis reveals administrative appointment record with witness attestation.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 3: PY Ae 303\n",
    "        example_3 = ET.SubElement(examples, \"example_3\")\n",
    "        sequence = \"i-je-ro-jo pu-ro i-je-re-ja do-e-ra e-ne-ka ku-ru-so-jo MUL 14\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_3, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_3, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_3, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_3, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \":       14\",\n",
    "    \"English translation\": \"Pylos: slaves of the priestess for the sake of sacred gold: 14 women.\",\n",
    "    \"reasoning\": \"Grammatical analysis: pu-ro is a topical header (Pylos). do-e-ra =  (fem. pl.) 'female slaves'; i-je-re-ja () provides genitival possession ('of the priestess'). e-ne-ka () governs the genitive ku-ru-so-jo ( 'of gold'), with i-je-ro-jo ( 'sacred') as attributive genitive  'for the sake of sacred gold'. MUL 14 is the logogram and tally for women (14). Logical/Syntactic analysis: nominal, paratactic inventory entry with structure [Topic] : [Possessed group] [Purpose PP] [Logographic count]; no finite verb required. Subordinate clause analysis: none present. Notes: respects administrative roster style, uses -jo genitives and +GEN, treats logogram as a counted NP head.\"\n",
    "}\"\"\"\n",
    "        \n",
    "        return ET.tostring(examples, encoding=\"unicode\")\n",
    "\n",
    "    examples_xml = create_examples_section()\n",
    "\n",
    "    # Build conversation history with comprehensive examples\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the historical context of Linear B and Mycenaean Greek.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will apply the syllabogram-to-character mapping carefully, especially for double consonants.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Extract grammatical information from this XML:\\n{create_grammatical_info()}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will use this declension and grammatical information to validate predictions and analyze Linear B sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': morphological_markers}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will carefully consider these prefixes and suffixes as morphological markers.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': create_main_prompt()}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the translation task methodology and output requirements.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Study these translation examples:\\n{examples_xml}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I have studied the examples and understand the expected format for Ancient Greek translation, English translation, and detailed reasoning that includes grammatical, logical, and syntactic analysis. I will do the same on your input.'}]}\n",
    "    ]\n",
    "\n",
    "    # Generate input data XML for your actual data structure\n",
    "    def create_input_data():\n",
    "        input_data = ET.Element(\"input_data\")\n",
    "        \n",
    "        # Document text\n",
    "        document = ET.SubElement(input_data, \"document\")\n",
    "        ET.SubElement(document, \"linear b\").text = linear_b_document\n",
    "        ET.SubElement(document, \"ancient greek translations\").text = translations\n",
    "        ET.SubElement(document, \"classifiers info\").text = json.dumps(out_classifiers, ensure_ascii=False)\n",
    "        \n",
    "        return ET.tostring(input_data, encoding=\"unicode\")\n",
    "\n",
    "    # Configure and use Gemini\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.1,     # Slightly increased for better reasoning\n",
    "            top_p=0.95,          # Slightly more focused\n",
    "            top_k=40,             # Allow more vocabulary options\n",
    "            response_mime_type=\"application/json\"  # <- enforce JSON\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(create_input_data())\n",
    "    logging.debug(f\"Gemini response: {response.text}\")\n",
    "    \n",
    "    return quick_parse_gemini_response(response.text.strip())\n",
    "\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Helper function to parse Gemini JSON response\"\"\"\n",
    "    # Clean up the response if needed\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:-3]\n",
    "    elif response_text.startswith('```'):\n",
    "        response_text = response_text[3:-3]\n",
    "        \n",
    "    return json.loads(response_text)\n",
    "\n",
    "# Call the function:\n",
    "# s, t, c = document_input(corpus.get_document_no(0))\n",
    "# make_translation_prompt(s, t, c, api_keys[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Document: 4774\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "DEBUG: ma-ro-pi qe-re-wa-o pa-ra-jo OVISm 136 ma-ro-pi to-ro-wi-ko pa-ra-jo OVISm 133 [...] ma-ro-pi ke-ro-wo-jo OVISm 85 ma-ro-pi ra-pa-sa-ko-jo OVISm 69 ma-ro-pi pu-wi-no a-pi-me-de-o a-ko-ra OVISm 190 ma-ro-pi i-wa-so we-da-ne-wo a-ko-ra OVISm 70 ma-ro-pi ti-re-wo pa-ra-jo OVISm 70 ma-ro-pi o-ka-ri-jo pa-ra-jo OVISm 95 ma-ro-pi e-ti-ra-wo pa-ra-jo OVISm 70 ma-ro-pi a-ta-ma-ne-u pa-ra-jo OVISm 60 ma-ro-pi qi-ri-ta-ko a-ke-o-jo a-ko-ra OVISm 90 [...] ma-ro-pi a-ri-wo a-ke-o-jo a-ko-ra 14 [...] ma-ro-pi ro-ko-jo we-da-ne-wo a-ko-ra 80 [...] ma-ro-pi o-pe-re-ta we-da-ne-wo OVISf 86 ma-ro-pi po-ro-qa-ta-jo we-da-ne-wo OVISf 63 ma-ro-pi to-ru-ko-ro we-da-ne-wo OVISf 88 ma-ro-pi [...] ma-ma-ro we-da-ne-wo OVISm 90 ma-ro-pi ma-du-ro we-da-ne-wo OVISm 100 ma-ro-pi se-no we-da-ne-wo OVISf 40 ma-ro-pi ta-ta-ke-u we-da-ne-wo [...] OVISf 30\n",
      "Error occurred: block_reason: PROHIBITED_CONTENT\n",
      ". Retrying in 65 seconds...\n",
      "Current Document: 4775\n",
      "Current Document: 4941\n",
      "Current Document: 4967\n",
      "Current Document: 5401\n",
      "document: qe-qi-no-me-no e-re-pa [...] a-no-po a-ko-so-ta ZE e-wi-su-*79-ko 4 ro-i-ko 2\n",
      "answer: {'Ancient Greek translation': ' . [...] , .  f 4,  2.', 'English translation': 'Decorated ivory/elephant. [...] Anopos, Axotas. 4 enclosed pairs, 2 crooked items.', 'reasoning': \"Grammatical analysis: The document appears to be an administrative record listing items and individuals. 'qe-qi-no-me-no' () is interpreted as a masculine nominative singular perfect passive participle, meaning 'having been made' or 'decorated'. It modifies 'e-re-pa' (), a masculine nominative singular noun meaning 'elephant' or 'ivory'. 'a-no-po' () and 'a-ko-so-ta' () are both masculine nominative singular proper nouns (anthroponyms), as indicated by their classifiers. 'ZE' is a logogram representing '' (pair), implying a plural count of items. 'e-wi-su-*79-ko' (f) is an adjective derived from 'f' ('enclosed' or 'fenced in'), here rendered in the neuter nominative/accusative plural to agree with the implied plural items within the 'pairs'. Similarly, 'ro-i-ko' () is an adjective derived from '' ('crooked' or 'bent'), also in the neuter nominative/accusative plural. The numbers '4' and '2' are quantifiers for the respective items. Logical/Syntactic analysis: The '(...)' separator indicates a break in the text, suggesting that ' ' is a distinct entry or heading. Following the break, ', ' are listed, likely as individuals associated with the subsequent inventory. The structure 'Name, Name. Logogram Adjective Count, Adjective Count' is typical for Mycenaean administrative records, detailing items attributed to or managed by specific persons. Subordinate clause analysis: No subordinate clauses are present; the text consists of nominal phrases and an inventory list.\"}\n",
      "Current Document: 5402\n",
      "document: a-ko-so-ne ka-zo-e 32\n",
      "answer: {'Ancient Greek translation': '  32', 'English translation': '32 inferior axes', 'reasoning': \"Grammatical analysis: 'a-ko-so-ne' is translated as '' (axes), which is the nominative/accusative plural of the noun ''. This translation is directly supported by the critical hint provided in the prompt regarding the derivation of double consonants like  from 'ko-so'. 'ka-zo-e' is translated as '' (worse, inferior), which is the nominative/accusative plural of the comparative adjective ''. This adjective agrees in number and case with ''. '32' is a numerical quantity. Logical/Syntactic analysis: The structure is a simple nominal phrase, typical of administrative records, listing a quantity of items with an associated quality. The adjective '' modifies the noun '', indicating that these are axes of inferior quality. Subordinate clause analysis: No subordinate clauses are present in this short entry.\"}\n",
      "Current Document: 5403\n",
      "DEBUG: e-ke-i-ja 30 pe-di-je-wi-ja 20 a-ko-so-ne 2\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "document: e-ke-i-ja 30 pe-di-je-wi-ja 20 a-ko-so-ne 2\n",
      "answer: {'Ancient Greek translation': ' 30, f 20,  2', 'English translation': 'Spears: 30, Foot-soldiers (or those of the plain): 20, Axes: 2', 'reasoning': \"Grammatical analysis: The document presents a series of inventory entries, each consisting of a noun followed by a numeral. 'e-ke-i-ja' () is a common noun, feminine plural, likely nominative or accusative, meaning 'spears'. It follows the thematic in -a declension pattern. 'pe-di-je-wi-ja' (f) is also a common noun, feminine plural, likely nominative or accusative, meaning 'foot-soldiers' or 'those of the plain', also following the thematic in -a declension. 'a-ko-so-ne' () is a common noun, masculine plural, likely nominative or accusative, meaning 'axes' or 'axles', following the thematic in -o declension. The classifier for 'a-ko-so-ne' as 'anthroponym/animal name/theonym' appears to be an error in the provided data, as '' (axes) is a well-attested common noun in Linear B contexts, as indicated by the example provided in the prompt. The numbers 30, 20, and 2 are simple quantities. Logical/Syntactic analysis: The document is a paratactic list, typical of Mycenaean administrative records. Each line item functions as a nominal phrase indicating an item and its quantity. There are no finite verbs or complex sentence structures, implying a simple enumeration or inventory. Subordinate clause analysis: No subordinate clauses are present in this document. The translation reflects a direct inventory of items and their counts, consistent with the administrative nature of Linear B texts.\"}\n",
      "Current Document: 5404\n",
      "DEBUG: o-di-do-si du-ru-to-mo a-mo-te-jo-na-de e-pi-*19-ta 50 a-ko-so-ne-qe 50 to-sa-de ro-u-si-jo a-ko-ro a-ko-so-ne 100 to-sa-de e-pi-*19-ta 100\n",
      "Error occurred: 429 Resource has been exhausted (e.g. check quota).. Retrying in 65 seconds...\n",
      "document: o-di-do-si du-ru-to-mo a-mo-te-jo-na-de e-pi-*19-ta 50 a-ko-so-ne-qe 50 to-sa-de ro-u-si-jo a-ko-ro a-ko-so-ne 100 to-sa-de e-pi-*19-ta 100\n",
      "answer: {'Ancient Greek translation': '  :  50   50.   :  100.  : 100.', 'English translation': \"The woodcutters give to the workshop: 50 overseers and 50 axles. Total for Rousios' field: 100 axles. Total for overseers: 100.\", 'reasoning': \"Grammatical analysis: The verb 'o-di-do-si' () is a 3rd person plural present active indicative, meaning 'they give'. The subject is 'du-ru-to-mo' (), a nominative plural masculine noun meaning 'woodcutters'. The destination is 'a-mo-te-jo-na-de' (), a noun with the allative suffix '-de', indicating 'to the workshop' or 'to the chariot-maker's shop'. The items being given are 'e-pi-*19-ta' (), a nominative plural masculine noun meaning 'overseers', and 'a-ko-so-ne-qe' ( ), a nominative plural masculine noun meaning 'axles', with the enclitic conjunction '-qe' () meaning 'and'. The numbers '50' quantify these items. The second part of the document uses 'to-sa-de' (), a nominative plural neuter adjective meaning 'so many' or 'these many', functioning as a quantifier or summary marker. 'ro-u-si-jo a-ko-ro' ( ) consists of 'ro-u-si-jo' as a nominative singular masculine adjective modifying 'a-ko-ro' (nominative singular masculine noun), together meaning 'Rousios' field' or 'the Rousian field'. 'a-ko-so-ne' () is again 'axles', quantified by '100'. The final phrase 'to-sa-de e-pi-*19-ta 100' repeats the quantifier and 'overseers', also quantified by '100'.\\n\\nLogical/Syntactic analysis: The document is structured as an administrative record, typical of Linear B texts. The first sentence describes an action: 'The woodcutters give to the workshop'. This is followed by a list of items and their quantities, which are interpreted as the direct objects of the verb 'give'. The inclusion of 'overseers' as an item being 'given' is unusual but follows the direct translation of the provided options in an administrative context, possibly implying their assignment or provision. The second part of the document, introduced by 'to-sa-de', functions as a separate summary or inventory entry. It lists quantities associated with 'Rousios' field' (100 axles) and 'overseers' (100 units, implicitly the same or related items as previously mentioned, or 100 overseers themselves). This paratactic structure is common in such records, where items are listed without complex grammatical subordination.\\n\\nSubordinate clause analysis: There are no explicit subordinate clauses in this document. The sentences are structured paratactically, listing actions and inventory items in a direct, administrative style.\"}\n",
      "Current Document: 5405\n",
      "document: pi-*82 sa-pi-de [...] me-ta-pa sa-pi-de [...] pe-to-no sa-pi-de 200 [...] pa-ki-ja-ne sa-pi-de 80 [...] sa-pi-de 60 sa-pi-de [...] 40 ro [...] sa-pi-de sa-pi-de\n",
      "answer: {'Ancient Greek translation': '  [...]   [...]   200 [...]   80 [...]  60  [...] 40  [...]  ', 'English translation': 'Pisfa: hides [...] Metapa: hides [...] Petonos: 200 hides [...] Phakianes: 80 hides [...] 60 hides [...] 40 hides [...] Ro [...] hides, hides', 'reasoning': \"Grammatical analysis: The document is an administrative inventory. `pi-*82` (), `me-ta-pa` (), `pe-to-no` (), and `pa-ki-ja-ne` () are interpreted as toponyms or ethnonyms in the nominative case, functioning as headings or locative references for the listed items. `sa-pi-de` is consistently translated as `` (nominative/accusative plural), an athematic noun meaning 'hides' or 'sacks'. The numbers (200, 80, 60, 40) are quantities. `ro` is an incomplete word, classified as an incomplete noun (anthroponym/animal name/theonym), and is left as `` in Ancient Greek to reflect its fragmentary nature. Logical/Syntactic analysis: The document follows a clear, paratactic inventory structure: `[Location/Group] [Item] [Quantity]`. Each segment represents an entry in this list. The `[...]` indicates missing or fragmented text. The repetition of `sa-pi-de` at the end suggests a continuation of the inventory, possibly with unspecified quantities or further items associated with the fragmented `ro` entry. Subordinate clause analysis: No subordinate clauses are present; the text consists of a series of nominal entries typical of Linear B administrative records. Semantic coherence: The translation maintains the administrative and inventory nature of Linear B texts, listing items and quantities associated with specific locations or groups. The choice of `` for `sa-pi-de` is a well-attested interpretation for 'hides' or 'sacks' in Mycenaean Greek, fitting the context of goods being recorded.\"}\n",
      "Current Document: 5406\n",
      "document: o-ha e-pi-de-da-to pa-ra-we-wo wo-no pi-*82-de 50 me-ta-pa-de 50 pe-to-no-de 100 pa-ki-ja-na-de 35 a-phu-de 35 a-ke-re-wa-de 30 e-ra-to-de 50 ka-ra-do-ro-de 40 ri-jo-de 20\n",
      "answer: {'Ancient Greek translation': ' f  ff:  50,  50,  100,  35,  35, f 30,  50,  40,  20', 'English translation': 'Thus, wine distributed by Parawewos: to Pisas 50, to Metapa 50, to Petono 100, to Pakijana 35, to Apu 35, to Akerewa 30, to Erato 50, to Karadoro 40, to Rion 20.', 'reasoning': \"Grammatical analysis: The document begins with `o-ha` (), an adverb meaning 'thus' or 'in this way', introducing the record. `wo-no` (f) is a noun in the nominative masculine singular, meaning 'wine', which is the commodity being recorded. `e-pi-de-da-to` () is a perfect passive participle, masculine nominative singular, agreeing with `wo-no`, meaning 'distributed' or 'allotted'. `pa-ra-we-wo` (ff) is a personal name in the genitive singular, indicating the agent or source of the distribution ('by Parawewos'). The subsequent words (`pi-*82-de`, `me-ta-pa-de`, `pe-to-no-de`, `pa-ki-ja-na-de`, `a-phu-de`, `a-ke-re-wa-de`, `e-ra-to-de`, `ka-ra-do-ro-de`, `ri-jo-de`) are all toponyms (place names) followed by the allative suffix `-de`, meaning 'to' or 'towards'. This indicates the destination of the distributed wine. Each toponym is followed by a numeral representing the quantity of wine distributed to that specific location. The Linear B sign `*82` is interpreted as `sa` or `sfa`, leading to `pi-sa-de` (). Logical/Syntactic analysis: The document follows a typical administrative record structure, common in Linear B texts. It starts with a general statement about the commodity and its distribution, then lists specific destinations with their respective quantities. The structure is paratactic, presenting a series of entries without complex subordinate clauses. The implied verb 'is distributed' connects the initial statement to the subsequent list. Semantic coherence: The translation provides a clear and coherent administrative record detailing the allocation of wine from a specific individual to various locations, which is consistent with the nature of Mycenaean palatial archives.\"}\n",
      "Current Document: 5407\n",
      "DEBUG: me-ti-ja-no-ro wo-di-je-ja 1 ka-e-sa-me-no-jo a-*64-ja 1 e-to-mo-jo *63-o-wa 1 de-ki-si-wo-jo pi-ri-ta 1 a-ta-o-jo pi-ro-pa-ta-ra 1 qa-ko-jo ma-ra-me-na [...] 1 [...] pe-rya [...] 1 [...] a-ri [...] 1 [...] 1 wa-o [...] o [...] 1\n",
      "Error occurred: Invalid control character at: line 4 column 175 (char 729). Retrying in 65 seconds...\n",
      "document: me-ti-ja-no-ro wo-di-je-ja 1 ka-e-sa-me-no-jo a-*64-ja 1 e-to-mo-jo *63-o-wa 1 de-ki-si-wo-jo pi-ri-ta 1 a-ta-o-jo pi-ro-pa-ta-ra 1 qa-ko-jo ma-ra-me-na [...] 1 [...] pe-rya [...] 1 [...] a-ri [...] 1 [...] 1 wa-o [...] o [...] 1\n",
      "answer: {'Ancient Greek translation': ' F 1\\n f 1\\n f 1\\nf  1\\n  1\\n  1\\n[...]  1\\n[...]  1\\n[...] 1\\nF [...]  1', 'English translation': 'Woidigeia, of Methianor: 1\\nAsfija, of Kalesamenos: 1\\nUiofa, of Etoimos: 1\\nPrita, of Dexiwo: 1\\nPhilopatra, of Ataos: 1\\nMaramena, of Bakchos: 1\\n[...] Perya: 1\\n[...] Aris: 1\\n[...] 1\\nFaos, [...] a sheep: 1', 'reasoning': \"Grammatical analysis: The document presents a series of administrative entries, primarily following a consistent pattern of `[Genitive Anthroponym] [Nominative Noun/Anthroponym] 1`. Each entry lists an individual (or item) and attributes it to another individual in the genitive case, followed by a count of '1'.\\n\\n1.  `me-ti-ja-no-ro` () is a thematic -o stem noun in the genitive singular, indicating possession ('of Methianor'). `wo-di-je-ja` (F) is a thematic -a stem noun in the nominative singular, likely an anthroponym or a role.\\n2.  `ka-e-sa-me-no-jo` () is a thematic -o stem adjective/participle in the genitive singular ('of Kalesamenos'). `a-*64-ja` (f) is a thematic -a stem noun in the nominative singular, interpreted as an ethnonym or proper name (from `a-si-f-ja`).\\n3.  `e-to-mo-jo` () is a thematic -o stem noun in the genitive singular ('of Etoimos'). `*63-o-wa` (f) is a thematic -o stem noun in the nominative singular, interpreted as a proper name (from `u-i-o-wa`).\\n4.  `de-ki-si-wo-jo` (f) is a thematic -o stem noun in the genitive singular ('of Dexiwo'). `pi-ri-ta` () is a thematic -a stem noun in the nominative singular, a proper name.\\n5.  `a-ta-o-jo` () is a thematic -o stem noun in the genitive singular ('of Ataos'). `pi-ro-pa-ta-ra` () is a thematic -a stem noun in the nominative singular, a proper name.\\n6.  `qa-ko-jo` () is a thematic -o stem noun in the genitive singular ('of Bakchos'). `ma-ra-me-na` () is a thematic -a stem noun in the nominative singular, a proper name.\\n7.  `pe-rya` () is a thematic -a stem noun in the nominative singular. The preceding genitive anthroponym is lost due to fragmentation.\\n8.  `a-ri` () is an athematic noun, likely nominative singular. The preceding genitive anthroponym is lost due to fragmentation.\\n9.  A completely fragmented entry.\\n10. `wa-o` (F) is a thematic -o stem noun in the nominative singular, interpreted as an anthroponym. `o` () is a thematic -o stem common noun in the nominative singular, meaning 'sheep'. This entry, while fragmented, suggests a list of a person and an associated item.\\n\\nLogical/Syntactic analysis: The document is a list, likely an inventory or roster, where each line records an individual or item, often associated with another person (indicated by the genitive), followed by a numerical count of '1'. The consistent use of the genitive case for the first element and the nominative for the second establishes a possessive or associative relationship. The fragmented entries are interpreted based on the established pattern, assuming missing elements. The final entry deviates slightly by presenting two nominative nouns, suggesting a person and an associated item.\\n\\nSubordinate clause analysis: No subordinate clauses are present, which is typical for administrative Linear B texts that primarily function as records or inventories. The structure is paratactic, listing items sequentially.\"}\n",
      "Current Document: 5408\n",
      "document: pi-rai [...] ka-pi-ni-ja a-ti-ta 6 [...] ka-pi-ni-ja e-ru-mi-ni-ja 4 ka-pi-ni-ja ta-ra-nu-we 12 [...] *34-ki-no-o 81 o-pi-rai-te-re 40 e-to-ki-ja 23 [...] ke-te-re [...] 140 pi-ri-ja-o ta-ra-nu-we 6 qe-re-ti-ri-jo 2 me-ta-se-we 10 e-po-wo-ke pu-to-ro 16 *34-ki-no-o pu-to-ro 100 ta-to-mo a-ro-wo e-pi-*65-ko 1 e-ru-mi-ni-ja 2 ki-wo-qe 1\n",
      "answer: {'Ancient Greek translation': ' [...]   6.   4. ,  12 [...]  81.  40.  23 [...]  [...] 140.   6.  2. f 10. f  16.   100.  f  1.  2.   1.', 'English translation': 'Piraia [...] Hearth-items of Atitas: 6. Hearth-items of Erymnia: 4. Hearth-items, stools: 12 [...] Alkinoos: 81. Overseers: 40. Wall-hangings: 23 [...] Spurs [...] 140. Stools of Piria: 6. Throwers: 2. Middle-men: 10. Epowoke, total: 16. Alkinoos, total: 100. Station-master Arowos, overseer: 1. Erymnia: 2. And pillar: 1.', 'reasoning': \"Grammatical Analysis: The document is an administrative inventory or roster. Each entry typically consists of an item or person, possibly a descriptor or location, and a numerical count. \\n- `pi-rai`: Interpreted as  (Piraia), a toponym, serving as a fragmentary heading or location marker.\\n- `ka-pi-ni-ja`:  (kapniai), nominative plural of 'hearth-item' or 'smoke-house'.\\n- `a-ti-ta`:  (Atita), likely a personal name in the genitive case, indicating possession or association.\\n- `e-ru-mi-ni-ja`:  (Erymnia), a toponym or ethnonym, functioning as a descriptor or location.\\n- `ta-ra-nu-we`:  (thranes), nominative plural of 'stools', an athematic noun.\\n- `*34-ki-no-o`:  (Alkinoos), a proper noun (anthroponym) in the nominative case.\\n- `o-pi-rai-te-re`:  (opithetres), nominative plural of 'overseers' or 'inspectors', an athematic noun.\\n- `e-to-ki-ja`:  (entoichia), nominative plural of 'wall-hangings' or 'inlaid work'.\\n- `ke-te-re`:  (kentres), nominative plural of 'spurs' or 'goads', an athematic noun.\\n- `pi-ri-ja-o`:  (Piriao), a personal name in the genitive case, indicating possession.\\n- `qe-re-ti-ri-jo`:  (bltrioi), nominative plural of 'throwers' or 'shooters'.\\n- `me-ta-se-we`: f (metaswes), nominative plural of 'middle-men' or 'those in between', an athematic noun.\\n- `e-po-wo-ke`: f (epowoke), nominative plural, likely a type of item or a role, an athematic noun.\\n- `pu-to-ro`:  (pytros), here interpreted as a Mycenaean administrative term for 'total' or 'sum', despite its literal meaning 'loaf' or 'wheat'. This interpretation is supported by its use with personal names (`*34-ki-no-o pu-to-ro`).\\n- `ta-to-mo`:  (stathmos), nominative singular of 'station' or 'post'.\\n- `a-ro-wo`: f (Arowos), a proper noun (anthroponym) in the nominative case.\\n- `e-pi-*65-ko`:  (epikoos), a common noun in the nominative singular, meaning 'overseer' or 'listener'.\\n- `ki-wo-qe`:   (kn te), nominative singular of 'pillar' with the enclitic conjunction -qe meaning 'and'.\\n\\nLogical/Syntactic Analysis: The document follows a paratactic structure, typical of Linear B administrative records, listing items or individuals and their quantities. Each line or segment represents a distinct entry. The `[...]` markers indicate breaks or missing text within the tablet.\\n- The initial `pi-rai [...] ka-pi-ni-ja a-ti-ta 6` suggests a fragmentary heading followed by an itemized count: 'Hearth-items of Atitas: 6'.\\n- `ka-pi-ni-ja e-ru-mi-ni-ja 4` continues the pattern: 'Hearth-items of Erymnia: 4'.\\n- `ka-pi-ni-ja ta-ra-nu-we 12` is interpreted as a list of two items, 'Hearth-items' and 'stools', with the count '12' applying to the latter, or 'Hearth-items' as a category for 'stools'. The comma in the Ancient Greek translation clarifies this separation.\\n- Entries like `*34-ki-no-o 81` and `o-pi-rai-te-re 40` are straightforward counts of individuals or items associated with them.\\n- `pi-ri-ja-o ta-ra-nu-we 6` confirms `ta-ra-nu-we` as an item, with `pi-ri-ja-o` as a possessive genitive: 'Stools of Piria: 6'.\\n- The use of `pu-to-ro` with `e-po-wo-ke` and `*34-ki-no-o` strongly indicates a 'total' or 'sum' for the preceding item/person, fitting the administrative context.\\n- `ta-to-mo a-ro-wo e-pi-*65-ko 1` is interpreted as a single individual holding multiple roles or titles: 'Station-master Arowos, overseer: 1'. This is a common way to list personnel and their functions in Mycenaean texts.\\n- The final `ki-wo-qe 1` uses the enclitic `-qe` ('and'), suggesting it is the last item in a list.\\n\\nSubordinate Clause Analysis: No complex subordinate clauses are present, consistent with the nature of an administrative inventory.\"}\n",
      "Current Document: 5409\n",
      "document: ki-ka-ne-wi-jo-de 3 ma-so [...] 1 [...] e-re-wi-jo-po-ti-ni-ja 1 [...] 1 ki-wo-na-de 1 [...] 1 tu-ra-te-u-si 2 wo-no-wa-ti-si [...] 2 ta [...] 6 ka-ra-wi-po-ro 5\n",
      "answer: {'Ancient Greek translation': ' 3.  [...] 1.   1. [...] 1. f 1. [...] 1.  2. F [...] 2.  [...] 6. f 5.', 'English translation': 'To Kikanewion: 3. Masos: [...] 1. Lady of Erewion: 1. [...] 1. To the pillar: 1. [...] 1. To the doorkeepers: 2. To the wine-workers: [...] 2. Items: [...] 6. Key-bearers: 5.', 'reasoning': \"Grammatical analysis: The document is a series of administrative entries, each consisting of a noun (or noun phrase) followed by a numerical count. \\n- 'ki-ka-ne-wi-jo-de' () is a toponym (place name) 'Kikanewion' in the accusative case with the allative suffix '-de', indicating 'to Kikanewion'. It is followed by the count '3'.\\n- 'ma-so' () is an anthroponym (personal name) in the nominative singular, followed by the count '1'. The '[...]' indicates missing text, but the name itself is clear.\\n- 'e-re-wi-jo-po-ti-ni-ja' ( ) is a compound noun phrase, 'Lady of Erewion'. 'Erewion' is likely a place or epithet in the genitive case, modifying 'Potnia' (Mistress/Lady) which is in the nominative singular. It is followed by the count '1'.\\n- 'ki-wo-na-de' (f) is a common noun 'pillar' (f) in the accusative singular with the allative suffix '-de', meaning 'to the pillar'. It is followed by the count '1'.\\n- 'tu-ra-te-u-si' () is a common noun 'doorkeepers' (from , an athematic stem) in the dative plural, indicating recipients. It is followed by the count '2'.\\n- 'wo-no-wa-ti-si' (F) is a common noun 'wine-workers' or 'wine-distributors' (from f, an athematic stem) in the dative plural, indicating recipients. It is followed by the count '2'. The word itself is marked as incomplete, but the dative plural ending is clear.\\n- 'ta' () is a fragment. Given the classifier 'noun, common, thematic in -a' and the following number '6', it is interpreted as a neuter plural pronoun/article '' ('the things' or 'items') or a fragment of a noun like '' (talents). For a general administrative list, 'items' is a suitable placeholder. It is followed by the count '6'.\\n- 'ka-ra-wi-po-ro' (f) is a common noun 'key-bearer' (f, a thematic -o stem). Since it is followed by the count '5', it is interpreted as nominative plural 'key-bearers'.\\n\\nLogical/Syntactic analysis: The document presents a series of independent entries, each typically specifying a destination, recipient, or category, followed by a numerical quantity. This paratactic structure is characteristic of Mycenaean administrative records, which function as inventories or allocations. The use of the allative suffix '-de' clearly indicates a destination, while the dative plural suffix '-si' indicates recipients. The presence of '[...]' signifies missing or illegible text, a common feature in fragmentary Linear B tablets.\\n\\nSubordinate clause analysis: There are no subordinate clauses present in this document. The entries are simple, declarative statements of allocation or enumeration.\"}\n",
      "Current Document: 5410\n",
      "DEBUG: o-ze-to ke-sa-do-ro *34-to-pi pa-ro a-ke-ha me-ta-pa pe-ri-te 1 a-pi-no-e-wi-jo pa-ro e-ru-si-jo 1 a-pi-no-e-wi-jo pa-ro ai-ki-e-we 4 e-na-po-ro pa-ro wa-do-me-no 9 sa-ri-no-te pa-ro o-wo-to 5 pa-ki-ja-si pa-ro a-ta-no-re 4 ka-ra-do-ro pa-ro to-ro-wo 1 pa-ki-ja-si pa-ro e-ri-we-ro 3 e-wi-te-wi-jo pa-ro wi-sa-to 1 me-te-to [...] pa-ro ko-do 3 ro-u-so 24 me-te-to [...] pa-ro e-u-qo-ne 3\n",
      "Error occurred: Invalid \\escape: line 4 column 347 (char 1279). Retrying in 65 seconds...\n",
      "document: o-ze-to ke-sa-do-ro *34-to-pi pa-ro a-ke-ha me-ta-pa pe-ri-te 1 a-pi-no-e-wi-jo pa-ro e-ru-si-jo 1 a-pi-no-e-wi-jo pa-ro ai-ki-e-we 4 e-na-po-ro pa-ro wa-do-me-no 9 sa-ri-no-te pa-ro o-wo-to 5 pa-ki-ja-si pa-ro a-ta-no-re 4 ka-ra-do-ro pa-ro to-ro-wo 1 pa-ki-ja-si pa-ro e-ri-we-ro 3 e-wi-te-wi-jo pa-ro wi-sa-to 1 me-te-to [...] pa-ro ko-do 3 ro-u-so 24 me-te-to [...] pa-ro e-u-qo-ne 3\n",
      "answer: {'Ancient Greek translation': '  . :  1.    1.    4. :   9.    5.    4.    1.    3.    1.  [...]   3.  24.  [...]   3.', 'English translation': 'Kessandros recorded at Zotophi. Metapa: Perites, 1. Amphinoewios, 1 from Erysios. Amphinoewioi, 4 from Aikiewes. Enaporos: 9 from Wadomenos. Salinotes, 5 from Owotos. At Phakijai: 4 from Atanor. Charadros, 1 from Throwos. At Phakijai: 3 from Eriweros. Ewitewios, 1 from Wissatos. Metethon [...]: 3 from Kodos. Lousos: 24. Metethon [...]: 3 from Euphone.', 'reasoning': 'Grammatical Analysis:\\n*   `o-ze-to`: Interpreted as a 3rd person singular middle/passive verb, ``, meaning \\'recorded\\' or \\'assigned\\', setting the context for the administrative record. This verb is chosen for its plausible administrative function, despite the classical Greek `` meaning \\'to smell\\'.\\n*   `ke-sa-do-ro`: ``, a proper noun (anthroponym) in the nominative singular, functioning as the subject of `o-ze-to`.\\n*   `*34-to-pi`: ``, interpreted as a toponym in the locative singular (due to the `-pi` suffix), indicating the place where the recording occurred. The classifier as \\'anthroponym\\' is considered an error here, as the suffix strongly points to a locative place name (assuming *34 is ZO).\\n*   `pa-ro`: ``, a preposition taking the dative case, meaning \\'from\\' or \\'by\\'. The classifier as \\'noun\\' is incorrect.\\n*   Personal names following `pa-ro` (e.g., `a-ke-ha`, `e-ru-si-jo`, `ai-ki-e-we`, `wa-do-me-no`, `o-wo-to`, `a-ta-no-re`, `to-ro-wo`, `e-ri-we-ro`, `wi-sa-to`, `ko-do`, `e-u-qo-ne`) are all rendered in the dative case (e.g., ``, ``, ``, ``, ``, ``, ``, ``, ``, ``, ``).\\n*   Place names (e.g., `me-ta-pa`, `e-na-po-ro`, `ro-u-so`, `me-te-to`) are rendered in the nominative case as headings (e.g., ``, ``, ``, ``).\\n*   `pa-ki-ja-si`: ``, a toponym in the locative plural, meaning \\'at Phakijai\\'.\\n*   `pe-ri-te`, `sa-ri-no-te`, `ka-ra-do-ro`, `e-wi-te-wi-jo`: These are anthroponyms/ethnonyms in the nominative singular (e.g., ``, ``, ``, ``), functioning as the subject or main entry for a line.\\n*   `a-pi-no-e-wi-jo`: `` (singular) and `` (plural), interpreted as a type of person or item, agreeing in number with the associated quantity.\\n*   Quantities (1, 4, 9, 5, 4, 1, 3, 1, 3, 24, 3) are numerical values.\\n\\nLogical/Syntactic Analysis:\\n*   The document begins with an introductory statement: `o-ze-to ke-sa-do-ro *34-to-pi` (\"Kessandros recorded at Zotophi\"), establishing the recorder and location of the record.\\n*   The rest of the document consists of a series of administrative entries, often grouped under place headings (e.g., `me-ta-pa`, `e-na-po-ro`, `me-te-to`, `ro-u-so`).\\n*   The entries follow a consistent pattern: `[Item/Person] [Quantity] pa-ro [Source Person] [Quantity]`. The quantity immediately following `pa-ro [Source Person]` refers to the item/person listed before the `pa-ro` phrase. For example, `a-pi-no-e-wi-jo pa-ro e-ru-si-jo 1` means \"1 amphinoewios from Erysios\".\\n*   When a person\\'s name appears before a `pa-ro` phrase (e.g., `sa-ri-no-te pa-ro o-wo-to 5`), it implies that `Salinotes` is associated with 5 units received from `Owotos`.\\n*   Some entries, like `ro-u-so 24`, appear to be simple totals for a given place.\\n*   The fragments `[...]` indicate missing text, which is preserved in the translation.\\n\\nSubordinate Clause Analysis:\\n*   There are no explicit subordinate clauses in this document. The structure is primarily paratactic, typical of administrative lists and inventories in Linear B, where information is presented in a series of juxtaposed phrases and entries rather than complex sentences with conjunctions and dependent clauses. The `pa-ro` phrases function as prepositional phrases indicating source or agent.'}\n",
      "Current Document: 5411\n",
      "document: a-ke-ro e-po a-ke-rya-te pa-wa-wo ti-mi-ti-ja 20 o-to-ro-qa e-sa-re-wi-ja za-ma-e-wi-ja-qe 50 po-ro-u-te-u [...] e-ra-te-re-wa-pi 20 ja [...] 20 re-wi-ja-qe [...] 50 40 me-ta-pa-qe 20 pe-to-no [...] 25 ja [...] a-ke-re-wa-qe 20\n",
      "answer: {'Ancient Greek translation': ': ,  ,  20. :    50.  [...]  20.  [...] 20,   [...] 50, 40.   20,  [...] 25.  [...]   20.', 'English translation': 'Record: Angelos, collector of Pawawo, Thimitia 20. Group: Esarewians and Zamaewians 50. Plouteus [...] at Elatrewa 20. Ia [...] 20, and Lewiai [...] 50, 40. And Metapa 20, Petono [...] 25. Ia [...] and Agrewa 20.', 'reasoning': \"Grammatical analysis: The document is an administrative list. 'e-po' () is interpreted as a general heading 'Record' or 'List' in the nominative singular neuter. 'a-ke-ro' () is a proper noun in the nominative singular masculine. 'a-ke-rya-te' () is an athematic noun in the nominative singular masculine, functioning as an appositive to 'a-ke-ro', meaning 'collector' or 'gatherer'. 'pa-wa-wo' () is a proper noun in the genitive singular masculine, indicating possession ('of Pawawo') modifying 'a-ke-rya-te'. 'ti-mi-ti-ja' () is a toponym in the nominative singular feminine, indicating the location. 'o-to-ro-qa' () is a common noun in the nominative singular feminine, interpreted as 'group' or 'category' for the subsequent ethnonyms. 'e-sa-re-wi-ja' () and 'za-ma-e-wi-ja-qe' ( ) are ethnonyms in the nominative plural feminine, coordinated by the suffix '-qe' (), meaning 'and'. 'po-ro-u-te-u' () is a proper noun in the nominative singular masculine. 'e-ra-te-re-wa-pi' () is a toponym with the instrumental/locative suffix '-pi', meaning 'at Elatrewa'. 'ja [...]' ( [...]) is an incomplete proper noun, likely nominative singular. 're-wi-ja-qe' ( ) is a common noun in the nominative plural feminine, coordinated by '-qe'. 'me-ta-pa-qe' ( ) is a toponym or group name in the nominative singular/plural, coordinated by '-qe'. 'pe-to-no' () is a toponym in the nominative singular. 'a-ke-re-wa-qe' ( ) is a proper noun in the nominative singular, coordinated by '-qe'. The numbers (20, 50, 40, 25) are tallies associated with the preceding entries. Logical/Syntactic analysis: The document follows a paratactic administrative list format, with each line or segment representing a distinct entry. Entries typically consist of names or groups, associated locations, and numerical quantities. The absence of finite verbs in most entries is characteristic of Linear B administrative texts, which often use nominal phrases. The suffix '-qe' is consistently used to conjoin elements within an entry. The ' [...] ' indicates missing or illegible portions of the text, reflecting the fragmentary nature of the original. Subordinate clause analysis: No explicit subordinate clauses are present; the structure is primarily a series of coordinated nominal phrases.\"}\n",
      "Current Document: 5412\n",
      "document: de-mi-ni-ja e-ke-ri-jo-na [...] 1 ta-ra-ma-ta [...] 1 ka [...] da-je-we [...] 1 e [...] ru-ke-ja [...] 1 tu [...] e-po-me-ne-we 1 na [...] 1 wa-na-si-ja-ke 1 sa-nu-we 1 wi-ri-ke-ja 1 o-to-wo-we-i 1 e-ti-ri-ja 1 te-do-ne-ja 1 e-ti-je-ja 1 ne-ka-ta-ta 1 ta-zo-te-ja [...] 2 [...] ka-pa-si-ja [...] 1 re-u-ka-ta-ra-ja [...] te-qi-ri-jo-ne 1 ma-so-ni-jo 2 ra-i-jo [...] 1 [...] ko-jo [...] 5 [...] o 3 [...] 4 [...] o [...] 2\n",
      "answer: {'Ancient Greek translation': '\\n 1\\n 1\\n [...] 1\\n 1\\n [...] 1\\n 1\\n 1\\n 1\\n [...] 1\\n 1\\n 1\\n 1\\n 1\\n 1\\n 1\\n 1\\n 1\\n 2\\n 1\\n 1\\n 1\\n 2\\n 1\\n 5\\n 3\\n4\\n 2', 'English translation': 'Bedding:\\nEcheliona: 1.\\nTo Thalamatas: 1.\\n[Fragment] 1.\\nTo Daiewes: 1.\\n[Fragment] 1.\\nTo Lykeia: 1.\\nTo Tys: 1.\\nTo the attendant: 1.\\n[Fragment] 1.\\nWanasiage: 1.\\nTo Sanywes: 1.\\nTo Wilikeia: 1.\\nTo Ortowowei: 1.\\nTo Etria: 1.\\nTo Tedoneia: 1.\\nTo Etieia: 1.\\nTo Nektata: 1.\\nTo Tazoteia: 2.\\nTo Karpasia: 1.\\nTo Leukatarai: 1.\\nTethrione: 1.\\nTo the Masonioi: 2.\\nTo Rhaio: 1.\\nTo the Koioi: 5.\\nSheep: 3.\\n4.\\nSheep: 2.', 'reasoning': \"Grammatical analysis: The document begins with 'de-mi-ni-ja' (), a common noun in nominative plural, functioning as a heading for the inventory: 'Bedding'. Most subsequent entries are anthroponyms or common nouns followed by a number. The majority of these names/titles are interpreted in the dative singular or plural, indicating recipients of the listed item (bedding). For example, 'ta-ra-ma-ta' () is dative singular of a thematic -a noun, 'e-po-me-ne-we' () is dative singular of an athematic noun, and 'ma-so-ni-jo' () is dative plural of a thematic -o noun. This dative case is strongly supported by the provided Ancient Greek options for several words (e.g., 'o-to-wo-we-i' -> ). Some entries, like 'e-ke-ri-jo-na' () and 'te-qi-ri-jo-ne' (), are rendered in the nominative singular as their dative forms are not explicitly provided in the options or are less certain, or in the case of 'e-ke-ri-jo-na', it might function as the first listed individual in a nominative sense. 'wa-na-si-ja-ke' (), a toponym, is also rendered in the nominative, indicating a place associated with the item. The final entries 'o' followed by numbers are interpreted as the logogram '' (sheep). Fragments like 'ka', 'e', 'na' are left as they are due to their incompleteness. Numbers are preserved as digits. \\n\\nLogical/Syntactic analysis: The document functions as an administrative inventory or allocation record. The initial '' sets the category of items being accounted for. Each subsequent line typically lists a person, title, or place (the recipient/associated entity) followed by a numerical quantity. The consistent use of dative case for many of the names/titles strongly suggests an allocation, meaning 'X units of bedding to [person/place]'. The numbers thus quantify the bedding allocated to each listed entity. The final lines with '' represent a separate count of sheep, indicating a continuation of inventory but for a different commodity. The structure is paratactic, typical of Linear B administrative texts, with no complex subordinate clauses.\\n\\nSubordinate clause analysis: No subordinate clauses are present in this document; it consists of a series of nominal entries and counts.\"}\n",
      "Current Document: 5413\n",
      "document: na-u-do-mo ai-me-wa 1 sa-mu-ta-jo 1 qe-te-re-u 1 ta-ra-pe-se 1 wa-tu-o 1 ka-ro-qo 1 a-re-ki-si-to 1 a-wa-ne-u 1 ki-nu-ra 1 o-ro-do-ko 1 a-mi-ja [...] u-re-u 1\n",
      "answer: {'Ancient Greek translation': ':  1,  1,  1,  1,  1,  1,  1, f 1,  1,  1, [...]  1', 'English translation': 'Shipbuilders: Aimevas 1, Samuthas 1, Qetereus 1, Tarpes 1, Wastuos 1, Charops 1, Alexistos 1, Awaneus 1, Kinuras 1, Orodokhos 1, Amija[...] Hyleus 1.', 'reasoning': \"Grammatical analysis: The document begins with `na-u-do-mo` (), a common noun in the nominative singular, thematic in -o, meaning 'shipbuilder'. This word functions as a heading or category for the subsequent list. Each following word is an anthroponym (proper name of a person), primarily in the nominative singular, followed by the numeral '1'. This numeral indicates a count of one individual for each name. The names are: `ai-me-wa` (, thematic in -a), `sa-mu-ta-jo` (, thematic in -o), `qe-te-re-u` (, athematic), `ta-ra-pe-se` (, athematic), `wa-tu-o` (, thematic in -o), `ka-ro-qo` (, thematic in -o), `a-re-ki-si-to` (, thematic in -o), `a-wa-ne-u` (f, athematic), `ki-nu-ra` (, thematic in -a), `o-ro-do-ko` (, thematic in -o), `a-mi-ja` (, thematic in -a, noted as incomplete), and `u-re-u` (, athematic). The `[...]` signifies a physical break in the tablet, indicating missing text between 'Amija' and 'Hyleus'.\\n\\nLogical/Syntactic analysis: The document is structured as an administrative roster, typical of Linear B texts. It lists a profession or group ('shipbuilders') followed by a series of individuals belonging to that group, each explicitly counted as '1'. This paratactic structure, common in Mycenaean records, does not require a finite verb for semantic completeness, as it functions as an inventory. The names are presented as a simple enumeration under the initial heading.\\n\\nSubordinate clause analysis: There are no subordinate clauses present in this document. The text consists of a simple list of items (names) under a categorical heading, each with an associated count.\"}\n",
      "Current Document: 5414\n",
      "document: a-ti [...] pe-*65-ka 8 ko-ni-ti-ja-ja pe-*65-ka 24 e-to-ki-ja qa-ra-de-ro 10 pa-ke-te-re qa-ra-de-ro 86\n",
      "answer: {'Ancient Greek translation': ' [...]  8\\n  24\\n  10\\n  86', 'English translation': 'Anti [...], Peuka: 8.\\nKonitia, Peuka: 24.\\nEntoichia, Kraderos: 10.\\nContractors, Kraderos: 86.', 'reasoning': \"The document presents a series of administrative entries, each typically consisting of a descriptor (place or category), a personal name, and a numerical quantity. The relationships between these elements are implied by their juxtaposition, a common feature in Linear B administrative texts.\\n\\n**Grammatical Analysis:**\\n*   `a-ti`: Translated as ``, an incomplete anthroponym in the nominative singular, athematic inflection. The fragment `[...]` indicates missing text.\\n*   `pe-*65-ka`: Translated as ``, an anthroponym (personal name) in the nominative singular, thematic in -a inflection. `*65` is interpreted as ``, leading to ``.\\n*   `ko-ni-ti-ja-ja`: Translated as ``, a toponym (place name) or ethnonym in the nominative singular, thematic in -a inflection.\\n*   `e-to-ki-ja`: Translated as ``, a common noun in the nominative singular, thematic in -a inflection. In this context, it is interpreted as a place name or a specific designation/category.\\n*   `qa-ra-de-ro`: Translated as ``, an anthroponym (personal name) in the nominative singular, thematic in -o inflection.\\n*   `pa-ke-te-re`: Translated as ``, a common noun meaning 'contractors' in the nominative plural, athematic inflection.\\n*   Numbers (8, 24, 10, 86) represent quantities associated with the preceding entries.\\n\\n**Logical/Syntactic Analysis:**\\nEach line functions as an independent entry in an inventory or roster. No finite verbs are explicitly present, which is typical for such administrative records. The structure is generally `[Descriptor/Category] [Name] [Quantity]`. The implied relationships are:\\n*   `a-ti [...] pe-*65-ka 8`: A fragmentary entry, likely listing a person `` associated with `` and a quantity of 8.\\n*   `ko-ni-ti-ja-ja pe-*65-ka 24`: `` (Konitia) is interpreted as a place name or an ethnonym, indicating the origin or association of `` (Peuka), with a quantity of 24. The English translation 'Konitia, Peuka: 24' implies 'From Konitia, Peuka (is assigned/has): 24'.\\n*   `e-to-ki-ja qa-ra-de-ro 10`: `` (Entoichia) is interpreted as a place name or a category, associated with `` (Kraderos), with a quantity of 10. The English translation 'Entoichia, Kraderos: 10' implies 'Entoichia (category/place), Kraderos (is assigned/has): 10'.\\n*   `pa-ke-te-re qa-ra-de-ro 86`: `` (Contractors) is a group, and `` (Kraderos) is a personal name. The juxtaposition implies that Kraderos is associated with these contractors, possibly as their leader, supervisor, or the person for whom they work. The English translation 'Contractors, Kraderos: 86' implies 'Contractors (associated with) Kraderos: 86'.\\n\\n**Subordinate Clause Analysis:**\\nThere are no subordinate clauses in this document; it consists of a series of paratactic entries, characteristic of Linear B administrative records.\"}\n",
      "Current Document: 5415\n",
      "document: e-ke-ja 32 [...] ja [...] 127 [...] ki-wa [...] ki-ta-ro-ka-ta 6 a-ko-so-ne [...] 8 [...] 6\n",
      "answer: {'Ancient Greek translation': ' 32. [...]  [...] 127. [...] f [...].  6.  [...] 8 [...] 6.', 'English translation': 'Hekeia: 32. [...] violets [...] 127. [...] kifa [...]. Lyre-player: 6. Axes: [...] 8 [...] 6.', 'reasoning': \"Grammatical analysis: The document presents a series of inventory-style entries. 'e-ke-ja' is translated as '' (Hekeia), a proper noun (anthroponym) in the nominative singular, thematic in -a, consistent with the classifier and the pattern of listing individuals or roles with associated numbers. 'ja' and 'ki-wa' are incomplete; '' (violets) and 'f' (an unknown item) are plausible common nouns for inventory items, but their fragmentary nature is indicated by '[...]'. 'ki-ta-ro-ka-ta' is translated as '' (lyre-player), a common noun in the nominative singular, thematic in -o, referring to a person or role. 'a-ko-so-ne' is translated as '' (axes), a common noun in the nominative/accusative plural, thematic in -o, directly supported by the provided example. The numbers (32, 127, 6, 8, 6) are direct counts associated with the preceding entry. Logical/Syntactic analysis: The document functions as an administrative record, listing various entities (persons/roles and items) with their respective quantities. The structure is paratactic, with each entry forming an independent statement of 'Entity: Quantity'. The fragments '[...]' indicate missing or illegible portions of the text, which are preserved in the translation to reflect the document's incomplete state. The line 'a-ko-so-ne [...] 8 [...] 6' is interpreted as 'axes' with a quantity of '8', where the first '[...]' represents a missing descriptor for the axes, followed by a separate entry indicated by the second '[...]' and the quantity '6'. Subordinate clause analysis: No subordinate clauses are present; the text consists of simple nominal phrases and counts, typical of Mycenaean administrative documents.\"}\n",
      "Current Document: 5416\n",
      "document: ki-wa-ra-e-ru jo-pi-do-ja [...] to-ka-te-ri-ja ki-wa-ra [...] ru-e [...] ki-wa-ra [...] 5 pa-ta-jo [...] 200\n",
      "answer: {'Ancient Greek translation': '  [...]   [...]  [...] :   [...] ', 'English translation': 'Kiphalaerus Iopidia [...] birth offerings of Kiphara [...] Rhue [...] Kiphara: five javelins [...] 200.', 'reasoning': \"Grammatical analysis: The document appears to be an administrative record listing individuals and items, interspersed with fragments. \\n*   'ki-wa-ra-e-ru' () is interpreted as a proper noun, an anthroponym in the nominative singular, functioning as a subject or a list entry. It is classified as thematic in -a, and the chosen Ancient Greek form reflects this. \\n*   'jo-pi-do-ja' () is also interpreted as a proper noun, an anthroponym in the nominative singular. Although the classifier indicates 'thematic in -o', the provided Ancient Greek options strongly suggest a feminine -a stem (e.g., ), which is a common pattern for personal names. We prioritize the plausible Ancient Greek form . \\n*   'to-ka-te-ri-ja' () is a common noun, neuter plural nominative/accusative, meaning 'birth offerings' or 'things pertaining to childbirth'. It is classified as thematic in -a. \\n*   The first instance of 'ki-wa-ra' () is interpreted as a proper noun, an anthroponym in the genitive singular, indicating possession ('of Kiphara'), agreeing with ''. It is classified as thematic in -a. \\n*   'ru-e' () is interpreted as a proper noun, an anthroponym in the nominative singular, based on its position and the context of other names. It is classified as athematic. \\n*   The second instance of 'ki-wa-ra' () is interpreted as a proper noun, an anthroponym in the nominative singular, acting as a heading or owner for the subsequent inventory item. It is classified as thematic in -a. \\n*   '5' () is a numeral. \\n*   'pa-ta-jo' () is a common noun, neuter plural nominative/accusative, meaning 'javelins'. It is classified as thematic in -o. \\n*   '200' () is a numeral, likely referring to a quantity of an unstated item, hence rendered as neuter plural. \\n\\nLogical/Syntactic analysis: The document consists of several distinct, fragmented entries. The initial ' ' likely represents a listing of two individuals. The phrase ' ' forms a possessive construction, 'birth offerings of Kiphara'. The final section ':   [...] ' clearly functions as an inventory entry, where 'Kiphara' is the subject or owner, followed by a quantity ('five') and the item ('javelins'), with a subsequent large number possibly indicating another item or a total. The use of colons in the Ancient Greek translation helps to delineate these inventory-style entries, consistent with administrative Linear B texts. \\n\\nSubordinate clause analysis: No explicit subordinate clauses are present in this fragmented administrative text. The structure is primarily paratactic, listing names and inventory items.\"}\n",
      "Current Document: 5417\n",
      "document: me-ni-jo MUL pe-rai-ko-ra-i-ja ko\n",
      "answer: {'Ancient Greek translation': '   ', 'English translation': 'Menios: Woman from Peraia-goraia, Kos.', 'reasoning': \"Grammatical analysis: 'me-ni-jo' () is interpreted as a proper noun, an anthroponym in the nominative singular, thematic in -o, likely representing a male individual, possibly an overseer or a person associated with the entry. 'MUL' is a logogram for '' (woman), indicating the subject of the record. 'pe-rai-ko-ra-i-ja' () is a toponym, a place name in the dative singular, thematic in -a, functioning as a locative or indicating origin ('from Peraia-goraia'). 'ko' () is a mostly incomplete word classified as an anthroponym/theonym/animal name, thematic in -o. Given the context of an administrative list of people and places, and the provided options, '' (Kos, the island) is the most plausible completion, functioning as an additional place of origin or identification for the woman. Logical/Syntactic analysis: The document follows a typical paratactic structure for Linear B administrative records, which often list a primary individual (e.g., an overseer), followed by a category of people or goods, and then descriptive details such as origin or status. Here, '' acts as the heading or primary subject, '' introduces the woman being recorded, '' specifies her location or origin, and '' provides further geographical identification. No subordinate clauses are present in this simple inventory-style entry.\"}\n",
      "Current Document: 5418\n",
      "document: to-se [...] te-wi-ja\n",
      "answer: {'Ancient Greek translation': ' [...] F', 'English translation': 'so many [...] divine woman', 'reasoning': \"Grammatical analysis: 'to-se' is classified as an incomplete adjective, thematic in -o. The most fitting Ancient Greek translation from the options, considering the thematic -o inflection and the ending, is '' (masculine nominative plural or dual of '', meaning 'so many'). 'te-wi-ja' is classified as an incomplete common noun, thematic in -a. The form 'F' (nominative singular feminine of 'F', meaning 'goddess' or 'divine woman') aligns with this classification. Logical/Syntactic analysis: The presence of '[...]' indicates a significant portion of the text is missing, preventing a complete syntactic or logical analysis. '' would typically modify a plural noun, which is absent. 'F' stands as a noun, but its role (subject, object, or part of a larger phrase) cannot be determined without more context. Subordinate clause analysis: No subordinate clauses are present in this fragmentary text. The translation reflects the incomplete nature of the document, providing the most probable Ancient Greek equivalents for the visible words based on their classifications.\"}\n",
      "Current Document: 5419\n",
      "document: ta [...] a-ko\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'The [...] Agos', 'reasoning': \"Grammatical analysis: The document is highly fragmentary, indicated by `[...]` and the 'MOSTLY_INCOMPLETE' and 'INCOMPLETE' levels for the words. For 'ta', classified as a common noun with thematic in -a inflection, '' (neuter plural nominative/accusative article/pronoun, often functioning as 'the things' or 'these') is chosen as the most plausible interpretation, given its common occurrence in administrative texts and its fit with the thematic -a pattern. For 'a-ko', classified as an anthroponym/animal name/theonym with thematic in -o inflection, '' (Agos), a proper name in the nominative singular, is selected as it directly matches the classification. Logical/Syntactic analysis: Due to the extreme fragmentation and the presence of `[...]`, a complete syntactic or semantic connection between 'ta' and 'a-ko' cannot be established. The translation reflects a fragmented record, possibly indicating a list item or a partial entry where '' might refer to items or categories, and '' is a person's name associated with the entry. Subordinate clause analysis: No subordinate clauses are present in this highly fragmented text.\"}\n",
      "Current Document: 5420\n",
      "document: de [...] to-ra pe\n",
      "answer: {'Ancient Greek translation': ' [...]  ', 'English translation': 'De- [...] game Pe-', 'reasoning': \"Grammatical analysis: 'de' is classified as a mostly incomplete athematic noun, anthroponym. It is interpreted as the beginning of a personal name, '-'. 'to-ra' is classified as a complete common noun, thematic in -a. Among the possible Ancient Greek translations, '' (prey, game) is a thematic in -a noun, consistent with the classifier's inflection. This is a plausible item for an administrative record. It is likely in the nominative or accusative case. 'pe' is classified as a mostly complete athematic noun, anthroponym. It is interpreted as the beginning of a personal name, '-'.\\nLogical/Syntactic analysis: The fragment 'de [...] to-ra pe' suggests an administrative record, likely a list of names and an item. The ' [...] ' indicates missing information, making a precise grammatical connection between 'de', 'to-ra', and 'pe' impossible. The most coherent interpretation for a fragmentary administrative text is a simple enumeration or association. We assume 'de' and 'pe' are names, possibly in the nominative case, and '' is an item being recorded. No subordinate clauses are present in this short fragment.\\nError recovery: The ' [...] ' is explicitly acknowledged in both translations, indicating the fragmentary nature of the document.\"}\n",
      "Current Document: 5421\n",
      "document: ra-o [...] da-so-mo sa-ma-ra do-si-mi-jo-qe\n",
      "answer: {'Ancient Greek translation': ' [...]    ', 'English translation': 'People [...] tribute, Samara, and a payment.', 'reasoning': \"Grammatical analysis: 'ra-o' is interpreted as '' (people), a common noun in the nominative singular, referring to the populace. Its 'INCOMPLETE' status and the following '(...)' indicate a missing part of the text. 'da-so-mo' is translated as '' (tribute, division), a common noun in the nominative singular. 'sa-ma-ra' is rendered as '' (Samara), a proper noun (likely a toponym or anthroponym) in the nominative singular, consistent with its thematic -a inflection. 'do-si-mi-jo-qe' is broken down into '' (payment, due), a common noun in the nominative singular neuter, and the enclitic conjunction '-qe' (, 'and'), which connects it to the preceding element. Logical/Syntactic analysis: The document is fragmentary and appears to be an administrative record, likely a list or inventory, as is common for Linear B texts. There is no explicit finite verb, suggesting a nominal or paratactic structure. '' likely serves as a general heading or a reference to the people involved in the context of the record. '' is the central item, indicating a 'tribute' or 'division'. '' and '' are listed as items or entities associated with this tribute, with '' (and) linking the latter. The '(...)' signifies missing information, which prevents a more complete sentence reconstruction but does not hinder the interpretation of the listed items. Subordinate clause analysis: No subordinate clauses are present in this fragment.\"}\n",
      "Current Document: 5422\n",
      "document: do-so-mo o-wi-de-ta\n",
      "answer: {'Ancient Greek translation': ' f', 'English translation': 'Contribution: Witness', 'reasoning': \"Grammatical analysis: 'do-so-mo' () is interpreted as a noun in the nominative singular, meaning 'contribution' or 'payment', following the thematic -o declension pattern. Although classified as a 'verb', in this form and context, it functions as a noun. 'o-wi-de-ta' (f) is a noun in the nominative singular, meaning 'witness' or 'overseer', following the thematic -a declension pattern for masculine nouns. The digamma (f) is retained for Mycenaean authenticity. Logical/Syntactic analysis: This phrase functions as a nominal entry, typical of Linear B administrative records. '' identifies the item or transaction, and 'f' identifies the individual associated with it, likely as the one who recorded or oversaw the contribution. The structure implies a heading or a direct association between the contribution and the witness, without an explicit finite verb. Subordinate clause analysis: There are no subordinate clauses present in this short phrase.\"}\n",
      "Current Document: 5423\n",
      "document: to-ra-ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'breastplates', 'reasoning': \"Grammatical analysis: The Linear B word 'to-ra-ke' is classified as a common noun with athematic inflection. The most fitting Ancient Greek translation from the provided options is '' (thorakes), which is the nominative/accusative plural form of the athematic noun '' (thorax), meaning 'breastplate' or 'cuirass'. This aligns perfectly with the given grammatical classification. The syllabogram 'to' can represent '', 'ra' can represent '', and 'ke' can represent '', consistent with the mapping rules. Logical/Syntactic analysis: As a single word entry, '' functions as a simple noun, likely representing an item in an inventory or a list, which is a common characteristic of Linear B administrative texts. No complex syntactic structure or subordinate clauses are present in this single-word document.\"}\n",
      "Current Document: 5424\n",
      "document: wi-to-te-ra\n",
      "answer: {'Ancient Greek translation': 'F', 'English translation': 'Vitothera', 'reasoning': \"Grammatical analysis: The Linear B word 'wi-to-te-ra' is classified as an 'INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym', with a 'thematic in -a' inflection. This strongly suggests a feminine proper noun in the nominative singular, which typically ends in - in Ancient Greek. The 'w' in 'wi-' corresponds to the digamma (F) in Ancient Greek. Logical/Syntactic analysis: As a single, incomplete word, there is no complex syntactic structure to analyze. It functions as a standalone entry, most likely a proper name within an administrative context. Subordinate clause analysis: Not applicable for a single word. The translation 'F' is chosen as a plausible Ancient Greek proper name consistent with the given Linear B syllabograms and the specified grammatical classification.\"}\n",
      "Current Document: 5425\n",
      "document: o-na-te-re [...] ko-to-na-o\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Tenants [...] of plots', 'reasoning': \"Grammatical analysis: 'o-na-te-re' is translated as '' (onateres), a nominative plural noun meaning 'tenants' or 'occupiers'. This form is consistent with an athematic noun, as indicated by the classifier. 'ko-to-na-o' is translated as '' (ktoinan), a genitive plural noun meaning 'of plots' or 'of estates'. This form is derived from 'ko-to-na' (), which is a thematic -a stem noun, and the Mycenaean genitive plural ending '-a-o' directly corresponds to Classical Greek '-'. While the classifier for 'ko-to-na-o' states 'thematic in -o', this is likely an error or simplification, as 'ko-to-na' itself is consistently an -a stem in Linear B. The alternative '' (ktmatn) is also a genitive plural of 'plots/possessions' but from a different declension. '' is preferred due to its direct derivation from the Linear B form 'ko-to-na'.\\nLogical/Syntactic analysis: The fragment presents a genitive construction: 'tenants of plots'. This structure is typical of administrative records in Linear B, which often list individuals in relation to land or resources. The ' [...] ' indicates a break in the document, suggesting that this phrase is part of a larger entry, but the core grammatical relationship is clear.\\nSubordinate clause analysis: No subordinate clauses are present in this short fragment.\\nSemantic coherence: The translation 'Tenants of plots' is highly plausible and semantically coherent within the context of Mycenaean administrative documents, which frequently detail land tenure and the roles of individuals associated with land.\"}\n",
      "Current Document: 5426\n",
      "document: o-da-sa-to [...] a-ko-so-ta [...] e-qe-ta [...] e-re-u-te-re\n",
      "answer: {'Ancient Greek translation': '   ', 'English translation': 'Axotas, the heqetas, allotted to the free (person/entity).', 'reasoning': \"Grammatical analysis: The word 'o-da-sa-to' () is interpreted as a 3rd person singular aorist middle verb from , meaning 'to divide' or 'to allot'. 'a-ko-so-ta' () is a proper noun, an anthroponym, in the nominative singular masculine, functioning as the subject. 'e-qe-ta' () is a common noun, also in the nominative singular masculine, acting in apposition to 'a-ko-so-ta', identifying Axotas as 'the heqetas' (a high-ranking official). 'e-re-u-te-re' () is an adjective in the dative singular masculine/neuter, meaning 'to/for a free one', serving as the indirect object or beneficiary of the allotment. The classifier for 'e-re-u-te-re' as 'athematic' is consistent with a dative singular ending in -e/-i, as seen in the declension table for athematic stems. Logical/Syntactic analysis: The sentence structure is Subject (Axotas, the heqetas) + Verb (allotted) + Indirect Object (to the free one). This forms a coherent administrative statement, typical of Linear B documents, where an official performs an action of distribution or assignment. The direct object of the allotment (what was allotted) is implied, which is common in such concise records. Subordinate clause analysis: There are no subordinate clauses present in this fragmentary document. The translation reflects an administrative record of an allotment made by an official to a free individual or entity.\"}\n",
      "Current Document: 5427\n",
      "document: re-ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'To/for the bed/couch', 'reasoning': \"Grammatical analysis: The Linear B word 're-ke' is classified as an 'INCOMPLETE' noun with 'athematic' inflection. Among the provided Ancient Greek options, '' () is the dative singular form of the athematic noun '' (lechos), meaning 'bed' or 'couch'. This form aligns with the 'athematic' inflection. Another plausible option is '' (), dative singular of '' (sakos), meaning 'shield', which also fits the athematic inflection. While the classifier suggests 'anthroponym/animal name/theonym' for the noun type, the most direct and common interpretations of the provided Ancient Greek options ('', '') are common nouns. Given the 'INCOMPLETE' status of the word and the document, and the administrative nature of Linear B texts, a dative singular form indicating an item or its purpose/recipient is a plausible interpretation. We prioritize '' as it is the first listed plausible noun form. Logical/Syntactic analysis: As a single-word document, there is no syntactic structure to analyze. Logically, '' could represent an item in an inventory or a destination/recipient for an implied action or allocation. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5428\n",
      "document: a-ri\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Aris', 'reasoning': \"Grammatical analysis: 'a-ri' is classified as an incomplete athematic noun, specifically an anthroponym. Among the provided Ancient Greek options, '' (Aris) is the most suitable proper name for this syllabogram sequence and its classification. Logical/Syntactic analysis: As a standalone, incomplete word, it most likely represents a proper name, possibly functioning as a heading or a subject in a fragmentary record. Its incompleteness prevents a more detailed syntactic analysis or the formation of a complete sentence. The translation reflects its probable identity as a personal name.\"}\n",
      "Current Document: 5429\n",
      "document: do-ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'gift', 'reasoning': \"Grammatical analysis: The Linear B word 'do-ro' is classified as an incomplete noun, thematic in -o. In Mycenaean Greek, as written in Linear B, final consonants, particularly - (nu) or - (sigma), are frequently omitted. Given the options, 'do-ro' can plausibly represent a thematic -o noun such as  (dron, neuter nominative/accusative singular meaning 'gift') or  (doros, masculine nominative singular meaning 'gift' or 'spear'). Both fit the thematic -o inflection. Logical/Syntactic analysis: As a standalone entry, 'do-ro' most likely functions as a simple noun, possibly indicating an item in an inventory, a record of a transaction, or an offering. The meaning 'gift' () is highly common and contextually appropriate for administrative Linear B texts, which often document distributions, offerings, or tributes. The 'INCOMPLETE' status is consistent with the omission of the final - of . Subordinate clause analysis: Not applicable for a single word entry.\"}\n",
      "Current Document: 5430\n",
      "document: de-we-ro-ai-ko-ra\n",
      "answer: {'Ancient Greek translation': 'F', 'English translation': 'Depheros Aigora', 'reasoning': \"Grammatical analysis: The Linear B word 'de-we-ro-ai-ko-ra' is classified as an incomplete noun, specifically a toponym, with thematic in -a inflection. This suggests a place name, likely feminine, ending in -. Syllabogram mapping: 'de-we-ro' can be interpreted as 'F' (Mycenaean form of '' or ''), where the final sigma is unwritten, a common feature in Linear B. 'ai-ko-ra' directly maps to '', a noun related to '' (goat), often used in place names. Compound formation: Combining these elements, we derive the compound toponym 'F'. Inflection consistency: The final component '' is a feminine noun of the first declension (thematic in -a), which aligns perfectly with the classifier's 'inflection: thematic in -a' for the overall word. Semantic coherence: 'F' (meaning 'terrible' or 'second') combined with '' ('goat-place') forms a plausible and descriptive place name, such as 'Terrible Goat-place' or 'Second Goat-place'. As a toponym, it is best rendered as a proper name. Selection of translation: Among the provided Ancient Greek options, 'f' is the most comprehensive and phonetically consistent interpretation of the Linear B sequence 'de-we-ro-ai-ko-ra', especially considering the common omission of final consonants in Linear B. The Ancient Greek translation 'F' is derived from this option with appropriate diacritics and the Mycenaean digamma.\"}\n",
      "Current Document: 5431\n",
      "document: na\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'na (fragment of a noun)', 'reasoning': \"Grammatical analysis: The Linear B word 'na' is classified as an 'INCOMPLETE' 'noun'. As a single syllabogram, it represents a fragment of a larger word. The direct syllabic transcription into Ancient Greek is ''.\\nLogical/Syntactic analysis: Since 'na' is an incomplete fragment and the entire document consists only of this fragment, it is not possible to construct a meaningful complete sentence or phrase. The translation reflects the fragmentary nature of the input.\\nSubordinate clause analysis: Not applicable as the input is a single, incomplete word.\\nError recovery: The translation acknowledges the 'INCOMPLETE' status of the word 'na' and its classification as a noun. Without additional context or syllabograms, a full and definitive translation of a complete noun cannot be provided. The Ancient Greek translation '' represents the direct phonetic value of the syllabogram, while the English translation explicitly states its fragmentary nature and part of speech.\"}\n",
      "Current Document: 5432\n",
      "document: MUL o-si-to\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'Woman, unfed', 'reasoning': \"Grammatical analysis: 'MUL' is a logogram representing 'woman' (). 'o-si-to' () is classified as an incomplete noun, thematic in -o. Given the context, it functions as an adjective modifying 'MUL', meaning 'unfed' or 'hungry'. It is in the nominative singular, agreeing with the implied singular 'woman'. Logical/Syntactic analysis: This is a nominal entry, typical of administrative records, classifying a person by their status or condition. The structure is simply a logogram followed by a descriptive adjective. No finite verb is present, as is common in such inventory-style entries. Subordinate clause analysis: There are no subordinate clauses in this short phrase. Semantic coherence: The translation 'Woman, unfed' fits the administrative and practical nature of Linear B texts, which often record personnel and their conditions or allocations.\"}\n",
      "Current Document: 5433\n",
      "document: me-te\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Metes', 'reasoning': \"Grammatical analysis: The Linear B word 'me-te' is classified as an 'INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -o' inflection. Among the provided Ancient Greek translation options ('', '', '', ''), '' (Mtes) is the most suitable. While '' typically functions as a nominative plural of a thematic -o noun (e.g., from a singular **), it aligns with the 'anthroponym' and 'thematic in -o' classifications. The syllabogram 'me-te' can represent '' (using eta for the 'e' sound, common in proper names). The 'INCOMPLETE' status of the Linear B word suggests it might be a fragment, but '' is the most plausible complete word interpretation from the given choices that fits the grammatical profile. Logical/Syntactic analysis: As a single-word document, '' functions as a standalone entry, most likely a proper name or a group of individuals in an administrative context. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5434\n",
      "document: a-mo-ta e-qe-si-ja we-je-ke-a\n",
      "answer: {'Ancient Greek translation': '  ', 'English translation': 'Suitable fittings for the companions/chariot-fighters.', 'reasoning': \"Grammatical analysis: The phrase consists of three words, all in the neuter plural nominative/accusative case, indicating agreement. \\n*   `a-mo-ta` is translated as `` (harmota), a common noun meaning 'fittings' or 'wheels', often referring to chariot parts. It is a thematic in -a noun, fitting the neuter plural. \\n*   `e-qe-si-ja` is translated as `` (ephesia), a common noun or adjective, neuter plural. This term is widely interpreted in Mycenaean studies as referring to the `e-qe-ta` (companions, chariot warriors), thus meaning 'belonging to the companions' or 'cavalry-related'. It is a thematic in -a noun/adjective. \\n*   `we-je-ke-a` is translated as `` (weikea), a common adjective, neuter plural, meaning 'suitable' or 'proper'. It is a thematic in -a adjective. \\n\\nLogical/Syntactic analysis: The phrase forms a nominal entry, typical of Linear B administrative records. `` serves as the head noun, modified by two adjectives: `` specifies the type or purpose (for the `e-qe-ta`), and `` describes the quality or condition (suitable). The structure is Noun-Adjective-Adjective, which is grammatically sound for descriptive inventory entries. The overall meaning is 'suitable fittings for the companions/chariot-fighters'.\\n\\nSubordinate clause analysis: There are no subordinate clauses present in this short phrase; it is a simple descriptive nominal construction.\\n\\nLinguistic authenticity and semantic coherence: The chosen Ancient Greek forms and their meanings are consistent with attested Mycenaean vocabulary and the administrative context of Linear B tablets, which frequently inventory equipment for specific groups like the `e-qe-ta`.\"}\n",
      "Current Document: 5435\n",
      "document: jo [...] ko\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Ios [...] Cos', 'reasoning': \"Grammatical analysis: Both 'jo' () and 'ko' () are classified as mostly incomplete nouns, likely proper names or toponyms, with thematic -o inflection. In the absence of further context, interpreting them as nominative singular is the most straightforward for a list-like entry. Logical/Syntactic analysis: The presence of '[...]' indicates a significant gap in the document. With only two fragmented words, a complete sentence cannot be formed. The most logical interpretation for such an entry in a Linear B administrative document is a partial list of names or places. Subordinate clause analysis: No subordinate clauses are present. The translation reflects the fragmentary nature of the input, presenting the most probable interpretations of the isolated words as proper names or toponyms.\"}\n",
      "Current Document: 5436\n",
      "document: ra-ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'He/she/it obtained.', 'reasoning': \"Grammatical analysis: 'ra-ke' is classified as a verb. The most fitting Ancient Greek translation is '', which is the 3rd person singular aorist indicative active form of the verb '' (to obtain by lot, to receive, to get). This form aligns perfectly with the provided part of speech. Logical/Syntactic analysis: As a standalone verb, '' functions as a complete predicate, implying a subject (he/she/it) that obtained something. In the context of Linear B administrative records, this often refers to the acquisition or allocation of resources, land, or duties. Semantic coherence: The meaning 'obtained' or 'received' is highly plausible for a single verb entry in an administrative document, indicating a completed action of acquisition.\"}\n",
      "Current Document: 5437\n",
      "document: we [...] da-je\n",
      "answer: {'Ancient Greek translation': '- [...] f', 'English translation': 'Six- [...] Dais', 'reasoning': \"Grammatical analysis: The word 'we' is interpreted as the numerical prefix - ('six'), based on the critical morphological markers provided, despite its classification as a 'noun' in the input. This prefix typically modifies a following noun or adjective. The word 'da-je' is an incomplete noun, classified as an 'anthroponym/animal name/theonym' with athematic inflection. Among the possible Ancient Greek translations, f (Dais) is chosen as a plausible proper name, consistent with the 'anthroponym' classification. Logical/Syntactic analysis: The presence of '[...]' indicates a significant portion of missing text between 'we' and 'da-je'. This fragmentation prevents the construction of a complete sentence or the establishment of a clear syntactic relationship between the two words. The prefix '-' would normally precede the word it modifies, but that word is absent. 'f' stands as an isolated, incomplete noun. Subordinate clause analysis: No subordinate clauses can be identified due to the extreme fragmentation of the document. The translation reflects the highly fragmentary nature of the Linear B text, providing the most probable interpretations of the isolated elements based on the available linguistic data and critical morphological markers.\"}\n",
      "Current Document: 5439\n",
      "document: ka-ra-ni-jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kranion', 'reasoning': \"Grammatical analysis: The Linear B word 'ka-ra-ni-jo' is classified as a complete noun, thematic in -o, and an anthroponym. Among the provided Ancient Greek options, '' (kranion) fits the thematic -o inflection. As an anthroponym, it is best rendered as the proper name  (Kranion), which is a masculine noun in the nominative singular, following the second declension (-o stem) pattern. Logical/Syntactic analysis: As a single, complete word, 'Kranion' likely functions as a standalone entry, possibly a name in a list or a subject of a record, typical for Linear B administrative texts. Subordinate clause analysis: Not applicable for a single word entry. Semantic coherence: 'Kranion' is a plausible personal name, consistent with the 'anthroponym' classification.\"}\n",
      "Current Document: 5440\n",
      "document: ai-wo-di-jo-no\n",
      "answer: {'Ancient Greek translation': '', 'English translation': \"Of Aiwodion / Aiwodion's\", 'reasoning': \"Grammatical analysis: The Linear B word 'ai-wo-di-jo-no' is classified as a complete noun, an anthroponym, with 'thematic in -o' inflection. However, the most phonetically and morphologically plausible Ancient Greek translation from the given options is ''. This form is the genitive singular of the proper noun ''. While '' is typically a third declension (athematic) noun in Classical Greek, the Linear B script often omits final consonants (like the final sigma in -). Thus, 'ai-wo-di-jo-no' represents '', where 'ai' corresponds to '', 'wo' to '', 'di' to '', 'jo' to '', and 'no' to '' (with the final sigma unwritten). The classifier's 'thematic in -o' might be a broader categorization in Mycenaean context or a simplification, as some third declension nouns could exhibit thematic-like tendencies or be grouped for classification purposes. The ending '-no' for '-' is consistent with Linear B orthography. Logical/Syntactic analysis: As a genitive singular, '' indicates possession or a relationship 'of Aiwodion' or 'Aiwodion's'. Without further context, its precise syntactic role within a larger sentence cannot be determined, but it functions as a possessive modifier. Subordinate clause analysis: Not applicable for a single word. Linguistic authenticity: '' is an attested Mycenaean name, and its transcription into Linear B as 'ai-wo-di-jo-no' is consistent with known orthographic conventions.\"}\n",
      "Current Document: 5441\n",
      "document: SUSx [...] 350 pe-re-i-to\n",
      "answer: {'Ancient Greek translation': ' 350 ', 'English translation': 'Swine: 350, for Peleito.', 'reasoning': \"Grammatical analysis: 'SUSx' is a logogram representing 'swine' (). The 'x' likely denotes a specific type or variant of swine, but the core meaning is clear. '350' is a numerical quantity. 'pe-re-i-to' is classified as a toponym with a thematic -o inflection. In the context of administrative records, a dative form (e.g., ) is the most plausible interpretation, indicating the destination or location 'for/at Peleito'. Logical/Syntactic analysis: The document follows a common Linear B inventory structure: [Item Logogram] [Quantity] [Location]. This paratactic structure lists an item, its count, and its associated place, without requiring a finite verb. Subordinate clause analysis: No subordinate clauses are present in this fragmentary entry. The translation reflects a typical administrative record of livestock allocation or inventory.\"}\n",
      "Current Document: 5442\n",
      "document: pa-ke-te-re 1\n",
      "answer: {'Ancient Greek translation': ' 1', 'English translation': 'Fasteners: 1', 'reasoning': \"Grammatical analysis: 'pa-ke-te-re' () is identified as a common noun, likely in the nominative plural, following an athematic declension pattern. The ending '-te-re' corresponds to the Ancient Greek nominative plural suffix '-' for certain athematic nouns (e.g., '' meaning 'fasteners' or 'joiners'). The numeral '1' indicates the quantity of the item. Logical/Syntactic analysis: This entry represents a simple inventory record, characteristic of Linear B administrative documents. It consists of a noun identifying an item followed by its numerical count. There is no complex sentence structure or verb present. Subordinate clause analysis: No subordinate clauses are present in this short entry. The translation reflects a direct enumeration of goods.\"}\n",
      "Current Document: 5443\n",
      "document: CAPm o-pa\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'He-goat, equipment/arms', 'reasoning': \"Grammatical analysis: 'CAPm' is a logogram representing '' (tragos), meaning 'he-goat' or 'male goat', functioning as a noun in the nominative singular. 'o-pa' is a noun, and from the provided options ('||'), '' (hopla) meaning 'arms, equipment, tools' is the most semantically appropriate for an administrative context. Although '' is a neuter plural noun of the second declension in Classical Greek, its nominative/accusative plural form ends in -a, which aligns with the 'thematic in -a' classification for Mycenaean forms, as neuter plural endings often correspond to this pattern in Linear B. It functions here as a descriptive noun, indicating the purpose or category associated with the he-goat. Logical/Syntactic analysis: This entry is a typical administrative record, likely an inventory item. It consists of a logogram (the item being recorded) followed by a noun that specifies its purpose, type, or destination. There is no finite verb, which is common in such lists. The structure is paratactic, simply juxtaposing the item and its descriptor. Subordinate clause analysis: No subordinate clauses are present in this short entry.\"}\n",
      "Current Document: 5444\n",
      "document: de-mi-ni-jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'bed / mattress', 'reasoning': \"Grammatical analysis: 'de-mi-ni-jo' is classified as a complete common noun with a thematic -o inflection. This corresponds directly to the Ancient Greek neuter noun '' (demnion), which belongs to the second declension (thematic -o stems). The Linear B syllabogram 'jo' accurately represents the Ancient Greek neuter nominative/accusative singular ending '-'. Logical/Syntactic analysis: As a single, complete word, '' functions as a simple noun, likely an entry in an inventory or a label for an item. There are no complex syntactic structures or subordinate clauses to analyze. Semantic coherence: The translation 'bed' or 'mattress' is highly consistent with the administrative and inventory nature of Linear B texts, which frequently list household items, provisions, and goods.\"}\n",
      "Current Document: 5445\n",
      "document: pe-di-e [...] wi\n",
      "answer: {'Ancient Greek translation': ' [...] F', 'English translation': 'To/for the man from the plain [...] Fia', 'reasoning': \"Grammatical analysis: The Linear B word 'pe-di-e' is interpreted as '', the dative singular of the athematic noun '' (man from the plain). This choice aligns with the classifier's indication of an 'athematic' noun and 'common' type, and '' is one of the provided Ancient Greek options. Although 'pe-di-e' is marked as 'INCOMPLETE', '' is a plausible complete word that fits the context. The word 'wi' is interpreted as 'F', a proper noun (anthroponym). This aligns with the classifier's 'noun' and 'anthroponym/animal name/theonym' types. While 'wi' is marked as 'MOSTLY_INCOMPLETE' and the classifier indicates 'thematic in -o' inflection, 'F' (thematic in -a) is chosen from the provided options ('f|f') as the most plausible anthroponym, prioritizing semantic coherence and the 'error recovery' instruction over a strict adherence to the inflection classifier in this instance of conflicting data. 'F' (strength) is less likely as a proper name. Logical/Syntactic analysis: The document is highly fragmentary, indicated by the ' [...] ' separator. '' suggests a dative relationship ('to/for the man from the plain') or a locative ('in the plain'), while 'F' is likely a name. Without further context, the precise relationship between these two elements remains unclear, but it suggests an administrative record involving a person or entity associated with a 'man from the plain' and a name 'Fia'. Subordinate clause analysis: No subordinate clauses are present in this fragmentary document.\"}\n",
      "Current Document: 5446\n",
      "document: 20\n",
      "answer: {'Ancient Greek translation': '', 'English translation': '20', 'reasoning': \"Grammatical analysis: The input '20' is a numerical value. Logical analysis: This is a direct numerical entry, likely representing a count or quantity in an administrative record. Syntactic analysis: As a standalone number, it functions as a simple statement of quantity without complex sentence structure. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5447\n",
      "document: o-pa\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'flask', 'reasoning': \"Grammatical analysis: The Linear B word 'o-pa' is classified as a 'MOSTLY_COMPLETE' common noun with 'thematic in -a' inflection. This morphological pattern strongly suggests a feminine noun of the first declension in Ancient Greek. Among the provided options, '' (olpa), meaning 'flask' or 'jar', is a feminine noun that perfectly aligns with the 'thematic in -a' inflection. While '' (hopla, 'weapons') is also an option, it is typically neuter plural in Classical Greek, making '' a more direct and grammatically consistent choice for a singular feminine noun of the first declension in this context. Logical/Syntactic analysis: As a single word, 'o-pa' likely functions as a label or an item in an inventory, which is consistent with the administrative nature of Linear B documents. Subordinate clause analysis: Not applicable as this is a single word entry.\"}\n",
      "Current Document: 5448\n",
      "document: OVISm o-pa\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'Ram, equipment.', 'reasoning': \"Grammatical analysis: OVISm is a logogram representing 'ram' (, krios), a male sheep. o-pa is classified as a common noun, thematic in -a. The most plausible Ancient Greek translation for o-pa in this context, given the options and administrative nature of Linear B, is  (hopla), meaning 'equipment' or 'implements'. While  is typically the neuter plural of  (a thematic -o stem), its nominative/accusative plural form ending in -a can be broadly aligned with the 'thematic in -a' classification, or it represents a specific Mycenaean declensional pattern. Syntactic analysis: This is a nominal phrase, typical of inventory or administrative entries in Linear B, where a logogram (OVISm) is followed by a descriptive noun (o-pa). There is no explicit verb, implying a simple listing or designation. Logical analysis: The phrase likely indicates a 'ram' designated for or associated with 'equipment' or 'implements', perhaps for a workshop, as a contribution, or for a specific purpose within the palatial economy. Semantic coherence: The translation 'Ram, equipment' fits the administrative and practical nature of Linear B texts, suggesting an item and its associated category or purpose.\"}\n",
      "Current Document: 5449\n",
      "document: WI o-pa\n",
      "answer: {'Ancient Greek translation': 'WI ', 'English translation': 'WI oil-flasks.', 'reasoning': \"Grammatical analysis: The word 'o-pa' is classified as a noun, common, with 'thematic in -a' inflection. This strongly indicates a first declension feminine noun. Among the provided Ancient Greek options, '' (plural of , meaning 'oil-flask' or 'jar') is a first declension feminine noun, making it the most grammatically consistent choice. It is interpreted as being in the nominative or accusative plural. 'WI' is an isolated uppercase letter, functioning as a logogram or abbreviation, and is therefore retained as 'WI' in the Ancient Greek translation, as per instructions for units of measure or logograms. Logical/Syntactic analysis: The entry 'WI o-pa' represents a simple inventory item, typical of Linear B administrative documents. The structure is paratactic, listing a logogram followed by a noun, indicating a quantity or type of item. Subordinate clause analysis: No subordinate clauses are present in this short entry. Semantic coherence: 'Oil-flasks' or 'jars' are plausible items to be recorded in an administrative inventory, especially if 'WI' denotes a material or category of goods.\"}\n",
      "Current Document: 5450\n",
      "document: o-pa\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'equipment', 'reasoning': \"Grammatical analysis: The Linear B word 'o-pa' is classified as a common noun, thematic in -a, and mostly complete. In Ancient Greek, '' (hopla) is a neuter plural noun meaning 'arms, equipment, tools'. The nominative/accusative plural form of neuter nouns often ends in -, which aligns with the 'thematic in -a' inflection provided. The omission of the liquid consonant '' in the Linear B transcription 'o-pa' for '' is a common feature of Mycenaean orthography, where liquids and nasals are often not written before other consonants or at the end of a syllable. Logical/Syntactic analysis: As a single entry in a Linear B document, 'o-pa' most likely represents an item being inventoried or listed. '' (equipment) fits this context well, indicating a quantity of general equipment or arms. Semantic coherence: Given the administrative and inventory-focused nature of most Linear B texts, the translation 'equipment' or 'arms' is highly plausible and semantically coherent within such a context. Subordinate clause analysis: Not applicable for a single word entry.\"}\n",
      "Current Document: 5451\n",
      "document: CAPm\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'he-goat', 'reasoning': \"Grammatical analysis: 'CAPm' is a logogram, representing a male goat. The 'm' suffix indicates masculine gender. The Ancient Greek translation '' is a masculine noun in the nominative singular, meaning 'he-goat' or 'buck'. Logical/Syntactic analysis: This is a simple inventory entry, functioning as a noun phrase. It denotes a single male goat, typical of administrative records. No complex sentence structure or subordinate clauses are present. Semantic coherence: The translation is direct and consistent with the common use of logograms in Linear B for livestock counts.\"}\n",
      "Current Document: 5452\n",
      "document: VIN\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'WINE', 'reasoning': \"Grammatical analysis: 'VIN' is a logogram, representing the noun '' (wine). In Linear B texts, logograms function as direct labels for commodities. Logical/Syntactic analysis: This entry is a simple inventory item, not a complete sentence. It serves to identify the type of good being recorded. Subordinate clause analysis: No subordinate clauses are present as this is a single logogram entry.\"}\n",
      "Current Document: 5453\n",
      "document: VIN e-ti-wa-i\n",
      "answer: {'Ancient Greek translation': ' f', 'English translation': 'Wine at Etiwa.', 'reasoning': \"Grammatical analysis: 'VIN' is a logogram representing  (wine), a common noun. 'e-ti-wa-i' is classified as an incomplete noun, a toponym, and thematic in -a. The syllabogram sequence 'wa-i' (f) strongly suggests a dative singular ending for a first declension (thematic in -a) noun, functioning as a locative. Thus, 'f' means 'at Etiwa'.\\nLogical/Syntactic analysis: The document presents a commodity ('wine') followed by a place name in the locative case. This is a typical structure for administrative records in Linear B, indicating the location where the commodity is found, stored, or designated for. The entry is a nominal phrase, characteristic of inventory or ledger entries, and does not require a finite verb.\\nSubordinate clause analysis: No subordinate clauses are present in this short entry. The translation reflects a direct administrative statement.\"}\n",
      "Current Document: 5454\n",
      "document: VIN me-ri-ti [...] jo\n",
      "answer: {'Ancient Greek translation': '  [...] ', 'English translation': 'Wine with honey [...] Ios.', 'reasoning': \"Grammatical analysis: 'VIN' is the logogram for 'wine' (). 'me-ri-ti' is interpreted as '', the dative singular of '' (honey), an athematic noun, indicating 'with honey' or 'honeyed'. 'jo' is interpreted as '', a proper noun (anthroponym) in the nominative case, thematic in -o. Its 'MOSTLY_INCOMPLETE' status suggests it might be a shortened form or part of a longer name, but '' is a plausible standalone name. Logical/Syntactic analysis: This is an administrative record, likely an inventory or allocation. 'VIN' establishes the item. 'me-ri-ti' describes the item (honeyed wine). 'jo' likely indicates the recipient or a person associated with this specific entry. The '[...]' signifies a missing portion of the record, which could contain further descriptors or quantities. Subordinate clause analysis: No subordinate clauses are present; the structure is a simple nominal inventory entry.\"}\n",
      "Current Document: 5455\n",
      "document: VIN\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'WINE', 'reasoning': \"Grammatical analysis: 'VIN' is a logogram representing a commodity. Its Ancient Greek equivalent is '' (wine), which is a common noun, masculine, singular, nominative. Logical/Syntactic analysis: This entry is a simple logogram, functioning as a noun to identify an item in an inventory or record. It stands alone as a complete, albeit minimal, statement of a commodity. Subordinate clause analysis: No subordinate clauses are present in this single-item entry. The translation reflects a direct identification of the commodity.\"}\n",
      "Current Document: 5456\n",
      "document: TELAI+PU\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'textiles', 'reasoning': \"Grammatical analysis: 'TELAI+PU' is a logogram, where 'TELAI' represents 'textile' and 'PU' is a phonetic complement. The Ancient Greek translation '' is a neuter plural noun, meaning 'textiles' or 'woven materials'. Logical/Syntactic analysis: This entry functions as a simple inventory item, indicating a category of goods. It is a nominal entry without a verb, typical for administrative records. Subordinate clause analysis: None present.\"}\n",
      "Current Document: 5457\n",
      "document: 180 1\n",
      "answer: {'Ancient Greek translation': '180 1', 'English translation': '180 1', 'reasoning': \"Grammatical analysis: The input consists solely of numerical values, '180' and '1'. These are not words subject to grammatical inflection but rather quantitative or identificatory markers. Logical/Syntactic analysis: In the context of Linear B administrative documents, such numerical sequences often represent inventory numbers, item counts, or document identifiers. Without further linguistic context, their precise function remains general, but they clearly serve to quantify or label. Subordinate clause analysis: No linguistic clauses are present, as the input is purely numerical.\"}\n",
      "Current Document: 5458\n",
      "document: A+RE+PA\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ointment', 'reasoning': \"Grammatical analysis: The Linear B sign A+RE+PA is a logogram representing a commodity. The Ancient Greek translation '' (aleiphar) is a neuter noun meaning 'ointment' or 'oil', typically found in the nominative or accusative singular when used as an inventory item. Logical/Syntactic analysis: As a single logogram, it functions as a direct entry, likely an item in an inventory or a heading in an administrative record. There is no complex sentence structure or verb to analyze; it simply identifies the commodity. Subordinate clause analysis: No subordinate clauses are present in this single-item entry. The translation is straightforward, reflecting the common use of logograms in Linear B for listing goods.\"}\n",
      "Current Document: 5459\n",
      "document: *152 a-pu-do-si\n",
      "answer: {'Ancient Greek translation': '*152 ', 'English translation': 'Item *152: Payment/Delivery', 'reasoning': \"Grammatical analysis: The term '*152' is a logogram, which in Linear B texts typically represents a commodity, a unit of measure, or a specific item. It is retained as such in the Ancient Greek translation and described in English. 'a-pu-do-si' () is a noun in the nominative singular, meaning 'payment,' 'delivery,' or 'rendering.' It is formed from the prefix 'a-pu-' (- 'from, away from') and the root '-do-si' ( 'giving'). Although classified as 'thematic in -o' in the input, the Ancient Greek equivalent '' is a third declension noun, which is a common simplification or categorization in Linear B transliteration systems. Logical/Syntactic analysis: This phrase functions as a heading or title for an administrative record. There is no finite verb, which is characteristic of many Linear B inventory or transaction entries. The logogram '*152' specifies the subject of the record, and '' describes the type of transaction or action being recorded. Subordinate clause analysis: No subordinate clauses are present in this short phrase. The translation reflects a typical administrative record format, indicating a payment or delivery related to the item designated by '*152'.\"}\n",
      "Current Document: 5461\n",
      "document: e-ri\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Eris (goddess of strife)', 'reasoning': \"Grammatical analysis: The Linear B word 'e-ri' is classified as an 'INCOMPLETE' noun of type 'anthroponym/animal name/theonym'. Among the provided Ancient Greek options (, , , ), '' (Eris) is the only one that functions as a theonym (the goddess of strife). In Linear B, final consonants, such as the - in , are frequently omitted, making 'e-ri' a plausible representation for the nominative singular form. A notable discrepancy exists with the classifier's 'inflection: thematic in -a', as Classical Greek  is an athematic (third declension) noun. This could indicate a broader Mycenaean grammatical categorization, a simplification in the classification data, or a specific Mycenaean declension pattern not perfectly aligned with later Classical Greek. Given the 'INCOMPLETE' status, a definitive resolution of this inflectional conflict is challenging, but the semantic fit for 'theonym' is strong. Logical/Syntactic analysis: As a single, incomplete word, 'e-ri' functions as a standalone entry, most likely a proper name or label within an administrative context. Semantic coherence: Choosing 'Eris' provides the most semantically coherent translation that aligns with the 'anthroponym/animal name/theonym' classification, despite the inflectional ambiguity.\"}\n",
      "Current Document: 5462\n",
      "document: WA pa-ta-jo do-ka-ma\n",
      "answer: {'Ancient Greek translation': 'WA  ', 'English translation': \"WA: Paltios's beam/timber.\", 'reasoning': \"Grammatical analysis: 'WA' is a logogram, likely representing a commodity or unit of measure, and is left untranslated as per instructions. 'pa-ta-jo' () is interpreted as the genitive singular of a thematic -o stem noun, specifically an anthroponym like  (Paltios), indicating possession. The Mycenaean genitive ending -jo corresponds to Classical Greek - or -. 'do-ka-ma' () is a common noun in the nominative singular, a thematic -o stem, referring to a 'beam' or 'timber'. Logical/Syntactic analysis: The phrase forms a simple nominal entry, typical of administrative records. It lists an item ('') and attributes it to a person ('' in the genitive case). The logogram 'WA' likely serves as a header or quantifier for the entry. Subordinate clause analysis: No subordinate clauses are present in this short entry.\"}\n",
      "Current Document: 5463\n",
      "document: ze-i-ja-ka-ra-na\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Zeiakrana', 'reasoning': \"Grammatical analysis: The Linear B word 'ze-i-ja-ka-ra-na' is classified as an 'INCOMPLETE' 'noun' of 'toponym' type, with 'thematic in -a' inflection. This indicates it is a proper noun, a place name, likely in the nominative or accusative singular, consistent with the common declension patterns for such names in Mycenaean Greek. Logical/Syntactic analysis: As a single, isolated word, it functions as a standalone entry, most probably a heading or an item within an administrative list, which is characteristic of Linear B documents. It directly refers to a specific geographical location. Subordinate clause analysis: Not applicable as this is a single word. Translation justification: The syllabograms 'ze-i-ja-ka-ra-na' are transliterated into Ancient Greek as ''. The syllabogram 'ra' can represent either '' (rho) or '' (lambda); '' is a plausible Mycenaean toponym. The final '-a' is consistent with the thematic -a declension, typically found in feminine singular nouns and place names. Despite its 'INCOMPLETE' status, as a toponym, it is translated as the place name itself.\"}\n",
      "Current Document: 5464\n",
      "document: pa-ki-ja-ne te-re\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'At Pakijanes, for a rite.', 'reasoning': \"Grammatical analysis: 'pa-ki-ja-ne' is interpreted as the dative plural of the toponym '' (Pakijanes), a significant cultic site. The '-ne' syllabogram represents the dative plural ending, with the final sigma often omitted in Linear B. The classifier indicates 'thematic in -a' and 'ethnonym', but its primary function here is locative. 'te-re' is interpreted as the dative singular of the athematic noun '' (telos), meaning 'rite', 'fulfillment', 'tax', or 'office'. The '-e' ending corresponds to the dative singular ''. Although classified as 'INCOMPLETE', '' is a highly plausible reconstruction given the context of Mycenaean administrative and religious records. Logical/Syntactic analysis: The phrase ' ' forms a concise administrative entry, specifying a location ('At Pakijanes') and a purpose or context ('for a rite'). This structure is characteristic of Linear B documents, which often record offerings, distributions, or activities associated with specific places and their administrative or religious functions without an explicit verb. Subordinate clause analysis: No subordinate clauses are present in this short phrase.\"}\n",
      "Current Document: 5465\n",
      "document: pe-re-wo-te pa-ro i-qa-ne [...] pe-re-wo-te pa-ro\n",
      "answer: {'Ancient Greek translation': '   [...]  ', 'English translation': 'At Pelewos, from Ikanes [...] At Pelewos, from [someone].', 'reasoning': \"Grammatical analysis:\\n*   **pe-re-wo-te**: This word is classified as a toponym with thematic -o inflection. The form `f` (Pelewote) is interpreted as the dative singular `` (Pelewotei), indicating a location ('at Pelewos'). The digamma () is restored from the Linear B 'w' syllabogram.\\n*   **pa-ro**: Although classified as a noun, `pa-ro` is consistently interpreted in Mycenaean Greek as the preposition `` (para), meaning 'from' or 'at the hands of', which governs the dative case. This interpretation is prioritized for semantic coherence and consistency with attested Mycenaean usage.\\n*   **i-qa-ne**: This word is classified as an anthroponym with athematic inflection and marked as 'INCOMPLETE'. Given the context with `pa-ro`, it is interpreted as the dative singular `` (Hikanes), derived from an athematic name like ``. The dative case indicates the source or agent from whom something is received.\\n\\nLogical/Syntactic analysis:\\n*   The phrase `[toponym in dative] pa-ro [anthroponym in dative]` establishes a common administrative pattern in Linear B texts, signifying a record of items or transactions 'at [place], from [person]'.\\n*   The repetition of `pe-re-wo-te pa-ro` (` `) suggests a list or a series of entries where the location and the 'from' preposition remain constant, while the specific individual (or other details) would vary or be listed in the missing section indicated by `[...]`.\\n*   The fragmentary nature of the document, marked by `[...]`, implies that the second instance of `pe-re-wo-te pa-ro` would be followed by another personal name or similar information, which is currently missing.\\n\\nSubordinate clause analysis: No subordinate clauses are present. The document consists of paratactic phrases, typical of administrative records and inventories in Linear B.\"}\n",
      "Current Document: 5466\n",
      "document: ka [...] di-da-ma-o\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'ka [...] Didamas', 'reasoning': \"Grammatical analysis: 'di-da-ma-o' is classified as a complete proper noun (anthroponym) with thematic in -o inflection. According to the provided declension table, the nominative singular for thematic -o stems ends in '-o'. Therefore, '' (Didamas) is a plausible translation, with the final - of the Ancient Greek nominative often omitted in Linear B transcription. 'ka' is classified as a mostly incomplete common noun with thematic in -a inflection. Due to its incompleteness and the ' [...] ' separator, its full form and precise meaning cannot be determined, and it is best left as a fragment. Logical/Syntactic analysis: The document is highly fragmentary, consisting of an incomplete word followed by a complete proper noun. Without further context, a complete sentence structure cannot be established. It likely represents an entry in an administrative list, possibly indicating an item or descriptor associated with the individual Didamas. Subordinate clause analysis: No subordinate clauses are present in this short, fragmentary document. Semantic coherence: The translation maintains the fragmentary nature of the original, identifying the complete proper name while acknowledging the uncertainty of the preceding fragment. This is consistent with the nature of many Linear B administrative records.\"}\n",
      "Current Document: 5467\n",
      "document: ma-to\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Mato', 'reasoning': \"Grammatical analysis: The Linear B word 'ma-to' is classified as an 'INCOMPLETE' noun, specifically a 'toponym' with 'thematic in -o' inflection. Among the provided Ancient Greek options (||), '' is the most appropriate translation for a place name. '' is a known Mycenaean toponym. Logical/Syntactic analysis: As a single-word document, 'ma-to' most likely functions as a simple entry or heading, identifying a location. The 'INCOMPLETE' classification likely refers to the document's brevity rather than the word's morphological truncation, as '' is a complete and plausible form for a place name in the nominative case. No complex sentence structure or subordinate clauses are present.\"}\n",
      "Current Document: 5468\n",
      "document: wa-wo-u [...] i-sa-ma\n",
      "answer: {'Ancient Greek translation': 'FF [...] ', 'English translation': 'Of Wawou, [...] Isama.', 'reasoning': \"Grammatical analysis: 'wa-wo-u' (FF) is interpreted as a proper noun (anthroponym or theonym) in the genitive singular, consistent with a thematic -o stem (e.g., from a nominative *Wawous*). The ending -u often represents the genitive - in Mycenaean Greek. 'i-sa-ma' () is interpreted as a proper noun (anthroponym or toponym) in the nominative singular, consistent with a thematic -a stem (e.g., a feminine proper name). Logical/Syntactic analysis: The presence of '[...]' indicates missing text between the two words, making a definitive syntactic relationship impossible without further context. However, in administrative Linear B documents, a genitive followed by a nominative often implies a possessive relationship (e.g., 'Isama, belonging to Wawou') or simply a listing of entities. The translation reflects this by presenting the genitive 'Of Wawou' followed by the nominative 'Isama', acknowledging the missing information. Subordinate clause analysis: No subordinate clauses are present in this fragmentary text. Semantic coherence: The translation maintains the administrative and documentary nature typical of Linear B records by identifying potential names or entities, even in a fragmented state.\"}\n",
      "Current Document: 5469\n",
      "document: ka-da-po da-ma\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'Kardapos, wife.', 'reasoning': \"Grammatical analysis: 'ka-da-po' () is identified as a complete proper noun (anthroponym) in the nominative singular, following a thematic in -o inflection pattern. 'da-ma' () is classified as an incomplete athematic noun, which is best translated as 'wife' (, nominative singular). The 'INCOMPLETE' tag for 'da-ma' likely refers to its Linear B representation rather than the completeness of the Ancient Greek word '' itself, which is a full word. Logical/Syntactic analysis: The phrase consists of a proper name followed by a relational noun. This paratactic structure is common in Linear B administrative documents for identifying individuals and their associated roles or family members. The nominative case for both words suggests an appositive relationship, meaning 'Kardapos, (his) wife'. Semantic coherence: This translation fits the typical administrative context of Linear B tablets, which often list individuals and their familial connections or status.\"}\n",
      "Current Document: 5470\n",
      "document: to-ko [...] di-we si-po-ro ti-mi-to-qo\n",
      "answer: {'Ancient Greek translation': ' [...] f  ', 'English translation': 'Yield [...] to Zeus, (from) Timitoqos the attendant.', 'reasoning': \"Grammatical analysis: 'to-ko' () is a noun, common, thematic in -o, likely in the nominative singular, meaning 'yield' or 'interest' in an administrative context, or 'offspring'. 'di-we' (f) is a noun, theonym, athematic, in the dative singular, meaning 'to Zeus'. 'si-po-ro' () is a noun, common, thematic in -o, likely in the nominative singular, meaning 'shepherd' or 'attendant'. 'ti-mi-to-qo' () is a noun, anthroponym, thematic in -o, likely in the nominative singular, representing a proper name. Logical/Syntactic analysis: The dative 'f' indicates the recipient of an offering or dedication. The phrase ' ' functions as a nominative subject or agent, identifying 'Timitoqos' as 'the attendant'. '' is the item being recorded or dedicated. The ' [...] ' signifies missing information, likely a verb (e.g., 'gives', 'is dedicated') or a quantity, which is common in fragmentary Linear B documents. Semantic coherence: The translation reflects a typical administrative record of an offering or dedication to a deity (Zeus) from a specific individual (Timitoqos) in a particular role (attendant), concerning a 'yield' or 'offering'. 'Yield' is chosen as the most plausible interpretation for '' in an administrative context. No explicit subordinate clauses are present; the structure is paratactic, characteristic of Linear B inventory or dedication texts.\"}\n",
      "Current Document: 5472\n",
      "document: e-ra [...] re\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Era [...] Las', 'reasoning': \"Grammatical analysis: The document is highly fragmentary. 'e-ra' is classified as an uncertain noun, anthroponym, thematic in -a. The most direct interpretation is a proper noun in the nominative singular, such as . 're' is classified as a mostly incomplete noun, anthroponym, athematic. From the given options,  (a proper name or 'stone') is a plausible interpretation for an athematic noun, especially given the anthroponym classification. Logical/Syntactic analysis: Due to the extreme fragmentation indicated by '[...]', a complete syntactic or logical analysis is not possible. The most coherent interpretation for such a short, fragmented entry with anthroponym classifications is a list of names or a very abbreviated administrative record. The '...' explicitly denotes missing information, preventing any further connection between 'e-ra' and 're'. Subordinate clause analysis: No subordinate clauses are present in this fragmentary text.\"}\n",
      "Current Document: 5473\n",
      "document: ke-re-te-u-ti-no\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kreteutinos', 'reasoning': \"Grammatical analysis: The Linear B word 'ke-re-te-u-ti-no' is classified as a complete noun, specifically an anthroponym (personal name), with athematic inflection. This indicates it is a proper name. Syllabogram mapping: 'ke' maps to '', 're' maps to '' (a common representation of /r/ or /r/ in Mycenaean, often reflecting a lengthened vowel or specific dialectal pronunciation), 'te' maps to '', 'u' maps to '', 'ti' maps to '', and 'no' maps to ''. Candidate selection: Among the provided Ancient Greek options, '' (Kreteutinos) is the most phonetically consistent and semantically plausible for a proper name. The initial '-' element strongly suggests a connection to Crete (), which is highly relevant given the context of Linear B. The suffix '-' is a common adjectival or nominal ending in Greek, often used for personal names or to denote origin. Logical/Syntactic analysis: As a standalone proper noun, 'Kreteutinos' would function as a name in an administrative record, likely identifying an individual. Without further context, its role is simply to name. Semantic coherence: The translation 'Kreteutinos' is a plausible Mycenaean Greek personal name, fitting the administrative and documentary nature of Linear B texts.\"}\n",
      "Current Document: 5474\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 1 [...] o-pe-ro\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: 1 [...] o-pe-ro\n",
      "answer: {'Ancient Greek translation': '1 [...] ', 'English translation': '1 [...] debt/deficit', 'reasoning': \"Grammatical analysis: The numeral '1' indicates a quantity. 'o-pe-ro' is classified as a noun with thematic -o inflection. Among the provided Ancient Greek options (||), '' (neuter, thematic -o) meaning 'debt' or 'deficit' is the most appropriate translation in the context of administrative Linear B documents, where 'o-pe-ro' frequently denotes an outstanding amount or obligation. While '' also means 'debt', its feminine gender is less consistent with the thematic -o inflection of 'o-pe-ro' than ''. '' and '' mean 'benefit' or 'advantage', which is less likely in this context. Logical/Syntactic analysis: The fragment '1 [...] o-pe-ro' suggests an entry in an administrative record, likely indicating a quantity (1) of something that is a 'debt' or 'deficit'. The '...' signifies missing context, but the core meaning remains clear for 'o-pe-ro'. Subordinate clause analysis: None present in this fragment. The translation reflects a typical administrative entry.\"}\n",
      "Current Document: 5475\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: po-ra\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: po-ra\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Sowing / Crop', 'reasoning': \"Grammatical analysis: The Linear B word 'po-ra' is classified as an incomplete common noun with a thematic in -a inflection. The Ancient Greek translation '' (sowing, seed, crop) is a feminine noun of the first declension (thematic in -a), nominative singular, which perfectly aligns with the provided grammatical classification. Logical/Syntactic analysis: As an isolated and incomplete word, 'po-ra' most likely represents a single entry in an administrative record. The meaning 'sowing' or 'crop' is highly consistent with the typical content of Linear B tablets, which frequently document agricultural activities and produce. Subordinate clause analysis: Not applicable for a single word entry. Semantic coherence: '' provides a semantically coherent and contextually appropriate translation for an agricultural term in a Mycenaean administrative document.\"}\n",
      "Current Document: 5476\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e-pe [...] 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: e-pe [...] 6\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'Six words/verses.', 'reasoning': \"Grammatical analysis: The Linear B word 'e-pe' is classified as an incomplete noun, thematic in -o. The most fitting Ancient Greek translation for a noun that can be counted and corresponds to 'e-pe' is  (plural of , meaning 'word', 'utterance', or 'epic poem'). While  is a neuter noun of the thematic -o declension, its plural nominative/accusative form is . Linear B often simplifies vowel endings, making 'e-pe' a plausible representation of . The numeral '6' directly quantifies this noun. Logical/Syntactic analysis: The fragment 'e-pe [...] 6' follows a common administrative pattern of listing an item followed by a quantity. The '...' indicates missing context, but the core structure of 'X number of Y' is clear. Subordinate clause analysis: No subordinate clauses are present in this brief fragment. Semantic coherence: Counting 'words' or 'verses' () is a semantically coherent interpretation for an administrative record, possibly referring to entries, texts, or sections within a document.\"}\n",
      "Current Document: 5477\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 6\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: 6\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'six', 'reasoning': \"Grammatical analysis: The item '6' is a numeral. In Linear B texts, numerals typically represent quantities or tallies. Logical/Syntactic analysis: As the sole element in the document, '6' functions as a direct numerical statement. There is no complex sentence structure or logical relationship to analyze beyond its inherent numerical value. Subordinate clause analysis: No subordinate clauses are present in this single-item document. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5478\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ro-si-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ro-si-ja\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'dew / moisture', 'reasoning': \"Grammatical analysis: The Linear B word 'ro-si-ja' is classified as an incomplete common noun, thematic in -a. Among the provided Ancient Greek options, '' (rosia) directly corresponds to the syllabograms 'ro-si-ja' and is a feminine noun of the first declension (thematic in -a), meaning 'dew' or 'moisture'. This aligns perfectly with the given grammatical information. Logical/Syntactic analysis: As a single, incomplete word, 'ro-si-ja' most likely represents a standalone entry in an administrative list or a fragment of a larger phrase. Its function as a common noun suggests it would denote an item or substance. Subordinate clause analysis: No subordinate clauses are present as the input consists of a single word.\"}\n",
      "Current Document: 5479\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 31\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: 31\n",
      "answer: {'Ancient Greek translation': '31', 'English translation': '31', 'reasoning': \"Grammatical analysis: The input '31' is a numeral. In Linear B documents, numerals often appear as standalone entries, representing counts or identifiers. Logical/Syntactic analysis: As a single numeral, it functions as a direct quantity or a label. There is no complex sentence structure or grammatical relationship to analyze beyond its numerical value. Subordinate clause analysis: Not applicable as there are no clauses present. The translation directly reflects the numerical value, consistent with the administrative nature of Linear B texts where numbers are frequently recorded without explicit linguistic context.\"}\n",
      "Current Document: 5480\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: 21\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: 21\n",
      "answer: {'Ancient Greek translation': '21', 'English translation': '21', 'reasoning': \"Grammatical analysis: The input '21' is a numerical value. In Linear B texts, numbers typically represent counts, quantities, or identifiers. Logical/Syntactic analysis: As a standalone numerical entry, it functions as a direct statement of quantity or an index. No complex sentence structure or subordinate clauses are present. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5481\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-ja-no [...] se-wa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ti-ja-no [...] se-wa\n",
      "answer: {'Ancient Greek translation': ' [...] F', 'English translation': 'Thianos [...] Sewa', 'reasoning': \"Grammatical analysis: 'ti-ja-no' is classified as an incomplete noun, an anthroponym with thematic -o inflection. The most plausible Ancient Greek translation is '' (Thianos), interpreted as a nominative singular proper noun. 'se-wa' is also classified as an incomplete noun, an anthroponym with thematic -a inflection. The most fitting Ancient Greek translation is 'F' (Sewa), interpreted as a nominative singular proper noun. Logical/Syntactic analysis: The presence of '[...]' indicates a significant gap in the Linear B document. Both 'ti-ja-no' and 'se-wa' are proper names in the nominative case. In administrative texts, such names frequently appear in lists or as subjects of entries. Without further context, the fragment suggests a simple mention or listing of two individuals, separated by an unknown amount of text. Subordinate clause analysis: No subordinate clauses are present in this short, fragmentary text.\"}\n",
      "Current Document: 5482\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ne-re-we\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ne-re-we\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'pitiless ones / ruthless ones', 'reasoning': \"Grammatical analysis: The Linear B word 'ne-re-we' is classified as an 'INCOMPLETE' athematic noun. The most plausible Ancient Greek translation from the given options is 'f' (nlewes). This form is an athematic adjective/noun, typically found in the nominative or accusative plural (masculine/feminine or neuter). The Linear B ending '-we' corresponds to the Ancient Greek '-f', where 'f' represents the digamma, a sound present in Mycenaean Greek but later lost in Classical Greek. Logical/Syntactic analysis: As a single, isolated word, 'f' functions as a standalone descriptor or a plural noun. Without further context, its specific syntactic role (e.g., subject, object, or predicate adjective) cannot be definitively determined. Subordinate clause analysis: Not applicable as the input consists of a single word. The 'INCOMPLETE' classification for 'ne-re-we' suggests it might be a fragment of a larger Mycenaean word, but 'f' is a complete and attested Ancient Greek word that morphologically fits the Linear B transcription.\"}\n",
      "Current Document: 5483\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ke-wa-o [...] o-pe-ro 8 [...] a-ka-na-jo\n",
      "answer: {'Ancient Greek translation': 'FF   ', 'English translation': 'Kewawos: debt of 8 (units) at Akanaios.', 'reasoning': \"Grammatical analysis: 'ke-wa-o' (FF) is interpreted as a proper noun (anthroponym) in the nominative singular, thematic in -o, functioning as the subject or a heading for the record. 'o-pe-ro' () is interpreted as a common noun in the nominative singular, thematic in -o, meaning 'debt' or 'deficit'. While the classifier suggests an anthroponym, the overwhelming evidence from Linear B administrative texts, especially when followed by a numeral, points to 'debt' or 'deficit' as the most semantically coherent translation. The numeral '8' () quantifies the preceding 'debt'. 'a-ka-na-jo' () is interpreted as a toponym in the nominative singular, thematic in -o, indicating the location associated with the record. Logical/Syntactic analysis: The structure follows a typical Linear B administrative pattern: [Person/Entity] [Item/Quantity] [Location]. The implied verb 'is' or 'has' is common in such inventory-style records. The fragmentary nature of the document, indicated by '[...]', means that connecting words or phrases are missing, but the core information is discernible. Subordinate clause analysis: No subordinate clauses are present in this fragmentary record.\"}\n",
      "Current Document: 5484\n",
      "document: e\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'O [missing noun]!', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'e' is classified as a 'MOSTLY_INCOMPLETE' 'noun' with 'thematic in -o' inflection. In Ancient Greek, the vocative singular of a thematic -o stem noun ends in -. Given the extreme brevity and incompleteness of the input, and the classifier's information, the most plausible grammatical interpretation for 'e' as a standalone noun form is the vocative singular ending. The full noun stem is missing, preventing a complete semantic translation of the specific noun. Logical/Syntactic analysis: As a single, incomplete vocative ending, it implies an address to an unspecified individual or entity. Subordinate clause analysis: Not applicable for a single syllabogram.\"}\n",
      "Current Document: 5485\n",
      "document: ra-na-i\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'wool / fleeces', 'reasoning': \"Grammatical analysis: The Linear B word 'ra-na-i' is classified as an 'INCOMPLETE' common noun with 'thematic in -a' inflection. The syllabograms 'ra-na-i' correspond to Ancient Greek '' (or '', but '' for wool is more common). The ending '-ai' is characteristic of the nominative plural or dative singular of first declension (thematic in -a) nouns. '' (, 'wool' or 'fleece') fits this pattern. Logical/Syntactic analysis: As a single, incomplete word, there is no complex sentence structure. It likely represents an entry in an administrative record, possibly indicating a quantity of wool (nominative plural) or a recipient/destination related to wool (dative singular). Semantic coherence: 'Wool' or 'fleeces' is a highly plausible commodity in Mycenaean administrative texts, making this translation semantically coherent within the typical context of Linear B documents. Due to the 'INCOMPLETE' status and lack of further context, the precise case (nominative plural or dative singular) remains ambiguous, but the core meaning of 'wool' is clear.\"}\n",
      "Current Document: 5486\n",
      "document: ra-te-o\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Lateos (a personal name)', 'reasoning': \"Grammatical analysis: The Linear B word 'ra-te-o' is classified as an incomplete noun, specifically an anthroponym (personal name), with a thematic in -o inflection. The syllabogram 'ra' can correspond to Ancient Greek '' or ''. The final 'o' syllabogram in Linear B frequently represents the nominative singular ending '-' for thematic (-o stem) nouns in Ancient Greek (e.g., ko-to-no-o-ko  ). Among the provided Ancient Greek options, '' (Lateos) is a plausible proper name in the nominative singular, consistent with a thematic -o stem noun. While '' is also phonetically possible, '' is a more commonly attested form or derivation. Logical/Syntactic analysis: As a single, incomplete word, 'ra-te-o' most likely functions as a standalone entry, such as a name in a list or a heading within an administrative document. The nominative case is the most common for such isolated entries. Subordinate clause analysis: Not applicable as this is a single word entry. Semantic coherence: Translating 'ra-te-o' as the proper name 'Lateos' maintains the administrative and documentary nature typical of Linear B texts, which often list individuals.\"}\n",
      "Current Document: 5487\n",
      "document: pe-se-wi-ja\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Pesewia', 'reasoning': \"Grammatical analysis: The word 'pe-se-wi-ja' is classified as an incomplete noun, specifically an anthroponym/animal name/theonym, with a thematic in -a inflection. The '-ja' ending is a common marker for feminine nouns of the first declension in Ancient Greek (e.g., -). The 'wi' syllabogram indicates the presence of a digamma (). Logical/Syntactic analysis: As a single, isolated word, 'Pesewia' most likely functions as a proper name, possibly a nominative subject or a heading in an administrative context. Without further context, its precise syntactic role cannot be definitively determined beyond being a proper noun. Subordinate clause analysis: Not applicable for a single word. Translation justification: The Ancient Greek translation '' (Pesewia) is a direct and phonetically plausible transcription of the Linear B syllabograms, incorporating the digamma () for 'w' as is characteristic of Mycenaean Greek. The English translation is a direct transliteration of this proper name.\"}\n",
      "Current Document: 5488\n",
      "document: jo-i\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'to/for the (ones)', 'reasoning': \"Grammatical analysis: The Linear B word 'jo-i' is classified as an incomplete common noun with thematic -o inflection. Based on the syllabogram mapping (jo  , , ; i  ) and the thematic -o inflection, the most direct and grammatically consistent Ancient Greek translation is ''. This form can represent either the nominative plural or the dative plural of a thematic -o stem noun in Mycenaean Greek. Logical/Syntactic analysis: As a single, incomplete word, its full syntactic role is undetermined without further context. However, if '' is taken as a complete form, it would function as a subject (nominative plural) or an indirect object/locative (dative plural). Subordinate clause analysis: Not applicable for a single word. The English translation reflects the potential dative case, which is a common function for such an ending, while acknowledging the lack of specific semantic context.\"}\n",
      "Current Document: 5489\n",
      "document: 30\n",
      "answer: {'Ancient Greek translation': '30', 'English translation': '30', 'reasoning': \"Grammatical analysis: The input '30' is a numeral, representing a quantity. Logical/Syntactic analysis: This document consists solely of a numeral, which in Linear B administrative texts typically denotes a count or a total. It functions as a data point rather than a complete sentence. Subordinate clause analysis: No clauses are present. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5490\n",
      "document: 210\n",
      "answer: {'Ancient Greek translation': '210', 'English translation': '210', 'reasoning': \"Grammatical analysis: The input '210' is a numerical value. In Linear B texts, numbers typically represent quantities, tallies, or identifiers. Logical/Syntactic analysis: As a standalone numerical entry, it functions as a direct statement of quantity or an identifier without requiring complex sentence structure. No verbs, nouns, or other grammatical elements are present to form a sentence. Subordinate clause analysis: Not applicable as there are no clauses. The translation is a direct representation of the numerical value.\"}\n",
      "Current Document: 5491\n",
      "document: di-wo-nu-so [...] tu-ni-jo [...] i-pe-ne-o [...] wo-no-wa-ti-si\n",
      "answer: {'Ancient Greek translation': 'f [...]  [...]  [...] ff', 'English translation': 'Dionysus [...] Thynios [...] of Ipheneus [...] to/for the wine-makers/wine-vats', 'reasoning': \"Grammatical analysis: The document is highly fragmentary, consisting of a series of nouns or epithets. 'di-wo-nu-so' (f) is interpreted as the nominative singular of the deity Dionysus, a thematic -o stem. 'tu-ni-jo' () is also a thematic -o stem, likely an epithet for Dionysus or another proper name in the nominative singular. 'i-pe-ne-o' () is a thematic -o stem in the genitive singular, likely a proper name or an adjective meaning 'of Ipheneus' or 'of the mighty one'. 'wo-no-wa-ti-si' (ff) is an athematic noun in the dative plural, derived from a word related to 'wine' (), likely referring to 'wine-makers' or 'wine-vats', indicating a recipient or purpose. The '-si' ending is a clear marker for dative plural in athematic stems. Logical/Syntactic analysis: Due to the extensive fragmentation indicated by '[...]', a complete sentence structure cannot be definitively established. The sequence appears to be a list of names, possibly a deity and an associated epithet, followed by a genitive indicating possession or origin, and then a dative plural indicating a recipient or purpose. This structure is common in administrative Linear B tablets, which often record allocations, offerings, or personnel lists. The presence of Dionysus and a term related to wine suggests a religious or cultic context, possibly detailing offerings made to Dionysus or allocations to individuals involved in wine production. Subordinate clause analysis: No subordinate clauses can be identified given the fragmentary nature of the text.\"}\n",
      "Current Document: 5492\n",
      "document: ra-ja-mo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Laiamos', 'reasoning': \"Grammatical analysis: The Linear B word 'ra-ja-mo' is classified as a complete noun, specifically an anthroponym (proper name), with a thematic in -o inflection. This indicates it follows the pattern of Ancient Greek second declension nouns. The most fitting Ancient Greek translation from the provided options that matches this grammatical profile and is a plausible proper name is '' (Laiamos), which is a nominative singular form of a thematic -o stem noun. Syntactic analysis: As a single, standalone word, 'ra-ja-mo' functions as a proper noun, likely representing an individual's name in an administrative record or list. Semantic coherence: The translation 'Laiamos' maintains semantic coherence by identifying the entry as a personal name, which is common in Linear B documents.\"}\n",
      "Current Document: 5493\n",
      "document: ko-wo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'boy', 'reasoning': \"Grammatical analysis: 'ko-wo' is classified as a noun, common type, with thematic in -o inflection. This corresponds to the nominative singular of a masculine noun of the second declension in Ancient Greek. The most direct and widely accepted translation is '' (kouros), meaning 'boy' or 'young man', which preserves the thematic -o ending. The 'w' in 'ko-wo' represents the digamma (f), which was present in Mycenaean Greek (f/f) and later dropped in Classical Greek, resulting in . Logical/Syntactic analysis: As a single word, 'ko-wo' functions as a simple noun, likely representing an entry in an administrative list or a designation of an individual. Subordinate clause analysis: Not applicable as this is a single word entry.\"}\n",
      "Current Document: 5494\n",
      "document: 4 [...] 4\n",
      "answer: {'Ancient Greek translation': '4 [...] 4', 'English translation': '4 [...] 4', 'reasoning': \"Grammatical analysis: The input consists solely of numerals, which function as simple counts. Logical/Syntactic analysis: The '4' at the beginning and end, separated by '[...]', indicates a fragmented record. This could represent a count of four items, or two separate entries each tallying four, with the intervening text lost. Without further context, it is interpreted as a direct numerical transcription of a fragmentary administrative record. Subordinate clause analysis: No verbs or complex sentence structures are present, thus no subordinate clauses are identified.\"}\n",
      "Current Document: 5495\n",
      "document: 2\n",
      "answer: {'Ancient Greek translation': '2', 'English translation': '2', 'reasoning': \"Grammatical analysis: The input '2' is a numeral, functioning as a quantifier. In Linear B, numerals are often found as standalone entries or associated with logograms to denote quantities in administrative records. Logical/Syntactic analysis: As a single numerical entry, it represents a count or an item identifier within a list or inventory, which is characteristic of Linear B documents. There is no complex sentence structure or grammatical relationship to analyze beyond its quantitative function. Subordinate clause analysis: No subordinate clauses are present as the input consists solely of a numeral. Semantic coherence: The most direct and coherent translation is simply the number '2'.\"}\n",
      "Current Document: 5499\n",
      "document: 1\n",
      "answer: {'Ancient Greek translation': '1', 'English translation': '1', 'reasoning': \"Grammatical analysis: The input '1' is a numeral, functioning as a quantifier. In Linear B, numerals are represented by specific signs, and in administrative contexts, they denote counts of items, individuals, or quantities. Logical/Syntactic analysis: As a standalone entry, '1' most likely represents a single unit, item, or a sequential identifier within a list or inventory. Without additional context, it serves as a direct numerical statement. Subordinate clause analysis: No subordinate clauses are present as the input consists solely of a numeral. Semantic coherence: The most direct and contextually appropriate translation for the numeral '1' is simply '1', reflecting its use in administrative records.\"}\n",
      "Current Document: 5504\n",
      "document: 2\n",
      "answer: {'Ancient Greek translation': '2', 'English translation': '2', 'reasoning': \"Grammatical analysis: The input '2' is a numeral. In Linear B documents, numerals are typically represented by digits, functioning as quantifiers. Logical/Syntactic analysis: As a standalone entry, this numeral most likely represents a count or an item number within an administrative record. Without additional context, it serves as a direct numerical statement. Subordinate clause analysis: No subordinate clauses are present as the input consists solely of a numeral. Semantic coherence: The most direct and coherent translation for the numeral '2' is simply '2', consistent with the administrative and inventory-keeping nature of Linear B texts.\"}\n",
      "Current Document: 5511\n",
      "document: 1\n",
      "answer: {'Ancient Greek translation': '1', 'English translation': '1', 'reasoning': \"Grammatical analysis: The input '1' is a numeral. In Linear B texts, numerals often serve as identifiers or counts. Logical/Syntactic analysis: As a standalone numeral, it likely functions as a document identifier, a list item number, or a simple count. Without further context, its precise role is limited to being a numerical marker. Subordinate clause analysis: Not applicable as this is a single numeral.\"}\n",
      "Current Document: 5512\n",
      "document: 1\n",
      "answer: {'Ancient Greek translation': '1', 'English translation': '1', 'reasoning': \"Grammatical analysis: The input '1' is a numeral. In Linear B documents, numerals typically represent quantities of items, individuals, or units of measure. Logical/Syntactic analysis: As a standalone entry, '1' functions as a simple count. Without additional context (such as a preceding logogram or noun), the specific item being counted is not specified, but its role as a numerical value is clear. This is a common form of entry in administrative records. Subordinate clause analysis: No subordinate clauses are present as the input consists solely of a numeral.\"}\n",
      "Current Document: 5513\n",
      "document: pa-ra\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Pala (incomplete noun)', 'reasoning': \"Grammatical analysis: The Linear B word 'pa-ra' is classified as an 'INCOMPLETE' 'noun' of 'common' type with 'athematic' inflection. This means it does not follow the typical -o or -a stem declension patterns. Lexical choice: Among the provided Ancient Greek options, '' and '', '' is selected as the most plausible translation for a noun. While '' is overwhelmingly a preposition or adverb in Ancient Greek, '' could represent a noun, potentially a truncated form of a Mycenaean-specific term. Semantic interpretation: Given the word's 'INCOMPLETE' status and its 'athematic' inflection, which does not align with common Classical Greek nouns like '' (wrestling) or '' (palm/hand) that are typically thematic, the precise meaning remains highly uncertain. It likely refers to an object, place, or concept relevant to the administrative context of Linear B, but a definitive semantic translation is not possible without further context or a complete form. The translation 'Pala' reflects this incompleteness and ambiguity.\"}\n",
      "Current Document: 5515\n",
      "document: ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'you lie / you are placed', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ke' is classified as a 'MOSTLY_INCOMPLETE' verb. From the provided Ancient Greek options, '' (2nd person singular present indicative of , 'to lie' or 'to be placed') is chosen as a complete verb form that plausibly begins with 'ke'. While 'ke' alone is a single syllable, the instruction to provide 'complete translations' necessitates inferring a full verb form through 'error recovery' for the fragment. Logical/Syntactic analysis: As a standalone, incomplete word, its syntactic role is ambiguous without further context. However, assuming it represents a verb, '' would function as the predicate of a sentence with an implied second-person singular subject. Subordinate clause analysis: Not applicable for a single, incomplete word. Linguistic authenticity/Semantic coherence: '' is a common verb in Ancient Greek, and its forms are plausible for Mycenaean. The choice of '' is an attempt at 'error recovery' to provide a complete verb translation for a 'MOSTLY_INCOMPLETE' fragment, as required by the task.\"}\n",
      "Current Document: 5519\n",
      "document: e\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'a fragment of a noun', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'e' is classified as a 'mostly incomplete' common noun with 'thematic in -o' inflection. As a single syllabogram, it is too short to definitively identify a specific noun or its full inflectional form. The provided Ancient Greek translation options are '', '', ''. '' (one) is a numeral/adjective and does not fit the 'thematic in -o' inflection. '' and '' are simply letters representing the sound of the syllabogram. Given the extreme incompleteness, '' is chosen as the most direct phonetic representation. Logical/Syntactic analysis: As a single, incomplete syllabogram, 'e' cannot form a coherent sentence or phrase. It represents a fragment of a larger, unknown word. Subordinate clause analysis: Not applicable. Translation decision: Due to the extreme incompleteness of the input and the inherent ambiguity of a single syllabogram, a definitive translation into a complete Ancient Greek word is not possible. The Ancient Greek translation '' represents the phonetic value of the syllabogram. The English translation 'a fragment of a noun' acknowledges its fragmentary nature and aligns with the provided grammatical classification as a noun, without attempting to reconstruct a specific word that is not present in the input or explicitly among the provided options.\"}\n",
      "Current Document: 5520\n",
      "document: pa\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'all (feminine singular)', 'reasoning': \"Grammatical analysis: The Linear B word 'pa' is classified as an adjective with 'thematic in -a' inflection and a 'MOSTLY_COMPLETE' preservation level. The Ancient Greek adjective '' (all, every) has a feminine nominative singular form '', which belongs to the first declension (thematic in -a). Although 'pa' is a very short representation, its classification as 'thematic in -a' strongly suggests it represents '' rather than the masculine '' (which is athematic). This interpretation aligns with the provided morphological information. Logical/Syntactic analysis: As a single, possibly incomplete, adjective, '' would modify an implied feminine noun. Without further context, its specific syntactic role is limited to being an attributive or predicative adjective. Subordinate clause analysis: Not applicable for a single word. Error recovery: The provided Ancient Greek translation option '' is masculine and athematic, which conflicts with the classifier's 'thematic in -a' inflection. To ensure consistency with the detailed morphological classification, 'pa' is recovered as '', which is the feminine form of the adjective 'all' and fits the 'thematic in -a' pattern.\"}\n",
      "Current Document: 5524\n",
      "document: za-ka-wo\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'Zakawos', 'reasoning': \"Grammatical analysis: 'za-ka-wo' is identified as an incomplete noun, specifically an anthroponym (personal name), with a thematic in -o inflection. This suggests a proper noun, likely in the nominative or vocative case, as is common for entries in administrative texts. The final '-o' syllabogram directly corresponds to the vowel sound. Logical/Syntactic analysis: As a single, isolated word, 'za-ka-wo' functions as a standalone entry, most probably a name in a list or a heading. There is no complex sentence structure to analyze. Subordinate clause analysis: No subordinate clauses are present. Translation justification: The Ancient Greek translation 'f' is selected as the most direct and phonetically consistent transliteration of the Linear B syllabograms, representing the proper name. The English translation 'Zakawos' is a conventional rendering of such a Mycenaean name. Given the 'INCOMPLETE' status and lack of further context, the translation focuses on identifying the name itself rather than a specific grammatical case.\"}\n",
      "Current Document: 5525\n",
      "document: ki-ri\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'cloak', 'reasoning': \"Grammatical analysis: The Linear B word 'ki-ri' is classified as an incomplete, common, athematic noun. This immediately rules out '' (krin), which is a verb. We are left with '' (kilis) and '' (kiris), both of which are athematic (third declension) nouns. Lexical choice: Both '' (meaning 'cloak' or 'Cilician') and '' (meaning 'hawk' or 'falcon') are plausible common nouns that could appear in administrative Linear B texts. Given the general nature of these records, '' referring to a 'cloak' or a 'Cilician' (person/item from Cilicia) is a reasonable choice for a common noun. Completeness: Although 'ki-ri' is marked as 'INCOMPLETE', the task requires a complete translation. Therefore, the full reconstructed word '' is provided. Syntactic/Logical analysis: As a single, incomplete noun, '' likely functions as an entry in an inventory or a descriptor within a fragmentary administrative record, typical of Linear B documents. Without further context, its precise role (e.g., subject, object, item in a list) cannot be definitively determined, but its presence as a common noun is consistent with the document type.\"}\n",
      "Current Document: 5526\n",
      "document: 4\n",
      "answer: {'Ancient Greek translation': '4', 'English translation': '4', 'reasoning': \"Grammatical analysis: The item '4' is a numeral. Logical/Syntactic analysis: This is a standalone numerical entry, likely representing a count or an identifier within an administrative record. There are no complex grammatical structures or relationships to analyze. Subordinate clause analysis: No subordinate clauses are present. The translation is direct as it is a simple numerical value.\"}\n",
      "Current Document: 5527\n",
      "document: 3\n",
      "answer: {'Ancient Greek translation': '3', 'English translation': '3', 'reasoning': \"Grammatical analysis: The input '3' is a numeral. Logical/Syntactic analysis: This is a standalone numerical entry, likely representing a count, an identifier, or a quantity in an administrative context. No complex grammatical structure or syntax is present. Subordinate clause analysis: Not applicable as there are no clauses.\"}\n",
      "Current Document: 5528\n",
      "document: e-re-e\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'he/she/it pities', 'reasoning': \"Grammatical analysis: The Linear B word 'e-re-e' is classified as a verb. Among the provided Ancient Greek options, '' () is the 3rd person singular present active indicative form of the verb , meaning 'to pity' or 'to have mercy'. This form directly corresponds to the syllabic structure 'e-re-e' in Mycenaean Greek, where the final 'e' can represent the contracted '' ending, and the 're' syllabogram can represent the '' sound (as '' is an option for 're'). Other provided options like '' () are less direct syllabic matches. '' () is also a plausible verb (infinitive 'to ask/row'), but '' provides a complete finite verb form. Logical/Syntactic analysis: As a single word, 'e-re-e' functions as a complete predicate. In the absence of a stated subject, it implies an unstated third-person singular subject (he, she, or it). Subordinate clause analysis: Not applicable as the document consists of a single word. Semantic coherence: While context is missing for a single word, 'pities' is a plausible action for a verb in any context.\"}\n",
      "Current Document: 5529\n",
      "document: 1 [...] wo-tu-ko [...] 1 [...] po-ki\n",
      "answer: {'Ancient Greek translation': '1 [...] F [...] 1 [...] ', 'English translation': '1 Wotyko [...] 1 Pokion', 'reasoning': \"Grammatical analysis: The numbers '1' indicate a count for each entry. 'wo-tu-ko' is classified as an incomplete noun, likely an anthroponym with thematic -o inflection. From the options, 'F' (Wotyko) is the most suitable completion, appearing in the nominative singular. Similarly, 'po-ki' is an incomplete noun, also likely an anthroponym with thematic -o inflection. '' (Pokion) fits this description best, also in the nominative singular. Logical/Syntactic analysis: The document is highly fragmentary, indicated by '[...]'. However, the recurring pattern of 'number [...] name' strongly suggests an administrative list or inventory, enumerating individuals or items. The absence of verbs or complex sentence structures points to a simple, paratactic listing format. Subordinate clause analysis: No subordinate clauses are present in this fragmentary document. Semantic coherence: The translation as a simple list of names with associated counts is consistent with the typical administrative and record-keeping nature of Linear B texts.\"}\n",
      "Current Document: 5531\n",
      "document: to-sa o\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'So many donkeys.', 'reasoning': \"Grammatical analysis: 'to-sa' is classified as an adjective, thematic in -a. The most fitting Ancient Greek translation is '' (tosa), which is the neuter plural nominative/accusative form of '' (tosos), meaning 'so many' or 'such a quantity'. 'o' is classified as a noun, common, thematic in -o. In Linear B administrative texts, single letters often serve as logograms or abbreviations for commodities. Given the classification 'thematic in -o', the most common and plausible commodity represented by 'o' is '' (onos, donkey), which is a masculine thematic -o noun. Although the provided Ancient Greek options for 'o' ('||') do not directly offer '' or its plural, the classifier's explicit description of 'o' as a 'thematic in -o' noun guides this interpretation, prioritizing the grammatical type over potentially misleading phonetic options for a logogram. Logical/Syntactic analysis: The phrase 'to-sa o' functions as a quantification, where '' quantifies the commodity represented by 'o'. While '' is masculine singular, and '' is neuter plural, it is a known feature of Linear B administrative records that neuter plural adjectives can be used as general quantifiers for 'so many (units/head of)' a commodity, even if the commodity noun itself is masculine or feminine. To accurately reflect the plural quantity implied by '', the noun '' is rendered in its masculine plural form, ''. Subordinate clause analysis: This fragment is too short to contain any subordinate clauses.\"}\n",
      "Current Document: 5532\n",
      "document: ka-ra-do-ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Plover (a bird)', 'reasoning': \"Grammatical analysis: The Linear B word 'ka-ra-do-ro' is classified as a noun, specifically an 'anthroponym/animal name/theonym', with a 'thematic in -o' inflection. This aligns perfectly with the Ancient Greek word '' (charadros), which is a masculine noun of the second declension (thematic -o stem) meaning 'plover' (a type of bird). The ending '-ro' in Linear B corresponds to the nominative singular '-' in Ancient Greek. Logical/Syntactic analysis: As a single word entry, 'ka-ra-do-ro' most likely functions as a label or an item in an inventory. Given its classification as an 'animal name', '' is the most direct and semantically coherent translation. It could also potentially be an anthroponym (personal name) derived from the bird, but without further context, the primary meaning of the bird itself is the most straightforward interpretation. Semantic coherence: The translation 'Plover' () is consistent with the 'animal name' classification and the administrative nature of Linear B texts, where animals might be listed.\"}\n",
      "Current Document: 5533\n",
      "document: na\n",
      "answer: {'Ancient Greek translation': '', 'English translation': '[Incomplete noun]', 'reasoning': \"Grammatical analysis: The Linear B word 'na' is classified as an 'INCOMPLETE' common noun with 'thematic in -a' inflection. In Linear B, 'na' represents the syllable /na/, which directly transcribes to '' in Ancient Greek. Given its incompleteness and isolation, it is impossible to determine the full word or its precise meaning. The 'thematic in -a' inflection suggests it might be a fragment of a feminine noun. Syntactic/Logical analysis: As a single, incomplete word, 'na' cannot form a coherent sentence or convey a complete logical statement. It is likely a fragment of a larger administrative entry or a damaged part of a document. Semantic coherence: Due to its fragmentary nature, semantic coherence cannot be established. The translation reflects the direct syllabic transcription while acknowledging the word's incompleteness in both Ancient Greek and English.\"}\n",
      "Current Document: 5534\n",
      "document: we-jo\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'shrine', 'reasoning': \"Grammatical analysis: The Linear B word 'we-jo' is classified as an incomplete common noun with a thematic -o inflection. Among the provided Ancient Greek options, 'f' (veion) is the most appropriate, representing a neuter noun in the nominative or accusative singular, meaning 'shrine' or 'sacred place'. This form perfectly matches the thematic -o declension pattern for neuter nouns. Logical/Syntactic analysis: As a standalone, incomplete word, 'we-jo' most likely functions as a nominal entry in an administrative context, possibly indicating a location or an item associated with a sacred site. Its brevity suggests it could be a heading or a single item in a list. Subordinate clause analysis: No subordinate clauses are present as the input consists of a single word. Semantic coherence: The translation 'shrine' or 'sacred place' is highly plausible and semantically coherent within the typical administrative and religious inventory nature of Linear B documents.\"}\n",
      "Current Document: 5536\n",
      "document: 4\n",
      "answer: {'Ancient Greek translation': '4', 'English translation': '4', 'reasoning': \"Grammatical analysis: The input '4' is a numeral. In Linear B documents, numerals typically represent quantities or identifiers. Logical/Syntactic analysis: As a standalone entry, it most likely indicates a count or a sequential number within a list or inventory. Without additional context, its precise function (e.g., 'item 4', 'quantity 4') cannot be determined, but its numerical value is clear. Subordinate clause analysis: No clauses are present in this single-item document.\"}\n",
      "Current Document: 5537\n",
      "document: jo [...] o-no\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Sheep [...] donkey', 'reasoning': \"Grammatical analysis: 'jo' is classified as a noun, thematic in -o, and is best translated as '' (sheep) in the nominative singular, fitting the 'animal name' type. 'o-no' is also a noun, thematic in -o, and is translated as '' (donkey) in the nominative singular, again fitting the 'animal name' type. Both are simple nouns, likely part of a list. Logical/Syntactic analysis: The document is fragmentary, indicated by '[...]'. The presence of two animal names ('sheep' and 'donkey') strongly suggests an administrative inventory or list of livestock, a common type of Linear B record. The absence of a verb is typical for such lists. Subordinate clause analysis: No subordinate clauses are present in this short, fragmentary entry. The translation reflects a simple, paratactic listing of items.\"}\n",
      "Current Document: 5538\n",
      "document: o [...] e-ke [...] ka [...] ka\n",
      "answer: {'Ancient Greek translation': '   ', 'English translation': 'Orestes has an urn, an urn.', 'reasoning': \"Grammatical analysis: 'o' is interpreted as  (Orestes), a proper noun (anthroponym) in the nominative singular, thematic in -o, functioning as the subject. 'e-ke' is translated as  (has), a verb in the 3rd person singular present active indicative, agreeing with the singular subject. The first 'ka' is completed as  (urn), a common noun, thematic in -a, in the accusative singular, serving as a direct object. The second 'ka' is similarly completed as  (urn), another common noun in the accusative singular, indicating a second item of the same type. Logical/Syntactic analysis: The sentence follows a simple subject-verb-object structure: 'Orestes' (subject) 'has' (verb) 'an urn, an urn' (direct objects). This structure is typical of administrative records detailing possessions or inventories. Subordinate clause analysis: No subordinate clauses are present in this fragmentary sentence. Fragmentation handling: The 'MOSTLY_COMPLETE' and 'MOSTLY_INCOMPLETE' classifications, along with the '[...]' separators, indicate that the original Linear B document is fragmentary. For the purpose of providing a 'complete translation' as required, the legible syllabograms have been completed into plausible Ancient Greek words (e.g., 'o' to '', 'ka' to '') that fit the grammatical classifications and the administrative context of Linear B. The explicit '[...]' gaps are acknowledged as representing illegible or missing parts of the original tablet and are not explicitly rendered in the final Ancient Greek or English translation, as the instruction is to provide a complete, coherent sentence from the decipherable parts.\"}\n",
      "Current Document: 5539\n",
      "document: na-u-do-mo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'shipbuilder', 'reasoning': \"Grammatical analysis: The Linear B word 'na-u-do-mo' is classified as a common noun with a thematic -o inflection. This directly corresponds to the Ancient Greek '' (naudomos), a masculine noun of the second declension, meaning 'shipbuilder'. The final '-o' in Linear B often represents the nominative singular ending '-' for thematic -o stems. Logical/Syntactic analysis: As a single word, '' functions as a simple noun, likely referring to a profession or an individual engaged in that profession within an administrative record. Subordinate clause analysis: Not applicable as this is a single word entry. The translation is semantically coherent with the typical content of Linear B tablets, which often list professions and personnel.\"}\n",
      "Current Document: 5540\n",
      "document: pe-o\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'penis', 'reasoning': \"Grammatical analysis: The Linear B word 'pe-o' is classified as an incomplete common noun with a thematic -o inflection. Among the provided Ancient Greek options, '' (penis) is a noun. While in Classical Greek '' is typically a 3rd declension neuter noun, in Mycenaean Greek, it could represent a masculine nominative singular of a thematic -o stem, aligning with the given inflectional classification. Logical/Syntactic analysis: As a single, incomplete word, 'pe-o' lacks a broader syntactic context. It likely functions as a standalone entry or label within a document, possibly referring to an object or a descriptive term. Semantic coherence: '' is a direct and plausible translation of one of the provided options, fitting the noun classification. Its specific administrative context remains unknown due to the word's isolation.\"}\n",
      "Current Document: 5541\n",
      "document: wo [...] 1\n",
      "answer: {'Ancient Greek translation': ' 1', 'English translation': '1 house', 'reasoning': \"Grammatical analysis: The Linear B word 'wo' is classified as an incomplete noun, thematic in -o inflection. The numeral '1' indicates a quantity. Logical/Syntactic analysis: The structure 'wo [...] 1' strongly suggests an inventory entry, where 'wo' represents an item and '1' its quantity. Despite the incompleteness of 'wo' and the intervening '...', the most semantically coherent and contextually appropriate completion for 'wo' in Mycenaean administrative records is '' (house or household). This interpretation aligns with the common practice of listing items and their counts in such documents. Semantic coherence: '' (house/household) is a frequently attested term in Linear B inventories, making it a highly plausible completion for the fragment 'wo'. The translation '1 house' reflects this administrative context.\"}\n",
      "Current Document: 5542\n",
      "document: jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ios (a proper name)', 'reasoning': \"Grammatical analysis: The Linear B word 'jo' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -o' inflection. Among the provided Ancient Greek translation options, '' () is the most suitable, representing a proper name (anthroponym) that aligns with the thematic -o declension pattern (nominative singular). Logical/Syntactic analysis: As a single, isolated word, 'jo' functions as a standalone noun. In administrative contexts, single proper names often appear as entries in lists or as headings. Discourse analysis: Without further context, the word stands as an independent entry. The 'MOSTLY_INCOMPLETE' status suggests it might be a fragment, but for a complete translation of the given input, '' is the most plausible interpretation as a complete proper name.\"}\n",
      "Current Document: 5543\n",
      "document: u\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Pig', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'u' is extremely incomplete, making definitive identification challenging. However, given the classifier information indicating an 'INCOMPLETE' noun of type 'anthroponym/animal name/theonym' with 'athematic' inflection, '' (hys), meaning 'pig', is a plausible and simple translation. '' is an athematic noun in Ancient Greek. Logical/Syntactic analysis: As a single, incomplete word, no complex syntactic structure can be inferred. It most likely represents a fragment of a noun, possibly an animal listed in an administrative context. Subordinate clause analysis: Not applicable due to the single-word nature of the input. Semantic coherence: 'Pig' is a common animal found in Mycenaean administrative records, making this a contextually plausible, albeit highly speculative, interpretation for such an incomplete fragment.\"}\n",
      "Current Document: 5545\n",
      "document: wo-wo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'boundary / field', 'reasoning': \"Grammatical analysis: The Linear B word 'wo-wo' is classified as a common noun with a 'thematic in -o' inflection. This corresponds to the Ancient Greek second declension. Linguistic authenticity: 'wo-wo' is a well-attested Linear B term, often reconstructed as Mycenaean  (worwos) or  (wowos), meaning 'boundary' or 'field'. For the Ancient Greek translation, the classical equivalent  (horos) is chosen, which is a masculine noun of the second declension, perfectly matching the 'thematic in -o' inflection. Semantic coherence: In the context of administrative Linear B documents, terms related to land division, such as 'boundary' or 'field', are highly relevant and common. Logical/Syntactic analysis: As a single word entry, 'wo-wo' likely functions as a label, a heading, or an item in an inventory, which is a typical usage in Mycenaean administrative records. No complex syntactic structure or subordinate clauses are present for a single word.\"}\n",
      "Current Document: 5546\n",
      "document: wa-ta\n",
      "answer: {'Ancient Greek translation': 'F', 'English translation': 'Wata', 'reasoning': \"Grammatical analysis: The Linear B word 'wa-ta' is classified as an incomplete noun, thematic in -a, and an anthroponym/animal name/theonym. The most direct Ancient Greek transcription, consistent with a nominative singular form for a thematic -a noun, is 'F' (Wata), which could represent a proper name or a common noun. The digamma (F) is restored based on Mycenaean phonology. Logical/Syntactic analysis: As a single, incomplete word, there is no complex syntactic structure to analyze. It most likely functions as a standalone entry, possibly a name in a list or an inventory item, typical of administrative Linear B texts. Subordinate clause analysis: Not applicable for a single word. The English translation 'Wata' reflects the proper name interpretation, which is common for such isolated entries in Linear B documents.\"}\n",
      "Current Document: 5547\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ka-pa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ka-pa\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kapa', 'reasoning': \"Grammatical analysis: The Linear B word 'ka-pa' is classified as an uncertain noun, specifically a toponym, with a thematic in -a inflection. Among the provided Ancient Greek options, '' is the most direct and plausible transcription for a place name. The thematic -a inflection aligns with a first declension noun in Ancient Greek, and '' serves as the nominative singular form, suitable for a standalone place name. Logical/Syntactic analysis: As a single word constituting the entire document, 'ka-pa' most logically functions as a heading or a simple identification of a location, typical for administrative Linear B texts. There are no other words or grammatical structures to suggest a more complex sentence or a different case. Subordinate clause analysis: No subordinate clauses are present in this single-word document. The translation reflects a straightforward administrative entry identifying a geographical location.\"}\n",
      "Current Document: 5548\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: te\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: te\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'serf', 'reasoning': \"Grammatical analysis: The Linear B word 'te' is classified as a 'MOSTLY_INCOMPLETE' common noun with 'athematic' inflection. Consulting the syllabogram mapping, 'te' can represent the sounds '' and ''. Among the provided Ancient Greek translation options ('||||'), '' is selected as the most suitable stem. This stem is then completed to the Ancient Greek word '' (ths), which is an athematic common noun meaning 'serf' or 'laborer' (e.g., nominative singular , genitive singular ). This reconstruction aligns perfectly with the specified grammatical classification of 'te'.\\nLogical/Syntactic analysis: As a single, 'MOSTLY_INCOMPLETE' word, 'te' is interpreted as a standalone entry, likely part of an administrative list or inventory. The translation 'serf' represents a category of personnel frequently documented in Mycenaean administrative records. The brevity of the entry precludes complex syntactic structures or subordinate clauses.\\nSemantic coherence: The translation 'serf' is highly plausible within the context of Linear B documents, which often detail human resources, land tenure, and other administrative matters. This interpretation provides a meaningful and contextually appropriate translation for the fragmentary Linear B word.\"}\n",
      "Current Document: 5549\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ra-qe\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ra-qe\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Rupture', 'reasoning': \"Grammatical analysis: The Linear B word 'ra-qe' is classified as an incomplete noun, thematic in -a. Among the provided Ancient Greek options, '' (rag) fits the syllabogram mapping ('ra'  '' (rho + alpha), 'qe'  '' (gamma + eta)) and is a 1st declension noun (thematic in -a). The rough breathing and acute accent are added for proper Ancient Greek orthography. Logical/Syntactic analysis: As a single, incomplete word, its full syntactic role is uncertain without further context. However, as a noun, it would function as a subject, object, or part of a nominal phrase. The classification as 'anthroponym/animal name/theonym' suggests it might be a proper name or a descriptive term for a living entity. While '' typically means 'rupture' or 'fracture' (a common noun), it is plausible in Mycenaean administrative records for such a term to be used as a descriptive nickname for a person or animal, especially given the 'INCOMPLETE' status which might imply a shortened or informal usage. Subordinate clause analysis: Not applicable for a single word. Semantic coherence: The translation 'Rupture' () is semantically coherent with the word's meaning and can be reconciled with the 'anthroponym/animal name/theonym' classification through the interpretation of a descriptive nickname.\"}\n",
      "Current Document: 5550\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: wa-ja\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: wa-ja\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'Land', 'reasoning': \"Grammatical analysis: The Linear B word 'wa-ja' is classified as an incomplete noun, common, and thematic in -a. The provided Ancient Greek translation option 'f' (pronounced 'waia') is interpreted as a Mycenaean form of '' (earth/land). This interpretation aligns perfectly with the grammatical classification, as '' is a feminine noun of the first declension (thematic in -a). The initial 'w' sound of the Linear B syllabogram 'wa' is represented by the digamma 'f' in 'f', a common phonetic feature in Mycenaean Greek. Logical/Syntactic analysis: As a single word in a fragmentary document, 'wa-ja' most likely represents an entry in an administrative record. 'Land' or 'earth' is a highly plausible subject in Linear B tablets, which frequently document land tenure, agricultural resources, and property. Subordinate clause analysis: None present in this single-word document.\"}\n",
      "Current Document: 5551\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ma\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ma\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'barley-cake', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ma' is classified as a 'MOSTLY_INCOMPLETE' noun of 'thematic in -a' inflection. Among the provided Ancient Greek translation options, '' (barley-cake) is the only noun that fits the 'thematic in -a' declension pattern. Logical/Syntactic analysis: Given the extreme brevity and the 'MOSTLY_INCOMPLETE' status, 'ma' is interpreted as the initial syllable of ''. This interpretation is chosen for semantic coherence, as 'barley-cake' is a plausible item to be recorded in Mycenaean administrative documents. Subordinate clause analysis: Not applicable as the document consists of a single, incomplete word.\"}\n",
      "Current Document: 5552\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: si\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: si\n",
      "answer: {'Ancient Greek translation': '-', 'English translation': '(to/for/at/in) [plural nouns]', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'si' is classified as an 'INCOMPLETE' common noun with 'athematic' inflection. According to the provided declension table, '-si' is the dative plural ending for athematic nouns (e.g.,  from ). Given the 'INCOMPLETE' status, 'si' is best interpreted as this suffix, indicating that the noun stem it belongs to is missing from the document. Logical/Syntactic analysis: As a standalone suffix, '-' indicates the dative plural case, signifying 'to/for' (indirect object) or 'at/in' (locative) for an unspecified plural noun. It does not form a complete sentence or phrase on its own, reflecting the fragmentary nature of the document. Subordinate clause analysis: Not applicable as the input is a single, incomplete word. The translation reflects the grammatical function of the suffix, acknowledging the incompleteness of the word as indicated by the classifier.\"}\n",
      "Current Document: 5553\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ta [...] ne [...] qa\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ta [...] ne [...] qa\n",
      "answer: {'Ancient Greek translation': ' [...]  [...] ...', 'English translation': 'The [...] ships [...] Ga...', 'reasoning': \"Grammatical analysis:\\n*   **ta**: Classified as a 'MOSTLY_INCOMPLETE' common noun, thematic in -a. '' (neuter plural nominative/accusative) is a plausible interpretation, functioning as an article or demonstrative, or as the ending of a noun. Given the fragmentary nature, it is treated as referring to 'the (things)' or 'these (things)'.\\n*   **ne**: Classified as an 'UNCERTAIN' common noun, athematic. Among the provided options, '' (nominative plural of , 'ships') is a well-attested athematic noun in Linear B administrative contexts and fits the grammatical classification.\\n*   **qa**: Classified as an 'INCOMPLETE' noun, specifically an anthroponym/animal name/theonym, with thematic in -a inflection. The provided Ancient Greek translation options ('|||') are verbs or participles, which directly contradict the classifier's 'part of speech: noun'. Prioritizing the grammatical classification, 'qa' is interpreted as the beginning of an incomplete proper noun (an anthroponym). The syllabogram 'qa' can represent sounds such as , , , , . For the Ancient Greek translation, '...' is chosen as a plausible initial sound for an incomplete name, acknowledging its fragmentary status and the conflict in the provided options.\\n\\nLogical/Syntactic analysis: The document is highly fragmentary, indicated by the '[...]' separators, which prevents the formation of a coherent sentence or a clear subject-predicate structure. Each word is treated as an isolated entry or part of a list, reflecting the typical administrative nature of Linear B texts. The translation maintains this fragmented structure, avoiding speculative reconstruction of a complete sentence.\\n\\nSubordinate clause analysis: Due to the extreme fragmentation of the document, no subordinate clauses can be identified.\"}\n",
      "Current Document: 5554\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti-wo [...] ta\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ti-wo [...] ta\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Tiwos [...] of land.', 'reasoning': \"Grammatical analysis: 'ti-wo' is translated as '' (Tiwos), a proper noun (anthroponym) in the nominative singular. This choice aligns with the classifier 'thematic in -o' for 'ti-wo', as thematic -o stems typically form their nominative singular in -os (with the final -s often unwritten in Linear B). 'ta' is translated as '' (gas), the genitive singular of '' (earth/land). This choice aligns with the classifier 'thematic in -a' for 'ta', as Mycenaean 'a'-stem nouns can form their genitive singular in -as or -ao. '' represents a common noun in a possessive case. Logical/Syntactic analysis: The fragment ' [...] ' suggests a record where an individual named Tiwos is associated with 'land' in a genitival relationship (e.g., 'Tiwos's land' or 'Tiwos [something] of land'). The '[...]' indicates a significant gap, preventing a complete sentence structure. However, the combination of a proper name and a genitive noun related to land is consistent with the administrative nature of Linear B documents, which often record land ownership or transactions. Subordinate clause analysis: No subordinate clauses are present in this highly fragmented text. Semantic coherence: This interpretation provides a plausible administrative fragment, linking a person to land, which is a common theme in Linear B records.\"}\n",
      "Current Document: 5555\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: de-ke\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: de-ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Receive!', 'reasoning': \"Grammatical analysis: 'de-ke' is classified as an incomplete verb. The most plausible Ancient Greek translation is '' (present imperative, 2nd person singular) or '' (aorist imperative, 2nd person singular) from the verb , meaning 'to receive' or 'to take'. Logical/Syntactic analysis: As an imperative verb, it functions as a direct command or instruction, common in administrative contexts for directing actions related to goods or personnel. No subordinate clauses are present.\"}\n",
      "Current Document: 5556\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: re\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Rheos', 'reasoning': \"Grammatical analysis: The Linear B word 're' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. This indicates it is likely a proper name or a divine/animal designation. Linguistic Authenticity/Semantic Coherence: Among the provided Ancient Greek translation options, '' () is a noun that directly corresponds to the 're' syllabogram (-). While '' typically means 'stream' or 'flow', it can plausibly function as a proper name or a theonym (e.g., a river god or a name derived from a natural feature), which aligns with the 'anthroponym/animal name/theonym' classification. Other options either do not match the initial sound ('', '', ''), are verbs (''), or are less standard forms (''). Syntactic/Logical analysis: As a single, mostly incomplete word, its specific syntactic role within a larger document is not determinable without further context. It most likely represents an entry in an administrative list, possibly a name. Error Recovery: Given the extreme incompleteness of the word, selecting '' (capitalized as a proper noun) as the most plausible noun starting with 're' that fits the semantic category of an anthroponym/theonym is the best available recovery, acknowledging the inherent uncertainty due to fragmentation.\"}\n",
      "Current Document: 5557\n",
      "document: we\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'Six (items)', 'reasoning': \"Grammatical analysis: The Linear B word 'we' is classified as a 'MOSTLY_INCOMPLETE' 'common noun' with 'athematic' inflection. The syllabogram 'we' phonetically corresponds to 'f' in Ancient Greek. The provided Ancient Greek options for 'we' are 'f', 'f', 'f', 'f'. We select 'f' as a plausible phonetic rendering, which could represent a fragment of a noun, potentially a neuter plural nominative/accusative form. Syntactic/Logical analysis: As a single, incomplete word, its precise syntactic role is ambiguous without further context. However, the 'CRITICAL MORPHOLOGICAL MARKERS' section identifies 'we-' as a numerical prefix meaning 'six' (-). Given this strong association and the administrative nature of Linear B texts, it is highly probable that 'we', even as an incomplete noun, refers to a quantity of 'six' or a group of 'six items'. Semantic coherence: Interpreting 'we' as 'six (items)' provides a semantically coherent meaning consistent with numerical entries in administrative documents, despite its fragmentary nature. The choice of 'f' in Ancient Greek reflects one of the provided phonetic possibilities for the syllabogram 'we', used here to represent the incomplete noun. Subordinate clause analysis: Not applicable as this is a single, incomplete word.\"}\n",
      "Current Document: 5558\n",
      "document: e\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'e (fragment)', 'reasoning': \"Grammatical analysis: The Linear B entry 'e' is classified as a 'MOSTLY_INCOMPLETE' noun, thematic in -o. As a single syllabogram, it cannot form a complete, meaningful word in Mycenaean Greek. It is likely a fragment of a larger word, either its initial or final part, or an isolated vowel. The provided Ancient Greek options (, , ) are either vowels or a preposition, none of which fit the 'thematic in -o' noun classification as a complete word. Logical/Syntactic analysis: Without any surrounding context, it is impossible to determine the logical role or syntactic position of this fragment within a sentence or document. It stands as an isolated, untranslatable element. Subordinate clause analysis: Not applicable for a single, incomplete syllabogram. Translation decisions: Due to its 'MOSTLY_INCOMPLETE' status and lack of context, a definitive translation is not possible. The Ancient Greek translation provides the direct transliteration of the syllabogram, and the English translation explicitly notes its fragmentary nature.\"}\n",
      "Current Document: 5559\n",
      "document: ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'you lie / you are placed', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ke' is classified as a 'MOSTLY_INCOMPLETE' verb. Among the provided Ancient Greek translation options (|||||), '' (keis) is the only complete verb form. It corresponds to the 2nd person singular present active indicative of the verb  (keimai), meaning 'to lie' or 'to be placed'.\\nLogical/Syntactic analysis: Given the extreme incompleteness of the word, providing a definitive translation without further context is challenging. However, adhering strictly to the classification as a verb and selecting the only complete verb form from the provided options leads to ''. In administrative contexts, verbs like 'to be placed' or 'to lie' can refer to the location or status of items or individuals, though the 2nd person singular is less common than the 3rd person in such texts.\\nSubordinate clause analysis: Not applicable as this is a single, incomplete word. The choice of '' represents an attempt at 'error recovery' to provide a complete verb translation as requested, despite the fragmentary nature of the input, by selecting the most grammatically complete verb option available.\"}\n",
      "Current Document: 5560\n",
      "document: ta\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'These (things)', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ta' most commonly corresponds to the Ancient Greek neuter plural nominative/accusative article or demonstrative pronoun ''. While strictly an article or pronoun, in administrative Linear B texts, '' frequently functions substantively, referring to 'these things' or 'the items' being discussed, which aligns with the classifier's 'common noun' designation. The 'thematic in -a' inflection can be understood in relation to the - ending of the neuter plural, even if it's not a typical -a stem noun declension. The 'MOSTLY_INCOMPLETE' status for a single syllabogram suggests it's a very short word, possibly implying it's usually part of a larger phrase or list, but '' itself is a complete and frequently occurring word. Logical/Syntactic analysis: As a standalone word, '' serves to introduce or refer to a set of multiple, inanimate objects or concepts. Without further context, its precise referent is unspecified, but its function is demonstrative or anaphoric, pointing to items relevant to the administrative record. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5561\n",
      "document: ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': '[Fragmentary]', 'reasoning': \"Grammatical analysis: The Linear B word 'ke' is classified as 'MOSTLY_INCOMPLETE' and 'verb'. In Ancient Greek, '' is a common enclitic particle, often a shortened form of '' (and) or a modal particle. However, as a standalone verb, it is highly ambiguous and likely represents a fragment of a longer word or an incomplete form. Given its 'MOSTLY_INCOMPLETE' status and lack of context, a definitive verbal translation is not possible. Syntactic analysis: As a single, incomplete word, no syntactic structure can be determined. Logical analysis: Without further context, the semantic meaning is entirely unclear. The provided Ancient Greek options '|||||' show various possibilities, but '' as a particle or an incomplete verb form is the most direct transliteration. Due to its fragmentary nature, the most accurate English translation is to indicate it as such.\"}\n",
      "Current Document: 5563\n",
      "document: pa [...] to-ro [...] te\n",
      "answer: {'Ancient Greek translation': ' [...]  [...] ', 'English translation': 'All [...] Toros [...] beast', 'reasoning': \"Grammatical analysis: \\n- 'pa' is classified as a 'MOSTLY_COMPLETE' adjective, 'thematic in -a'. The most fitting Ancient Greek translation is '' (feminine nominative singular or neuter nominative/accusative plural of '', meaning 'all' or 'every'), which aligns with the thematic -a inflection.\\n- 'to-ro' is classified as an 'INCOMPLETE' noun, 'anthroponym/animal name/theonym', 'thematic in -o'. Among the options '' and '', '' (a proper name) is chosen as it fits the 'anthroponym' and 'thematic in -o' classifications better than '' (Trojan), which is typically athematic in Ancient Greek. The 'INCOMPLETE' status indicates it might be a fragment of a longer word, but based on the provided options and classifiers, '' is the most plausible complete word.\\n- 'te' is classified as a 'MOSTLY_INCOMPLETE' noun, 'common', 'athematic'. Among the options, '' (beast) is the most suitable as it is an athematic common noun. Other options like '' (goddess) are typically thematic ('' thematic -a, '' thematic -o), and '' (conjunction) is not a noun. The 'MOSTLY_INCOMPLETE' status suggests it could be a fragment, but '' is the best fit for a complete word based on the classifiers.\\n\\nLogical/Syntactic analysis: Due to the extreme fragmentation indicated by '[...]' separators, a coherent sentence structure or logical connection between the words cannot be established. The words are translated as isolated fragments, likely representing entries in a list or parts of a severely damaged administrative record. No subject-predicate relationship or complex sentence structure can be inferred.\\n\\nSubordinate clause analysis: No subordinate clauses are present or discernible due to the fragmentary nature of the document.\"}\n",
      "Current Document: 5564\n",
      "document: ja-ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': \"Iaros (a proper name, or 'Sacred')\", 'reasoning': \"Grammatical analysis: 'ja-ro' is classified as an incomplete noun, thematic in -o, and an anthroponym/animal name/theonym. This aligns with the nominative singular masculine form of '' (or ''), meaning 'sacred' or 'holy'. Given the 'anthroponym/animal name/theonym' classification, it is most likely a proper name, 'Iaros', or an epithet for a sacred individual or entity. Logical/Syntactic analysis: As a standalone word, it functions as a proper noun or a descriptive epithet. Without further context, its precise role (e.g., subject, heading) cannot be definitively determined, but its classification strongly suggests a named entity. Subordinate clause analysis: Not applicable for a single word entry.\"}\n",
      "Current Document: 5565\n",
      "document: to-so O qe [...] e-ni a-pu-ki-si\n",
      "answer: {'Ancient Greek translation': ' O  [...]  ', 'English translation': 'So much O, and [...] in headbands.', 'reasoning': \"Grammatical analysis:\\n*   `to-so` (): Adjective, thematic in -o, interpreted as neuter singular nominative/accusative 'so much', agreeing with the implied commodity represented by the logogram `O`.\\n*   `O`: Logogram, representing an unspecified commodity or unit of measure. It is left untranslated as per instructions.\\n*   `qe` (): Interpreted as the enclitic conjunction 'and', as listed in the 'CRITICAL MORPHOLOGICAL MARKERS'. This interpretation is chosen for semantic coherence in an administrative context, despite the classifier indicating it as an incomplete noun. The `[...]` indicates missing text following the conjunction.\\n*   `e-ni` (): Interpreted as the preposition 'in' or 'among', a common Mycenaean form of Ancient Greek '', which governs the dative case. This choice prioritizes syntactic and semantic coherence with the following dative plural noun, aligning with 'linguistic authenticity' and 'semantic coherence' principles, despite the classifier indicating it as a noun.\\n*   `a-pu-ki-si` (): Noun, athematic, dative plural of  ('headband'). This form is grammatically complete and functions as the object of the preposition `e-ni`, indicating a location or context.\\n\\nLogical/Syntactic analysis: The document fragment describes a quantity ('so much') of a commodity (`O`), followed by a conjunction ('and') and a locative phrase ('in headbands'). The `[...]` signifies a break in the text, indicating that further details or items in the inventory are missing. The overall structure is characteristic of an administrative record, detailing quantities and their location or context.\\n\\nSubordinate clause analysis: No subordinate clauses are present in this fragment. The structure is paratactic, typical of inventory-style Linear B texts.\"}\n",
      "Current Document: 5566\n",
      "document: qe-re-te-u\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Keleter', 'reasoning': \"Grammatical analysis: The Linear B word 'qe-re-te-u' is classified as an incomplete noun, specifically an anthroponym, with athematic inflection. The final syllabogram '-u' is characteristic of nominative singular endings for athematic nouns in Mycenaean Greek, which can correspond to forms like '-' or '-' in later Ancient Greek. Among the provided options, '' (Keleter) is a well-attested anthroponym that fits this morphological pattern. Logical/Syntactic analysis: As a single, isolated word, 'qe-re-te-u' functions as a standalone entry, most likely a personal name in an administrative context. There is no complex syntax or sentence structure to analyze. Subordinate clause analysis: No subordinate clauses are present in this single-word entry. Translation choice: '' is selected as the most appropriate Ancient Greek translation, representing a proper name, which directly corresponds to the classification as an anthroponym.\"}\n",
      "Current Document: 5567\n",
      "document: 20\n",
      "answer: {'Ancient Greek translation': '20', 'English translation': '20', 'reasoning': \"Grammatical analysis: The input '20' is a numeral. In Linear B texts, numerals often appear as standalone entries, indicating a quantity or a document identifier. Logical/Syntactic analysis: As a standalone numeral, it does not form a complete sentence but rather represents a discrete piece of information, likely a count or an identifier within an administrative context. Subordinate clause analysis: No clauses are present. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5568\n",
      "document: o\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Bone', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'o' is classified as a 'MOSTLY_COMPLETE' common noun with 'thematic in -o' inflection. Among the provided Ancient Greek options ('', '', ''), '' () is the most direct and grammatically plausible interpretation for a thematic -o noun, meaning 'bone'. While '' can also be a relative pronoun ('who', 'which'), the classification as a 'noun, common' strongly suggests the substantive meaning. Logical/Syntactic analysis: As a single syllabogram, this entry is extremely fragmentary. It likely represents an isolated item in an administrative list or a severely damaged record. Without further context, a complete sentence cannot be formed. The translation reflects the most probable noun interpretation given the limited information. Subordinate clause analysis: Not applicable due to the extreme brevity of the input.\"}\n",
      "Current Document: 5569\n",
      "document: we\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'word', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'we' is classified as a 'MOSTLY_INCOMPLETE' common 'noun' with 'athematic' inflection. Among the provided Ancient Greek options ('f|f|f|f'), 'f' best corresponds to the stem of '' (wepos), an athematic s-stem noun meaning 'word' or 'statement'. This choice aligns with the 'athematic' inflection specified in the classifier, unlike other common interpretations of 'we' as a noun (e.g., '' 'year', '' 'garment', '' 'yearling') which are thematic o-stems. Logical/Syntactic analysis: As a single, 'MOSTLY_INCOMPLETE' word, 'we' likely represents the beginning of a noun. In administrative texts, 'word' or 'statement' could refer to an entry, a decree, or a record. The fragmentary nature prevents a full syntactic analysis, but '' is a plausible common noun in such contexts. Subordinate clause analysis: Not applicable for a single word. Linguistic authenticity: '' is an attested Mycenaean word (e.g., 'we-po' in KN As 1517). Semantic coherence: 'Word' or 'statement' is a semantically plausible common noun for an administrative document, even if incomplete.\"}\n",
      "Current Document: 5570\n",
      "document: ke\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'you lie / you are placed', 'reasoning': \"Grammatical analysis: The Linear B word 'ke' is classified as a 'MOSTLY_INCOMPLETE' verb. Among the provided Ancient Greek translation options, '' (from , 'to lie, to be placed') is the only form identified as a verb. It is the 2nd person singular present indicative. Logical/Syntactic analysis: As a single syllabogram, 'ke' is highly fragmentary. Given its 'MOSTLY_INCOMPLETE' status and classification as a verb, '' is selected as the most plausible full verb form that 'ke' could represent, despite the phonetic challenges of mapping a single syllabogram to a multi-sound word like ''. The extreme brevity of the input prevents further syntactic or logical analysis. Subordinate clause analysis: Not applicable for a single, incomplete word.\"}\n",
      "Current Document: 5571\n",
      "document: de\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'De', 'reasoning': \"Grammatical analysis: The Linear B word 'de' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an anthroponym/animal name/theonym, with athematic inflection. This suggests it represents a proper name, possibly a fragment or a very short form. Logical/Syntactic analysis: As a single, incomplete word, its specific syntactic function within a sentence is indeterminate without further context. However, given its classification as a proper noun, it would typically function as a subject or object. Subordinate clause analysis: Not applicable for a single word. The Ancient Greek translation '' is a direct transliteration of the syllabogram, and in English, it is rendered as 'De', consistent with its interpretation as a proper name, despite its brevity and the general ambiguity of 'de' as a particle or suffix in Ancient Greek.\"}\n",
      "Current Document: 5572\n",
      "document: jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ios', 'reasoning': \"Grammatical analysis: The Linear B word 'jo' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -o' inflection. This strongly suggests a proper name. The syllabogram 'jo' phonetically corresponds to /io/ or /yo/. Among the provided Ancient Greek options, '' (Ios) is a suitable proper name (masculine, second declension, thematic -o stem) that aligns with both the phonetic transcription and the grammatical classification. Logical/Syntactic analysis: As a single, mostly incomplete word, 'jo' likely functions as a standalone entry in an administrative record, possibly a name of a person, animal, or deity, or a place name. This is common in Linear B documents where individual entries or headings are listed. Subordinate clause analysis: Not applicable as this is a single word entry.\"}\n",
      "Current Document: 5573\n",
      "document: wo\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'sheep', 'reasoning': \"Grammatical analysis: The Linear B word 'wo' is classified as an incomplete common noun with thematic -o inflection. In Linear B orthography, final consonants are often omitted. Among the provided Ancient Greek translation options, 'f' (, 'sheep') is the most plausible complete word that could be represented by the single syllabogram 'wo', where the final sigma is unwritten. While classically '' is often an i-stem or consonant stem, Mycenaean forms can exhibit thematic -o inflection, consistent with the classifier information. The initial 'f' (digamma) is characteristic of Mycenaean Greek. Logical/Syntactic analysis: As a single-word document, 'wo' likely functions as a label or an inventory item, which is a common format in Linear B administrative texts. Subordinate clause analysis: Not applicable for a single word entry. Semantic coherence: 'Sheep' is a highly attested and semantically coherent entry for a fragmentary administrative document, fitting the typical content of Linear B records.\"}\n",
      "Current Document: 5574\n",
      "document: ha\n",
      "answer: {'Ancient Greek translation': 'h', 'English translation': 'A (fragment)', 'reasoning': \"Grammatical analysis: The Linear B word 'ha' is classified as an 'INCOMPLETE' common noun with 'thematic in -a' inflection. Given its incompleteness and single syllabogram nature, it is most likely a fragment of a larger word. Among the provided Ancient Greek options, 'h' is the direct transliteration of the syllabogram. While 'h' typically functions as a feminine definite article or an interjection in Classical Greek, its classification as a 'noun, thematic in -a' in this context suggests it represents an extremely abbreviated or fragmentary noun, possibly a commodity or a place name, consistent with administrative texts. Without further context, a definitive full word cannot be recovered. Logical/Syntactic analysis: As a single, incomplete word, no complex syntactic structure can be determined. It stands as an isolated entry, likely a heading or a label for a subsequent entry that is missing. Subordinate clause analysis: Not applicable due to the fragmentary nature of the input.\"}\n",
      "Current Document: 5575\n",
      "document: o\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'bone', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'o' is classified as a 'MOSTLY_COMPLETE' common noun with 'thematic in -o' inflection. Among the provided Ancient Greek options, '' (os), meaning 'bone', is a thematic -o stem noun in the nominative singular, which perfectly aligns with the given classification. The alternative '' (ous, 'ear') is a third declension noun (, ) and therefore does not fit the specified 'thematic in -o' inflection. '' as a relative pronoun is grammatically possible but less likely as a standalone entry in an administrative context without a verb or other clause elements. Logical/Syntactic analysis: As a single, isolated word without further context, it is most likely an entry for an item or material. In administrative texts, 'bone' is a plausible item to be listed. Subordinate clause analysis: No subordinate clauses are present as it is a single word. Semantic coherence: 'Bone' fits the typical inventory-like nature of Linear B administrative records.\"}\n",
      "Current Document: 5576\n",
      "document: 2\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'two', 'reasoning': \"Grammatical analysis: The input '2' is a numeral. In Linear B texts, numerals often appear as standalone entries, indicating a count or an identifier. Logical/Syntactic analysis: As a single numeral, it functions as a direct count or a label. No complex sentence structure or subordinate clauses are present. The translation directly reflects the numerical value.\"}\n",
      "Current Document: 5577\n",
      "document: qo-wi-ro [...] o-ma\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Bophilos [...] company/band', 'reasoning': \"Grammatical analysis: 'qo-wi-ro' is interpreted as the anthroponym '' (Bophilos), a masculine noun of the thematic -o declension, in the nominative singular. This is a common type of entry in Linear B records. 'o-ma' is interpreted as the common noun '' (homs), a feminine noun of the thematic -a declension, in the nominative singular, meaning 'company' or 'band'. This fits the part of speech and inflection. Logical/Syntactic analysis: The fragment appears to be a partial entry, likely from a list or an administrative record. The structure '[Name] [...] [Group/Item]' is consistent with such documents. The '(...)' signifies missing information, preventing the formation of a complete sentence. Subordinate clause analysis: No subordinate clauses are present in this short fragment. Semantic coherence: The combination of a personal name and a term for a group or company is plausible in an administrative context, perhaps indicating a person associated with a specific group or a record of a group led by that person.\"}\n",
      "Current Document: 5578\n",
      "document: ja\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ia (a name)', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ja' is classified as an incomplete noun, specifically an anthroponym with thematic -a inflection. Among the provided Ancient Greek translation options (, , ), '' (Ia) is the most fitting choice to represent a noun, likely a proper name or a fragment thereof, consistent with the given grammatical classification. The thematic -a inflection often corresponds to feminine nouns or names. Logical/Syntactic analysis: As a single, incomplete word, 'ja' most plausibly functions as a standalone entry, such as a name in an administrative list. Its exact syntactic role within a larger sentence cannot be determined without additional context, but as an anthroponym, it would typically serve as a subject or object. Subordinate clause analysis: Not applicable for a single word. Semantic coherence: Given the administrative nature of Linear B texts, the appearance of a name, even in an abbreviated or incomplete form, is a common occurrence.\"}\n",
      "Current Document: 5584\n",
      "document: qo-wa\n",
      "answer: {'Ancient Greek translation': 'f', 'English translation': 'cow / ox', 'reasoning': \"Grammatical analysis: The Linear B word 'qo-wa' is classified as an 'INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -a' inflection. Phonologically, the 'qo' syllabogram represents a labiovelar consonant, which typically corresponds to '' in later Ancient Greek. The 'wa' syllabogram represents the vowel '' preceded by the digamma 'f'. Therefore, 'qo-wa' is best transliterated as 'f', which is the Mycenaean Greek precursor to the Classical Greek word '' (bous), meaning 'ox' or 'cow'. The 'thematic in -a' inflection is consistent with a feminine singular nominative/accusative form (like '') or a neuter plural nominative/accusative form (like ''), both of which could apply to 'cow' or 'cattle' in a Mycenaean context. Logical/Syntactic analysis: As a single word, 'qo-wa' functions as a simple noun. In the context of Linear B documents, such isolated entries typically represent items in an inventory or a list, which is common for administrative texts. Semantic coherence: The translation 'cow' or 'ox' is semantically coherent and highly plausible, as livestock inventories are a frequent subject in Linear B tablets. The 'INCOMPLETE' status of the word likely refers to the fragmentary nature of the document itself, rather than the word 'qo-wa' being morphologically incomplete, as 'qo-wa' is a well-attested term.\"}\n",
      "Current Document: 5606\n",
      "document: 1\n",
      "answer: {'Ancient Greek translation': '1', 'English translation': '1', 'reasoning': \"Grammatical analysis: The input '1' is a numeral. In Linear B texts, numerals are typically represented by strokes or specific signs and denote quantities. Logical/Syntactic analysis: As a standalone entry, '1' functions as a simple numerical count. It is likely an item number or a quantity in an administrative list, common in Linear B documents. Subordinate clause analysis: No subordinate clauses are present as the input consists solely of a numeral. The translation reflects a direct numerical representation without further contextual interpretation.\"}\n",
      "Current Document: 5620\n",
      "document: ma\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'barley-cake', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ma' is classified as a 'MOSTLY_INCOMPLETE' common noun with 'thematic in -a' inflection. Among the provided Ancient Greek translation options, '' (maza, meaning 'barley-cake' or 'bread') is a common noun that aligns with the 'thematic in -a' declension pattern (e.g., like feminine nouns such as ). The other options either do not fit the specified part of speech (e.g., '' is a particle) or the required inflection (e.g., '', '', '' are athematic or thematic in other patterns). Logical/Syntactic analysis: As a single, incomplete syllabogram, 'ma' is highly likely a fragment of a larger word. However, adhering to the instruction to provide a complete translation and selecting the most fitting option based on the provided grammatical classifiers, '' is chosen as the most probable intended word. In an administrative context, it would function as a simple noun, possibly representing an item in an inventory. Subordinate clause analysis: Not applicable for a single word. Error recovery: The 'MOSTLY_INCOMPLETE' status of 'ma' indicates its fragmentary nature. The translation '' represents the most plausible completion of this fragment into a full word, guided by the provided grammatical classifiers and translation options.\"}\n",
      "Current Document: 5621\n",
      "document: 2\n",
      "answer: {'Ancient Greek translation': '2', 'English translation': '2', 'reasoning': \"Grammatical analysis: The item '2' is a numerical logogram. In Linear B, isolated numbers typically represent a quantity or an identifier. Logical/Syntactic analysis: This is a standalone entry, likely indicating a count or a document number. Without further context, its exact function is limited to representing the numeral itself. Subordinate clause analysis: None present, as it is a single numerical entry.\"}\n",
      "Current Document: 5622\n",
      "document: a\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'A', 'reasoning': \"Grammatical analysis: The Linear B word 'a' is classified as a noun, specifically an anthroponym/animal name/theonym, with a thematic -a inflection. Its completeness level is 'MOSTLY_INCOMPLETE', indicating it is likely a fragment or an abbreviation. From the provided Ancient Greek options, '' is the direct transliteration of the syllabogram. Logical/Syntactic analysis: Given its extreme brevity and 'MOSTLY_INCOMPLETE' status, 'a' is most plausibly interpreted as an abbreviation for a proper name or an initial, which is common in Linear B administrative texts. The thematic -a inflection is difficult to apply to a single syllabogram, further suggesting an abbreviated form. Semantic coherence: Translating 'a' as '' (Ancient Greek) and 'A' (English) as a proper name or initial provides the most direct and least speculative 'complete' translation, acknowledging its fragmentary nature while adhering to the requirement for a full translation. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5623\n",
      "document: ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'People', 'reasoning': \"Grammatical analysis: The Linear B word 'ro' is classified as an incomplete noun, specifically an anthroponym/animal name/theonym with thematic -o inflection. Among the provided Ancient Greek translation options, '' (people, army, or a proper name) is the most suitable choice as it is a thematic -o noun and can function as an anthroponym. Despite the 'INCOMPLETE' status of the Linear B word, the task requires a complete translation, and '' represents the most plausible full word from the given choices that aligns with the morphological and semantic classification. Logical/Syntactic analysis: As a single, isolated word, '' functions as a standalone noun. Without further context, its specific syntactic role (e.g., subject, object) cannot be determined. Subordinate clause analysis: Not applicable as there are no clauses present.\"}\n",
      "Current Document: 5624\n",
      "document: wi-ja\n",
      "answer: {'Ancient Greek translation': 'F', 'English translation': 'Wia', 'reasoning': \"Grammatical analysis: The Linear B word 'wi-ja' is classified as an incomplete noun, likely an anthroponym, animal name, or theonym, with a thematic -a inflection. This suggests a feminine singular form, possibly nominative or accusative. The syllabograms 'wi' and 'ja' combine to form 'F' (Wia), which is a plausible Ancient Greek transcription for such a noun, preserving the digamma (F). Logical/Syntactic analysis: As a single, incomplete word, there is no broader sentence structure to analyze. It most likely represents a fragment of a proper name or a common noun within an administrative list or record. Subordinate clause analysis: Not applicable for a single word. Translation decision: Given the 'INCOMPLETE' status and the classification as an anthroponym/theonym, the most direct and contextually appropriate translation is a proper name, 'Wia', reflecting the Mycenaean pronunciation with the digamma.\"}\n",
      "Current Document: 5625\n",
      "document: *86 [...] *166 [...] ko-u-ta-po\n",
      "answer: {'Ancient Greek translation': '*86 [...] *166 [...] ', 'English translation': '*86 [...] *166 [...] Koutapos', 'reasoning': \"Grammatical analysis: The Linear B signs *86 and *166 are unidentified logograms or incomplete words, and no specific Ancient Greek equivalents are provided, so they are retained as is. The word 'ko-u-ta-po' is classified as an incomplete noun, an anthroponym/animal name/theonym, with thematic -o inflection. Among the given Ancient Greek options, '' (Koutapos) is chosen as the most direct transliteration that fits a thematic -o noun in the nominative singular. Logical/Syntactic analysis: The document is highly fragmentary, indicated by the '...' separators and the 'INCOMPLETE'/'MOSTLY_INCOMPLETE' classifications. The presence of unidentified signs and an incomplete proper noun suggests an administrative record where an individual named Koutapos might be listed, possibly in connection with the items or entities represented by *86 and *166. Without further context, the precise syntactic role of 'Koutapos' is speculative, but its nominative form suggests it could be a subject or a listed entity. Subordinate clause analysis: No subordinate clauses are present in this extremely fragmentary text. The translation reflects the high degree of incompleteness and uncertainty, preserving the unidentified elements and providing the most plausible form for the identifiable proper noun.\"}\n",
      "Current Document: 5627\n",
      "document: a-ke-re\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ageles', 'reasoning': \"Grammatical analysis: The classifier indicates 'a-ke-re' is a complete noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. The Linear B syllabogram 'a-ke-re' can be reconstructed as the Ancient Greek proper name  (Agels). The syllabograms 'a', 'ke', 're' correspond to '', '' (where 'ke' represents '' and 're' represents ''), and the final '-e' can represent the nominative singular ending '-' for athematic nouns, with the final sigma often unwritten in Linear B. Logical/Syntactic analysis: As a single, complete word identified as an anthroponym, '' functions as a proper noun, likely representing an individual's name in an administrative record. No complex syntactic structure is present for a single word. Subordinate clause analysis: Not applicable as this is a single word.\"}\n",
      "Current Document: 5628\n",
      "document: da-*22-to da-phu-ra-zo wa\n",
      "answer: {'Ancient Greek translation': '  f', 'English translation': 'Daton: Daphurazos, the Queen.', 'reasoning': \"Grammatical analysis: 'da-*22-to' is identified as a toponym, '', in the nominative singular, consistent with a thematic -o stem. 'da-phu-ra-zo' is an anthroponym, '', also in the nominative singular, following a thematic -o declension. 'wa' is classified as a mostly incomplete noun, an anthroponym/theonym with thematic -a inflection. Given its context in an administrative list and the 'thematic in -a' classification, 'wa' is plausibly interpreted as an abbreviation for 'wa-na-sa' (f), meaning 'queen', which is a feminine noun of the first declension (thematic -a). This interpretation provides a complete and meaningful term for the fragment. Logical/Syntactic analysis: The sequence of a toponym followed by two personal identifiers (an anthroponym and a title/anthroponym) strongly suggests an administrative record or roster. The structure is paratactic, listing entities without a finite verb, typical for Linear B inventory or personnel documents. '' functions as a heading or location marker, followed by the individuals associated with it. Subordinate clause analysis: No subordinate clauses are present in this short, nominal entry.\"}\n",
      "Current Document: 5629\n",
      "document: wa\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'king', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'wa' is classified as a 'MOSTLY_INCOMPLETE' noun, of 'anthroponym/animal name/theonym' type, with 'thematic in -a' inflection. This strongly suggests it is the initial part of a longer, well-attested Mycenaean word. Logical/Syntactic analysis: Considering the common vocabulary of Linear B administrative texts and the provided classification, 'wa' is most plausibly the beginning of the word 'wa-na-ka' (), which means 'king' or 'lord'. This title is a central figure in Mycenaean administration and religion. The classifier information for 'wa-na-ka' in example 2 (noun, common, thematic in -a) perfectly aligns with the provided classifiers for 'wa', reinforcing this interpretation. Subordinate clause analysis: Not applicable as this is a single, incomplete word. The translation assumes the most probable completion of the fragment based on linguistic authenticity and semantic coherence within the context of Linear B documents.\"}\n",
      "Current Document: 5630\n",
      "document: ai-ai-ta\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ai-ai-tas', 'reasoning': \"Grammatical analysis: The Linear B word 'ai-ai-ta' is classified as an 'INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -a' inflection. Given these classifications, the most probable Ancient Greek translation is '', which is a masculine proper noun in the nominative singular, following the first declension pattern for masculine nouns ending in - (e.g., ). The '-ta' ending in Linear B often corresponds to this declension. Logical/Syntactic analysis: As a single, isolated word, 'ai-ai-ta' most likely functions as a proper name, possibly a subject in an implied sentence or an entry in a list. The nominative case is the default for such standalone entries. Subordinate clause analysis: Not applicable as this is a single word.\"}\n",
      "Current Document: 5632\n",
      "document: ka-u-no o-du-ru-wi-jo wa-na-ka-te-ro\n",
      "answer: {'Ancient Greek translation': '  ', 'English translation': 'Kaunos Odruwios, the royal one.', 'reasoning': \"Grammatical analysis: Each word is interpreted as a nominative singular masculine form of a thematic -o stem. 'ka-u-no' () is a proper noun, likely an anthroponym. 'o-du-ru-wi-jo' () is also treated as a proper noun, possibly a patronymic or an ethnonym, given its classification as an anthroponym. 'wa-na-ka-te-ro' () is an adjective meaning 'royal' or 'of the king', used substantively here to denote a royal official or someone belonging to the king, consistent with its classification as a common noun with thematic -o inflection. Logical/Syntactic analysis: The sequence forms a descriptive entry, typical of Linear B administrative records. It identifies an individual (Kaunos Odruwios) and assigns them a status or role ('the royal one'). The absence of a finite verb is common in such nominal lists or inventory entries. Subordinate clause analysis: No subordinate clauses are present in this short entry.\"}\n",
      "Current Document: 5633\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kruzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a 'COMPLETE' 'noun' of 'anthroponym/animal name/theonym' type, with 'thematic in -o' inflection. This indicates a proper noun, likely a personal name, in the nominative singular case, following the second declension pattern. Among the provided Ancient Greek options, '' (Kruzos) fits this description as a plausible personal name or a name related to an animal (a type of bird or a cry), consistent with the 'anthroponym/animal name/theonym' classification and the thematic -o ending. Logical/Syntactic analysis: As a single word entry, 'ku-ru-zo' most likely functions as a standalone item, such as a name in a list or a heading, typical of administrative Linear B records. No complex syntactic structure or subordinate clauses are present. The translation 'Kruzos' is a direct and phonetically consistent transliteration into Ancient Greek, representing a proper noun.\"}\n",
      "Current Document: 5634\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kruzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a COMPLETE noun, specifically an anthroponym (proper name), with a thematic in -o inflection. This strongly suggests a masculine singular nominative form. Among the provided Ancient Greek options, '' (Kruzos) is the most suitable as a proper name that fits the thematic -o declension pattern (ending in -). The other options are either verbs (''), adjectives (''), or common nouns ('') less likely to be used as a standalone proper name in this context. Logical/Syntactic analysis: As a single-word document, 'ku-ru-zo' most likely functions as a heading or a personal name, which is a common occurrence in Linear B administrative records. No complex syntactic structure or subordinate clauses are present.\"}\n",
      "Current Document: 5635\n",
      "document: a-do-we\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Hadws', 'reasoning': \"Grammatical analysis: The Linear B word 'a-do-we' is classified as a 'mostly incomplete' noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. Among the provided Ancient Greek options, 'f' () is the most fitting. It represents a masculine proper name, which is an athematic noun in the nominative singular, perfectly aligning with the given morphological information. The syllabograms 'a', 'do', 'we' directly correspond to '', '', and 'f' respectively, forming . Logical/Syntactic analysis: As a single, mostly incomplete word, 'a-do-we' most plausibly functions as a standalone entry or a subject/topic in an administrative record, which is common for proper names in Linear B documents. Subordinate clause analysis: Not applicable as this is a single word entry. Semantic coherence: Translating 'a-do-we' as the proper name 'Hadws' is semantically coherent within the context of Linear B tablets, which frequently list individuals.\"}\n",
      "Current Document: 5636\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Klytos', 'reasoning': \"Grammatical analysis: The Linear B word 'ku-ru-zo' is classified as a 'COMPLETE' noun, specifically an 'anthroponym/animal name/theonym', with a 'thematic in -o' inflection. This indicates a nominative singular form, which in Ancient Greek thematic -o declension typically ends in -. Among the provided Ancient Greek options, '' (Klytos) fits this grammatical profile perfectly, as it is an adjective meaning 'famous' or 'renowned' that is frequently used as a proper name (anthroponym) and follows the thematic -o declension pattern (nominative singular ). Logical/Syntactic analysis: As a single, complete word classified as an anthroponym, 'ku-ru-zo' most logically represents a proper name. Semantic coherence: 'Klytos' is a well-attested Greek name, making it a semantically coherent choice for an anthroponym in a Mycenaean context.\"}\n",
      "Current Document: 5637\n",
      "document: ku-ja-ni\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kyanis', 'reasoning': \"Grammatical analysis: The Linear B word 'ku-ja-ni' is classified as a noun, specifically an anthroponym (personal name), with an athematic inflection. Among the provided Ancient Greek options, '' (Kyanis) is the most fitting transcription for a proper name. The syllabograms 'ku-ja-ni' directly correspond to '', with 'ja' often representing an 'ia' or 'ya' sound, and the final 'ni' representing ''. As a proper noun, it is likely in the nominative case, functioning as a standalone entry or subject. Logical/Syntactic analysis: As a single, isolated word, 'ku-ja-ni' most logically functions as a label or a name in an administrative record, which is common for Linear B texts. There are no other words to form a sentence, so it stands as a direct identification. Subordinate clause analysis: Not applicable as this is a single word without a complex sentence structure.\"}\n",
      "Current Document: 5638\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kruzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a complete noun, specifically an anthroponym, with a thematic in -o inflection. This corresponds to the Ancient Greek nominative singular ending - for thematic nouns. Among the provided options, '' (Kruzos) fits this grammatical pattern and classification as a proper name. Logical/Syntactic analysis: As a single word, 'ku-ru-zo' functions as a standalone proper noun, likely a personal name, without further syntactic complexity. Semantic coherence: The choice of '' is semantically coherent with the classification as an anthroponym, representing a plausible Mycenaean personal name.\"}\n",
      "Current Document: 5639\n",
      "document: pi-pi wa-to su-ro-no\n",
      "answer: {'Ancient Greek translation': ', F, ', 'English translation': 'Piphys, Watos, Syronos', 'reasoning': \"Grammatical analysis: All three words, 'pi-pi', 'wa-to', and 'su-ro-no', are classified as proper nouns (anthroponyms) in the nominative singular case. 'pi-pi' () is an athematic noun. 'wa-to' (F) and 'su-ro-no' () are thematic -o stem nouns. The chosen Ancient Greek forms reflect these classifications and are consistent with the provided translation options. Logical/Syntactic analysis: The document consists of a simple sequence of three proper nouns without any verbs, conjunctions, or other grammatical markers. This structure is characteristic of administrative lists or enumerations of individuals, which are common in Linear B tablets. There are no complex sentences or subordinate clauses to analyze. Semantic coherence: Interpreting the sequence as a list of names is the most semantically coherent and contextually appropriate translation, aligning with the typical content and format of Mycenaean administrative records.\"}\n",
      "Current Document: 5641\n",
      "document: ku-ja-ni\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kyiani', 'reasoning': \"Grammatical analysis: The word 'ku-ja-ni' is classified as a noun, specifically an anthroponym/animal name/theonym, with an athematic inflection. This indicates it is a proper name. The direct transliteration of the syllabograms 'ku-ja-ni' into Ancient Greek characters yields '--', which corresponds to ''. Logical/Syntactic analysis: As a single, standalone word, '' functions as a proper noun, likely representing an individual's name within an administrative record. There is no complex sentence structure or subordinate clauses to analyze. Semantic coherence: Translating 'ku-ja-ni' as the proper name 'Kyiani' is consistent with the nature of Linear B texts, which frequently list names of individuals, places, or deities.\"}\n",
      "Current Document: 5642\n",
      "document: a-re-zo-me-ne wa-to re-u-ko-jo\n",
      "answer: {'Ancient Greek translation': ' F ', 'English translation': 'The Protected Woman, Watos of Leukos.', 'reasoning': \"Grammatical analysis: 'a-re-zo-me-ne' () is interpreted as a feminine nominative singular participle from  ('to ward off, protect'), functioning as a descriptive title or a proper name, consistent with its classification as an adjective, thematic in -a. 'wa-to' (F) is a masculine nominative singular noun, likely an anthroponym (proper name), consistent with its classification as a noun, thematic in -o. 're-u-ko-jo' () is a masculine genitive singular noun, meaning 'of Leukos', indicating parentage or association, consistent with its classification as a noun, thematic in -o. Logical/Syntactic analysis: The phrase appears to be a nominal list, common in Linear B administrative documents. '' stands as a distinct entry, likely referring to a woman by a descriptive title or name. 'F ' forms a second entry, identifying a man 'Watos' and his patronymic or association 'of Leukos'. No explicit verb is present, which is typical for such records. Subordinate clause analysis: No subordinate clauses are present in this short phrase. The translation reflects a simple listing of individuals or entities.\"}\n",
      "Current Document: 5643\n",
      "document: e-wa-ko-ro ka-ma-ti-jo-jo\n",
      "answer: {'Ancient Greek translation': ' ', 'English translation': 'Euagros of Kamatios', 'reasoning': \"Grammatical analysis: 'e-wa-ko-ro' () is identified as a proper noun (anthroponym) in the nominative singular, following the thematic -o declension pattern. 'ka-ma-ti-jo-jo' () is also a proper noun (anthroponym), but in the genitive singular, indicated by the characteristic Mycenaean '-jo-jo' ending for thematic -o stems, which corresponds to Classical Greek '-'.\\nLogical/Syntactic analysis: The phrase consists of a nominative noun followed by a genitive noun. This is a common Mycenaean syntactic structure used to denote filiation or belonging, meaning 'X son of Y' or 'X of Y'. In this administrative context, it serves to identify an individual, Euagros, by his patronymic or association with Kamatios.\\nSubordinate clause analysis: There are no subordinate clauses present in this simple nominal phrase. The structure is a direct identification of a person.\"}\n",
      "Current Document: 5644\n",
      "document: a-re-zo-me-ne wa-to re-u-ko-jo\n",
      "answer: {'Ancient Greek translation': ' , ', 'English translation': 'Wato of Leukos, for the protecting one.', 'reasoning': \"Grammatical analysis: 'a-re-zo-me-ne' is interpreted as , a dative singular feminine participle from the verb  ('to ward off, protect'), following the thematic in -a declension pattern for feminine participles. 'wa-to' is interpreted as , a proper noun (likely a toponym) in the nominative singular, following the thematic in -o declension. 're-u-ko-jo' is interpreted as , the genitive singular of the proper noun  (a personal name), following the thematic in -o declension. Logical/Syntactic analysis: The phrase ' ' functions as a compound nominal phrase, identifying a specific location as 'Wato belonging to Leukos' or 'Wato of Leukos'. The dative participle '' acts as a dative complement, indicating the recipient or purpose ('for the protecting one'). This structure is common in administrative records where a location or item is designated for a specific person or entity. Subordinate clause analysis: No subordinate clauses are present in this nominal phrase. The overall interpretation suggests an administrative entry, possibly designating a location and its association with a person (Leukos), and a dedication or purpose related to a 'protecting one', who could be a deity or an official.\"}\n",
      "Current Document: 5645\n",
      "document: a-re-me-ne wa-to re-u-ko-jo\n",
      "answer: {'Ancient Greek translation': ' F ', 'English translation': 'Areimenes. Watos of Leukos.', 'reasoning': \"Grammatical analysis: 'a-re-me-ne' () is interpreted as a proper noun (anthroponym) in the nominative singular, masculine, following the thematic in -a declension pattern (e.g., like ). 'wa-to' (F) is also a proper noun (anthroponym) in the nominative singular, masculine, following the thematic in -o declension. 're-u-ko-jo' () is a proper noun (anthroponym) in the genitive singular, masculine, following the thematic in -o declension, with the Mycenaean genitive ending -jo. Logical/Syntactic analysis: The sequence presents a list of individuals. '' is a distinct individual. 'F ' is interpreted as 'Watos, son of Leukos' or 'Watos of Leukos', a common patronymic construction in Linear B administrative texts where the genitive indicates filiation or belonging. The absence of a finite verb is typical for such administrative lists or rosters. No subordinate clauses are present.\"}\n",
      "Current Document: 5646\n",
      "document: e-u-da-mo wa-to ri-*82-ta-o\n",
      "answer: {'Ancient Greek translation': ' F ', 'English translation': 'Eudamos, Watos, Litaos.', 'reasoning': \"Grammatical analysis: The document presents three words, all classified as proper nouns (anthroponyms) with thematic -o stem inflection. 'e-u-da-mo' () is interpreted as a nominative singular form. 'wa-to' (F) is similarly interpreted as a nominative singular form. 'ri-*82-ta-o' is translated as ''. Although the syllabogram *82 maps to ['', '', 'f'], leading to a more complex reconstruction like 'F', the provided Ancient Greek option '' is chosen, representing a simplified or conventionalized form of the name. Given the context of a list of names, '' is also interpreted as a nominative singular, maintaining consistency with the preceding names. Logical/Syntactic analysis: The document consists of a simple enumeration of three proper nouns, all in the nominative case. This paratactic structure is characteristic of many Linear B administrative records, which often list individuals without complex verbal constructions. There is no verb or other grammatical marker to suggest a more elaborate sentence structure. Subordinate clause analysis: No subordinate clauses are present in this document. The text functions as a straightforward list.\"}\n",
      "Current Document: 5647\n",
      "document: pi-pi [...] wa-to su-ro-no\n",
      "answer: {'Ancient Greek translation': ' [...] F ', 'English translation': 'Pips [...] Watos, son of Syron', 'reasoning': \"Grammatical analysis: 'pi-pi' is an uncertain, athematic noun, most plausibly translated as the proper name '' (Pips) in the nominative singular. The fragment '[...]' indicates a break, separating 'pi-pi' from the subsequent words. 'wa-to' is a thematic -o noun, translated as the proper name 'F' (Watos) in the nominative singular. 'su-ro-no' is also a thematic -o noun. While it could be a nominative singular like '', the option '' (Syronos) is a genitive form of the name ''. In Linear B administrative texts, it is common to find an individual's name followed by a patronymic in the genitive case. This interpretation makes '' the most fitting choice for 'su-ro-no', indicating 'son of Syron'.\\nLogical/Syntactic analysis: The document is fragmentary. 'pi-pi' likely represents a standalone entry or the beginning of a separate record. The sequence 'wa-to su-ro-no' is interpreted as a personal name ('F') followed by a patronymic (''), a common structure for identifying individuals in Mycenaean records. This structure is consistent with the administrative nature of Linear B texts, which often list individuals for various purposes (e.g., land tenure, personnel rosters).\\nSubordinate clause analysis: No subordinate clauses are present in this short, fragmentary document.\"}\n",
      "Current Document: 5649\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kruzos', 'reasoning': \"Grammatical analysis: The Linear B word 'ku-ru-zo' is classified as a 'COMPLETE' noun, specifically an 'anthroponym/animal name/theonym', with 'thematic in -o' inflection. This indicates a nominative singular form ending in - in Ancient Greek. Among the provided options, '' (kruzos) perfectly matches this morphological pattern, being a thematic -o stem noun in the nominative singular. While '' is also a thematic -o stem, '' is a more direct phonetic match and is attested as a name or a type of bird. Logical/Syntactic analysis: As a single, complete word, 'ku-ru-zo' most likely functions as a proper name (anthroponym) or an animal name, possibly as a heading or an entry in an administrative list. Without further context, interpreting it as a proper name 'Kruzos' is the most straightforward and common approach for such entries in Linear B documents. Semantic coherence: 'Kruzos' as a proper name is semantically coherent with the classification 'anthroponym/animal name/theonym'.\"}\n",
      "Current Document: 5650\n",
      "document: di-no-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Dinozos', 'reasoning': \"Grammatical analysis: The Linear B word 'di-no-zo' is classified as a complete noun, specifically an anthroponym/animal name/theonym, with a thematic in -o inflection. This corresponds to a second declension noun in Ancient Greek. Among the provided options, '' (Dinozos) is the most fitting for a proper name in the nominative singular, consistent with the thematic -o inflection (ending in -). Logical/Syntactic analysis: As a single, complete word in an administrative context, '' most likely functions as a nominative singular, serving as a personal name or a heading for an entry. No complex sentence structure or subordinate clauses are present.\"}\n",
      "Current Document: 5651\n",
      "document: di-no-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Dinozos', 'reasoning': \"Grammatical analysis: The Linear B word 'di-no-zo' is classified as a complete noun, specifically an anthroponym (proper name), with a thematic in -o inflection. This indicates it follows the second declension pattern. The Ancient Greek translation '' (Dinozos) is the nominative singular form of a masculine noun of the second declension, perfectly matching the provided classification and common Mycenaean naming conventions. Logical/Syntactic analysis: As a single, complete word, '' most likely functions as a standalone entry, a heading, or the subject of an implied sentence, typical for administrative records listing individuals. Subordinate clause analysis: Not applicable as this is a single word without a complex sentence structure. Semantic coherence: The translation as a proper name is consistent with the 'anthroponym' classification and the nature of Linear B documents, which often record names of individuals.\"}\n",
      "Current Document: 5652\n",
      "document: a-nu-to\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Anytos', 'reasoning': \"Grammatical analysis: 'a-nu-to' is classified as a complete noun, specifically an anthroponym (proper name), with a thematic in -o inflection. This corresponds to the Ancient Greek masculine proper noun '' in the nominative singular, which follows the second declension pattern. Logical/Syntactic analysis: As a single word, '' functions as a standalone entry, most likely a name in a list or a heading within an administrative document. There is no complex syntax or subordinate clauses to analyze.\"}\n",
      "Current Document: 5653\n",
      "document: a-nu-to\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Anutos', 'reasoning': \"Grammatical analysis: 'a-nu-to' is classified as a complete noun, specifically an anthroponym (personal name), with a thematic -o inflection. This corresponds to the nominative singular form in Ancient Greek, typically ending in -. '' is a plausible transcription for a Mycenaean personal name. Logical/Syntactic analysis: As a single-word document, 'a-nu-to' functions as a standalone entry, most likely a name or a heading in an administrative record. Subordinate clause analysis: Not applicable for a single-word entry.\"}\n",
      "Current Document: 5654\n",
      "document: a-nu-to\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Anutos', 'reasoning': \"Grammatical analysis: The Linear B word 'a-nu-to' is classified as a complete noun, specifically an anthroponym (proper name), with a thematic -o inflection. This directly corresponds to the Ancient Greek masculine nominative singular form ''. Logical/Syntactic analysis: As a single word entry, '' functions as a proper noun, likely representing an individual's name in an administrative record. No complex syntactic structure or subordinate clauses are present in this single-word document.\"}\n",
      "Current Document: 5655\n",
      "document: i-ru\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Irys', 'reasoning': \"Grammatical analysis: The Linear B word 'i-ru' is classified as a complete noun, specifically an anthroponym (personal name), with athematic inflection. The Ancient Greek translation '' (Irys) is chosen as a plausible nominative singular form for an athematic noun, consistent with the syllabic transcription 'i-ru'. The final - is a common nominative ending for masculine/feminine athematic nouns in Ancient Greek. Logical/Syntactic analysis: As a single, isolated word, 'i-ru' most likely represents a proper name, functioning as a standalone entry, possibly in a list or as a heading within an administrative document. There is no complex syntax or sentence structure to analyze. Subordinate clause analysis: No subordinate clauses are present in this single-word entry. Semantic coherence: Translating 'i-ru' as the proper name 'Irys' aligns perfectly with its classification as an anthroponym.\"}\n",
      "Current Document: 5656\n",
      "document: i-ru\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Irys', 'reasoning': \"Grammatical analysis: The Linear B word 'i-ru' is classified as a complete noun, an anthroponym (personal name), with athematic inflection. The proposed Ancient Greek translation '' (Irys) is a proper noun, consistent with an athematic declension in the nominative singular, which is a common form for names in Linear B records. Logical/Syntactic analysis: As a single, standalone word, 'i-ru' most logically functions as a name or a label within an administrative context, typical of Linear B documents. There is no complex syntactic structure or subordinate clauses to analyze, as it is a single lexical item. Semantic coherence: '' is a plausible Mycenaean personal name, fitting the common practice of listing individuals in administrative records. The choice of '' over '' is based on '' being a more commonly attested proper name in Greek contexts, often associated with mythology (e.g., Iris, the messenger goddess).\"}\n",
      "Current Document: 5657\n",
      "document: ru-i\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Lys', 'reasoning': \"Grammatical analysis: The Linear B word 'ru-i' is classified as a complete noun, specifically an anthroponym (proper name), with an athematic inflection. Among the provided Ancient Greek translation options (||), '' (Lys) is a well-attested Greek proper name that fits the athematic declension pattern (third declension). The syllabogram 'ru' can correspond to '' or '' and 'u', while 'i' corresponds to ''. The combination 'ru-i' most plausibly represents '' in the nominative singular. Logical/Syntactic analysis: As a single, complete word, '' functions as a standalone entry, likely a name in the nominative case, typical for lists or as a subject in a very short, implied sentence. Subordinate clause analysis: Not applicable as this is a single word.\"}\n",
      "Current Document: 5658\n",
      "document: ta-de-so\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Tadesos', 'reasoning': \"Grammatical analysis: 'ta-de-so' is classified as a 'MOSTLY_COMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. The most direct and plausible Ancient Greek transliteration from the given options is '', which functions as a proper noun in the nominative singular. Logical/Syntactic analysis: As a single word, '' stands as a proper name, likely representing an individual or a deity, common in Linear B administrative records as an entry or subject. Subordinate clause analysis: No subordinate clauses are present as the input consists of a single word.\"}\n",
      "Current Document: 5659\n",
      "document: de-so\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Desos', 'reasoning': \"Grammatical analysis: The Linear B word 'de-so' is classified as an incomplete noun, specifically an anthroponym (proper name), with thematic -o inflection. This strongly suggests a masculine singular nominative form ending in - in Ancient Greek. Logical/Syntactic analysis: As a standalone, incomplete word, it most likely represents a proper name, possibly an individual's name in an administrative record. Subordinate clause analysis: Not applicable for a single word. Translation choice: Among the provided Ancient Greek options, '' () is the most direct and plausible transliteration for an anthroponym with thematic -o inflection, fitting the typical administrative context of Linear B documents where names are frequently listed. Other options like '' (fear) or '' (enemy) are common nouns, and '' is a genitive form, making '' the most appropriate choice for a nominative anthroponym.\"}\n",
      "Current Document: 5660\n",
      "document: ta-*22-de-so\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Tamidesos', 'reasoning': \"Grammatical analysis: The Linear B word 'ta-*22-de-so' is classified as a complete noun, specifically an anthroponym/animal name/theonym, with a thematic in -o inflection. Among the provided Ancient Greek translation options, '' (Tamidesos) is the most consistent with this classification. It ends in -, which is the characteristic nominative singular ending for masculine nouns of the second declension (thematic -o stems) in Ancient Greek, aligning perfectly with the 'thematic in -o' inflection. The option '' is a common noun and follows a third declension (athematic) pattern, making it inconsistent. '' is too short to correspond to four syllabograms. '' ends in -, which is less typical for a nominative singular masculine thematic -o stem compared to -. Logical/Syntactic analysis: As an anthroponym, '' functions as a proper noun, likely representing an individual's name within an administrative record. Without further context, its specific syntactic role (e.g., subject, object) cannot be determined, but its identity as a named entity is clear. Subordinate clause analysis: Not applicable, as this is a single word translation.\"}\n",
      "Current Document: 5661\n",
      "document: ta-*22-de-so\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Tamidesos', 'reasoning': \"Grammatical analysis: The Linear B word 'ta-*22-de-so' is classified as a 'COMPLETE' 'noun' of 'anthroponym/animal name/theonym' type, with 'thematic in -o' inflection. Among the provided Ancient Greek translation options, '' (Tamidesos) is a proper noun that fits the 'anthroponym/theonym' category. Its ending '-' is characteristic of a masculine nominative singular noun of the second declension (thematic -o stems), which perfectly aligns with the 'thematic in -o' inflection specified in the classifier. The other options are less suitable: '' (peacock) is an animal name but typically follows a third declension pattern, not thematic -o; '' (tripods) is a common noun and also third declension, thus not fitting either the noun type or inflection; '' (Tatuedeso) is a plausible proper name, but the - ending for a masculine nominative singular thematic -o noun is less common than -. Logical/Syntactic analysis: As a proper noun in the nominative singular, '' would function as a subject in a sentence or as a standalone entry in an administrative list, which is a common format for Linear B texts. Subordinate clause analysis: No subordinate clauses are present as this is a single word entry. Linguistic authenticity and semantic coherence: '' is a plausible Mycenaean proper name, consistent with the onomastic patterns found in Linear B, and its translation as a proper name maintains semantic coherence for an administrative record.\"}\n",
      "Current Document: 5662\n",
      "document: ta-*22-de-so\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Tamidesos', 'reasoning': \"Grammatical analysis: The Linear B word 'ta-*22-de-so' is classified as a 'COMPLETE' noun, specifically an 'anthroponym/animal name/theonym', with 'thematic in -o' inflection. Among the provided Ancient Greek options, '' (tripods) is a common noun and belongs to the third declension, making it inconsistent with the given classifiers. '' (peacock) is an animal name, but its classical Greek form () is a third declension noun, not thematic in -o. While Mycenaean forms can vary, the 'thematic in -o' classification strongly suggests a nominative singular ending in - for masculine nouns. '' ends in -, which is less typical for a masculine nominative singular thematic -o noun. '' ends in -, which is the standard nominative singular ending for masculine thematic -o nouns (e.g., ). This option perfectly aligns with all the provided grammatical classifiers: it is a noun, can function as an anthroponym/theonym, and its ending is consistent with a thematic -o declension in the nominative singular. Logical/Syntactic analysis: As a single proper noun, '' likely functions as a standalone entry in an administrative record, possibly a name of an individual, animal, or deity. Subordinate clause analysis: Not applicable for a single word entry.\"}\n",
      "Current Document: 5663\n",
      "document: ti-tu\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Tityus', 'reasoning': \"Grammatical analysis: The Linear B word 'ti-tu' is classified as an INCOMPLETE noun, specifically an anthroponym/animal name/theonym with athematic inflection. The proposed Ancient Greek translation '' (Tityus) is an athematic proper noun in the accusative case. The Linear B script frequently omits final consonants, making 'ti-tu' a plausible representation for '' where the final nu () is not explicitly written. The syllabograms 'ti' and 'tu' directly correspond to '' and '' respectively, making '' a more direct phonetic match than '' from the given options. Logical/Syntactic analysis: As a single, incomplete word, 'ti-tu' lacks complex syntactic structure. It most likely functions as an entry in a list, a label, or a damaged part of a larger record, typical of Mycenaean administrative texts. Subordinate clause analysis: No subordinate clauses are present in this single-word entry.\"}\n",
      "Current Document: 5664\n",
      "document: wa-to su-ro-no\n",
      "answer: {'Ancient Greek translation': 'F ', 'English translation': 'Watos, Syronos', 'reasoning': \"Grammatical analysis: Both 'wa-to' (F) and 'su-ro-no' () are classified as nouns, specifically anthroponyms, with a thematic in -o inflection. This morphological pattern strongly suggests they are in the nominative singular case, indicating two distinct individuals. Logical/Syntactic analysis: The document consists of two proper nouns in succession, without any verbs, prepositions, or conjunctions. This structure is characteristic of simple lists of names, which are very common in Linear B administrative records. There is no complex sentence structure or subordinate clauses to analyze. The translation reflects a straightforward enumeration of two individuals.\"}\n",
      "Current Document: 5665\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kryzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a COMPLETE noun, specifically an anthroponym (proper name), with a thematic -o inflection. This corresponds to the nominative singular form of a thematic -o stem noun in Ancient Greek, which typically ends in -. Among the provided options, '' (Kryzos) is the only one that fits this grammatical description as a proper noun. '' is a verb, '' is an adjective, and '' is a common noun, not a proper name. Logical/Syntactic analysis: As a single word, 'ku-ru-zo' functions as a standalone entry, most likely a name in an administrative list or a heading. There is no complex syntax or subordinate clauses to analyze.\"}\n",
      "Current Document: 5666\n",
      "document: ni-jo-jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'of Nioios', 'reasoning': \"Grammatical analysis: The Linear B word 'ni-jo-jo' is classified as an incomplete noun, specifically an anthroponym/animal name/theonym, with a thematic in -o inflection. The ending '-jo-jo' is a characteristic genitive singular suffix for thematic -o stems in Mycenaean Greek (equivalent to Classical Greek -). Among the provided Ancient Greek options, '' directly corresponds to this form, representing the genitive singular of a name like ''. Logical/Syntactic analysis: As a genitive singular, '' would typically indicate possession ('Nioios's') or a relationship ('of Nioios'). Given its 'INCOMPLETE' status and the nature of Linear B texts, this word likely functions as a possessive modifier or a heading in an administrative record, even without further context. Subordinate clause analysis: Not applicable as this is a single, inflected noun.\"}\n",
      "Current Document: 5667\n",
      "document: ni\n",
      "answer: {'Ancient Greek translation': '-', 'English translation': 'Ni- (fragment)', 'reasoning': \"Grammatical analysis: The Linear B word 'ni' is classified as an incomplete noun, specifically an anthroponym, animal name, or theonym, with a thematic -o inflection. Its incompleteness is explicitly stated. Logical/Syntactic analysis: As a single, incomplete word, 'ni' cannot form a complete sentence or convey a full meaning on its own. In the context of Linear B administrative texts, incomplete words, especially those classified as proper nouns, often represent fragments of names or toponyms. Without additional syllabograms, its full form and precise meaning remain unknown. Subordinate clause analysis: Not applicable for a single word fragment. Translation decision: The translation reflects the word's incomplete nature by presenting it as '-' in Ancient Greek and 'Ni- (fragment)' in English, indicating that it is the beginning of a word, most likely a name, consistent with its classification.\"}\n",
      "Current Document: 5668\n",
      "document: ne [...] wa-to [...] re-u-ko-jo\n",
      "answer: {'Ancient Greek translation': ' [...] F [...] ', 'English translation': 'Island [...] Watos [...] of Leukos', 'reasoning': \"Grammatical analysis: 'ne' is interpreted as '' (island), a common noun in the nominative singular, consistent with its classification as an athematic noun. 'wa-to' is translated as 'F' (Watos), a proper noun (anthroponym) in the nominative singular, following the thematic in -o declension. 're-u-ko-jo' is rendered as '' (Leukoio), a proper noun (anthroponym) in the genitive singular, also thematic in -o, indicating possession or a patronymic ('of Leukos'). Logical/Syntactic analysis: The fragmentary nature of the document, indicated by '[...]', prevents the formation of a complete sentence. However, the sequence of a potential location (''), followed by an individual's name ('F'), and then a possessive or patronymic (''), suggests a typical administrative entry or list. This pattern is common in Linear B texts for identifying individuals and their associations or locations. Subordinate clause analysis: No subordinate clauses are present in this highly fragmented sequence.\"}\n",
      "Current Document: 5669\n",
      "document: e-wa-ko-ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Euagoras', 'reasoning': \"Grammatical analysis: The Linear B word 'e-wa-ko-ro' is classified as a complete noun, specifically an anthroponym (personal name), with a thematic in -o inflection. This suggests a nominative singular form ending in - in Ancient Greek. The syllabogram 'ko' can represent both /ko/ and /go/ (as per the provided mapping 'ko': ['', '', '', '', '']), making '' (Euagoras) a plausible reconstruction. '' is a well-attested Greek personal name meaning 'good speaker' or 'good in the assembly'. Logical/Syntactic analysis: As a single, complete word, 'e-wa-ko-ro' most likely functions as a standalone entry, perhaps a name in a list or a heading, typical of administrative Linear B texts. There are no other words to form a sentence or subordinate clauses. The translation directly reflects the identified proper noun.\"}\n",
      "Current Document: 5670\n",
      "document: e-wa-ko-ro\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ewagoros', 'reasoning': \"Grammatical analysis: The Linear B word 'e-wa-ko-ro' is classified as a complete noun, specifically an anthroponym (personal name), with a thematic -o inflection. This indicates a masculine singular nominative form. Logical/Syntactic analysis: As a standalone word, it functions as a proper noun, likely a name or a heading in an administrative context. Subordinate clause analysis: Not applicable for a single word. The chosen Ancient Greek translation '' (Ewagoros) is the most phonetically consistent and commonly accepted transcription for this Linear B syllabogram sequence, fitting the given grammatical classification as a proper name.\"}\n",
      "Current Document: 5671\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ti\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ti\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ti', 'reasoning': \"Grammatical analysis: The Linear B word 'ti' is classified as an incomplete noun, specifically an anthroponym/animal name/theonym, with thematic in -o inflection. As a single syllabogram, it is highly fragmentary, making a definitive grammatical analysis challenging. The thematic -o inflection typically implies a masculine nominative singular ending in - or a neuter ending in -, but this cannot be fully realized from a single syllable. Logical/Syntactic analysis: Due to its extreme incompleteness, 'ti' cannot form a coherent sentence or phrase on its own. It most likely represents a fragment of a personal name or an abbreviation within an administrative context. Syntactic structure is not applicable here. Translation decision: Given the 'INCOMPLETE' status and the requirement for 'error recovery' and 'complete translations' for even fragmentary inputs, the most direct and least speculative approach is to transliterate 'ti' as '' in Ancient Greek and 'Ti' in English. This acknowledges its fragmentary nature while providing a plausible representation as a potential name or name fragment, consistent with its classification as an anthroponym. Among the provided Ancient Greek options ('|||'), '' is the most direct transliteration of the syllabogram. While '' is a complete word, it is a pronoun and not consistent with the 'thematic in -o' noun inflection provided in the classifier information. Therefore, '' is chosen as the most appropriate representation for this highly incomplete entry.\"}\n",
      "Current Document: 5672\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: re\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: re\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'stream', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 're' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. Among the provided Ancient Greek translation options, '' () is a noun of the third declension (athematic), meaning 'stream'. While 'stream' is typically a common noun, not an anthroponym or theonym, it is the only option that is a complete noun and fits the 'athematic' inflection pattern. Given the extreme incompleteness of a single syllabogram, a definitive identification as a proper name is impossible without further context. Logical/Syntactic analysis: As a single, incomplete word, 're' likely represents a fragment or a very short term. Interpreting it as  provides a grammatically coherent noun. Without further context, its role in a sentence cannot be determined, but it stands as a standalone nominal entry. Subordinate clause analysis: Not applicable for a single word. Semantic coherence: Choosing  provides a concrete, albeit common, noun translation for the fragment, adhering to the requirement for a complete translation. The administrative nature of Linear B texts often includes lists of items or individuals, where even a single noun can constitute an entry.\"}\n",
      "Current Document: 5673\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ios', 'reasoning': \"Grammatical analysis: The Linear B word 'jo' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -o' inflection. This strongly suggests a proper name. Among the provided Ancient Greek options ('|||'), '' (Ios) is a plausible proper name that aligns with the 'anthroponym' classification and the phonetic possibilities of 'jo'. Logical/Syntactic analysis: As a single, mostly incomplete word, its most probable function in a Linear B document is a standalone entry, likely a name in a list or record, which is common for such fragments. Semantic coherence: 'Ios' as a proper name provides semantic coherence for an anthroponym, fitting the administrative and listing nature of Linear B texts.\"}\n",
      "Current Document: 5674\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: o\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: o\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'bone', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'o' is classified as a 'MOSTLY_COMPLETE' common noun with 'thematic in -o' inflection. In Ancient Greek, the nominative singular masculine form for thematic -o stems is typically -. Among the provided Ancient Greek translation options, '' is the most direct fit for this classification. We select '' (bone) as a specific example of a thematic -o noun that could plausibly appear in an administrative record. Logical/Syntactic analysis: As a single word, 'o' functions as a simple noun entry, likely representing an item in a list or inventory, which is common in Linear B administrative documents. Subordinate clause analysis: No subordinate clauses are present in this single-word entry. The choice of '' ensures semantic coherence within the typical context of Mycenaean records, which often catalog goods or resources.\"}\n",
      "Current Document: 5675\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: e\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: e\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'O [implied noun]', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'e' is classified as a 'MOSTLY_INCOMPLETE' noun with 'thematic in -o' inflection. In Ancient Greek, the vocative singular of masculine thematic -o nouns typically ends in - (e.g.,  from ). Given that 'e' maps to '' and the word is incomplete, interpreting 'e' as the vocative ending of an unstated thematic -o noun is a plausible 'error recovery' strategy to assign a grammatical function consistent with the provided classification. Logical/Syntactic analysis: As a single, incomplete syllabogram, 'e' cannot form a complete logical or syntactic unit on its own. However, by interpreting it as a vocative ending, it implies an address to an unstated entity, fitting the 'noun' classification. Semantic coherence: This interpretation allows for a grammatically coherent, albeit fragmentary, understanding of the input, aligning with the administrative nature of Linear B texts where names or titles might be addressed. Linguistic authenticity: The vocative ending - for thematic -o nouns is a well-attested feature of Ancient Greek. The translation '' represents this ending, and 'O [implied noun]' conveys the vocative sense in English, acknowledging the missing stem.\"}\n",
      "Current Document: 5676\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kryzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a COMPLETE noun, specifically an anthroponym/animal name/theonym, with a thematic in -o inflection. This indicates a nominative singular form, which in Ancient Greek thematic -o stems typically ends in -. Logical/Syntactic analysis: As a single word, it functions as a proper noun, likely a personal name. Subordinate clause analysis: No subordinate clauses are present as this is a single word. Translation choice: Among the provided options, '' (Kryzos) best fits the phonetic transcription of 'ku-ru-zo' and the grammatical classification as a thematic -o noun functioning as an anthroponym. '' is an attested, albeit rare, personal name.\"}\n",
      "Current Document: 5677\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: ku-ru-zo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: ku-ru-zo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kryzos', 'reasoning': \"Grammatical analysis: 'ku-ru-zo' is classified as a noun, specifically an anthroponym/animal name/theonym, with a thematic -o inflection. The final '-o' syllabogram corresponds to the nominative singular ending '-' for thematic -o stem nouns in Ancient Greek. Phonetic analysis: The Linear B syllabograms 'ku-ru-zo' directly map to the Ancient Greek ''. 'ku' maps to '', 'ru' maps to '', and 'zo' maps to ''. Semantic coherence: '' means 'grunting' or 'squealing', and can refer to a 'grunter' or 'squealer'. As an anthroponym, it functions as a proper name, 'Kryzos', which is consistent with the classification. Syntactic analysis: As a single word, it stands as a nominative noun, likely representing a name or a descriptive epithet.\"}\n",
      "Current Document: 5678\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: a-nu-to\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: a-nu-to\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Anytos', 'reasoning': \"Grammatical analysis: The word 'a-nu-to' is classified as a complete noun, specifically an anthroponym (a personal name), with a thematic -o inflection. This corresponds to a masculine proper noun of the second declension in Ancient Greek. The most direct and attested translation is '', which is the nominative singular form of this name. Logical/Syntactic analysis: As a single word entry, 'a-nu-to' functions as a standalone proper noun, likely representing an individual's name in an administrative record. In Linear B documents, single names often appear as entries, headings, or subjects of a record. Subordinate clause analysis: No subordinate clauses are present as this is a single word.\"}\n",
      "Current Document: 5679\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ios', 'reasoning': \"Grammatical analysis: The Linear B word 'jo' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -o' inflection. Among the provided Ancient Greek options ('', '', '', ''), '' (Ios) is the most suitable choice as a proper noun (anthroponym) that fits the thematic -o declension pattern (nominative singular masculine). The other options are either articles, pronouns, or adjectives, which do not align as well with the 'noun type' and 'inflection' provided. Logical/Syntactic analysis: As a single, 'MOSTLY_INCOMPLETE' word, 'jo' likely represents a proper name or a fragment thereof, common in administrative records. Without further context, it functions as a standalone entry, possibly a name in a list or a heading. Subordinate clause analysis: Not applicable for a single word entry. The translation prioritizes the most plausible proper name given the grammatical classification.\"}\n",
      "Current Document: 5680\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "DEBUG: jo\n",
      "Error occurred: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.. Retrying in 65 seconds...\n",
      "document: jo\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ios', 'reasoning': \"Grammatical analysis: The Linear B word 'jo' is classified as a mostly incomplete noun, specifically an anthroponym/theonym with thematic -o inflection. Among the possible Ancient Greek translations, '' (Ios) is a proper name that perfectly fits this grammatical description, being a masculine nominative singular noun of the thematic -o declension. Logical/Syntactic analysis: As a standalone, mostly incomplete word, 'jo' most plausibly represents a short proper name or a fragment of a longer name. '' functions as a complete, albeit short, proper name. Without further context, its specific syntactic role (e.g., subject of a sentence, an entry in a list) cannot be definitively determined, but its classification as an anthroponym makes 'Ios' the most semantically coherent choice.\"}\n",
      "Current Document: 5681\n",
      "document: tu [...] jo\n",
      "answer: {'Ancient Greek translation': ' [...] ', 'English translation': 'Tu [...] Ios', 'reasoning': \"Grammatical analysis: 'tu' is classified as an uncertain, thematic -o stem noun, likely an anthroponym. The most direct Ancient Greek equivalent is ''. 'jo' is classified as a mostly incomplete, thematic -o stem noun, also likely an anthroponym. The most plausible Ancient Greek equivalent is ''. Both are interpreted as proper names in the nominative case, consistent with their provided classifications. Logical/Syntactic analysis: The presence of '[...]' indicates a significant gap in the text. Therefore, the syntactic relationship between 'Tu' and 'Ios' cannot be determined. They are presented as two distinct proper names, possibly part of a list or a fragmented sentence, which is common in administrative Linear B documents. Subordinate clause analysis: No subordinate clauses are present in this short, fragmented sequence. The translation reflects the fragmentary nature of the document, providing the most direct interpretation of the identified words as proper names while acknowledging the missing context.\"}\n",
      "Current Document: 5682\n",
      "document: a\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'A', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'a' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -a' inflection. Given the direct syllabogram mapping 'a' -> '', and the requirement for a complete translation, '' (Alpha, as a proper name) is chosen. While a single letter is highly unusual for a complete name, it is the most direct interpretation that aligns with the provided classifiers and the strict instruction to provide a complete translation. The 'thematic in -a' inflection suggests it might be an abbreviated form of a name that would typically follow this declension pattern, or it is a placeholder for a name whose full form is unknown. Logical/Syntactic analysis: As a standalone word, '' would function as a proper noun, likely a name or an initial in an administrative record. Without further context, its specific syntactic role (e.g., subject, object) cannot be determined, but it stands as an isolated entry. Subordinate clause analysis: Not applicable as this is a single, isolated word.\"}\n",
      "Current Document: 5683\n",
      "document: wa-wo\n",
      "answer: {'Ancient Greek translation': 'ff', 'English translation': 'Wawon', 'reasoning': \"Grammatical analysis: The Linear B word 'wa-wo' is classified as an incomplete noun, specifically an anthroponym (personal name), with a thematic in -o inflection. This indicates a masculine or neuter noun of the second declension. Among the provided Ancient Greek translation options, 'ff' (Wawon) is the most direct and commonly attested Mycenaean personal name that fits the thematic -o declension pattern for a nominative singular form. Logical/Syntactic analysis: As a single, incomplete word, 'wa-wo' most likely functions as a standalone entry, possibly a name in a list or the subject of a fragmentary sentence. The 'INCOMPLETE' completeness level suggests it is a fragment, but 'ff' provides the most coherent and attested interpretation for this fragment in an administrative context. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5685\n",
      "document: pi\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'fat, rich (land/produce)', 'reasoning': \"Grammatical analysis: The Linear B word 'pi' is classified as an 'INCOMPLETE' common noun with 'athematic' inflection. Among the provided Ancient Greek translation options ('|||'), '' (pon) is a plausible common noun meaning 'fat, rich (land/produce)'. This form is typically a thematic adjective/noun (neuter singular of ). There is a discrepancy between the classifier's 'athematic' inflection and the thematic nature of ''. Given that 'pi' is marked as 'INCOMPLETE', it likely represents a fragment of a larger word. However, to provide a 'complete translation' as required, '' is selected as the most semantically coherent common noun from the given options, representing what the fragment 'pi' could signify in an administrative context (e.g., referring to land quality or agricultural produce). The 'athematic' classification might refer to a potential underlying root or a broader category that doesn't perfectly align with the specific thematic forms provided as options for this incomplete word. Logical/Syntactic analysis: As a single, incomplete word, 'pi' (interpreted as '') would function as a common noun, likely part of a nominal phrase or an item in an inventory, typical of Linear B administrative records. Without further context, its precise syntactic role (e.g., subject, object) remains indeterminate. Subordinate clause analysis: Not applicable for a single, fragmentary word.\"}\n",
      "Current Document: 5686\n",
      "document: da\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Da', 'reasoning': \"Grammatical analysis: The Linear B word 'da' is classified as a 'MOSTLY_INCOMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'thematic in -a' inflection. Given its incompleteness and classification, '' is chosen as the most direct and plausible Ancient Greek equivalent, representing a short form of a proper name or the beginning of a longer name. Syntactic analysis: As a single, incomplete word, a full syntactic analysis is not possible. It functions as a standalone entry, likely a proper noun in the nominative case. Semantic coherence: The translation 'Da' () is semantically coherent as a proper name, consistent with the administrative nature of Linear B texts which often list individuals or entities.\"}\n",
      "Current Document: 5687\n",
      "document: ku-ja-ni\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Kyanis', 'reasoning': \"Grammatical analysis: The Linear B word 'ku-ja-ni' is classified as a 'MOSTLY_COMPLETE' noun, specifically an 'anthroponym/animal name/theonym' with 'athematic' inflection. Among the provided Ancient Greek options, '' (Kyanis) is the most phonetically consistent and plausible proper name or epithet. The athematic inflection suggests a nominative singular form. Logical/Syntactic analysis: As a single word entry, '' functions as a standalone label or name, typical of administrative records where individuals or entities are listed. There is no complex sentence structure to analyze. Subordinate clause analysis: Not applicable as this is a single word entry.\"}\n",
      "Current Document: 5699\n",
      "document: o-na-ta\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'benefits / usufructs / holdings', 'reasoning': \"Grammatical analysis: The Linear B word 'o-na-ta' is classified as a common noun with thematic in -a inflection. The ending '-ta' strongly suggests a neuter plural nominative or accusative form, consistent with Ancient Greek thematic nouns (e.g.,  -> ). The most direct and contextually appropriate Ancient Greek translation is  (onata), which means 'benefits,' 'usufructs,' or 'holdings.' This term is well-attested in Linear B texts, often referring to rights or portions of land. Logical/Syntactic analysis: As a standalone word, 'o-na-ta' likely functions as a heading or a single entry in an administrative document, categorizing a type of asset or right. It does not form a complete sentence but rather labels a concept crucial for understanding Mycenaean economic administration. Subordinate clause analysis: Not applicable for a single word entry.\"}\n",
      "Current Document: 5700\n",
      "document: ja\n",
      "answer: {'Ancient Greek translation': '', 'English translation': 'Ia', 'reasoning': \"Grammatical analysis: The Linear B syllabogram 'ja' is classified as an incomplete noun, specifically an anthroponym (proper name), with a thematic in -a inflection. Among the provided Ancient Greek translation options (, , ), '' directly corresponds to the syllabogram 'ja' (which maps to '' and ''). The Ancient Greek name '' (Ia) is a plausible feminine proper noun that fits the 'thematic in -a' inflection pattern (first declension). Although the completeness level is 'INCOMPLETE', the instruction 'ALWAYS provide complete translations' necessitates choosing the most probable complete form. Logical/Syntactic analysis: As a standalone entry in a Linear B document, a single proper name like '' could represent an individual in a list or an administrative record, which is consistent with the nature of Mycenaean texts. Without further context, its specific role in a larger sentence cannot be determined, but as a complete name, it fulfills the translation requirement. Subordinate clause analysis: Not applicable for a single word.\"}\n",
      "Current Document: 5701\n"
     ]
    }
   ],
   "source": [
    "output_file_path = os.path.join(prefix_path, \"translation_answers.csv\")\n",
    "TEST_ONLY = False\n",
    "\n",
    "already_done = set()\n",
    "file_exists = os.path.isfile(output_file_path)\n",
    "random.shuffle(api_keys)\n",
    "\n",
    "# Read existing rows to populate already_done\n",
    "if file_exists:\n",
    "    with open(output_file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            already_done.add(int(row[\"idx\"]))\n",
    "\n",
    "# Prepare test index filtering\n",
    "if TEST_ONLY:\n",
    "    todo = set(translated)    \n",
    "\n",
    "switch_key_interval = 10\n",
    "prec_idx = 0\n",
    "fail_count = 0\n",
    "with open(output_file_path, mode='a', newline='', encoding='utf-8') as output_file:\n",
    "    fieldnames = [\"idx\", \"original_document\", \"greek_translation\", \"english_translation\", \"reasoning\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if file is new\n",
    "    if not file_exists or os.stat(output_file_path).st_size == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i in range(corpus.num_documents):\n",
    "        doc = corpus.get_document_no(i)\n",
    "        idx = doc.idx\n",
    "        # Skip if already processed or TEST_ONLY and the index is not for testing\n",
    "        if idx in already_done or TEST_ONLY and idx not in todo:\n",
    "            continue\n",
    "        print(f\"Current Document: {idx}\")\n",
    "\n",
    "        seq, trans, clas = document_input(doc)\n",
    "\n",
    "        # Determine which API key to use\n",
    "        if i % 300 == 0 and i != 0:\n",
    "            time.sleep(60)\n",
    "        key_idx = (i % (switch_key_interval * len(api_keys))) // switch_key_interval\n",
    "        api_key = api_keys[key_idx]\n",
    "\n",
    "        # Delay if the API key changes\n",
    "        if key_idx != prec_idx:\n",
    "            time.sleep(5)\n",
    "        prec_idx = key_idx\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                gemini_answer = make_translation_prompt(seq, trans, clas, api_key)\n",
    "                print(f\"document: {seq}\")\n",
    "                print(f\"answer: {gemini_answer}\")\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"idx\": idx,\n",
    "                    \"original_document\": seq,\n",
    "                    \"greek_translation\": gemini_answer[\"Ancient Greek translation\"],\n",
    "                    \"english_translation\": gemini_answer[\"English translation\"],\n",
    "                    \"reasoning\": gemini_answer[\"reasoning\"]\n",
    "                })\n",
    "\n",
    "                if i % 20 == 0:\n",
    "                    output_file.flush()\n",
    "                fail_count = 0\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if fail_count >= len(api_keys) * switch_key_interval:\n",
    "                    break\n",
    "                elif fail_count >= switch_key_interval:\n",
    "                    api_key = api_keys[(key_idx + 1) % len(api_keys)]\n",
    "                    prec_idx = key_idx\n",
    "                    key_idx = (key_idx + 1) % len(api_keys)\n",
    "                \n",
    "                fail_count += 1\n",
    "                print(\"DEBUG:\", seq)\n",
    "                print(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sacrebleu in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: portalocker in ./.local/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in ./.local/lib/python3.10/site-packages (from sacrebleu) (2025.9.18)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.local/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu) (0.4.4)\n",
      "Requirement already satisfied: lxml in ./.local/lib/python3.10/site-packages (from sacrebleu) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.10/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge_score in ./.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.10/site-packages (from nltk->rouge_score) (8.3.0)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.10/site-packages (from nltk->rouge_score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk->rouge_score) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jiwer in ./.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: click>=8.1.8 in ./.local/lib/python3.10/site-packages (from jiwer) (8.3.0)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in ./.local/lib/python3.10/site-packages (from jiwer) (3.13.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu\n",
    "!pip install nltk\n",
    "!pip install rouge_score\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Sequence, Dict, Optional, Tuple\n",
    "import unicodedata\n",
    "import nltk  # noqa: F401\n",
    "import nltk.corpus  # noqa: F401\n",
    "import nltk.stem  # noqa: F401\n",
    "\n",
    "# --- Libraries ---\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from jiwer import wer as jiwer_wer\n",
    "\n",
    "# ------------- Normalization helpers (useful for Ancient Greek) -------------\n",
    "class WhitespaceTokenizer:\n",
    "    def tokenize(self, text: str):\n",
    "        return text.split()\n",
    "\n",
    "def normalize_text(\n",
    "    s: str,\n",
    "    form: str = \"NFC\",           # NFC/NFD/NFKC/NFKD\n",
    "    lowercase: bool = False,\n",
    "    strip_diacritics: bool = False,\n",
    "    remove_punct: bool = True,\n",
    "    collapse_spaces: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Normalize Unicode so both hyp and refs use the same canonical form.\n",
    "    For Ancient Greek, you may want NFC and (optionally) strip_diacritics=False to keep tonos/breathings.\n",
    "    Set strip_diacritics=True if you prefer accent-insensitive matching.\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(form, s)\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    if strip_diacritics:\n",
    "        s = \"\".join(ch for ch in unicodedata.normalize(\"NFD\", s) if not unicodedata.combining(ch))\n",
    "        s = unicodedata.normalize(\"NFC\", s)\n",
    "    if remove_punct:\n",
    "        s = \"\".join(ch for ch in s if not unicodedata.category(ch).startswith(\"P\"))\n",
    "        # This removes: period . comma , Greek question mark ; ano teleia  quotes, apostrophes, etc.\n",
    "    if collapse_spaces:\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_corpus(\n",
    "    texts: Sequence[str],\n",
    "    form: str = \"NFC\",\n",
    "    lowercase: bool = False,\n",
    "    strip_diacritics: bool = False,\n",
    "    remove_punct: bool = True,\n",
    "    collapse_spaces: bool = True,\n",
    ") -> List[str]:\n",
    "    return [normalize_text(t, form=form, lowercase=lowercase, strip_diacritics=strip_diacritics, remove_punct=remove_punct, collapse_spaces=collapse_spaces) for t in texts]\n",
    "\n",
    "# ------------------------- Metric wrappers (corpus) --------------------------\n",
    "\n",
    "def compute_bleu(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    "    smooth_method: str = \"exp\",    # \"exp\" (default in sacrebleu) is robust\n",
    "    tokenize: str = \"13a\",         # sacrebleu default tokenizer (language-agnostic)\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Pass hypotheses and references as a single list of strings (single-ref).\n",
    "    \"\"\"\n",
    "    hyps = list(hypotheses)\n",
    "    if len(hyps) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    refs_by_set = [list(references)]  # type: ignore[list-item]\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, refs_by_set, smooth_method=smooth_method, tokenize=tokenize)\n",
    "    return float(bleu.score) / 100.0  # return 0..1\n",
    "\n",
    "def compute_chrf(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    "    char_order: int = 6,\n",
    "    word_order: int = 0,     # set 0 to get chrF (not chrF++)\n",
    "    beta: float = 2.0,       # chrF2 (recall-weighted)\n",
    ") -> float:\n",
    "    chrf = sacrebleu.corpus_chrf(hypotheses, [references], char_order=char_order, word_order=word_order, beta=beta)\n",
    "    return float(chrf.score) / 100.0\n",
    "\n",
    "def compute_ter(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    "    case_sensitive: bool = False,\n",
    ") -> float:\n",
    "    # sacrebleu returns a percentage; lower is better\n",
    "    ter = sacrebleu.corpus_ter(hypotheses, [references], case_sensitive=case_sensitive)\n",
    "    return float(ter.score) / 100.0\n",
    "\n",
    "def compute_rouge(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    "    use_stemmer: bool = False      # stemming is English-centric; for Ancient Greek, keep False\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns micro-averaged ROUGE-1/2/L Precision/Recall/F1 over the dataset.\n",
    "    \"\"\"\n",
    "    if not use_stemmer:\n",
    "        scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=use_stemmer, tokenizer=WhitespaceTokenizer())\n",
    "    else:\n",
    "        scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=use_stemmer)\n",
    "        \n",
    "    sums = {\"rouge1_p\": 0.0, \"rouge1_r\": 0.0, \"rouge1_f\": 0.0,\n",
    "            \"rouge2_p\": 0.0, \"rouge2_r\": 0.0, \"rouge2_f\": 0.0,\n",
    "            \"rougeL_p\": 0.0, \"rougeL_r\": 0.0, \"rougeL_f\": 0.0}\n",
    "    n = 0\n",
    "    for hyp, ref in zip(hypotheses, references):\n",
    "        scores = scorer.score(ref, hyp)  # (target, prediction)\n",
    "        sums[\"rouge1_p\"] += scores[\"rouge1\"].precision\n",
    "        sums[\"rouge1_r\"] += scores[\"rouge1\"].recall\n",
    "        sums[\"rouge1_f\"] += scores[\"rouge1\"].fmeasure\n",
    "        sums[\"rouge2_p\"] += scores[\"rouge2\"].precision\n",
    "        sums[\"rouge2_r\"] += scores[\"rouge2\"].recall\n",
    "        sums[\"rouge2_f\"] += scores[\"rouge2\"].fmeasure\n",
    "        sums[\"rougeL_p\"] += scores[\"rougeL\"].precision\n",
    "        sums[\"rougeL_r\"] += scores[\"rougeL\"].recall\n",
    "        sums[\"rougeL_f\"] += scores[\"rougeL\"].fmeasure\n",
    "        n += 1\n",
    "    if n == 0:\n",
    "        return {k: 0.0 for k in sums}\n",
    "    return {k: v / n for k, v in sums.items()}\n",
    "\n",
    "def compute_meteor(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Corpus METEOR (average of sentence scores), single reference per example.\n",
    "    For Ancient Greek, keep use_wordnet_english_synonyms=False (exact matches).\n",
    "    \"\"\"\n",
    "    if not hypotheses:\n",
    "        return 0.0\n",
    "    if len(hypotheses) != len(references):\n",
    "        raise ValueError(\"hypotheses and references must have the same length\")\n",
    "\n",
    "    # Optional: enable English WordNet (NOT recommended for Ancient Greek)\n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "\n",
    "    scores: List[float] = []\n",
    "    for hyp, ref in zip(hypotheses, references):\n",
    "        hyp_tokens = hyp.split()\n",
    "        ref_tokens = ref.split()\n",
    "        # meteor_score expects: (list_of_reference_token_lists, hypothesis_token_list)\n",
    "        s = meteor_score([ref_tokens], hyp_tokens)\n",
    "        scores.append(float(s))\n",
    "\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def compute_wer(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Word Error Rate using jiwer. Returns a ratio in [0, 1+] (lower is better).\n",
    "    \"\"\"\n",
    "    return float(jiwer_wer(references, hypotheses))\n",
    "\n",
    "# ------------------------------- Convenience -------------------------------\n",
    "\n",
    "def evaluate_all(\n",
    "    hypotheses: Sequence[str],\n",
    "    references: Sequence[str],\n",
    "    normalize: bool = True,\n",
    "    unicode_form: str = \"NFC\",\n",
    "    lowercase: bool = False,\n",
    "    strip_diacritics: bool = True,\n",
    "    remove_punct: bool = True,\n",
    "    collapse_spaces: bool = True,\n",
    "    rouge_stemmer: bool = False,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Runs all metrics and returns a flat dict. If you pass multi-refs, BLEU & METEOR will use them;\n",
    "    ROUGE/TER/chrF/WER use the first reference per example.\n",
    "    \"\"\"\n",
    "    hyps = list(hypotheses)\n",
    "    refs = list(references)\n",
    "    if normalize:\n",
    "        hyps = normalize_corpus(hyps, unicode_form, lowercase, strip_diacritics, remove_punct=remove_punct, collapse_spaces=collapse_spaces)\n",
    "        refs = normalize_corpus(refs, unicode_form, lowercase, strip_diacritics, remove_punct=remove_punct, collapse_spaces=collapse_spaces)\n",
    "\n",
    "    out: Dict[str, float] = {}\n",
    "    out[\"BLEU\"] = compute_bleu(hyps, refs)\n",
    "    out[\"METEOR\"] = compute_meteor(hyps, refs)\n",
    "    rouge = compute_rouge(hyps, refs, use_stemmer=rouge_stemmer)\n",
    "    out[\"ROUGE1_P\"] = rouge[\"rouge1_p\"]; out[\"ROUGE1_R\"] = rouge[\"rouge1_r\"]; out[\"ROUGE1_F1\"] = rouge[\"rouge1_f\"]\n",
    "    out[\"ROUGE2_P\"] = rouge[\"rouge2_p\"]; out[\"ROUGE2_R\"] = rouge[\"rouge2_r\"]; out[\"ROUGE2_F1\"] = rouge[\"rouge2_f\"]\n",
    "    out[\"ROUGEL_P\"] = rouge[\"rougeL_p\"]; out[\"ROUGEL_R\"] = rouge[\"rougeL_r\"]; out[\"ROUGEL_F1\"] = rouge[\"rougeL_f\"]\n",
    "    case_sensitive = not lowercase\n",
    "    out[\"TER\"] = compute_ter(hyps, refs, case_sensitive=case_sensitive)\n",
    "    out[\"WER\"] = compute_wer(hyps, refs)\n",
    "    out[\"CHRF\"] = compute_chrf(hyps, refs)\n",
    "\n",
    "    per_doc: Dict[str, List[float]] = defaultdict(list)\n",
    "    for i in range(len(hyps)):\n",
    "        hyp, ref = [hyps[i]], [refs[i]]\n",
    "        per_doc[\"BLEU\"].append(compute_bleu(hyp, ref))\n",
    "        per_doc[\"METEOR\"].append(compute_meteor(hyp, ref))\n",
    "        rouge = compute_rouge(hyp, ref, use_stemmer=rouge_stemmer)\n",
    "        per_doc[\"ROUGE1_P\"].append(rouge[\"rouge1_p\"]); per_doc[\"ROUGE1_R\"].append(rouge[\"rouge1_r\"]); per_doc[\"ROUGE1_F1\"].append(rouge[\"rouge1_f\"])\n",
    "        per_doc[\"ROUGE2_P\"].append(rouge[\"rouge2_p\"]); per_doc[\"ROUGE2_R\"].append(rouge[\"rouge2_r\"]); per_doc[\"ROUGE2_F1\"].append(rouge[\"rouge2_f\"])\n",
    "        per_doc[\"ROUGEL_P\"].append(rouge[\"rougeL_p\"]); per_doc[\"ROUGEL_R\"].append(rouge[\"rougeL_r\"]); per_doc[\"ROUGEL_F1\"].append(rouge[\"rougeL_f\"])\n",
    "        case_sensitive = not lowercase\n",
    "        per_doc[\"TER\"].append(compute_ter(hyp, ref, case_sensitive=case_sensitive))\n",
    "        per_doc[\"WER\"].append(compute_wer(hyp, ref))\n",
    "        per_doc[\"CHRF\"].append(compute_chrf(hyp, ref))\n",
    "    return out, per_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'BLEU': 0.5830401928168866,\n",
       "  'METEOR': 0.6577408976773383,\n",
       "  'ROUGE1_P': 0.7,\n",
       "  'ROUGE1_R': 0.6666666666666666,\n",
       "  'ROUGE1_F1': 0.6818181818181818,\n",
       "  'ROUGE2_P': 0.625,\n",
       "  'ROUGE2_R': 0.6,\n",
       "  'ROUGE2_F1': 0.6111111111111112,\n",
       "  'ROUGEL_P': 0.7,\n",
       "  'ROUGEL_R': 0.6666666666666666,\n",
       "  'ROUGEL_F1': 0.6818181818181818,\n",
       "  'TER': 0.33333333333333326,\n",
       "  'WER': 0.3333333333333333,\n",
       "  'CHRF': 0.6975806861817233},\n",
       " defaultdict(list,\n",
       "             {'BLEU': [1.0000000000000004, 0.17491650626361255],\n",
       "              'METEOR': [0.9976851851851852, 0.3177966101694915],\n",
       "              'ROUGE1_P': [1.0, 0.4],\n",
       "              'ROUGE1_R': [1.0, 0.3333333333333333],\n",
       "              'ROUGE1_F1': [1.0, 0.3636363636363636],\n",
       "              'ROUGE2_P': [1.0, 0.25],\n",
       "              'ROUGE2_R': [1.0, 0.2],\n",
       "              'ROUGE2_F1': [1.0, 0.22222222222222224],\n",
       "              'ROUGEL_P': [1.0, 0.4],\n",
       "              'ROUGEL_R': [1.0, 0.3333333333333333],\n",
       "              'ROUGEL_F1': [1.0, 0.3636363636363636],\n",
       "              'TER': [0.0, 0.6666666666666665],\n",
       "              'WER': [0.0, 0.6666666666666666],\n",
       "              'CHRF': [1.0, 0.45366637602441784]}))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyps = [\n",
    "        \"     \",             # Ancient Greek example (with diacritics)\n",
    "        \"    \"             # Modern Greek (no diacritics, just for demo)\n",
    "    ]\n",
    "refs = [\n",
    "        \"     \",\n",
    "        \"     \"\n",
    "    ]\n",
    "\n",
    "evaluate_all(hyps, refs, lowercase=True, strip_diacritics=True, collapse_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18181818181818182"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ter(hyps, refs, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sequences from thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 documents in order:\n",
    "# 1) PY Ta 711\n",
    "# 2) KN Ra 1540\n",
    "# 3) PY Jn 310\n",
    "# 4) PY Jn 829\n",
    "# 5) KN So 4439\n",
    "# 6) KN Sd 4404\n",
    "# 7) PY An 657\n",
    "# 8) PY TA 641\n",
    "# 9) PY Er 312\n",
    "#10) KN Fp 1+31\n",
    "#11) PY Ab 573\n",
    "#12) PY Un 718\n",
    "\n",
    "greek_hyp = [\n",
    "    # 1\n",
    "    \"-      .      1.   .     1.   1.\",\n",
    "    # 2\n",
    "    \"   50.\",\n",
    "    # 3\n",
    "    \" [...]   :   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2.    [...]:  1,  1, [...] 1, [...] 1,  1.  :  1,  1,  1,  1.  [...]    :  [...]  M 3,   M 3,  M 3,   M 3. :  1.\",\n",
    "    # 4\n",
    "    \"              ,     . :   M 2,   N 3. :   M 2,   N 3.\",\n",
    "    # 5\n",
    "    \"  :   3,   1.\",\n",
    "    # 6\n",
    "    \"[...], --      . [...]    h   1.\",\n",
    "    # 7\n",
    "    \" , .   . , , , , , . :  50.  . , , , .   :  20.   :  10.     , , .   :  30.   :  20.     .\",\n",
    "    # 8\n",
    "    \" *201VAS [...]     *201VAS 2.     *201VAS 1.     .  *203VAS 3.   es *202VAS 1.    *202VAS 2.   es *202VAS 1.   es *202VAS 1.    *202VAS 1.\",\n",
    "    # 9\n",
    "    \" :    30.  :  10.   :  30.  :  3.  :    6.\",\n",
    "    #10\n",
    "    \" :    S 1.   S 2.   S 1.   1.   S 1 [...] :  S 1 [...]   V 3.   V 1.   V 4.   3 S 2 V 2.\",\n",
    "    #11\n",
    "    \":  5 T 1 DA TA.   16,  3,  7.  5 1.\",\n",
    "    #12\n",
    "    \":  .  .   :  4,  3,  1,   10,  *153 1,  V 3. h :  2,  2,  2,  5. :  V 2, *153 1.   :  2,   T 6,  S 2. h  :  [...] T 6,  S 1,  5,  [...] 1 [...] V 1.\"\n",
    "]\n",
    "\n",
    "greek_ref = [\n",
    "    # 1\n",
    "    \"      .      1.   .     1.   1.\",\n",
    "    # 2 (no change given)\n",
    "    \"   50.\",\n",
    "    # 3\n",
    "    \" [...]   :   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2,   M 1 N 2.    [...]:  1,  1, [...] 1, [...] 1,  1.  :  1,  1,  1,  1.  [...]    :  [...]  M 3,   M 3,  M 3,   M 3. :  1.\",\n",
    "    # 4\n",
    "    \"              ,     . :   M 2,   N 3. :   M 2,   N 3.\",\n",
    "    # 5\n",
    "    \"  :   3,   1.\",\n",
    "    # 6\n",
    "    \"[...],       . [...]    h   1.\",\n",
    "    # 7\n",
    "    \" , .   . , , , , .  :  50.  . , , , .   :  20.   :  10.     . ,    :  30.   :  20.     .\",\n",
    "    # 8\n",
    "    \"    *201VAS 2.     *201VAS 1.       *201VAS.  *203VAS 3.    *202VAS 1.    *202VAS 2.    *202VAS 1.    *202VAS 1.    *202VAS 1.\",\n",
    "    # 9\n",
    "    \" :    30.  :  10.   :  30.  :  3.  :    6.\",\n",
    "    #10\n",
    "    \" :    S 1.   S 2.   S 1.    1.   S 1 [...] :   [...]   V 3. *47-  V 1.   V 4.   3 S 2 V 2.\",\n",
    "    #11\n",
    "    \":  5 T 1 DA TA.   16,  3,  7.  5 1.\",\n",
    "    #12\n",
    "    \"  .  .   :  4,  3,  1,   10,  *153 1,  V 3. h :  2,  2,  2,  5. :  V 2, *153 1.   :  2,   T 6,  S 2. h  :  [...] T 6,  S 1,  5,  [...] 1 [...] V 1.\"\n",
    "]\n",
    "\n",
    "english_hyp = [\n",
    "    # 1\n",
    "    \"Phygekles witnessed when the king appointed Augeus as damoklos. A royal, ox-head-shaped, spiral-decorated krater (vessel): 1. A krater with fittings, with a handle. A royal, female, ox-head-shaped krater: 1. A spiral-designed vessel: 1.\",\n",
    "    # 2\n",
    "    \"So many swords: 50 SWORDS.\",\n",
    "    # 3\n",
    "    \"Agrewa: [...] bronze-smiths having allotment: Tithaios: BRONZE M 1 N 2, Ketawo: BRONZE M 1 N 2, Aisonios: BRONZE M 1 N 2, Tamieus: BRONZE M 1 N 2, Eurywotas: BRONZE M 1 N 2, Eudonos: BRONZE M 1 N 2, Plouteus: BRONZE M 1 N 2, Widuwako: BRONZE M 1 N 2. Total bronze-smiths without allotment [...]: Pamphisios 1, Kewetos 1, [...] 1, [...] 1, Petalos 1. Total slaves: Of Kewetos 1, Of Iwaka 1, Of Pamphisios 1, Of Plouteus 1. A [...] Potnia-related bronze-smiths having allotment: Himadios [...] BRONZE M 3, Tucheneus: BRONZE M 3, BRONZE M 3, Iwaka: BRONZE M 3. Without allotment: Physiarchos 1.\",\n",
    "    # 4\n",
    "    \"Thus will give the mayors and magistrates and deputy mayors and key-bearers and overseers of figs and overseers of digging, temple bronze, and spearheads for the javelin-men and for the spears. Pisa: mayor, bronze M 2, deputy mayor, bronze N 3. Metapa: mayor, bronze M 2, deputy mayor, bronze N 3.\",\n",
    "    # 5\n",
    "    \"Chariot fittings, willow (wood), rimmed: 3 pairs of wheels, 1 single wheel.\",\n",
    "    # 6\n",
    "    \"[...], and horse-equipped (parts), leathern eye-guards, with horn-like parts, with reins, a chariot. [...] A Cydonian, red-painted, fitted, red-dyed chariot: 1 chariot.\",\n",
    "    # 7\n",
    "    \"Guards at Opihara, auxiliaries. Maleus' company at Owitnos. Aperitawos, Orelas, Etewas, Kokkios, Suwerowios, Owitinios. Orcharai: 50 men. Nedawatas' company. Echemedes, Amphigetas, Maltheus, Tanikos. At Harywos, Kekkides Kyparissioi: 20 men. At Aithaleus, Kyparissioi Kekkides: 10 men. And with them, the follower Kerkios, Aelipotas, Elaphos. At Limen, Orchara at Owitnos: 30 men. And Kekkides Aphuganes: 20 men. And with them, Aikotas, the follower.\",\n",
    "    # 8\n",
    "    \"Tripod with legs [...] Two tripods, Aigeus, Cretan-made. One tripod, with one foot, one-handled. One tripod, Cretan-made, burnt off. Three jars. One larger, four-handled cup. Two larger, three-handled cups. One smaller, four-handled cup. One smaller, three-handled cup. One smaller, handleless cup.\",\n",
    "    # 9\n",
    "    \"Royal temenos: of so much grain, GRA 30. Temenos of the lawagetas: GRA 10. Of the telestai, so much grain: GRA 30. And so many telestai: 3 men. Fallow land of Worgion: of so much grain, GRA 6.\",\n",
    "    #10\n",
    "    \"In the month of Deukios: To Dictaean Zeus: 1 S unit of oil. To the Daedaleion: 2 S units of oil. To Pandes: 1 S unit of oil. To all gods: 1 unit of oil. To Therasia: 1 S unit of oil. [...] Amnisos: To all gods: 1 S unit (of oil). [...] To Erinys: 3 V units of oil. To Oudade: 1 V unit of oil. To the priestess of the wind: 4 V units (of oil). Total oil: 3 units, 2 S units, 2 V units.\",\n",
    "    #11\n",
    "    \"Pylos: Grain 5 T 1. DA TA. Milatian women: 16 women, 3 girls, 7 boys. Figs: 5 1.\",\n",
    "    #12\n",
    "    \"Sarapeda: Offering to Poseidon. Offering to the overseers. Echelawa will give this much: Grain 4, Wine 3, Bull 1, Cheese 10, Boy *153 1, Honey V 3. Also, the community: Grain 2, Wine 2, Ram 2, Cheese 5. Aleiros: Ointment V 2, *153 1. This much the Lawagetas will give: Ram 2, Flour T 6, Wine S 2. Also, Wrogioneios' plot: Grain [...] T 6, Wine S 1, Cheese 5, Honey [...] 1 [...] V 1.\"\n",
    "]\n",
    "\n",
    "english_ref = [\n",
    "    # 1 (no change)\n",
    "    \"Phygekles witnessed when the king appointed Augeus as damoklos. A royal, ox-head-shaped, spiral-decorated krater (vessel): 1. A krater with fittings, with a handle. A royal, female, ox-head-shaped krater: 1. A spiral-designed vessel: 1.\",\n",
    "    # 2 (no change)\n",
    "    \"So many swords: 50 SWORDS.\",\n",
    "    # 3 (your improved)\n",
    "    \"Agrewa: [...] bronze-smiths having allotment: Tithaios: BRONZE M 1 N 2, Ketawo: BRONZE M 1 N 2, Aisonios: BRONZE M 1 N 2, Tamieus: BRONZE M 1 N 2, Eurywotas: BRONZE M 1 N 2, Eudonos: BRONZE M 1 N 2, Plouteus: BRONZE M 1 N 2, Widuwako: BRONZE M 1 N 2. Total bronze-smiths without allotment [...]: Pamphisios 1, Kewetos 1, [...] 1, [...] 1, Petalos 1. Total slaves: Of Kewetos 1, Of Iwakas 1, Of Pamphisios 1, Of Plouteus 1. A [...] At the place of Potnia: bronze-smiths having allotment: Himadios [...] BRONZE M 3, Tucheneus: BRONZE M 3, BRONZE M 3, Iwakas: BRONZE M 3. Without allotment: Physiarchos 1.\",\n",
    "    # 4 (your improved)\n",
    "    \"Thus will give the mayors and magistrates and deputy mayors and key-bearers and overseers of figs and overseers of digging, bronze for the temple, and spearheads for the javelin-men and for the spears. Piswa: mayor, bronze M 2, deputy mayor, bronze N 3. Metapa: mayor, bronze M 2, deputy mayor, bronze N 3.\",\n",
    "    # 5 (no explicit corrected English given; left as-is)\n",
    "    \"Chariot fittings, willow (wood), rimmed: 3 pairs of wheels, 1 single wheel.\",\n",
    "    # 6 (your improved with marked adjustments; kept your wording)\n",
    "    \"[...], (equipped with) two horses, with leathern eye-guards, with horn-like parts, with reins, a chariot. [...] A Cydonian, purple red-painted, assembled, chariot: 1 chariot.\",\n",
    "    # 7 (your improved)\n",
    "    \"Guards at the coast, auxiliaries. Maleus' company at Owitnos: Aperitawos, Orelas, Etewas, Kolchios, Suwerowios. (People) from Owitnos and Oichala: 50 men. Nedawatas' company. Echemedes, Amphigetas, Maltheus, Tanikos. At Harywos, (people) from Kelchis and Kyparisos: 20 men. At Aithaleus, (people) from Kelchis and Kyparisos: 10 men. And with them, the attendant Kerkios. Aelipotas, at the deers' port in Oichala at Owitnos: 30 men. (People) from Kelchis and Aphy: 20 men. And with them, Aikotas, the attendant.\",\n",
    "    # 8 (your refined)\n",
    "    \"Two tripods, Aigeus, Cretan-made. One tripod, with one foot, one-handled. One tripod, Cretan-made, burnt off at the legs. Three jars. One larger, four-handled cup. Two larger, three-handled cups. One smaller, four-handled cup. One smaller, three-handled cup. One smaller, handleless cup.\",\n",
    "    # 9 (your refined)\n",
    "    \"Royal land holding: of so much grain, 30 units of grain. Land holding of the lawagetas: 10 units of grain. Of the telestai, so much grain: 30 units of grain. And so many telestai: 3 men. Remote land of the Worgiones: of so much grain, 6 units of grain.\",\n",
    "    #10 (your improved)\n",
    "    \"In the month of Deukios: To Dictaean Zeus: 1 S unit of oil. To the city of Daidalos: 2 S units of oil. To Pandes: 1 S unit of oil. To all gods: 1 unit of oil. To Therasia: 1 S unit of oil. [...] Amnisos: To all gods: 1 S unit (of oil). [...] To Erinys: 3 V units of oil. To ?-dade: 1 V unit of oil. To the priestess of the winds: 4 V units (of oil). Total oil: 3 units, 2 S units, 2 V units.\",\n",
    "    #11 (your refined)\n",
    "    \"Pylos: Grain 5 T 1. DA TA. Miletian women: 16 women, 3 girls, 7 boys. Figs: 5 1.\",\n",
    "    #12 (your refined)\n",
    "    \"Offering to Srapedas Poseidon. Offering to the overseers. Echelawon will give this much: Grain 4, Wine 3, Bull 1, Cheese 10, Boy *153 1, Honey V 3. Also, the community: Grain 2, Wine 2, Ram 2, Cheese 5. Aleiros: Ointment V 2, *153 1. This much the Lawagetas will give: Ram 2, Flour T 6, Wine S 2. Also, the plot of the Worgiones: Grain [...] T 6, Wine S 1, Cheese 5, Honey [...] 1 [...] V 1.\"\n",
    "]\n",
    "#thx gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /mnt/beegfs/home/amaiola/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "greek = [greek_hyp, greek_ref, \"Greek\"]\n",
    "english = [english_hyp, english_ref, \"English\"]\n",
    "avg = []\n",
    "per_doc = []\n",
    "for hyp, ref, lang in [greek, english]:\n",
    "    \n",
    "    strip_diacritics = lang == \"Greek\"\n",
    "    use_stemmer = lang == \"English\"\n",
    "    res = evaluate_all(hyp, ref, strip_diacritics=strip_diacritics, rouge_stemmer=use_stemmer, lowercase=False)\n",
    "    avg.append(res[0])\n",
    "    per_doc.append(res[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greek results:\n",
      "{'BLEU': 0.6407504239116995, 'METEOR': 0.8128516452296308, 'ROUGE1_P': 0.818415294542122, 'ROUGE1_R': 0.818415294542122, 'ROUGE1_F1': 0.818415294542122, 'ROUGE2_P': 0.6982833846392199, 'ROUGE2_R': 0.6982833846392199, 'ROUGE2_F1': 0.6982833846392199, 'ROUGEL_P': 0.8148691952513419, 'ROUGEL_R': 0.8148691952513419, 'ROUGEL_F1': 0.8148691952513419, 'TER': 0.1872146118721461, 'WER': 0.18949771689497716, 'CHRF': 0.8359568366679149}\n",
      "English results:\n",
      "{'BLEU': 0.7683766015991749, 'METEOR': 0.8800937915912642, 'ROUGE1_P': 0.9267140674761718, 'ROUGE1_R': 0.8828394601699325, 'ROUGE1_F1': 0.9029709408028528, 'ROUGE2_P': 0.8559298861587509, 'ROUGE2_R': 0.8205666631123022, 'ROUGE2_F1': 0.8368747774950847, 'ROUGEL_P': 0.9229967472147339, 'ROUGEL_R': 0.8791837997925741, 'ROUGEL_F1': 0.8992850433669553, 'TER': 0.15771230502599654, 'WER': 0.1611785095320624, 'CHRF': 0.8478466538103436}\n",
      "Greek results:\n",
      "Document PY Ta 711 Metric BLEU: 0.3942302221292177\n",
      "Document PY Ta 711 Metric METEOR: 0.656494140625\n",
      "Document PY Ta 711 Metric ROUGE1_P: 0.6666666666666666\n",
      "Document PY Ta 711 Metric ROUGE1_R: 0.6666666666666666\n",
      "Document PY Ta 711 Metric ROUGE1_F1: 0.6666666666666666\n",
      "Document PY Ta 711 Metric ROUGE2_P: 0.4782608695652174\n",
      "Document PY Ta 711 Metric ROUGE2_R: 0.4782608695652174\n",
      "Document PY Ta 711 Metric ROUGE2_F1: 0.4782608695652174\n",
      "Document PY Ta 711 Metric ROUGEL_P: 0.6666666666666666\n",
      "Document PY Ta 711 Metric ROUGEL_R: 0.6666666666666666\n",
      "Document PY Ta 711 Metric ROUGEL_F1: 0.6666666666666666\n",
      "Document PY Ta 711 Metric TER: 0.33333333333333326\n",
      "Document PY Ta 711 Metric WER: 0.3333333333333333\n",
      "Document PY Ta 711 Metric CHRF: 0.7935076339478979\n",
      "Document KN Ra 1540 Metric BLEU: 1.0000000000000004\n",
      "Document KN Ra 1540 Metric METEOR: 0.9921875\n",
      "Document KN Ra 1540 Metric ROUGE1_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE1_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE1_F1: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_F1: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_F1: 1.0\n",
      "Document KN Ra 1540 Metric TER: 0.0\n",
      "Document KN Ra 1540 Metric WER: 0.0\n",
      "Document KN Ra 1540 Metric CHRF: 1.0\n",
      "Document PY Jn 310 Metric BLEU: 0.8164449606363189\n",
      "Document PY Jn 310 Metric METEOR: 0.9163223140495868\n",
      "Document PY Jn 310 Metric ROUGE1_P: 0.9166666666666666\n",
      "Document PY Jn 310 Metric ROUGE1_R: 0.9166666666666666\n",
      "Document PY Jn 310 Metric ROUGE1_F1: 0.9166666666666666\n",
      "Document PY Jn 310 Metric ROUGE2_P: 0.8421052631578947\n",
      "Document PY Jn 310 Metric ROUGE2_R: 0.8421052631578947\n",
      "Document PY Jn 310 Metric ROUGE2_F1: 0.8421052631578947\n",
      "Document PY Jn 310 Metric ROUGEL_P: 0.9166666666666666\n",
      "Document PY Jn 310 Metric ROUGEL_R: 0.9166666666666666\n",
      "Document PY Jn 310 Metric ROUGEL_F1: 0.9166666666666666\n",
      "Document PY Jn 310 Metric TER: 0.08333333333333331\n",
      "Document PY Jn 310 Metric WER: 0.08333333333333333\n",
      "Document PY Jn 310 Metric CHRF: 0.906307952367095\n",
      "Document PY Jn 829 Metric BLEU: 0.6935374502793354\n",
      "Document PY Jn 829 Metric METEOR: 0.8658112222705525\n",
      "Document PY Jn 829 Metric ROUGE1_P: 0.868421052631579\n",
      "Document PY Jn 829 Metric ROUGE1_R: 0.868421052631579\n",
      "Document PY Jn 829 Metric ROUGE1_F1: 0.868421052631579\n",
      "Document PY Jn 829 Metric ROUGE2_P: 0.7297297297297297\n",
      "Document PY Jn 829 Metric ROUGE2_R: 0.7297297297297297\n",
      "Document PY Jn 829 Metric ROUGE2_F1: 0.7297297297297297\n",
      "Document PY Jn 829 Metric ROUGEL_P: 0.868421052631579\n",
      "Document PY Jn 829 Metric ROUGEL_R: 0.868421052631579\n",
      "Document PY Jn 829 Metric ROUGEL_F1: 0.868421052631579\n",
      "Document PY Jn 829 Metric TER: 0.13157894736842105\n",
      "Document PY Jn 829 Metric WER: 0.13157894736842105\n",
      "Document PY Jn 829 Metric CHRF: 0.8752690474639461\n",
      "Document KN So 4439 Metric BLEU: 0.5873949094699212\n",
      "Document KN So 4439 Metric METEOR: 0.6651234567901234\n",
      "Document KN So 4439 Metric ROUGE1_P: 0.6666666666666666\n",
      "Document KN So 4439 Metric ROUGE1_R: 0.6666666666666666\n",
      "Document KN So 4439 Metric ROUGE1_F1: 0.6666666666666666\n",
      "Document KN So 4439 Metric ROUGE2_P: 0.625\n",
      "Document KN So 4439 Metric ROUGE2_R: 0.625\n",
      "Document KN So 4439 Metric ROUGE2_F1: 0.625\n",
      "Document KN So 4439 Metric ROUGEL_P: 0.6666666666666666\n",
      "Document KN So 4439 Metric ROUGEL_R: 0.6666666666666666\n",
      "Document KN So 4439 Metric ROUGEL_F1: 0.6666666666666666\n",
      "Document KN So 4439 Metric TER: 0.33333333333333326\n",
      "Document KN So 4439 Metric WER: 0.3333333333333333\n",
      "Document KN So 4439 Metric CHRF: 0.7800446534300078\n",
      "Document KN Sd 4404 Metric BLEU: 0.7744031410142029\n",
      "Document KN Sd 4404 Metric METEOR: 0.8551587301587302\n",
      "Document KN Sd 4404 Metric ROUGE1_P: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric ROUGE1_R: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric ROUGE1_F1: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric ROUGE2_P: 0.7692307692307693\n",
      "Document KN Sd 4404 Metric ROUGE2_R: 0.7692307692307693\n",
      "Document KN Sd 4404 Metric ROUGE2_F1: 0.7692307692307693\n",
      "Document KN Sd 4404 Metric ROUGEL_P: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric ROUGEL_R: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric ROUGEL_F1: 0.8571428571428571\n",
      "Document KN Sd 4404 Metric TER: 0.14285714285714285\n",
      "Document KN Sd 4404 Metric WER: 0.14285714285714285\n",
      "Document KN Sd 4404 Metric CHRF: 0.9309263829391666\n",
      "Document PY An 657 Metric BLEU: 0.282688644666681\n",
      "Document PY An 657 Metric METEOR: 0.6274074557681008\n",
      "Document PY An 657 Metric ROUGE1_P: 0.6415094339622641\n",
      "Document PY An 657 Metric ROUGE1_R: 0.6415094339622641\n",
      "Document PY An 657 Metric ROUGE1_F1: 0.6415094339622641\n",
      "Document PY An 657 Metric ROUGE2_P: 0.4230769230769231\n",
      "Document PY An 657 Metric ROUGE2_R: 0.4230769230769231\n",
      "Document PY An 657 Metric ROUGE2_F1: 0.4230769230769231\n",
      "Document PY An 657 Metric ROUGEL_P: 0.6415094339622641\n",
      "Document PY An 657 Metric ROUGEL_R: 0.6415094339622641\n",
      "Document PY An 657 Metric ROUGEL_F1: 0.6415094339622641\n",
      "Document PY An 657 Metric TER: 0.3584905660377358\n",
      "Document PY An 657 Metric WER: 0.3584905660377358\n",
      "Document PY An 657 Metric CHRF: 0.7450840754588287\n",
      "Document PY TA 641 Metric BLEU: 0.3034564942914212\n",
      "Document PY TA 641 Metric METEOR: 0.6857100787749393\n",
      "Document PY TA 641 Metric ROUGE1_P: 0.7021276595744681\n",
      "Document PY TA 641 Metric ROUGE1_R: 0.7021276595744681\n",
      "Document PY TA 641 Metric ROUGE1_F1: 0.7021276595744681\n",
      "Document PY TA 641 Metric ROUGE2_P: 0.43478260869565216\n",
      "Document PY TA 641 Metric ROUGE2_R: 0.43478260869565216\n",
      "Document PY TA 641 Metric ROUGE2_F1: 0.43478260869565216\n",
      "Document PY TA 641 Metric ROUGEL_P: 0.6595744680851063\n",
      "Document PY TA 641 Metric ROUGEL_R: 0.6595744680851063\n",
      "Document PY TA 641 Metric ROUGEL_F1: 0.6595744680851063\n",
      "Document PY TA 641 Metric TER: 0.3617021276595745\n",
      "Document PY TA 641 Metric WER: 0.3829787234042553\n",
      "Document PY TA 641 Metric CHRF: 0.7197645991581938\n",
      "Document PY Er 312 Metric BLEU: 0.426864158766622\n",
      "Document PY Er 312 Metric METEOR: 0.7892000000000002\n",
      "Document PY Er 312 Metric ROUGE1_P: 0.8\n",
      "Document PY Er 312 Metric ROUGE1_R: 0.8\n",
      "Document PY Er 312 Metric ROUGE1_F1: 0.8000000000000002\n",
      "Document PY Er 312 Metric ROUGE2_P: 0.5833333333333334\n",
      "Document PY Er 312 Metric ROUGE2_R: 0.5833333333333334\n",
      "Document PY Er 312 Metric ROUGE2_F1: 0.5833333333333334\n",
      "Document PY Er 312 Metric ROUGEL_P: 0.8\n",
      "Document PY Er 312 Metric ROUGEL_R: 0.8\n",
      "Document PY Er 312 Metric ROUGEL_F1: 0.8000000000000002\n",
      "Document PY Er 312 Metric TER: 0.2\n",
      "Document PY Er 312 Metric WER: 0.2\n",
      "Document PY Er 312 Metric CHRF: 0.7752167698667634\n",
      "Document KN Fp 1+31 Metric BLEU: 0.7127585847997483\n",
      "Document KN Fp 1+31 Metric METEOR: 0.8367448316166265\n",
      "Document KN Fp 1+31 Metric ROUGE1_P: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric ROUGE1_R: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric ROUGE1_F1: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric ROUGE2_P: 0.7727272727272727\n",
      "Document KN Fp 1+31 Metric ROUGE2_R: 0.7727272727272727\n",
      "Document KN Fp 1+31 Metric ROUGE2_F1: 0.7727272727272727\n",
      "Document KN Fp 1+31 Metric ROUGEL_P: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric ROUGEL_R: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric ROUGEL_F1: 0.8666666666666667\n",
      "Document KN Fp 1+31 Metric TER: 0.15555555555555556\n",
      "Document KN Fp 1+31 Metric WER: 0.15555555555555556\n",
      "Document KN Fp 1+31 Metric CHRF: 0.9281313584279041\n",
      "Document PY Ab 573 Metric BLEU: 0.8282477531331043\n",
      "Document PY Ab 573 Metric METEOR: 0.9402573529411764\n",
      "Document PY Ab 573 Metric ROUGE1_P: 0.9411764705882353\n",
      "Document PY Ab 573 Metric ROUGE1_R: 0.9411764705882353\n",
      "Document PY Ab 573 Metric ROUGE1_F1: 0.9411764705882353\n",
      "Document PY Ab 573 Metric ROUGE2_P: 0.875\n",
      "Document PY Ab 573 Metric ROUGE2_R: 0.875\n",
      "Document PY Ab 573 Metric ROUGE2_F1: 0.875\n",
      "Document PY Ab 573 Metric ROUGEL_P: 0.9411764705882353\n",
      "Document PY Ab 573 Metric ROUGEL_R: 0.9411764705882353\n",
      "Document PY Ab 573 Metric ROUGEL_F1: 0.9411764705882353\n",
      "Document PY Ab 573 Metric TER: 0.0588235294117647\n",
      "Document PY Ab 573 Metric WER: 0.058823529411764705\n",
      "Document PY Ab 573 Metric CHRF: 0.9077896575979003\n",
      "Document PY Un 718 Metric BLEU: 0.8314939187541919\n",
      "Document PY Un 718 Metric METEOR: 0.9238026597607355\n",
      "Document PY Un 718 Metric ROUGE1_P: 0.8939393939393939\n",
      "Document PY Un 718 Metric ROUGE1_R: 0.8939393939393939\n",
      "Document PY Un 718 Metric ROUGE1_F1: 0.8939393939393939\n",
      "Document PY Un 718 Metric ROUGE2_P: 0.8461538461538461\n",
      "Document PY Un 718 Metric ROUGE2_R: 0.8461538461538461\n",
      "Document PY Un 718 Metric ROUGE2_F1: 0.8461538461538461\n",
      "Document PY Un 718 Metric ROUGEL_P: 0.8939393939393939\n",
      "Document PY Un 718 Metric ROUGEL_R: 0.8939393939393939\n",
      "Document PY Un 718 Metric ROUGEL_F1: 0.8939393939393939\n",
      "Document PY Un 718 Metric TER: 0.10606060606060605\n",
      "Document PY Un 718 Metric WER: 0.10606060606060606\n",
      "Document PY Un 718 Metric CHRF: 0.8784313281861613\n",
      "English results:\n",
      "Document PY Ta 711 Metric BLEU: 1.0000000000000004\n",
      "Document PY Ta 711 Metric METEOR: 0.9999860867629463\n",
      "Document PY Ta 711 Metric ROUGE1_P: 1.0\n",
      "Document PY Ta 711 Metric ROUGE1_R: 1.0\n",
      "Document PY Ta 711 Metric ROUGE1_F1: 1.0\n",
      "Document PY Ta 711 Metric ROUGE2_P: 1.0\n",
      "Document PY Ta 711 Metric ROUGE2_R: 1.0\n",
      "Document PY Ta 711 Metric ROUGE2_F1: 1.0\n",
      "Document PY Ta 711 Metric ROUGEL_P: 1.0\n",
      "Document PY Ta 711 Metric ROUGEL_R: 1.0\n",
      "Document PY Ta 711 Metric ROUGEL_F1: 1.0\n",
      "Document PY Ta 711 Metric TER: 0.0\n",
      "Document PY Ta 711 Metric WER: 0.0\n",
      "Document PY Ta 711 Metric CHRF: 1.0\n",
      "Document KN Ra 1540 Metric BLEU: 1.0000000000000004\n",
      "Document KN Ra 1540 Metric METEOR: 0.996\n",
      "Document KN Ra 1540 Metric ROUGE1_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE1_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE1_F1: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGE2_F1: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_P: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_R: 1.0\n",
      "Document KN Ra 1540 Metric ROUGEL_F1: 1.0\n",
      "Document KN Ra 1540 Metric TER: 0.0\n",
      "Document KN Ra 1540 Metric WER: 0.0\n",
      "Document KN Ra 1540 Metric CHRF: 1.0\n",
      "Document PY Jn 310 Metric BLEU: 0.8888112029917962\n",
      "Document PY Jn 310 Metric METEOR: 0.9559752386129703\n",
      "Document PY Jn 310 Metric ROUGE1_P: 0.9901960784313726\n",
      "Document PY Jn 310 Metric ROUGE1_R: 0.9528301886792453\n",
      "Document PY Jn 310 Metric ROUGE1_F1: 0.9711538461538463\n",
      "Document PY Jn 310 Metric ROUGE2_P: 0.9801980198019802\n",
      "Document PY Jn 310 Metric ROUGE2_R: 0.9428571428571428\n",
      "Document PY Jn 310 Metric ROUGE2_F1: 0.9611650485436893\n",
      "Document PY Jn 310 Metric ROUGEL_P: 0.9901960784313726\n",
      "Document PY Jn 310 Metric ROUGEL_R: 0.9528301886792453\n",
      "Document PY Jn 310 Metric ROUGEL_F1: 0.9711538461538463\n",
      "Document PY Jn 310 Metric TER: 0.0660377358490566\n",
      "Document PY Jn 310 Metric WER: 0.0660377358490566\n",
      "Document PY Jn 310 Metric CHRF: 0.9478822508358334\n",
      "Document PY Jn 829 Metric BLEU: 0.8513312457757861\n",
      "Document PY Jn 829 Metric METEOR: 0.9456704545454545\n",
      "Document PY Jn 829 Metric ROUGE1_P: 0.9803921568627451\n",
      "Document PY Jn 829 Metric ROUGE1_R: 0.9433962264150944\n",
      "Document PY Jn 829 Metric ROUGE1_F1: 0.9615384615384616\n",
      "Document PY Jn 829 Metric ROUGE2_P: 0.9\n",
      "Document PY Jn 829 Metric ROUGE2_R: 0.8653846153846154\n",
      "Document PY Jn 829 Metric ROUGE2_F1: 0.8823529411764707\n",
      "Document PY Jn 829 Metric ROUGEL_P: 0.9607843137254902\n",
      "Document PY Jn 829 Metric ROUGEL_R: 0.9245283018867925\n",
      "Document PY Jn 829 Metric ROUGEL_F1: 0.9423076923076923\n",
      "Document PY Jn 829 Metric TER: 0.07547169811320754\n",
      "Document PY Jn 829 Metric WER: 0.09433962264150944\n",
      "Document PY Jn 829 Metric CHRF: 0.9369829406473742\n",
      "Document KN So 4439 Metric BLEU: 1.0000000000000004\n",
      "Document KN So 4439 Metric METEOR: 0.9997106481481481\n",
      "Document KN So 4439 Metric ROUGE1_P: 1.0\n",
      "Document KN So 4439 Metric ROUGE1_R: 1.0\n",
      "Document KN So 4439 Metric ROUGE1_F1: 1.0\n",
      "Document KN So 4439 Metric ROUGE2_P: 1.0\n",
      "Document KN So 4439 Metric ROUGE2_R: 1.0\n",
      "Document KN So 4439 Metric ROUGE2_F1: 1.0\n",
      "Document KN So 4439 Metric ROUGEL_P: 1.0\n",
      "Document KN So 4439 Metric ROUGEL_R: 1.0\n",
      "Document KN So 4439 Metric ROUGEL_F1: 1.0\n",
      "Document KN So 4439 Metric TER: 0.0\n",
      "Document KN So 4439 Metric WER: 0.0\n",
      "Document KN So 4439 Metric CHRF: 1.0\n",
      "Document KN Sd 4404 Metric BLEU: 0.5367659014253399\n",
      "Document KN Sd 4404 Metric METEOR: 0.7282110091743119\n",
      "Document KN Sd 4404 Metric ROUGE1_P: 0.75\n",
      "Document KN Sd 4404 Metric ROUGE1_R: 0.6818181818181818\n",
      "Document KN Sd 4404 Metric ROUGE1_F1: 0.7142857142857143\n",
      "Document KN Sd 4404 Metric ROUGE2_P: 0.631578947368421\n",
      "Document KN Sd 4404 Metric ROUGE2_R: 0.5714285714285714\n",
      "Document KN Sd 4404 Metric ROUGE2_F1: 0.6\n",
      "Document KN Sd 4404 Metric ROUGEL_P: 0.75\n",
      "Document KN Sd 4404 Metric ROUGEL_R: 0.6818181818181818\n",
      "Document KN Sd 4404 Metric ROUGEL_F1: 0.7142857142857143\n",
      "Document KN Sd 4404 Metric TER: 0.36363636363636365\n",
      "Document KN Sd 4404 Metric WER: 0.36363636363636365\n",
      "Document KN Sd 4404 Metric CHRF: 0.7353234339871367\n",
      "Document PY An 657 Metric BLEU: 0.40847128245159203\n",
      "Document PY An 657 Metric METEOR: 0.6161903098545244\n",
      "Document PY An 657 Metric ROUGE1_P: 0.7540983606557377\n",
      "Document PY An 657 Metric ROUGE1_R: 0.6133333333333333\n",
      "Document PY An 657 Metric ROUGE1_F1: 0.676470588235294\n",
      "Document PY An 657 Metric ROUGE2_P: 0.5833333333333334\n",
      "Document PY An 657 Metric ROUGE2_R: 0.47297297297297297\n",
      "Document PY An 657 Metric ROUGE2_F1: 0.5223880597014926\n",
      "Document PY An 657 Metric ROUGEL_P: 0.7540983606557377\n",
      "Document PY An 657 Metric ROUGEL_R: 0.6133333333333333\n",
      "Document PY An 657 Metric ROUGEL_F1: 0.676470588235294\n",
      "Document PY An 657 Metric TER: 0.41333333333333333\n",
      "Document PY An 657 Metric WER: 0.41333333333333333\n",
      "Document PY An 657 Metric CHRF: 0.6296798866273786\n",
      "Document PY TA 641 Metric BLEU: 0.8874713442155334\n",
      "Document PY TA 641 Metric METEOR: 0.9497662742382272\n",
      "Document PY TA 641 Metric ROUGE1_P: 0.95\n",
      "Document PY TA 641 Metric ROUGE1_R: 0.95\n",
      "Document PY TA 641 Metric ROUGE1_F1: 0.9500000000000001\n",
      "Document PY TA 641 Metric ROUGE2_P: 0.8974358974358975\n",
      "Document PY TA 641 Metric ROUGE2_R: 0.8974358974358975\n",
      "Document PY TA 641 Metric ROUGE2_F1: 0.8974358974358975\n",
      "Document PY TA 641 Metric ROUGEL_P: 0.925\n",
      "Document PY TA 641 Metric ROUGEL_R: 0.925\n",
      "Document PY TA 641 Metric ROUGEL_F1: 0.925\n",
      "Document PY TA 641 Metric TER: 0.125\n",
      "Document PY TA 641 Metric WER: 0.15\n",
      "Document PY TA 641 Metric CHRF: 0.9558229727105618\n",
      "Document PY Er 312 Metric BLEU: 0.3201999491274141\n",
      "Document PY Er 312 Metric METEOR: 0.5465883441700213\n",
      "Document PY Er 312 Metric ROUGE1_P: 0.8157894736842105\n",
      "Document PY Er 312 Metric ROUGE1_R: 0.6326530612244898\n",
      "Document PY Er 312 Metric ROUGE1_F1: 0.7126436781609196\n",
      "Document PY Er 312 Metric ROUGE2_P: 0.5405405405405406\n",
      "Document PY Er 312 Metric ROUGE2_R: 0.4166666666666667\n",
      "Document PY Er 312 Metric ROUGE2_F1: 0.47058823529411764\n",
      "Document PY Er 312 Metric ROUGEL_P: 0.8157894736842105\n",
      "Document PY Er 312 Metric ROUGEL_R: 0.6326530612244898\n",
      "Document PY Er 312 Metric ROUGEL_F1: 0.7126436781609196\n",
      "Document PY Er 312 Metric TER: 0.46938775510204084\n",
      "Document PY Er 312 Metric WER: 0.46938775510204084\n",
      "Document PY Er 312 Metric CHRF: 0.49886402675253033\n",
      "Document KN Fp 1+31 Metric BLEU: 0.8888687509545825\n",
      "Document KN Fp 1+31 Metric METEOR: 0.9564429891166792\n",
      "Document KN Fp 1+31 Metric ROUGE1_P: 0.9767441860465116\n",
      "Document KN Fp 1+31 Metric ROUGE1_R: 0.9545454545454546\n",
      "Document KN Fp 1+31 Metric ROUGE1_F1: 0.9655172413793104\n",
      "Document KN Fp 1+31 Metric ROUGE2_P: 0.9529411764705882\n",
      "Document KN Fp 1+31 Metric ROUGE2_R: 0.9310344827586207\n",
      "Document KN Fp 1+31 Metric ROUGE2_F1: 0.941860465116279\n",
      "Document KN Fp 1+31 Metric ROUGEL_P: 0.9767441860465116\n",
      "Document KN Fp 1+31 Metric ROUGEL_R: 0.9545454545454546\n",
      "Document KN Fp 1+31 Metric ROUGEL_F1: 0.9655172413793104\n",
      "Document KN Fp 1+31 Metric TER: 0.056818181818181816\n",
      "Document KN Fp 1+31 Metric WER: 0.056818181818181816\n",
      "Document KN Fp 1+31 Metric CHRF: 0.9223029040504009\n",
      "Document PY Ab 573 Metric BLEU: 0.8394327083733333\n",
      "Document PY Ab 573 Metric METEOR: 0.943675509419454\n",
      "Document PY Ab 573 Metric ROUGE1_P: 0.9444444444444444\n",
      "Document PY Ab 573 Metric ROUGE1_R: 0.9444444444444444\n",
      "Document PY Ab 573 Metric ROUGE1_F1: 0.9444444444444444\n",
      "Document PY Ab 573 Metric ROUGE2_P: 0.8823529411764706\n",
      "Document PY Ab 573 Metric ROUGE2_R: 0.8823529411764706\n",
      "Document PY Ab 573 Metric ROUGE2_F1: 0.8823529411764706\n",
      "Document PY Ab 573 Metric ROUGEL_P: 0.9444444444444444\n",
      "Document PY Ab 573 Metric ROUGEL_R: 0.9444444444444444\n",
      "Document PY Ab 573 Metric ROUGEL_F1: 0.9444444444444444\n",
      "Document PY Ab 573 Metric TER: 0.05555555555555555\n",
      "Document PY Ab 573 Metric WER: 0.05555555555555555\n",
      "Document PY Ab 573 Metric CHRF: 0.9308621620986188\n",
      "Document PY Un 718 Metric BLEU: 0.8503381257828848\n",
      "Document PY Un 718 Metric METEOR: 0.9229086350524357\n",
      "Document PY Un 718 Metric ROUGE1_P: 0.958904109589041\n",
      "Document PY Un 718 Metric ROUGE1_R: 0.9210526315789473\n",
      "Document PY Un 718 Metric ROUGE1_F1: 0.9395973154362416\n",
      "Document PY Un 718 Metric ROUGE2_P: 0.9027777777777778\n",
      "Document PY Un 718 Metric ROUGE2_R: 0.8666666666666667\n",
      "Document PY Un 718 Metric ROUGE2_F1: 0.8843537414965987\n",
      "Document PY Un 718 Metric ROUGEL_P: 0.958904109589041\n",
      "Document PY Un 718 Metric ROUGEL_R: 0.9210526315789473\n",
      "Document PY Un 718 Metric ROUGEL_F1: 0.9395973154362416\n",
      "Document PY Un 718 Metric TER: 0.09210526315789473\n",
      "Document PY Un 718 Metric WER: 0.09210526315789473\n",
      "Document PY Un 718 Metric CHRF: 0.8920536833446274\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"PY Ta 711\",\n",
    "    \"KN Ra 1540\",\n",
    "    \"PY Jn 310\",\n",
    "    \"PY Jn 829\",\n",
    "    \"KN So 4439\",\n",
    "    \"KN Sd 4404\",\n",
    "    \"PY An 657\",\n",
    "    \"PY TA 641\",\n",
    "    \"PY Er 312\",\n",
    "    \"KN Fp 1+31\",\n",
    "    \"PY Ab 573\",\n",
    "    \"PY Un 718\"\n",
    "]\n",
    "for dic, lang in zip(avg, [\"Greek\", \"English\"]):\n",
    "    print(f\"{lang} results:\")\n",
    "    print(dic)\n",
    "\n",
    "for dic, lang in zip(per_doc, [\"Greek\", \"English\"]):\n",
    "    print(f\"{lang} results:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        for k, v in dic.items():\n",
    "            print(f\"Document {doc} Metric {k}: {v[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUW1774iLcV3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### BOH Esperimenti credo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[\"e-ke-si-qe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wz_J6HoTrh87"
   },
   "outputs": [],
   "source": [
    "brnn_args = args\n",
    "brnn_args[\"saved_path\"] = os.path.join(brnn_args[\"log_dir\"], \"saved.latest\")\n",
    "clear_stages()\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args)\n",
    "\n",
    "brnn_args[\"saved_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RcRcei3607A"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"testing_hypers1\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"transliterated_linear_b-greek.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 25, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 256, # changed\n",
    "    \"hidden_size\" : 256, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.7), # PORCODIO HAI CAMBIATO QUI OCHO\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 256, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "#CAMBIAMENTI FATTI IN: MAGIC_TENSOR\n",
    "args[\"saved_path\"] = SAVED_PATH\n",
    "manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "model = manager.get_trained_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bxm8ZGt0mfo"
   },
   "outputs": [],
   "source": [
    "#additional_words = set([word.form for word in v.get_words])\n",
    "words = v.get_words\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "cog_path = \"/content/drive/MyDrive/DMPROJECT/transliterated_linear_b-greek.cog\"\n",
    "lost_lang = \"transliterated_linear_b\"\n",
    "known_lang = \"greek\"\n",
    "build_vocabs(cog_path, lost_lang, known_lang)#, additional_words=additional_words)\n",
    "data_loader = LostKnownDataLoader(lost_lang, known_lang, batch_size, cognate_only=False)\n",
    "\n",
    "dataset = data_loader.entire_batch\n",
    "model.eval()\n",
    "model_ret = model(dataset, mode=\"flow\", num_cognates=args[\"num_cognates\"], edit=False, capacity=args[\"capacity\"])\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fi0YywR9Oqh"
   },
   "outputs": [],
   "source": [
    "greek_sequences = model_ret.log_probs\n",
    "greek_sequences = greek_sequences.permute(2, 0, 1)\n",
    "greek_sequences = torch.argmax(greek_sequences, dim=-1).cpu().numpy()\n",
    "greek_sequences = get_charset(\"greek\").id2char(greek_sequences)\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "preds_flow = model_ret.flow.get_best()\n",
    "idx = 9\n",
    "greek_sequences[idx], model_ret.valid_log_probs.row_words[idx], preds[model_ret.valid_log_probs.row_words[idx]],preds_flow[model_ret.valid_log_probs.row_words[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNenqn9M0w9Y"
   },
   "outputs": [],
   "source": [
    "boh = [word.form for word in get_words(lost_lang)]\n",
    "for w in additional_words:\n",
    "  if w not in boh:\n",
    "      print(w, \"wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzRAy1H75ymQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eZVPovYqqVT"
   },
   "source": [
    "## GAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_JGjDSQQcRJ"
   },
   "source": [
    "### Get corpuses and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMzmz-ksOdrL"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "    documents = {}\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq:\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    for seq in sequences:\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "    return seq, mis\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(missing, sequences):\n",
    "    assert len(missing) == len(sequences), \"Both lists must have the same length\"\n",
    "\n",
    "    # Create a shuffled index array\n",
    "    indices = np.random.permutation(len(missing))\n",
    "\n",
    "    # Apply the shuffled indices to both lists\n",
    "    missing_shuffled = [missing[i] for i in indices]\n",
    "    sequences_shuffled = [sequences[i] for i in indices]\n",
    "\n",
    "    # Determine split sizes\n",
    "    train_size = int(0.9 * len(missing))  # 90% for training and LOOCV\n",
    "    test_size = len(missing) - train_size  # 10% for testing\n",
    "\n",
    "    # Split the datasets\n",
    "    train_x, test_x = missing_shuffled[:train_size], missing_shuffled[train_size:]\n",
    "    train_y, test_y = sequences_shuffled[:train_size], sequences_shuffled[train_size:]\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    tensor_sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "    return padded_sequences\n",
    "\n",
    "def pad_sequences_to_length(sequences, target_length=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a batch of sequences to the max sequence length or a specified target length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of lists): List of tokenized sequences (each sequence is a list of integers).\n",
    "        target_length (int, optional): If specified, all sequences will be padded to this length.\n",
    "        padding_value (int, optional): Value used for padding (default is 0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_length)\n",
    "    \"\"\"\n",
    "    tensor_sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    # Determine max length: either given target_length or max sequence length in batch\n",
    "    max_length = target_length if target_length is not None else max(len(seq) for seq in tensor_sequences)\n",
    "    # Pad sequences using pad_sequence, ensuring all sequences are target_length long\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "    # If target_length is specified and some sequences are shorter, add extra padding manually\n",
    "    if target_length is not None and padded_sequences.shape[1] < target_length:\n",
    "        padding_needed = target_length - padded_sequences.shape[1]\n",
    "        padded_sequences = torch.nn.functional.pad(padded_sequences, (0, padding_needed), value=padding_value)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qGjdDrCuaVn"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_signs(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sign_number']).groupby('id')\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        signs = []\n",
    "        for w in group['sign']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                signs.append(\"NUM\")\n",
    "            elif \"?\" not in w:\n",
    "                signs.append(w)\n",
    "        documents[document_name] = signs\n",
    "\n",
    "    return documents\n",
    "\n",
    "def create_documents_dict():\n",
    "    filename = os.path.join(prefix_path, \"LIBER_documents.csv\")\n",
    "    documents_dict = {}\n",
    "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            document_id = int(row['id'])\n",
    "            documents_dict[document_id] = {key: value for key, value in row.items() if key != 'id'}\n",
    "    return documents_dict\n",
    "\n",
    "docs_info = create_documents_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10eHUq9O08Oy"
   },
   "outputs": [],
   "source": [
    "def preprocess_doc_info(doc_info):\n",
    "    doc_processed_info = {}\n",
    "    find_places = {}\n",
    "    counter = 0\n",
    "    chrono_dict = {\n",
    "        '': 0,\n",
    "        \"LM III B\": 1250,\n",
    "        \"LM II-III A\": 1400,\n",
    "        \"LH III C\": 1125,\n",
    "        \"LH III A2\": 1340,\n",
    "        \"LH III B2-C\": 1200,\n",
    "        \"LH III B2\": 1225,\n",
    "        \"LH III A2-B\": 1300,\n",
    "        \"LH III B1\": 1275,\n",
    "        \"LH III B\":  1250,\n",
    "        \"LM III B1\": 1275,\n",
    "        \"LH III A\": 1350,\n",
    "        \"LM III A2\": 1340,\n",
    "        'LH III A2-B1': 1320,\n",
    "        'LH III A2-C': 1213,\n",
    "        'LM III C': 1125,\n",
    "        'LM III B2-C1': 1210,\n",
    "        'LM III B2': 1225,\n",
    "        'LH III B-C': 1175\n",
    "    }\n",
    "\n",
    "    # gpt reconstructed place coorinates\n",
    "    places_coordinates = {\n",
    "        # Widely attested sites with confirmed data:\n",
    "        'KN': {\n",
    "            'name': 'Knossos (Crete)',\n",
    "            'coordinates': (35.2989, 25.1597)  # cite:WikipediaKnossos2025\n",
    "        },\n",
    "        'MY': {\n",
    "            'name': 'Mycenae (Argolid)',\n",
    "            'coordinates': (37.7300, 22.7500)  # cite:WikipediaMycenae2025\n",
    "        },\n",
    "        'TI': {\n",
    "            'name': 'Tiryns (Argolid)',\n",
    "            'coordinates': (37.5710, 22.7460)  # cite:WikipediaTiryns2025\n",
    "        },\n",
    "        'PY': {\n",
    "            'name': 'Pylos (Peloponnese)',\n",
    "            'coordinates': (36.8819, 21.6794)  # cite:WikipediaPylos2025\n",
    "        },\n",
    "        'TH': {\n",
    "            'name': 'Thebes (Boeotia)',\n",
    "            'coordinates': (38.3250, 23.3180)  # cite:WikipediaThebesBoeotia2025\n",
    "        },\n",
    "        'MAM': {\n",
    "            'name': 'Malia (Crete)',\n",
    "            'coordinates': (35.7514, 25.9329)  # cite:WikipediaMalia2025\n",
    "        },\n",
    "        'EL': {\n",
    "            'name': 'Eleusis',\n",
    "            'coordinates': (38.0425, 23.5367)  # cite:WikipediaEleusis2025\n",
    "        },\n",
    "        'OR': {\n",
    "            'name': 'Orchomenos (Boeotia)',\n",
    "            'coordinates': (38.3170, 22.7670)  # cite:WikipediaOrchomenos2025\n",
    "        },\n",
    "        'IK': {\n",
    "            'name': 'Ikaria',\n",
    "            'coordinates': (37.7500, 26.3000)  # cite:WikipediaIkaria2025\n",
    "        },\n",
    "        'VOL': {\n",
    "            'name': 'Volos',\n",
    "            'coordinates': (39.3667, 22.9500)  # cite:WikipediaVolos2025\n",
    "        },\n",
    "        'MA': {\n",
    "            'name': 'Marathon',\n",
    "            'coordinates': (38.1170, 23.9670)  # cite:WikipediaMarathon2025\n",
    "        },\n",
    "        'KH': {\n",
    "            'name': 'Khania (Chania, Crete)',\n",
    "            'coordinates': (35.5110, 24.0150)  # cite:WikipediaChania2025\n",
    "        },\n",
    "        'KR': {\n",
    "            'name': 'Kritsa (Crete)',\n",
    "            'coordinates': (35.2167, 25.1833)  # cite:WikipediaKritsa2025\n",
    "        },\n",
    "\n",
    "        # Tentative identifications (scholarly debate remains):\n",
    "        'MI': {\n",
    "            'name': 'Midea? (Argolid)',\n",
    "            'coordinates': (37.7220, 22.7660)  # Approximate; identification not definitively confirmed\n",
    "        },\n",
    "        'GLA': {\n",
    "            'name': 'Gla? (Boeotia)',\n",
    "            'coordinates': (38.3100, 22.6600)  # Approximate; based on limited archaeological surveys\n",
    "        },\n",
    "        'DI': {\n",
    "            'name': 'Dilos? (tentative)',\n",
    "            'coordinates': (38.0000, 23.5000)  # Coordinates provided as a rough estimate\n",
    "        },\n",
    "        'MED': {\n",
    "            'name': 'Medeon? (tentative)',\n",
    "            'coordinates': (38.0500, 23.6000)  # Tentative identification; data from limited sources\n",
    "        },\n",
    "        'ARM': {\n",
    "            'name': 'Armi? (tentative)',\n",
    "            'coordinates': (38.1000, 21.7000)  # Tentative; identification remains debated\n",
    "        },\n",
    "        'PR': {\n",
    "            'name': 'Prinias? (Crete)',\n",
    "            'coordinates': (35.2500, 24.0000)  # Approximation based on archaeological records\n",
    "        },\n",
    "        'SI': {\n",
    "            'name': 'Sicyon',\n",
    "            'coordinates': (38.0000, 22.9500)  # Approximate; consistent with regional data\n",
    "        }\n",
    "    }\n",
    "    id_count = 0\n",
    "    for doc_id, doc_data in doc_info.items():\n",
    "        find_place = doc_data[\"document_name\"].split()[0]\n",
    "        #associate an integer to any (new) find place\n",
    "        if find_place not in find_places:\n",
    "            find_places[find_place] = counter\n",
    "            counter += 1\n",
    "        # Regex to match initial digits\n",
    "        result = re.match(r'^\\d+', doc_data[\"scribe\"])\n",
    "        if result:\n",
    "            scribe = int(result.group())\n",
    "        else:\n",
    "            scribe = 0\n",
    "        doc_processed_info[doc_id] = {\n",
    "            \"id\": id_count,\n",
    "            \"site\": find_places[find_place],\n",
    "            \"scribe\": scribe,\n",
    "            \"time\": chrono_dict[doc_data[\"chronology\"]],\n",
    "            \"coordinates\": places_coordinates[find_place][\"coordinates\"],\n",
    "            \"words\": int(doc_data[\"words\"])\n",
    "        }\n",
    "        id_count += 1\n",
    "\n",
    "    return doc_processed_info\n",
    "\n",
    "def invert_mapping(signs_by_doc):\n",
    "    signs = defaultdict(lambda: set())\n",
    "    for doc_id, signs_list in signs_by_doc.items():\n",
    "        for pos, s in signs_list:\n",
    "            signs[s].add((pos, doc_id))\n",
    "    res = {}\n",
    "    for s in signs:\n",
    "        res[s] = sorted(list(signs[s]))\n",
    "    return res\n",
    "\n",
    "docs_info_preprocessed = preprocess_doc_info(docs_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLXP2UgY2Dv"
   },
   "outputs": [],
   "source": [
    "#Get all sequences of linear B from the corpus\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing = create_missing_dataset(sequences)\n",
    "sequences, missing = clean_datasets(sequences, missing)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(missing, sequences)\n",
    "\n",
    "#Get input and output tokens to use for the encoding: input has all signs + <SOS> and <PAD>, output has <EOS> and <PAD>, all numbers different from 1 and 2 become  <NUM> for simplicity reasons.Space < > also needs to be encoded\n",
    "signs_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "df_lb = pd.read_csv(signs_path)\n",
    "signs = df_lb['sign'].unique()\n",
    "\n",
    "tokens = set()\n",
    "for s in signs:\n",
    "    if not s.isdigit() or s == \"1\" or s == \"2\":\n",
    "        if not \"?\" in s and not \"separatum\" in s:\n",
    "            tokens.add(s)\n",
    "    else:\n",
    "        tokens.add(\"NUM\")\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "input_tokens = [\" \"] + tokens\n",
    "output_tokens = [\" \"] + tokens\n",
    "input_tokens.sort()\n",
    "output_tokens.sort()\n",
    "# last one is the unknown symbol\n",
    "input_tokens = [\"PAD\", \"SOS\", \"EOS\"] + input_tokens\n",
    "output_tokens = [\"PAD\", \"SOS\", \"EOS\"] + output_tokens\n",
    "\n",
    "input_tokens.extend([\"SYL\", \"LOG\", \"NUMERAL\",\"?\"])\n",
    "\n",
    "def label_dictionary(tokens):\n",
    "    mapping = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        mapping[t] = i\n",
    "    return mapping\n",
    "\n",
    "#Get mapping of every symbol to its encoding\n",
    "input_mapping = label_dictionary(input_tokens)\n",
    "output_mapping = label_dictionary(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PYetgzDWVs0J"
   },
   "outputs": [],
   "source": [
    "#Get dictionary of anthroponyms and toponyms\n",
    "LB_names = { \"anthroponyms\" : [], \"toponyms\" : []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "biPFBn-YAV9X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa-pa-ro-jo o-ka ne-wo-ki-to di-wi-je-u e-ri-ko-wo ha-di-je-u a-ki-wo-ni-jo wa-ka-ti-ja-ta ke-ki-de sa-pi-da me-ta-qe pe-i e-qe-ta pe-re-qo-ni-jo a-re-i-jo ne-wo-ki-to wo-wi-ja ko-ro-ku-ra-i-jo VIR NUM me-ta-qe pe-i e-qe-ta di-wi-je-u du-wo-jo-jo o-ka a-ke-re-wa ha-ku-ni-jo pe-ri-me-de phu-ti-ja a-phu-ka-ne ke-ki-de po-ra-i VIR NUM me-ta-qe pe-i e-qe-ta di-ko-na-ro a-da-ra-ti-jo u-wa-si ke-ki-de ne-wo VIR NUM me-ta-qe pe-i pe-re-u-ro-ni-jo e-qe-ta a-ke-re-wa ko-ro-ku-ra-i-jo VIR NUM me-ta-qe pe-i e-qe-ta ka-e-sa-me-no a-phu-ka\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i]) > 500:\n",
    "        print(sequences[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqSJBCKr3g3Q"
   },
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_CKSAe9bzcA9"
   },
   "outputs": [],
   "source": [
    "#Get all sequences of linear B from the corpus\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(sequences, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrEO8WDvcLJR"
   },
   "outputs": [],
   "source": [
    "def create_signs_dataset_with_positions(documents, add_SOS=True):\n",
    "    sequences = {}\n",
    "    curr_seq = ['SOS'] if add_SOS else []\n",
    "    for doc_id, doc in documents.items():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if not add_SOS and len(curr_seq) > 0 or add_SOS and len(curr_seq) > 1:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq:\n",
    "                         curr_seq.append(seq)\n",
    "                     signs = []\n",
    "                     sign_pos = 0\n",
    "                     for i, w in enumerate(curr_seq):\n",
    "                        signs.extend([(sign_pos+j, sign) for j, sign in enumerate(w.split(\"-\"))])\n",
    "                        sign_pos = len(signs)\n",
    "                        if i != len(curr_seq) - 1:\n",
    "                          signs.append(((sign_pos,\" \"))) # add spaces\n",
    "                          sign_pos += 1\n",
    "\n",
    "                     # normalize by length\n",
    "                     for i, (pos, sign) in enumerate(signs):\n",
    "                        signs[i] = (pos/(len(signs)-1+1e-10), sign)\n",
    "\n",
    "                     sequences[doc_id] = signs\n",
    "                     curr_seq = ['SOS'] if add_SOS else []\n",
    "        if not add_SOS and len(curr_seq) > 0 or add_SOS and len(curr_seq) > 1:\n",
    "            signs = []\n",
    "            sign_pos = 0\n",
    "            for i, w in enumerate(curr_seq):\n",
    "               signs.extend([(sign_pos+j, sign) for j, sign in enumerate(w.split(\"-\"))])\n",
    "               sign_pos = len(signs)\n",
    "               if i != len(curr_seq) - 1:\n",
    "                 signs.append(((sign_pos,\" \"))) # add spaces\n",
    "                 sign_pos += 1\n",
    "\n",
    "            for i, (pos, sign) in enumerate(signs):\n",
    "                signs[i] = (pos/(len(signs)-1+1e-10), sign)\n",
    "\n",
    "            sequences[doc_id] = signs\n",
    "            curr_seq = ['SOS'] if add_SOS else []\n",
    "\n",
    "    return sequences\n",
    "\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "signs_info = create_signs_dataset_with_positions(corpus_LB)\n",
    "signs_info = invert_mapping(signs_info)\n",
    "signs = {input_mapping[k]: v for k, v in signs_info.items()} # tokenized view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej-QMpmhkt3g"
   },
   "outputs": [],
   "source": [
    "# Recover features of documents where signs appear\n",
    "'''\n",
    "def define_token_features(signs_info, docs_info, input_tokens=input_tokens):\n",
    "    signs_list = [0 for _ in range(len(input_tokens))]\n",
    "    for i in range(len(signs_list)):\n",
    "        info_vec = []\n",
    "        docs = signs_info.get(i, [])\n",
    "        for (pos, key) in docs:\n",
    "            info_vec.append([pos, docs_info[key]['id'], docs_info[key]['site'], docs_info[key]['coordinates'][0],docs_info[key]['coordinates'][1], docs_info[key]['time'], docs_info[key]['scribe']])\n",
    "        signs_list[i] = info_vec\n",
    "    return signs_list\n",
    "\n",
    "def pad_signs_list(signs_list):\n",
    "    final_shape = max([len(signs_list[i]) for i in range(len(signs_list))])\n",
    "    for info_vec in signs_list:\n",
    "        info_vec.extend([torch.zeros(7) for _ in range(final_shape - len(info_vec))])\n",
    "    return signs_list\n",
    "\n",
    "signs_list = define_token_features(signs, docs_info_preprocessed)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH5RJI3Xv-aG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def aggregate_document_info(signs_list):\n",
    "    features = [0 for _ in range(len(signs_list))]\n",
    "    number_of_sites = 1\n",
    "    for s in signs_list:\n",
    "        for doc_info in s:\n",
    "            number_of_sites = max(number_of_sites, doc_info[2]+1)\n",
    "\n",
    "    for i, s in enumerate(signs_list):\n",
    "        if len(s) == 0:\n",
    "            features[i] = [0] * 7 + [0] * number_of_sites\n",
    "            continue\n",
    "        avg_pos = sum([doc_info[0] for doc_info in s]) / len(s)\n",
    "        num_docs = len(set([doc_info[1] for doc_info in s])) # n. of distinct documents where the token appears at least once\n",
    "        sites = set([doc_info[2] for doc_info in s])\n",
    "        one_hot_sites = [0 for _ in range(number_of_sites)]\n",
    "        for site in sites: one_hot_sites[site] = 1\n",
    "        avg_lat = sum([doc_info[3] for doc_info in s]) / len(s)\n",
    "        avg_long = sum([doc_info[4] for doc_info in s]) / len(s)\n",
    "        first_appearance = max([doc_info[5] for doc_info in s])\n",
    "        last_appearance = min([doc_info[5] if doc_info[5] != 0 else 1000000000 for doc_info in s]) # big number to avoid 0s encoding None dates\n",
    "        # remove scribe with key 0 (placeholder for no scribe)\n",
    "        num_scribes =  len(set([doc_info[6] for doc_info in s])) - 1 if 0 in set([doc_info[6] for doc_info in s]) else len(set([doc_info[6] for doc_info in s]))\n",
    "        features[i] = [avg_pos, num_docs, avg_lat, avg_long, first_appearance, last_appearance, num_scribes] + one_hot_sites\n",
    "    return features\n",
    "\n",
    "node_features = torch.tensor(aggregate_document_info(signs_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o03MhF2UwPis"
   },
   "outputs": [],
   "source": [
    "def define_nodes_features(docs_info, num_input_tokens=len(input_tokens)):\n",
    "    features = []\n",
    "    for _ in range(num_input_tokens):\n",
    "        features.append(torch.randn(6))\n",
    "    docs_info = sorted([(doc_id, doc_info) for doc_id, doc_info in docs_info.items()], key=lambda x: x[0])\n",
    "    for doc_id, doc_info in docs_info:\n",
    "        features.append(torch.tensor([doc_info['site'], doc_info['coordinates'][0], doc_info['coordinates'][1], doc_info['scribe'], doc_info['time'], doc_info[\"words\"]]))\n",
    "    return torch.stack(features)\n",
    "\n",
    "node_features = define_nodes_features(docs_info_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6EAy-xG15eN"
   },
   "outputs": [],
   "source": [
    "len(docs_info_preprocessed) + len(input_tokens), node_features.shape, len(docs_info_preprocessed), len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5pgT1dC3vtj"
   },
   "outputs": [],
   "source": [
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_6zyHpx1I7F"
   },
   "outputs": [],
   "source": [
    "def define_edges_document_token(docs_info, signs_dict, num_input_tokens=len(input_tokens)):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "    edges = set()\n",
    "    for sign in signs_dict.keys():\n",
    "        #filter out PAD, SOS, EOS\n",
    "        if sign > 2:\n",
    "            for pos, doc_key in signs_dict[sign]:\n",
    "                node_token_idx = sign\n",
    "                node_doc_idx = num_input_tokens + docs_info[doc_key]['id']\n",
    "                if (node_doc_idx, node_token_idx) not in edges:\n",
    "                    edges_src.extend([node_doc_idx, node_token_idx])\n",
    "                    edges_tgt.extend([node_token_idx, node_doc_idx])\n",
    "                    edges.add((node_doc_idx, node_token_idx))\n",
    "\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "\n",
    "    return edge_index\n",
    "edges_document_token = define_edges_document_token(docs_info_preprocessed, signs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jby_FGS-yY7"
   },
   "outputs": [],
   "source": [
    "def define_edges_token_token(input_tokens):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "    edges = set()\n",
    "    for i in range(len(input_tokens)):\n",
    "        for j in range(len(input_tokens)):\n",
    "            if i != j:\n",
    "                #syllabograms\n",
    "                if (input_tokens[i][0] == input_tokens[j][0] or input_tokens[i][-1] == input_tokens[j][-1]) and input_tokens[i].islower() and input_tokens[j].islower():\n",
    "                    if (i,j) not in edges:\n",
    "                        edges_src.extend([i, j])\n",
    "                        edges_tgt.extend([j, i])\n",
    "                        edges.add((i, j))\n",
    "                #logograms\n",
    "                if (not input_tokens[i].islower() and not input_tokens[j].islower()):\n",
    "                    pref1 = input_tokens[i].split(\"+\")[0] if \"+\" in input_tokens[i] else input_tokens[i]\n",
    "                    pref2 = input_tokens[j].split(\"+\")[0] if \"+\" in input_tokens[j] else input_tokens[j]\n",
    "                    suff1 = input_tokens[i].split(\"+\")[-1] if \"+\" in input_tokens[i] else input_tokens[i]\n",
    "                    suff2 = input_tokens[j].split(\"+\")[-1] if \"+\" in input_tokens[j] else input_tokens[j]\n",
    "                    if not input_tokens[i] == \"1\" and not input_tokens[j] == \"1\" and not input_tokens[i] == \"2\" and not input_tokens[j] == \"2\" and not input_tokens[i] == \"?\" and not input_tokens[j] == \"?\":\n",
    "                        if ((pref1 == pref2 or suff1 == suff2 or (len(input_tokens[i])==1 and len(input_tokens[j])==1)) or \"VAS\" in pref1 and \"VAS\" in pref2) and (i,j) not in edges:\n",
    "                            edges_src.extend([i, j])\n",
    "                            edges_tgt.extend([j, i])\n",
    "                            edges.add((i, j))\n",
    "                    #numerals only between them\n",
    "                    if (input_tokens[i] == \"NUM\" or input_tokens[i] == \"1\" or input_tokens[i] == \"2\") and (input_tokens[j] == \"NUM\" or input_tokens[j] == \"1\" or input_tokens[j] == \"2\"):\n",
    "                        if (i,j) not in edges:\n",
    "                            edges_src.extend([i, j])\n",
    "                            edges_tgt.extend([j, i])\n",
    "                            edges.add((i, j))\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "    return edge_index\n",
    "edges_token_token = define_edges_token_token(input_tokens)\n",
    "edges_document_token.shape, edges_token_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rYbguv0d4Nt"
   },
   "outputs": [],
   "source": [
    "def define_meta_paths_token_token(signs, num_input_tokens=len(input_tokens)):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "\n",
    "    edges_doc_tok = set()\n",
    "\n",
    "    sign_docs = defaultdict(lambda: set())\n",
    "    for sign, tup_list in signs.items():\n",
    "        for pos, doc in tup_list:\n",
    "            sign_docs[sign].add(doc)\n",
    "\n",
    "    # Create token-token edges based on shared document connections\n",
    "    for tok_i in range(num_input_tokens):\n",
    "        for tok_j in range(num_input_tokens):\n",
    "            if len(sign_docs[tok_i] & sign_docs[tok_j]) > 0:  # Ensure both tokens share the same document\n",
    "                edges_src.extend([tok_i, tok_j])\n",
    "                edges_tgt.extend([tok_j, tok_i])\n",
    "\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "meta_paths = define_meta_paths_token_token(signs)\n",
    "meta_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3NJY-NLKD22"
   },
   "outputs": [],
   "source": [
    "!pip install torch-cluster\n",
    "!pip install torch_geometric\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X0OnDrUoBjh"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Combine the two types of edges\n",
    "edges_src = torch.cat([edges_document_token[0], edges_token_token[0]], dim=0)\n",
    "edges_tgt = torch.cat([edges_document_token[1], edges_token_token[1]], dim=0)\n",
    "pos_edges = torch.stack([edges_src, edges_tgt], dim=0)\n",
    "\n",
    "# Define edge types\n",
    "edge_types = torch.cat([\n",
    "    torch.zeros(edges_document_token.shape[1], dtype=torch.long),   # Document-Token edges\n",
    "    torch.ones(edges_token_token.shape[1], dtype=torch.long)       # Token-Token edges\n",
    "])\n",
    "\n",
    "# Graph Data\n",
    "#graph_data = Data(x=node_features, edge_index=edge_index, edge_type=edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6dFbhBuyILo"
   },
   "outputs": [],
   "source": [
    "edge_types[:pos_edges.shape[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPGIhvnAwDrm"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx, negative_sampling, degree\n",
    "\n",
    "def sample_links(pos_edges, edge_types):\n",
    "    idxs = (edge_types[:pos_edges.shape[1]]) == 1\n",
    "    num_samples = pos_edges[:, idxs].shape[1]\n",
    "\n",
    "    pos_samples_tok_tok = pos_edges[:, idxs]\n",
    "    neg_samples_tok_tok = negative_sampling (\n",
    "                    edge_index=pos_samples_tok_tok,\n",
    "                    num_nodes=len(input_tokens),\n",
    "                    num_neg_samples=num_samples,\n",
    "                )\n",
    "\n",
    "    idxs = (edge_types[:pos_edges.shape[1]]) == 0\n",
    "    doc_tok_edges = pos_edges[:, idxs]\n",
    "    doc_tok_edges_set = set()\n",
    "\n",
    "    for i in range(doc_tok_edges.shape[1]):\n",
    "        e = doc_tok_edges[:, i]\n",
    "        doc_tok_edges_set.add((e[0], e[1]))\n",
    "        doc_tok_edges_set.add((e[1], e[0]))\n",
    "\n",
    "\n",
    "    def sample_negative_doc_tok_links(num_samples, num_tokens, num_docs, doc_tok_edges_set):\n",
    "        \"\"\"\n",
    "        Efficiently samples negative doc-token links while avoiding existing edges.\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of negative samples to generate.\n",
    "            num_tokens (int): Number of token nodes.\n",
    "            num_docs (int): Number of document nodes.\n",
    "            doc_tok_edges_set (set): Existing doc-token edges to avoid.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled negative edges of shape (2, num_samples).\n",
    "        \"\"\"\n",
    "        neg_edges = []\n",
    "        attempts = 0\n",
    "        max_attempts = num_samples * 10  # Avoid infinite loops\n",
    "\n",
    "        while len(neg_edges) < num_samples and attempts < max_attempts:\n",
    "            # Randomly sample token and document indices\n",
    "            tok_idx = torch.randint(0, num_tokens, (num_samples,))\n",
    "            doc_idx = torch.randint(num_tokens, num_tokens + num_docs, (num_samples,))\n",
    "\n",
    "            # Create candidate edges\n",
    "            candidate_edges = list(zip(tok_idx.tolist(), doc_idx.tolist()))\n",
    "\n",
    "            # Filter out existing edges\n",
    "            new_edges = [e for e in candidate_edges if e not in doc_tok_edges_set]\n",
    "\n",
    "            # Add to negative edges list\n",
    "            neg_edges.extend(new_edges)\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        # Convert to tensor\n",
    "        neg_edges = torch.tensor(neg_edges[:num_samples], dtype=torch.long).T  # Shape: (2, num_samples)\n",
    "\n",
    "        return neg_edges\n",
    "\n",
    "    neg_samples_doc_tok = sample_negative_doc_tok_links(num_samples, len(input_tokens), node_features.shape[0]-len(input_tokens), doc_tok_edges_set)\n",
    "\n",
    "    # Randomly sample `num_samples` positive edges from `doc_tok_edges`\n",
    "    perm = torch.randperm(doc_tok_edges.shape[1])[:num_samples]  # Random indices\n",
    "    pos_samples_doc_tok = doc_tok_edges[:, perm]  # Select random positives\n",
    "\n",
    "    edge_index = torch.cat([pos_samples_tok_tok, pos_samples_doc_tok, neg_samples_tok_tok, neg_samples_doc_tok], dim=1)\n",
    "    labels = torch.cat([torch.ones(num_samples*2), torch.zeros(num_samples*2)])\n",
    "    return edge_index, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khj9gfHLJ8-T"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "#Combine the two types of edges\n",
    "edges_src = torch.cat([edges_document_token[0], edges_token_token[0], meta_paths[0]], dim=0)\n",
    "edges_tgt = torch.cat([edges_document_token[1], edges_token_token[1], meta_paths[1]], dim=0)\n",
    "edge_index = torch.stack([edges_src, edges_tgt], dim=0)\n",
    "\n",
    "#Define edge types\n",
    "edge_types = torch.cat([\n",
    "    torch.zeros(edges_document_token.shape[1], dtype=torch.long),   #Document-Token edges\n",
    "    torch.ones(edges_token_token.shape[1], dtype=torch.long),       #Token-Token edges\n",
    "    torch.ones(meta_paths.shape[1], dtype=torch.long) * 2           #Meta-Path edges\n",
    "])\n",
    "\n",
    "# Graph Data\n",
    "graph_data = Data(x=node_features, edge_index=edge_index, edge_type=edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5l4FHaEmrKa"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Node2Vec\n",
    "from torch.optim import Adam, SparseAdam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Node2VecEmbeddings:\n",
    "    def __init__(self, data, params):\n",
    "        self.params = params\n",
    "        # initialize Node2Vec model with specified parameters\n",
    "        self.node2vec = Node2Vec(\n",
    "            data.edge_index,\n",
    "            embedding_dim=params['embedding_dim'],\n",
    "            walk_length=params.get('walk_length', 20),\n",
    "            context_size=params.get('context_size', 10),\n",
    "            walks_per_node=params.get('walks_per_node', 1),\n",
    "            num_negative_samples=params.get('num_negative_samples', 1),\n",
    "            p=params['p'],\n",
    "            q=params['q'],\n",
    "            sparse=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # prepare the data loader for Node2Vec training\n",
    "        self.loader = self.node2vec.loader(batch_size=params['batch_size'], shuffle=True)\n",
    "        self.optimizer_node2vec = SparseAdam(list(self.node2vec.parameters()), lr=params['node2vec_lr'])\n",
    "\n",
    "    def train_node2vec(self):\n",
    "        self.node2vec.train()\n",
    "        total_loss = 0\n",
    "        # iterate through the data loader and train Node2Vec\n",
    "        for pos_rw, neg_rw in self.loader:\n",
    "            self.optimizer_node2vec.zero_grad()\n",
    "\n",
    "            # compute the loss for the current batch\n",
    "            loss = self.node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            self.optimizer_node2vec.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.node2vec.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.node2vec()\n",
    "        return embeddings.detach()\n",
    "\n",
    "params = {\n",
    "    'embedding_dim': 128,          # test different embedding dimensions\n",
    "    'p': 0.1,               # tests different values for bias towards recent nodes\n",
    "    'q': 10.0,               # tests different values for bias towards outward and inward walks\n",
    "    'node2vec_lr': 0.01,        # learning rate for Node2Vec\n",
    "    'epochs_n2v': 200,        # number of epochs for training Node2Vec: smaller grid to avoid excessive training time\n",
    "    'batch_size': 128,              # batch size for training Node2Vec\n",
    "    'walk_length': 4,\n",
    "    'context_size': 3\n",
    "}\n",
    "\n",
    "n2v = Node2VecEmbeddings(graph_data, params)\n",
    "for i in range(params[\"epochs_n2v\"]):\n",
    "    loss = n2v.train_node2vec()\n",
    "    if not (i % 10):\n",
    "        print(f\"Epoch {i}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYb7m9FdjyDq"
   },
   "outputs": [],
   "source": [
    "edge_index.shape, edge_types.shape, node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pydncW4C2KH"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Custom Message Passing: Document-to-Token  #\n",
    "#############################################\n",
    "from torch_geometric.nn import MessagePassing, GATConv\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "class DocToTokenAttention(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DocToTokenAttention, self).__init__(aggr='add')  # We'll sum the messages.\n",
    "        # Linear transformation for both document and token features.\n",
    "        self.lin = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        # Learnable attention: takes concatenated [token, document] features.\n",
    "        self.att_lin = nn.Linear(2 * out_channels, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [num_nodes, in_channels] node features.\n",
    "        edge_index: [2, num_edges] for document-to-token edges.\n",
    "                    We assume that for each edge, source is a document and target is a token.\n",
    "        \"\"\"\n",
    "        # First, linearly transform all node features.\n",
    "        x_trans = self.lin(x)  # Now shape is [num_nodes, out_channels]\n",
    "        # Propagate messages along edges.\n",
    "        out = self.propagate(edge_index, x=x_trans)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, index):\n",
    "        \"\"\"\n",
    "        x_j: features of the source nodes (documents).\n",
    "        x_i: features of the target nodes (tokens).\n",
    "        index: target node indices for softmax aggregation.\n",
    "        \"\"\"\n",
    "        # Compute attention score from the concatenation of target (token) and source (document) features.\n",
    "        cat = torch.cat([x_i, x_j], dim=-1)  # shape: [num_edges, 2*out_channels]\n",
    "        alpha = self.att_lin(cat)  # shape: [num_edges, 1]\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        # Normalize the attention scores over all incoming edges for each target token.\n",
    "        alpha = softmax(alpha, index)\n",
    "        # Multiply the document features by the attention weight.\n",
    "        return x_j * alpha\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # Simply return the aggregated message.\n",
    "        return aggr_out\n",
    "\n",
    "#############################################\n",
    "# Overall Model: Documents influence tokens #\n",
    "#############################################\n",
    "\n",
    "class DocTokenGNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):#, num_tokens):\n",
    "        \"\"\"\n",
    "        in_channels: Dimension of input features.\n",
    "        out_channels: Dimension of output token embeddings.\n",
    "        num_tokens: Number of token nodes (assumed to be the first num_tokens in x).\n",
    "        \"\"\"\n",
    "        super(DocTokenGNN, self).__init__()\n",
    "        #self.num_tokens = num_tokens\n",
    "        #\n",
    "        #self.feature_transform = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "        # Branch for document-to-token message passing using learnable distance function.\n",
    "        self.doc2token = DocToTokenAttention(in_channels, out_channels)\n",
    "        # Branch for token-to-token interactions (using standard GAT).\n",
    "        self.token2token = GATConv(in_channels, out_channels, heads=1)\n",
    "        # Optional final linear layer to fuse the two branches.\n",
    "        self.final_lin = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        \"\"\"\n",
    "        x: [num_nodes, in_channels] input features for all nodes.\n",
    "           The first `num_tokens` rows correspond to tokens.\n",
    "           The remaining rows correspond to documents.\n",
    "        edge_index_doc2token: [2, num_edges_doc2token] edges from documents (source) to tokens (target).\n",
    "        edge_index_token2token: [2, num_edges_token2token] edges among tokens.\n",
    "        \"\"\"\n",
    "        mask_document_token = (edge_type == 0)\n",
    "        mask_token_token = (edge_type == 1)\n",
    "\n",
    "        edge_index_doc2token = edge_index[:, mask_document_token]\n",
    "\n",
    "        # Get the token-token edge index\n",
    "        edge_index_token2token = edge_index[:, mask_token_token]\n",
    "\n",
    "        # Compute messages from document nodes to tokens.\n",
    "        doc2token_out = self.doc2token(x, edge_index_doc2token)\n",
    "        # Compute messages among tokens.\n",
    "        token2token_out = self.token2token(x, edge_index_token2token)\n",
    "        # Combine both branches.\n",
    "        tokens_updated = doc2token_out + token2token_out\n",
    "        tokens_updated = self.final_lin(tokens_updated)\n",
    "        #out = self.feature_transform(x)[self.num_tokens:]\n",
    "        #\n",
    "        #out = torch.cat([tokens_updated[:self.num_tokens], out], dim=0)\n",
    "\n",
    "        return tokens_updated\n",
    "\n",
    "model = DocTokenGNN(in_channels=node_features.shape[1], out_channels=8)#, num_tokens=len(input_tokens))\n",
    "output = model(node_features, edge_index, edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr3ZfhZKVr6X"
   },
   "outputs": [],
   "source": [
    "# inspired by https://arxiv.org/abs/1903.07293\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class HAGN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads):\n",
    "        super(HAGN, self).__init__()\n",
    "\n",
    "        self.gat_doc_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_token_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_meta_path = GATConv(in_channels, out_channels, heads)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.semantic_transform = nn.Linear(out_channels * heads, out_channels)  # MLP Transformation (W)\n",
    "        self.semantic_attention_vector = nn.Parameter(torch.randn(out_channels))  # Attention vector (q)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.final_proj = nn.Linear(out_channels * heads, out_channels)\n",
    "        #self.reconstruct = nn.Linear(out_channels, in_channels)\n",
    "        #self.embeddings = None\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        edge_doc_tok = edge_index[:, (edge_type == 0)]\n",
    "        edge_token_tok = edge_index[:, (edge_type == 1)]\n",
    "        edge_meta_path = edge_index[:, (edge_type == 2)]\n",
    "        doc_tok = self.sigmoid(self.gat_doc_token(x, edge_doc_tok))\n",
    "        tok_tok = self.sigmoid(self.gat_token_token(x, edge_token_tok))\n",
    "        meta_tok = self.sigmoid(self.gat_meta_path(x, edge_meta_path))\n",
    "\n",
    "        # Stack node-level embeddings to a tensor of shape [num_nodes, num_meta_paths, hidden_size]\n",
    "        node_level_embeddings = torch.stack([doc_tok, tok_tok, meta_tok], dim=1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "\n",
    "        # Apply the semantic transformation (MLP) and tanh across all embeddings at once\n",
    "        transformed = self.semantic_transform(node_level_embeddings.view(-1, node_level_embeddings.size(-1)))  # [num_nodes * num_meta_paths, hidden_size]\n",
    "        transformed = transformed.view(node_level_embeddings.size(0), node_level_embeddings.size(1), -1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "        transformed = self.tanh(transformed)\n",
    "\n",
    "        # Compute attention scores in parallel for all meta-paths\n",
    "        attention_scores = torch.matmul(transformed, self.semantic_attention_vector)  # [num_nodes, num_meta_paths]\n",
    "\n",
    "        # Compute the average importance score across all nodes for each meta-path\n",
    "        meta_path_importance = attention_scores.mean(dim=0)  # [num_meta_paths]\n",
    "\n",
    "        # Apply softmax to get the final meta-path weights\n",
    "        meta_path_weights = self.softmax(meta_path_importance)  # [num_meta_paths]\n",
    "\n",
    "        # Expand meta_path_weights to match the dimensions of node_level_embeddings for broadcasting\n",
    "        meta_path_weights = meta_path_weights.view(1, -1, 1)  # Shape: [1, num_meta_paths, 1]\n",
    "\n",
    "        # Aggregate the final embeddings by weighting the node embeddings\n",
    "        z = torch.sum(meta_path_weights * node_level_embeddings, dim=1)  # [num_nodes, hidden_size]\n",
    "\n",
    "        z = self.final_proj(z)\n",
    "        #self.embeddings = z\n",
    "        #z = self.reconstruct(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "model = HAGN(in_channels=node_features.shape[1], out_channels=256, heads=16)\n",
    "output = model(node_features, edge_index, edge_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwSN2wRKt0FL"
   },
   "outputs": [],
   "source": [
    "class HAGN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads):\n",
    "        super(HAGN, self).__init__()\n",
    "\n",
    "        self.gat_doc_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_token_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_meta_path = GATConv(in_channels, out_channels, heads)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.semantic_transform = nn.Linear(out_channels * heads, out_channels)  # MLP Transformation (W)\n",
    "        self.semantic_attention_vector = nn.Parameter(torch.randn(out_channels))  # Attention vector (q)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.final_proj = nn.Linear(out_channels * heads, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_type):\n",
    "        edge_doc_tok = edge_index[:, (edge_type == 0)]\n",
    "        edge_token_tok = edge_index[:, (edge_type == 1)]\n",
    "        edge_meta_path = edge_index[:, (edge_type == 2)]\n",
    "        doc_tok = self.sigmoid(self.gat_doc_token(x, edge_doc_tok))\n",
    "        tok_tok = self.sigmoid(self.gat_token_token(x, edge_token_tok))\n",
    "        meta_tok = self.sigmoid(self.gat_meta_path(x, edge_meta_path))\n",
    "\n",
    "        # Stack node-level embeddings to a tensor of shape [num_nodes, num_meta_paths, hidden_size]\n",
    "        node_level_embeddings = torch.stack([doc_tok, tok_tok, meta_tok], dim=1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "\n",
    "        # Apply the semantic transformation (MLP) and tanh across all embeddings at once\n",
    "        transformed = self.semantic_transform(node_level_embeddings.view(-1, node_level_embeddings.size(-1)))  # [num_nodes * num_meta_paths, hidden_size]\n",
    "        transformed = transformed.view(node_level_embeddings.size(0), node_level_embeddings.size(1), -1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "        transformed = self.tanh(transformed)\n",
    "\n",
    "        # Compute attention scores in parallel for all meta-paths\n",
    "        attention_scores = torch.matmul(transformed, self.semantic_attention_vector)  # [num_nodes, num_meta_paths]\n",
    "\n",
    "        # Compute the average importance score across all nodes for each meta-path\n",
    "        meta_path_importance = attention_scores.mean(dim=0)  # [num_meta_paths]\n",
    "\n",
    "        # Apply softmax to get the final meta-path weights\n",
    "        meta_path_weights = self.softmax(meta_path_importance)  # [num_meta_paths]\n",
    "\n",
    "        # Expand meta_path_weights to match the dimensions of node_level_embeddings for broadcasting\n",
    "        meta_path_weights = meta_path_weights.view(1, -1, 1)  # Shape: [1, num_meta_paths, 1]\n",
    "\n",
    "        # Aggregate the final embeddings by weighting the node embeddings\n",
    "        z = torch.sum(meta_path_weights * node_level_embeddings, dim=1)  # [num_nodes, hidden_size]\n",
    "\n",
    "        z = self.final_proj(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        # extract source and target node indices\n",
    "        source_embeddings = z[edge_label_index[0]]\n",
    "        target_embeddings = z[edge_label_index[1]]\n",
    "\n",
    "        # dot product for the prediction\n",
    "        dot_product = torch.sum(source_embeddings * target_embeddings, dim=1)\n",
    "        return self.sigmoid(dot_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-rcLgkl0U43"
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5wkg7UNMFMh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def contrastive_loss(embeddings, pos_edge_index, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss without explicitly forming a large similarity matrix.\n",
    "\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    \"\"\"\n",
    "    num_tokens = embeddings.size(0)\n",
    "\n",
    "    # Normalize embeddings to compute cosine similarity efficiently\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "    losses = []\n",
    "    for src, tgt in pos_edge_index.t():  # Iterate over positive edges\n",
    "        # Compute positive similarity\n",
    "        pos_sim = torch.exp(torch.dot(embeddings[src], embeddings[tgt]) / temperature)\n",
    "\n",
    "        # Compute all negative similarities (excluding src itself)\n",
    "        neg_sims = torch.exp(embeddings[src] @ embeddings.T / temperature)  # [num_tokens]\n",
    "        neg_sims[src] = 0  # Mask self-similarity\n",
    "\n",
    "        # Compute denominator\n",
    "        denom = neg_sims.sum()\n",
    "\n",
    "        # Contrastive InfoNCE loss\n",
    "        loss = -torch.log(pos_sim / denom)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if losses:\n",
    "        return torch.stack(losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def triplet_loss(embeddings, pos_edge_index, margin=0.5, num_samples=10):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    margin: Margin hyperparameter for the triplet loss.\n",
    "    num_samples: Number of positive and negative samples per anchor.\n",
    "    \"\"\"\n",
    "    #embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "    num_tokens = embeddings.size(0)\n",
    "    triplet_losses = []\n",
    "\n",
    "    # Sample negative edges using torch_geometric's negative_sampling\n",
    "    neg_edge_index = negative_sampling(\n",
    "        pos_edge_index, num_nodes=num_tokens, num_neg_samples=num_samples * pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    # Convert positive edges to dictionary for fast lookup\n",
    "    pos_dict = {i: set() for i in range(num_tokens)}\n",
    "    for src, tgt in pos_edge_index.t().tolist():\n",
    "        pos_dict[src].add(tgt)\n",
    "        pos_dict[tgt].add(src)  # Assuming undirected edges\n",
    "\n",
    "    for anchor in range(num_tokens):\n",
    "        positives = list(pos_dict[anchor])\n",
    "        if not positives:\n",
    "            continue  # Skip tokens with no positive neighbors\n",
    "\n",
    "        # Sample up to `num_samples` positives\n",
    "        sampled_positives = random.sample(positives, min(num_samples, len(positives)))\n",
    "\n",
    "        # Find negatives corresponding to this anchor\n",
    "        neg_indices = (neg_edge_index[0] == anchor).nonzero(as_tuple=True)[0]\n",
    "        sampled_negatives = neg_edge_index[1, neg_indices].tolist()\n",
    "        sampled_negatives = random.sample(sampled_negatives, min(num_samples, len(sampled_negatives)))\n",
    "\n",
    "        if not sampled_negatives:\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings for all sampled positives and negatives\n",
    "        anchor_embed = embeddings[anchor].unsqueeze(0)  # [1, embedding_dim]\n",
    "        pos_embeds = embeddings[sampled_positives]  # [num_samples, embedding_dim]\n",
    "        neg_embeds = embeddings[sampled_negatives]  # [num_samples, embedding_dim]\n",
    "\n",
    "        # Compute cosine similarity (higher is better for positives)\n",
    "        pos_sim = F.cosine_similarity(anchor_embed, pos_embeds).mean()\n",
    "        neg_sim = F.cosine_similarity(anchor_embed, neg_embeds).mean()\n",
    "\n",
    "        # Compute triplet loss (maximize pos_sim, minimize neg_sim)\n",
    "        loss = F.relu(margin + neg_sim - pos_sim)  # Encourage higher pos_sim and lower neg_sim\n",
    "        triplet_losses.append(loss)\n",
    "\n",
    "    if triplet_losses:\n",
    "        return torch.stack(triplet_losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "'''\n",
    "def triplet_loss(embeddings, pos_edge_index, margin=0.5, num_samples=10):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    margin: Margin hyperparameter for the triplet loss.\n",
    "    num_samples: Number of positive and negative samples per anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    num_tokens = embeddings.size(0)\n",
    "    triplet_losses = []\n",
    "\n",
    "    # Sample negative edges using torch_geometric's negative_sampling\n",
    "    neg_edge_index = negative_sampling(\n",
    "        pos_edge_index, num_nodes=num_tokens, num_neg_samples=num_samples * pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    # Convert positive edges to dictionary for fast lookup\n",
    "    pos_dict = {i: set() for i in range(num_tokens)}\n",
    "    for src, tgt in pos_edge_index.t().tolist():\n",
    "        pos_dict[src].add(tgt)\n",
    "        pos_dict[tgt].add(src)  # Assuming undirected edges\n",
    "\n",
    "    for anchor in range(num_tokens):\n",
    "        positives = list(pos_dict[anchor])\n",
    "        if not positives:\n",
    "            continue  # Skip tokens with no positive neighbors\n",
    "\n",
    "        # Sample up to `num_samples` positives\n",
    "        sampled_positives = random.sample(positives, min(num_samples, len(positives)))\n",
    "\n",
    "        # Find negatives corresponding to this anchor\n",
    "        neg_indices = (neg_edge_index[0] == anchor).nonzero(as_tuple=True)[0]\n",
    "        sampled_negatives = neg_edge_index[1, neg_indices].tolist()\n",
    "        sampled_negatives = random.sample(sampled_negatives, min(num_samples, len(sampled_negatives)))\n",
    "\n",
    "        if not sampled_negatives:\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings for all sampled positives and negatives\n",
    "        anchor_embed = embeddings[anchor].unsqueeze(0)  # [1, embedding_dim]\n",
    "        pos_embeds = embeddings[sampled_positives]  # [num_samples, embedding_dim]\n",
    "        neg_embeds = embeddings[sampled_negatives]  # [num_samples, embedding_dim]\n",
    "\n",
    "        # Compute cosine similarity (higher is better for positives)\n",
    "        pos_sim = F.pairwise_distance(anchor_embed, pos_embeds).mean()\n",
    "        neg_sim = F.pairwise_distance(anchor_embed, neg_embeds).mean()\n",
    "\n",
    "        # Compute triplet loss (maximize pos_sim, minimize neg_sim)\n",
    "        loss = F.relu(margin + neg_sim - pos_sim)  # Encourage higher pos_sim and lower neg_sim\n",
    "        triplet_losses.append(loss)\n",
    "\n",
    "    if triplet_losses:\n",
    "        return torch.stack(triplet_losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "'''\n",
    "\n",
    "def graph_reconstruction_loss(token_emb, token_edge_index, neg_sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    For each tokentoken edge, predict a similarity score (dot product)\n",
    "    and use MSE loss: target 1 for positives and 0 for negatives.\n",
    "    \"\"\"\n",
    "    # Positive edges\n",
    "    pos_scores = (token_emb[token_edge_index[0]] * token_emb[token_edge_index[1]]).sum(dim=-1)\n",
    "    pos_loss = F.mse_loss(pos_scores, torch.ones_like(pos_scores))\n",
    "    # Negative sampling: sample random token pairs (not necessarily true negatives)\n",
    "    num_neg = int(neg_sample_ratio * token_edge_index.size(1))\n",
    "    num_tokens = token_emb.size(0)\n",
    "    neg_edge_index = torch.randint(0, num_tokens, (2, num_neg), device=token_emb.device)\n",
    "    neg_scores = (token_emb[neg_edge_index[0]] * token_emb[neg_edge_index[1]]).sum(dim=-1)\n",
    "    neg_loss = F.mse_loss(neg_scores, torch.zeros_like(neg_scores))\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "def graph_regularization_loss(embeddings, edge_index, edge_type, num_nodes, negative_ratio=1, num_negatives=10):\n",
    "    \"\"\"\n",
    "    Regularization term that encourages connected nodes to have similar embeddings\n",
    "    based on cosine similarity, and pushes unconnected nodes further apart.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to regularize.\n",
    "        edge_index: The edge index tensor.\n",
    "        edge_type: The edge types tensor.\n",
    "        num_nodes: The total number of nodes in the graph.\n",
    "        negative_ratio: The ratio of negative edges to positive edges for sampling.\n",
    "        num_negatives: The number of negative edges to sample for each token node.\n",
    "    \"\"\"\n",
    "    # Extract source and target nodes from edge_index\n",
    "    mask_token_token = (edge_type == 1)\n",
    "\n",
    "    # Get the token-token edge index (positive edges)\n",
    "    edge_index_token_token = edge_index[:, mask_token_token]\n",
    "\n",
    "    src, tgt = edge_index_token_token\n",
    "\n",
    "    # Get embeddings for the source and target nodes\n",
    "    src_embeddings = embeddings[src]\n",
    "    tgt_embeddings = embeddings[tgt]\n",
    "\n",
    "    # Compute cosine similarity between source and target embeddings\n",
    "    similarity = F.cosine_similarity(src_embeddings, tgt_embeddings, dim=-1)\n",
    "\n",
    "    # Positive loss: Minimize dissimilarity (maximize similarity)\n",
    "    positive_loss = 1 - similarity\n",
    "\n",
    "    # Negative sampling: Select random pairs of non-connected nodes\n",
    "    all_nodes = torch.arange(num_nodes)\n",
    "    non_edges = []\n",
    "\n",
    "    for i in range(len(src)):\n",
    "        # For each token (src), we will sample 'num_negatives' negative edges\n",
    "        src_node = src[i]\n",
    "\n",
    "        # Sample 'num_negatives' random non-connected nodes\n",
    "        for _ in range(num_negatives):\n",
    "            random_node = torch.randint(0, num_nodes, (1,)).item()\n",
    "            while random_node in edge_index_token_token[0, :] or random_node == src_node:\n",
    "                random_node = torch.randint(0, num_nodes, (1,)).item()\n",
    "\n",
    "            non_edges.append((src_node, random_node))\n",
    "\n",
    "    non_edges = torch.tensor(non_edges).T\n",
    "\n",
    "    # Get embeddings for negative pairs\n",
    "    src_negative = embeddings[non_edges[0]]\n",
    "    tgt_negative = embeddings[non_edges[1]]\n",
    "\n",
    "    # Compute cosine similarity for negative pairs (should be small)\n",
    "    negative_similarity = F.cosine_similarity(src_negative, tgt_negative, dim=-1)\n",
    "\n",
    "    # Negative loss: Maximize dissimilarity (minimize similarity)\n",
    "    negative_loss = torch.relu(negative_similarity)\n",
    "\n",
    "    # Combine positive and negative losses\n",
    "    loss = positive_loss.mean() + negative_loss.mean() * negative_ratio\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lhe0IBjteEy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def document_token_contrastive_loss(embeddings, edge_index, edge_types, num_samples=10, temperature=0.5, num_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss between tokens and documents with positive and negative sampling.\n",
    "\n",
    "    embeddings: Tensor of shape [num_nodes, embedding_dim] for all nodes (tokens and documents).\n",
    "    edge_index: LongTensor of shape [2, num_edges] for all edges.\n",
    "    edge_types: LongTensor of shape [num_edges] indicating edge types (0 for document-to-token).\n",
    "    num_samples: Number of positive and negative samples to use for contrastive loss.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    num_tokens: Number of token nodes (tokens are the first `num_tokens` nodes in `embeddings`).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = embeddings[:num_tokens]\n",
    "    documents = embeddings[num_tokens:]\n",
    "    # Normalize token and document embeddings\n",
    "    tokens_norm = F.normalize(tokens, p=2, dim=-1)\n",
    "    documents_norm = F.normalize(documents, p=2, dim=-1)\n",
    "\n",
    "    # Cosine similarity between tokens and documents\n",
    "    logits = torch.matmul(tokens_norm, documents_norm.t()) / temperature  # [N_tokens, N_documents]\n",
    "\n",
    "    # Extract document-to-token edges\n",
    "    doc_token_edges = edge_index[:, edge_types == 0]  # [2, num_positive_edges]\n",
    "\n",
    "    # Initialize the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Get the list of positive edges\n",
    "    positive_edges = set(zip(doc_token_edges[0].tolist(), doc_token_edges[1].tolist()))\n",
    "\n",
    "    for i in range(num_tokens):\n",
    "        # Get the positive edges for the current token\n",
    "        positive_edges_for_token = [edge for edge in positive_edges if edge[1] == i]\n",
    "\n",
    "        if len(positive_edges_for_token) == 0:\n",
    "            continue  # If there are no positive edges for this token, skip it.\n",
    "\n",
    "        # Sample num_samples positive edges (sampling randomly from the positive edges)\n",
    "        positive_sampled = random.sample(positive_edges_for_token, min(num_samples, len(positive_edges_for_token)))\n",
    "\n",
    "        # For negative sampling, we want to sample edges that do not already exist\n",
    "        # Generate valid negative document-token pairs\n",
    "        all_documents = set(range(num_tokens, embeddings.size(0)))  # Documents indices: [num_tokens, ...]\n",
    "\n",
    "        # Sample negative edges\n",
    "        negative_sampled = []\n",
    "        # Randomly sample a document and token\n",
    "        neg_docs = list(all_documents)\n",
    "        random.shuffle(neg_docs)\n",
    "\n",
    "        for neg_doc in neg_docs:\n",
    "            # Check if the sampled edge already exists as a positive edge\n",
    "            if (neg_doc, i) not in positive_edges:\n",
    "                negative_sampled.append((neg_doc, i))\n",
    "                if len(negative_sampled) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        # Now we have `positive_sampled` and `negative_sampled`\n",
    "        # Concatenate positive and negative logits\n",
    "        positive_logits = logits[i, [edge[0] - num_tokens for edge in positive_sampled]]  # directly index logits\n",
    "        negative_logits = logits[i, [edge[0] - num_tokens for edge in negative_sampled]]  # directly index logits\n",
    "\n",
    "        # Create the corresponding labels: 1 for positive, 0 for negative\n",
    "        pos_labels = torch.ones(len(positive_logits)).to(tokens.device)\n",
    "        neg_labels = torch.zeros(len(negative_logits)).to(tokens.device)\n",
    "\n",
    "        # Combine logits and labels\n",
    "        combined_logits = torch.cat([positive_logits, negative_logits], dim=0)\n",
    "        combined_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        # Compute contrastive loss using cross-entropy\n",
    "        loss = F.cross_entropy(combined_logits, combined_labels)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def token_token_contrastive_loss(embeddings, edge_index, edge_types, num_samples=10, temperature=0.5, num_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss between tokens using positive and negative sampling for token-token edges.\n",
    "\n",
    "    embeddings: Tensor of shape [num_nodes, embedding_dim] for all nodes (tokens and documents).\n",
    "    edge_index: LongTensor of shape [2, num_edges] for all edges.\n",
    "    edge_types: LongTensor of shape [num_edges] indicating edge types (1 for token-to-token).\n",
    "    num_samples: Number of positive and negative samples to use for contrastive loss.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    num_tokens: Number of token nodes (tokens are the first `num_tokens` nodes in `embeddings`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract token embeddings\n",
    "    tokens = embeddings[:num_tokens]\n",
    "\n",
    "    # Normalize token embeddings\n",
    "    tokens_norm = F.normalize(tokens, p=2, dim=-1)\n",
    "\n",
    "    # Cosine similarity between tokens (token-token similarity)\n",
    "    logits = torch.matmul(tokens_norm, tokens_norm.t()) / temperature  # [N_tokens, N_tokens]\n",
    "\n",
    "    # Extract token-to-token edges (positive edges for token-token pairs)\n",
    "    token_token_edges = edge_index[:, edge_types == 1]  # [2, num_token_token_edges]\n",
    "\n",
    "    # Initialize the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Get the list of positive edges\n",
    "    positive_edges = set(zip(token_token_edges[0].tolist(), token_token_edges[1].tolist()))\n",
    "\n",
    "    for i in range(num_tokens):\n",
    "        # Get the positive edges for the current token\n",
    "        positive_edges_for_token = [edge for edge in positive_edges if edge[0] == i]\n",
    "\n",
    "        if len(positive_edges_for_token) == 0:\n",
    "            continue  # If there are no positive edges for this token, skip it.\n",
    "\n",
    "        # Sample num_samples positive edges (sampling randomly from the positive edges)\n",
    "        positive_sampled = random.sample(positive_edges_for_token, min(num_samples, len(positive_edges_for_token)))\n",
    "\n",
    "        # For negative sampling, we want to sample token-token pairs that do not already exist\n",
    "        # Sample valid token-token pairs\n",
    "        all_tokens = set(range(num_tokens))  # Token indices: [0, ..., num_tokens-1]\n",
    "\n",
    "        # Sample negative edges\n",
    "        negative_sampled = []\n",
    "        # Randomly sample a token pair\n",
    "        neg_tokens = list(all_tokens)\n",
    "        random.shuffle(neg_tokens)\n",
    "\n",
    "\n",
    "        for neg_tok in neg_tokens:\n",
    "            # Check if the sampled edge already exists as a positive edge\n",
    "            if (i, neg_tok) not in positive_edges:\n",
    "                negative_sampled.append((i, neg_tok))\n",
    "                if len(negative_sampled) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        # Now we have `positive_sampled` and `negative_sampled`\n",
    "        # Concatenate positive and negative logits\n",
    "        positive_logits = logits[torch.tensor([edge[0] for edge in positive_sampled]), torch.tensor([edge[1] for edge in positive_sampled])].to(tokens.device)\n",
    "        negative_logits = logits[torch.tensor([edge[0] for edge in negative_sampled]), torch.tensor([edge[1] for edge in negative_sampled])].to(tokens.device)\n",
    "\n",
    "        # Create the corresponding labels: 1 for positive, 0 for negative\n",
    "        pos_labels = torch.ones(len(positive_logits)).to(tokens.device)\n",
    "        neg_labels = torch.zeros(len(negative_logits)).to(tokens.device)\n",
    "\n",
    "        # Combine logits and labels\n",
    "        combined_logits = torch.cat([positive_logits, negative_logits], dim=0)\n",
    "        combined_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        # Compute contrastive loss using cross-entropy\n",
    "        loss = F.cross_entropy(combined_logits, combined_labels)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zrOYKhODu-N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(embeddings, positive_mask, temperature=0.5):\n",
    "    \"\"\"\n",
    "    NT-Xent loss for contrastive learning.\n",
    "\n",
    "    Args:\n",
    "      embeddings (Tensor): shape [N, D], where N is the number of nodes and D is the embedding dimension.\n",
    "      positive_mask (Tensor): Boolean tensor of shape [N, N] where positive_mask[i, j] is True if sample i and sample j form a positive pair.\n",
    "      temperature (float): Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "      loss (Tensor): A scalar tensor representing the NT-Xent loss.\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    norm_embeddings = F.normalize(embeddings, p=2, dim=1)  # [N, D]\n",
    "\n",
    "    # Compute cosine similarity matrix: [N, N]\n",
    "    sim_matrix = torch.matmul(norm_embeddings, norm_embeddings.t())  # cosine similarity\n",
    "\n",
    "    # Scale the similarities by temperature\n",
    "    sim_matrix = sim_matrix / temperature  # [N, N]\n",
    "\n",
    "    # To avoid trivial self-comparison, set the diagonal to a very negative value.\n",
    "    # This ensures that the self-similarity does not contribute to the softmax.\n",
    "    diag_mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=sim_matrix.device)\n",
    "    sim_matrix = sim_matrix.masked_fill(diag_mask, -9e15)\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_sim = torch.exp(sim_matrix)  # [N, N]\n",
    "\n",
    "    # For each sample, compute numerator: sum of exp(similarity) over positive pairs.\n",
    "    # positive_mask is a Boolean mask [N, N]. Convert it to float.\n",
    "    pos_exp = exp_sim * positive_mask.float()  # [N, N]\n",
    "    positive_sum = pos_exp.sum(dim=1)  # [N]\n",
    "\n",
    "    # Denominator: sum over all (negative and positive) samples for each row.\n",
    "    negative_sum = exp_sim.sum(dim=1)  # [N]\n",
    "\n",
    "    # Avoid division by zero (if no positive exists, this should be handled appropriately)\n",
    "    eps = 1e-8\n",
    "    loss_per_sample = -torch.log((positive_sum + eps) / (negative_sum + eps))  # [N]\n",
    "\n",
    "    # Average over all samples\n",
    "    loss = loss_per_sample.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def nt_xent_loss_with_euclidean(embeddings, positive_mask, temperature=0.5):\n",
    "    \"\"\"\n",
    "    NT-Xent loss for contrastive learning using Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        embeddings (Tensor): shape [N, D], where N is the number of nodes and D is the embedding dimension.\n",
    "        positive_mask (Tensor): Boolean tensor of shape [N, N] where positive_mask[i, j] is True if sample i and sample j form a positive pair.\n",
    "        temperature (float): Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "        loss (Tensor): A scalar tensor representing the NT-Xent loss.\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    norm_embeddings = F.normalize(embeddings, p=2, dim=1)  # [N, D]\n",
    "\n",
    "    # Compute pairwise Euclidean distance: [N, N]\n",
    "    # Euclidean distance: dist(x, y) = sqrt(sum((x_i - y_i)^2))\n",
    "    dist_matrix = torch.cdist(norm_embeddings, norm_embeddings, p=2)  # [N, N]\n",
    "\n",
    "    # Scale the distances by temperature\n",
    "    dist_matrix = dist_matrix / temperature  # [N, N]\n",
    "\n",
    "    # To avoid trivial self-comparison, set the diagonal to a very large value.\n",
    "    # This ensures that the self-similarity does not contribute to the softmax.\n",
    "    diag_mask = torch.eye(dist_matrix.size(0), dtype=torch.bool, device=dist_matrix.device)\n",
    "    dist_matrix = dist_matrix.masked_fill(diag_mask, 9e15)\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_dist = torch.exp(-dist_matrix)  # [N, N] (negative because smaller distance means more similar)\n",
    "\n",
    "    # For each sample, compute numerator: sum of exp(-dist) over positive pairs.\n",
    "    pos_exp = exp_dist * positive_mask.float()  # [N, N]\n",
    "    positive_sum = pos_exp.sum(dim=1)  # [N]\n",
    "\n",
    "    # Denominator: sum over all (negative and positive) samples for each row.\n",
    "    negative_sum = exp_dist.sum(dim=1)  # [N]\n",
    "\n",
    "    # Avoid division by zero (if no positive exists, this should be handled appropriately)\n",
    "    eps = 1e-8\n",
    "    loss_per_sample = -torch.log((positive_sum + eps) / (negative_sum + eps))  # [N]\n",
    "\n",
    "    # Average over all samples\n",
    "    loss = loss_per_sample.mean()\n",
    "    return loss\n",
    "\n",
    "def create_positive_mask(num_nodes, edges_document_token, edges_token_token):\n",
    "    mask = torch.zeros((num_nodes, num_nodes), dtype=bool)\n",
    "    for i in range(edges_document_token.shape[1]):\n",
    "        e = edges_document_token[:, i]\n",
    "        if e[0] < num_nodes and e[1] < num_nodes:\n",
    "            mask[e[0]][e[1]] = True\n",
    "    for i in range(edges_document_token.shape[1]):\n",
    "        e = edges_document_token[:, i]\n",
    "        mask[e[0]][e[1]] = True\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPOwrmg6AKFj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "data['paper'].x = ... # [num_papers, num_features_paper]\n",
    "data['author'].x = ... # [num_authors, num_features_author]\n",
    "data['institution'].x = ... # [num_institutions, num_features_institution]\n",
    "data['field_of_study'].x = ... # [num_field, num_features_field]\n",
    "\n",
    "data['paper', 'cites', 'paper'].edge_index = ... # [2, num_edges_cites]\n",
    "data['author', 'writes', 'paper'].edge_index = ... # [2, num_edges_writes]\n",
    "data['author', 'affiliated_with', 'institution'].edge_index = ... # [2, num_edges_affiliated]\n",
    "data['paper', 'has_topic', 'field_of_study'].edge_index = ... # [2, num_edges_topic]\n",
    "\n",
    "data['paper', 'cites', 'paper'].edge_attr = ... # [num_edges_cites, num_features_cites]\n",
    "data['author', 'writes', 'paper'].edge_attr = ... # [num_edges_writes, num_features_writes]\n",
    "data['author', 'affiliated_with', 'institution'].edge_attr = ... # [num_edges_affiliated, num_features_affiliated]\n",
    "data['paper', 'has_topic', 'field_of_study'].edge_attr = ... # [num_edges_topic, num_features_topic]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnLnhISYWLZO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "#positive_mask = create_positive_mask(node_features.shape[0], edges_document_token, edges_token_token)\n",
    "\n",
    "def l1_reg(model, lambda_l1=0.01):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    return lambda_l1 * l1_norm\n",
    "\n",
    "def train(model, data, num_epochs=1500, learning_rate=1e-1, lambda_reg=1e-2, lambda_mse = 1e-4, num_input_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Trains the MultiLayerSeparateGAT model.\n",
    "\n",
    "    Arguments:\n",
    "        model: The MultiLayerSeparateGAT model.\n",
    "        data: A PyTorch Geometric Data object containing node features, edge_index, and edge_types.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        lambda_reg: Weight for the graph regularization loss.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    bce = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Forward pass\n",
    "        out = model.encode(data.x, data.edge_index, data.edge_type)\n",
    "        sampled_links, labels = sample_links(data.edge_index.cpu(), data.edge_type.cpu())\n",
    "\n",
    "        out = model.decode(out, sampled_links.to(out.device))\n",
    "\n",
    "        # Compute the final loss (here assuming no labels, just regularization)\n",
    "        loss = bce(out, labels.to(out.device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the training loss\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is a PyTorch Geometric Data object with the necessary fields: x, edge_index, edge_type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = DocTokenGNN(in_channels=node_features.shape[1], out_channels=128).to(device)\n",
    "#model = SeparateGAT(in_channels=node_features.shape[1], out_channels=8, heads=8, edge_types=edge_types).to(device)\n",
    "model = HAGN(in_channels=node_features.shape[1], out_channels=32, heads=8).to(device)\n",
    "trained_model = train(model, graph_data.to(device), num_epochs=150, learning_rate=1e-3, lambda_reg=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ5TKwJzVpWq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_embeddings(embeddings, labels=None, method='t-SNE', input_tokens=None):\n",
    "    \"\"\"\n",
    "    Visualizes the embeddings using either PCA or t-SNE, and optionally adds labels from `input_tokens`.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to visualize.\n",
    "        labels: The labels for coloring the embeddings (optional).\n",
    "        method: The dimensionality reduction method ('PCA' or 't-SNE').\n",
    "        input_tokens: A list of token labels (words) to display next to the points.\n",
    "    \"\"\"\n",
    "    # Apply dimensionality reduction (PCA or t-SNE)\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    elif method == 't-SNE':\n",
    "        tsne = TSNE(n_components=2)\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the reduced embeddings\n",
    "    plt.figure(figsize=(100, 80))\n",
    "\n",
    "    if labels is not None:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', s=30)\n",
    "        plt.colorbar()\n",
    "    else:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=30)\n",
    "\n",
    "    # Add labels from input_tokens\n",
    "    if input_tokens is not None:\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], token, fontsize=9, ha='center', va='center')\n",
    "\n",
    "    plt.title(f\"Node Embeddings Visualization ({method})\")\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after training\n",
    "# Assuming `trained_model` and `graph_data` are defined\n",
    "# Extract the token embeddings (assuming the first 354 nodes are tokens)\n",
    "#embeddings = trained_model(graph_data.x, graph_data.edge_index, graph_data.edge_type)\n",
    "#embeddings = trained_model.get_embeddings()\n",
    "#token_embeddings = embeddings.cpu().detach().numpy()  # Extract token embeddings (first 354 nodes are tokens)\n",
    "\n",
    "token_embeddings = n2v.get_embeddings().cpu().detach().numpy()\n",
    "# Assuming `input_tokens` is a list of tokens (words) corresponding to the first 354 tokens\n",
    "visualize_embeddings(token_embeddings, method='t-SNE', input_tokens=input_tokens)  # Or method='t-SNE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxwAxj4xUf_-"
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cD4QDHBIJFsf"
   },
   "outputs": [],
   "source": [
    "class LearnableDistance(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(num_features))  # Learnable weights for each feature\n",
    "\n",
    "    def forward(self, node1_features, node2_features):\n",
    "        distance = torch.sum(self.weights * (node1_features - node2_features) ** 2, dim=-1)\n",
    "        return torch.exp(-distance)  # Convert to similarity score (or edge weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWLFe1YAClPM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reduce vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0ExKGZEJeVt"
   },
   "outputs": [],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPBrw4ITCjvY"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sequences)):\n",
    "    sequences[i] = sequences[i].replace(\"VINb\", \"VIN\")\n",
    "pattern = r'\\b(TELHA|TELA|OVIS|SUS|CAP|EQU|OLE|GRA|CYP|BOS|TUN|OLIV|AROM|ROTA)\\w*'\n",
    "sequences = [re.sub(pattern, r'\\1', seq) for seq in sequences]\n",
    "\n",
    "pattern2 = r'\\*\\d+VAS\\S*'\n",
    "sequences = [re.sub(pattern2, 'VAS', seq) for seq in sequences]\n",
    "\n",
    "pattern3 = r'\\b(Z|V|T|S|Q|P|N|M|L)\\b'\n",
    "sequences = [re.sub(pattern3, 'MEASURE_UNIT', seq) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWf5up5PGTaG"
   },
   "outputs": [],
   "source": [
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9y6ZIZPD-7K"
   },
   "outputs": [],
   "source": [
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing = create_missing_dataset(sequences)\n",
    "sequences, missing = clean_datasets(sequences, missing)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(missing, sequences)\n",
    "#Get input and output tokens to use for the encoding: input has all signs + <SOS> and <PAD>, output has <EOS> and <PAD>, all numbers different from 1 and 2 become  <NUM> for simplicity reasons.Space < > also needs to be encoded\n",
    "signs_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "df_lb = pd.read_csv(signs_path)\n",
    "signs = df_lb['sign'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18HEQ-uaXmM8"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = set()\n",
    "for s in signs:\n",
    "    if not s.isdigit():\n",
    "        if not \"?\" in s:\n",
    "            if \"VIN\" in s:\n",
    "                tokens.add(\"VIN\")\n",
    "            elif \"VAS\" in s:\n",
    "                tokens.add(\"VAS\")\n",
    "            elif s == 'M' or s == 'N' or s == 'P' or s=='Q' or s=='L' or s=='Z' or s=='V' or s=='T' or s=='S':\n",
    "                tokens.add(\"MEASURE_UNIT\")\n",
    "            else:\n",
    "                tokens.add(s)\n",
    "    elif s == '1' or s == '2':\n",
    "      tokens.add(\"1\")\n",
    "      tokens.add(\"2\")\n",
    "    else:\n",
    "        tokens.add(\"NUM\")\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "input_tokens = [\" \"] + tokens\n",
    "output_tokens = [\" \"] + tokens\n",
    "input_tokens.sort()\n",
    "output_tokens.sort()\n",
    "# last one is the unknown symbol\n",
    "input_tokens = [\"PAD\", \"SOS\", \"EOS\"] + input_tokens\n",
    "output_tokens = [\"PAD\", \"SOS\", \"EOS\"] + output_tokens\n",
    "\n",
    "input_tokens.extend([\"SYL\", \"LOG\", \"NUMERAL\",\"?\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeshvmB2jRqU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Phonem2Vec\n",
    "To get phonetic embeddings we train in an unsupervised learning manner an lstm. We will use only \"encode\" method to get the latent embeddings. The key aspect is that we need this autoencoder to produce quality embeddings, so we don't really care about the result, more that similar sounding syllables have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "myhr-5l-jXAy"
   },
   "outputs": [],
   "source": [
    "class SyllableAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size=None, embedding_dim=None, hidden_dim=None, latent_dim=None, charsIndexes=None, verbose=False):\n",
    "        super(SyllableAutoencoder, self).__init__()\n",
    "        if not vocab_size or not embedding_dim or not hidden_dim or not latent_dim:\n",
    "            if verbose:\n",
    "                print(\"Missing parameters for autoencoder\")\n",
    "            return\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        #map hidden state to latent vector\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "        #output projection for each time step to vocabulary size (for reconstruction)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.charsIndexes = charsIndexes #vocabulary of the SyllableAutoencoder\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.encoder_rnn(packed)\n",
    "        if isinstance(hidden, tuple): #we don't need wider context\n",
    "            hidden = hidden[0]\n",
    "        last_hidden = hidden[-1]\n",
    "        latent = self.fc_mu(last_hidden)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent, target_seq, lengths):\n",
    "        #prepare the initial hidden state for decoder from latent vector\n",
    "        hidden_init = self.fc_latent(latent).unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        #embed the target sequence (teacher forcing during training)\n",
    "        embedded = self.embedding(target_seq)\n",
    "\n",
    "        #initialize the cell state as zeros (or you could use hidden_init too)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        #combine into a tuple for\n",
    "        hidden_init = (hidden_init, cell_init)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.decoder_rnn(packed, hidden_init)\n",
    "        decoder_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        logits = self.output_projection(decoder_out)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        latent = self.encode(x, lengths)\n",
    "        #For training, use the input sequence as target (teacher forcing)\n",
    "        logits = self.decode(latent, x, lengths)\n",
    "        return logits, latent\n",
    "\n",
    "    def get_probabilities(self, logits):\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    #Given syllable, returns its tensor one-hot-encoded representation\n",
    "    def syl2tensor(self, syl):\n",
    "        return torch.tensor([self.charsIndexes[c] for c in syl], dtype=torch.long).unsqueeze(0), torch.tensor([len(syl)])\n",
    "\n",
    "    #Given list of syllables, produces dictionary <syllable> : <embedding>\n",
    "    def get_embeddings(self, syllables, vocab=None): #original vocabulary of transformer\n",
    "\n",
    "      #Get mapping <syllable> : <one-hot-encoded-syllable> from original vocabulary, positions in the list are the encodings\n",
    "      if vocab != None:\n",
    "        mapping = {}\n",
    "        count = 0\n",
    "        for tok in vocab:\n",
    "          mapping[tok] = count\n",
    "          count += 1\n",
    "\n",
    "      embeddings_per_syllable = {}\n",
    "      for syl in syllables:\n",
    "        syl_, length = self.syl2tensor(syl)\n",
    "        out = self.encode(syl_, length).squeeze(0).detach().numpy()\n",
    "        syl = syl.replace(\"0\", \"\") #remove padding when saving\n",
    "\n",
    "        #Returns representation <one-hot-encoded-syllable> : <embedding>\n",
    "        if vocab != None:\n",
    "          syl = mapping[syl]\n",
    "\n",
    "        embeddings_per_syllable[syl] = out\n",
    "\n",
    "      return embeddings_per_syllable\n",
    "\n",
    "    def save_embeddings(self, filename, syllables, prefix_path='./DMPROJECT/Embeddings/', tokenList=None):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      phonetic_embeddings = self.get_embeddings(syllables, tokenList)\n",
    "      np.save(embeddings_path, phonetic_embeddings)\n",
    "\n",
    "    def load_embeddings(self, filename, prefix_path='./DMPROJECT/Embeddings/'):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      return np.load(embeddings_path, allow_pickle=True).item()\n",
    "\n",
    "\n",
    "class SyllableDataset(Dataset):\n",
    "    def __init__(self, syllables, char2idx):\n",
    "        self.syllables = syllables\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.syllables)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        syll = self.syllables[idx]\n",
    "        #Convert each character to its corresponding index\n",
    "        token_indices = [self.char2idx[c] for c in syll]\n",
    "        length = len(token_indices)\n",
    "        return torch.tensor(token_indices, dtype=torch.long), length\n",
    "\n",
    "def collate_fn_syl(batch):\n",
    "    sequences, lengths = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return sequences_padded, torch.tensor(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y3WyM7FIjhX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 3.2698\n",
      "Epoch 51/300, Loss: 0.1675\n",
      "Epoch 101/300, Loss: 0.0392\n",
      "Epoch 151/300, Loss: 0.0174\n",
      "Epoch 201/300, Loss: 0.0098\n",
      "Epoch 251/300, Loss: 0.0062\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "charsIndexes = {'0': 0,'a': 1,'b': 2,'c': 3,'d': 4,'e': 5,'f': 6,'g': 7,'h': 8,'i': 9,'j': 10,'k': 11,'l': 12,'m': 13,'n': 14,'o': 15,'p': 16,'q': 17,'r': 18,'s': 19,'t': 20,'u': 21,'v': 22,'w': 23,'x': 24,'y': 25,'z': 26}\n",
    "indexesChars = {v: k for k, v in charsIndexes.items()} #reverse mapping\n",
    "syllables = [] #dataset for unsupervised learning, '0' is padding\n",
    "\n",
    "#Get syllable tokens\n",
    "for tok in input_tokens:\n",
    "  if not tok.isupper() and not tok.isnumeric() and not '?' in tok and not '+' in tok and not '*' in tok and not 'separatum' in tok and not 'BOS' in tok and not 'CAP' in tok and not 'EQU' in tok and not 'OVIS' in tok and not 'SUS' in tok and not 'TELA' in tok and not 'VIN' in tok and not ' ' in tok:\n",
    "    syllables.append(tok)\n",
    "max_len_syl = max(len(syl) for syl in syllables)\n",
    "#Pad tokens to max length (3)\n",
    "for i in range(len(syllables)):\n",
    "  if len(syllables[i]) < max_len_syl:\n",
    "    syllables[i] += '0'*(max_len_syl-len(syllables[i]))\n",
    "\n",
    "#get dataset\n",
    "dataset = SyllableDataset(syllables, charsIndexes)\n",
    "data_loader_syl = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_syl)\n",
    "\n",
    "#define autoencoder\n",
    "vocab_size = len(charsIndexes)\n",
    "embedding_dim =  16\n",
    "hidden_dim = 32\n",
    "latent_dim = EMBEDDINGS_DIM #THIS IS THE SIZE THAT MUST BE EQUAL TO TRANSFORMER VECTOR, Dimension of encoder result\n",
    "\n",
    "syllable_autoencoder = SyllableAutoencoder(vocab_size, embedding_dim, hidden_dim, latent_dim, charsIndexes) ##IMPORTANT, this is the autoencoder model used later to create and save the embeddings\n",
    "optimizer = optim.Adam(syllable_autoencoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=charsIndexes['0'])\n",
    "\n",
    "#Train autoencoder\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch, lengths in data_loader_syl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, latent = syllable_autoencoder(batch, lengths)\n",
    "        logits_flat = logits.view(-1, vocab_size)  # shape: (batch_size*seq_length, vocab_size)\n",
    "        targets_flat = batch.view(-1)              # shape: (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader_syl)\n",
    "    if epoch % 50 == 0:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cCOLXMoi3m5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth & Predicted: \n",
      "[['m', '0', '0'], ['r', 'e', '0'], ['q', 's', '0'], ['w', 'o', '0'], ['t', 'u', '0'], ['m', 'i', '0'], ['n', 'a', '0'], ['n', 'e', '0']]\n",
      "[['m', 'o', 'o'], ['r', 'e', 'o'], ['q', 's', 'o'], ['w', 'o', 'o'], ['t', 'u', 'o'], ['m', 'i', 'i'], ['n', 'a', 'o'], ['n', 'e', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['t', 'o', '0'], ['j', 'a', '0'], ['s', 'i', '0'], ['d', 'a', '0'], ['k', 'a', '0'], ['k', 'o', '0'], ['t', 'a', '0'], ['q', 'i', '0']]\n",
      "[['t', 'o', 'o'], ['j', 'a', 'o'], ['s', 'i', 'i'], ['d', 'a', 'o'], ['k', 'a', 'o'], ['k', 'o', 'o'], ['t', 'a', 'o'], ['q', 'i', 'i']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['n', 'i', '0'], ['w', 'e', '0'], ['d', 'w', 'e'], ['m', 'e', '0'], ['t', 'e', '0'], ['f', '0', '0'], ['o', '0', '0'], ['r', 'i', '0']]\n",
      "[['n', 'i', 'i'], ['w', 'e', 'o'], ['d', 'w', 'e'], ['m', 'e', 'o'], ['t', 'e', 'o'], ['f', 'f', 'o'], ['o', 'o', 'o'], ['r', 'i', 'i']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['i', '0', '0'], ['z', 'o', '0'], ['a', '0', '0'], ['p', 'h', 'u'], ['d', 'e', '0'], ['m', 'u', '0'], ['p', 'e', '0'], ['s', 'e', '0']]\n",
      "[['i', 'i', 'i'], ['z', 'o', 'o'], ['a', 'o', 'o'], ['p', 'h', 'u'], ['d', 'e', 'o'], ['m', 'u', 'o'], ['p', 'e', 'o'], ['s', 'e', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['j', 'o', '0'], ['z', 'a', '0'], ['h', 'a', '0'], ['d', 'u', '0'], ['k', 'u', '0'], ['n', 'o', '0'], ['j', 'e', '0'], ['p', 't', 'e']]\n",
      "[['j', 'o', 'o'], ['z', 'a', 'o'], ['h', 'a', 'o'], ['d', 'u', 'o'], ['k', 'u', 'o'], ['n', 'o', 'o'], ['j', 'e', 'o'], ['p', 't', 'e']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['r', 'a', 'i'], ['s', 'u', '0'], ['p', 'a', '0'], ['n', 'u', '0'], ['e', '0', '0'], ['q', 'o', '0'], ['t', 'w', 'e'], ['p', 'u', '0']]\n",
      "[['r', 'a', 'i'], ['s', 'u', 'o'], ['p', 'a', 'o'], ['n', 'u', 'o'], ['e', 'o', 'o'], ['q', 'o', 'o'], ['t', 'w', 'e'], ['p', 'u', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['u', '0', '0'], ['q', 'e', '0'], ['k', 'e', '0'], ['r', 'y', 'o'], ['w', 'a', '0'], ['m', 'o', '0'], ['d', 'w', 'o'], ['s', 'o', '0']]\n",
      "[['u', 'o', 'o'], ['q', 'e', 'o'], ['k', 'e', 'o'], ['r', 'y', 'o'], ['w', 'a', 'o'], ['m', 'o', 'o'], ['d', 'w', 'o'], ['s', 'o', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['a', 'u', '0'], ['r', 'u', '0'], ['t', 'y', 'a'], ['t', 'i', '0'], ['r', 'y', 'a'], ['t', 'w', 'o'], ['z', 'e', '0'], ['p', 'o', '0']]\n",
      "[['a', 'u', 'o'], ['r', 'u', 'o'], ['t', 'y', 'a'], ['t', 'i', 'i'], ['r', 'y', 'a'], ['t', 'w', 'o'], ['z', 'e', 'o'], ['p', 'o', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['d', 'o', '0'], ['m', 'u', 't'], ['s', 'a', '0'], ['q', 'a', '0'], ['n', 'w', 'a'], ['k', 'i', '0'], ['r', 'o', '0'], ['m', 'a', '0']]\n",
      "[['d', 'o', 'o'], ['m', 'u', 't'], ['s', 'a', 'o'], ['q', 'a', 'o'], ['n', 'w', 'a'], ['k', 'i', 'i'], ['r', 'o', 'o'], ['m', 'a', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['r', 'a', '0'], ['p', 'i', '0'], ['d', 'i', '0'], ['a', 'i', '0'], ['w', 'i', '0']]\n",
      "[['r', 'a', 'o'], ['p', 'i', 'i'], ['d', 'i', 'i'], ['a', 'i', 'i'], ['w', 'i', 'i']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test just to see if it works as autoencoder, ignores padding, input == output for good autoencoding\n",
    "with torch.no_grad():\n",
    "  for batch, lengths in data_loader_syl:\n",
    "    logits, latent = syllable_autoencoder(batch, lengths)\n",
    "    g_truth = list(batch.detach().numpy())\n",
    "    for el in range(len(g_truth)):\n",
    "      g_truth[el] = [indexesChars[i] for i in g_truth[el]]\n",
    "    probs = syllable_autoencoder.get_probabilities(logits) #(batch, syllable, char)\n",
    "    predicted_indices = list(torch.argmax(probs, dim=-1).detach().numpy())\n",
    "    for el in range(len(predicted_indices)):\n",
    "      predicted_indices[el] = [indexesChars[i] for i in predicted_indices[el]]\n",
    "    print(\"GroundTruth & Predicted: \")\n",
    "    print(g_truth)\n",
    "    print(predicted_indices)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TDQ_yo33w7KX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['da', 'de', 'di', 'do', 'du', 'dwe', 'dwo']\n",
      "Cluster 1: ['i']\n",
      "Cluster 2: ['m', 'ma', 'me', 'mi', 'mo', 'mu', 'mut']\n",
      "Cluster 3: ['rya', 'ryo']\n",
      "Cluster 4: ['pa', 'pe', 'phu', 'pi', 'po', 'pte', 'pu']\n",
      "Cluster 5: ['qa', 'qe', 'qi', 'qo']\n",
      "Cluster 6: ['sa', 'se', 'si', 'so', 'su']\n",
      "Cluster 7: ['na', 'ne', 'ni', 'no', 'nu', 'nwa']\n",
      "Cluster 8: ['ta', 'te', 'ti', 'to', 'tu', 'twe', 'two', 'tya']\n",
      "Cluster 9: ['ja', 'je', 'jo']\n",
      "Cluster 10: ['za', 'ze', 'zo']\n",
      "Cluster 11: ['ka', 'ke', 'ki', 'ko', 'ku']\n",
      "Cluster 12: ['f']\n",
      "Cluster 13: ['a', 'u']\n",
      "Cluster 14: ['ha']\n",
      "Cluster 15: ['wa', 'we', 'wi', 'wo']\n",
      "Cluster 16: ['e', 'o']\n",
      "Cluster 17: ['ai', 'au']\n",
      "Cluster 18: ['qs']\n",
      "Cluster 19: ['ra', 'rai', 're', 'ri', 'ro', 'ru']\n"
     ]
    }
   ],
   "source": [
    "#Assess quality of embeddings through clustering\n",
    "from sklearn.cluster import KMeans\n",
    "lengths = [3] #each syllable is always of len 3\n",
    "embeddings_per_syllable = syllable_autoencoder.get_embeddings(syllables)\n",
    "\n",
    "syllable_keys = list(embeddings_per_syllable.keys())\n",
    "embedding_list = [embeddings_per_syllable[s] for s in syllable_keys]\n",
    "\n",
    "\n",
    "num_clusters = 20\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embedding_list)\n",
    "\n",
    "for cluster_id in range(num_clusters): #print clusters\n",
    "    cluster_syllables = [syllable_keys[i] for i in range(len(syllable_keys)) if clusters[i] == cluster_id]\n",
    "    print(f\"Cluster {cluster_id}: {cluster_syllables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "COyTr8eNCphK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1386787/2456792464.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAMWCAYAAAADI47PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde1zUVf4/8NdnuAzDwHDRAW8jMgIJCGqQhbprbJJ2Yauldrt4q6xU1LztfmnL9Yqsqe0Wi5fctqUsaxNz9yeKd5elMiuashAF5VIKiSLXkbl+fn/MMus4gwLOcNHX8/H4PIzzOZ9zzmduzXvOTRBFUQQREREREREROZWkuxtAREREREREdDNiwE1ERERERETkAgy4iYiIiIiIiFyAATcRERERERGRCzDgJiIiIiIiInIBBtxERERERERELsCAm4iIiIiIiMgFGHATERERERERuQADbiIiIiIiIiIXYMBN1EMNGTIE06dP7+5mdMrf//53CIKA8vLy7m7KNX3xxRcYM2YM5HI5BEGARqPpsrqvfn6PHDkCQRBw5MiRDpc1ffp0+Pj4tCuvIAhYtmxZh+vobq2vqS+//NLldU2fPh1Dhgy5br7y8nIIgoC///3v1rRly5ZBEATXNc4JmpqaEBQUhPfee6+7m9Lrddf7qSteZ3l5efDx8UFNTY1L6yEiutkx4CbqYqdPn8YLL7wAtVoNLy8vKBQKjB07Fq+//jouX77cJW3QarVYtmxZp4K7m4XBYMBjjz2G2tpa/OlPf8K7776LkJCQNvOXl5fj6aefxtChQ+Hl5YV+/frh5z//OZYuXdqFre55hgwZAkEQHB6TJk3q7uaRA6+//jp8fX3x+OOPW9N2797d4cCxpqYGL774IoYNGwaZTIagoCCMHj0a//d//4empiZrvunTp0MQBMTGxkIURbtyBEHAnDlzrH+3/pDR1vHHP/7xmu1qDUZbD29vb0RFReGVV15BQ0NDh+6xO3X35/SkSZMQFhaGjIyMbqmfiOhm4d7dDSC6leTm5uKxxx6DVCrF1KlTMXz4cOj1ehQUFOC3v/0tvv/+e7z55psub4dWq8Xy5csBAHfffbfTy58yZQoef/xxSKVSp5ftLKdPn0ZFRQW2bNmCGTNmXDNvaWkp7rjjDshkMjzzzDMYMmQIqqqqUFhYiDVr1lgfy1vVyJEjsWjRIrv0AQMGdENrutcrr7yCtLS07m5GmwwGA15//XUsWLAAbm5u1vTdu3cjKyur3UF3bW0t4uPj0dDQgGeeeQbDhg3DxYsX8e2332Ljxo2YNWuW3aiL48ePY8eOHUhJSWlXHU888QTuv/9+u/RRo0a16/qNGzfCx8cHTU1N2LdvH9LT03Ho0CF88sknTusdvnz5MtzdXfNV6lqf0131OnvhhRewePFiLF++HL6+vi6vj4joZsSAm6iLlJWV4fHHH0dISAgOHTqE/v37W8+lpqaitLQUubm53djCG9fc3Ay5XA43NzebL/M90fnz5wEA/v7+1837pz/9CU1NTdBoNHa94K3l3MoGDhyIyZMnd3czegR3d3eXBWDOsGvXLtTU1ODXv/71DZXz1ltvobKyEp988gnGjBljc66hoQGenp42aTKZDCqVCitWrMCvfvWrdgW8t99++w29rh599FH07dsXADBz5kykpKRgx44dOHr0KBISEhxeo9Vq4e3t3e46vLy8Ot2+G9FVr7OUlBTMnTsXH330EZ555hmX10dEdDPikHKiLvLqq6+iqakJb731lk2w3SosLAwvvvhim9e3NWfP0XzpL7/8EhMnTkTfvn0hk8kQGhpq/bJUXl4OpVIJAFi+fLl12OWVPVvFxcV49NFHERgYCC8vL8THx+Nf//qXw3r//e9/Y/bs2QgKCsKgQYPabNOQIUPw4IMPoqCgAKNHj4aXlxfUajXeeecdu3v69ttvMX78eMhkMgwaNAirVq3C22+/3e554YcOHcLPfvYzyOVy+Pv746GHHsKJEyes56dPn47x48cDAB577DEIgnDNnv7Tp09j0KBBDoecBwUFWf972rRp6Nu3LwwGg12+e++9F7fddtt1236l//znP3jssccwePBgSKVSqFQqLFiwoM2pB2fOnMHEiRMhl8sxYMAArFixwuEQ3qudPXsWzzzzDIKDgyGVShEdHY2//e1vHWrr9bTOM6+srMSDDz4IHx8fDBw4EFlZWQAsvZ+/+MUvIJfLERISgvfff99hOVqtFi+88AL69OkDhUKBqVOn4tKlS3b59uzZY30N+Pr64oEHHsD3339vl2/nzp0YPnw4vLy8MHz4cHz88ccO662rq8P06dPh5+cHf39/TJs2DXV1dXb5HL1PW4dMt9bV+hjn5eXZXX/kyBHEx8fDy8sLQ4cOxebNmx2WuX//fowbNw7+/v7w8fHBbbfdht///vcO2371/Q4ZMgRDhw61pk2fPt36PFw5FPtaTp8+DTc3N9x111125xQKhV0gKpFI8Morr+Dbb79t8zF2tV/84hcALD9+ApZe4+HDh+Orr77Cz3/+c3h7e1sfw/Pnz+PZZ59FcHAwvLy8MGLECGRnZ9uV6WgOd3vfTy0tLVi2bBkiIiLg5eWF/v3741e/+hVOnz593c9pR68Jo9GIlStXYujQoZBKpRgyZAh+//vfQ6fT2eTryGdxUFAQYmNj8c9//rMdjzARETnSc3+GJ7rJ/L//9/+gVqvteoOc7fz587j33nuhVCqRlpYGf39/lJeXY8eOHQAApVJpHfL5yCOP4Fe/+hUAIDY2FgDw/fffY+zYsRg4cCDS0tIgl8vxj3/8Aw8//DBycnLwyCOP2NQ3e/ZsKJVK/OEPf0Bzc/M121ZaWopHH30Uzz77LKZNm4a//e1vmD59OuLi4hAdHQ3A8mU1MTERgiDgpZdeglwux1//+td2D08/cOAA7rvvPqjVaixbtgyXL19GZmYmxo4di8LCQgwZMgQvvPACBg4ciNWrV2PevHm44447EBwc3GaZISEhOHDgAA4dOmT90u7IlClT8M4772Dv3r148MEHrenV1dU4dOhQh+d7f/TRR9BqtZg1axb69OmDY8eOITMzEz/++CM++ugjm7wmkwmTJk3CXXfdhVdffRV5eXlYunQpjEYjVqxY0WYdP/30E+666y5rUKhUKrFnzx48++yzaGhowPz586/bToPBgAsXLtily+VyyGQymzbed999+PnPf45XX30V7733HubMmQO5XI6XX34ZTz31FH71q19h06ZNmDp1KhISEhAaGmpT5pw5c+Dv749ly5bh5MmT2LhxIyoqKqyLzgHAu+++i2nTpmHixIlYs2YNtFotNm7ciHHjxuHrr7+2Loi2b98+pKSkICoqChkZGbh48SKefvpp6w9HrURRxEMPPYSCggLMnDkTkZGR+PjjjzFt2rTrPjatCgoKsGPHDsyePRu+vr544403kJKSgsrKSvTp0wcA8PXXX2PSpEno378/li9fDpPJhBUrVlgDr1bff/89HnzwQcTGxmLFihWQSqUoLS3FJ598ct12fPrpp7j99ttt0l544QWcO3cO+/fvx7vvvtuu+wkJCYHJZLI+1u3x5JNPYuXKlVixYgUeeeSR6wb1Wq3W4evK39+/U727p0+fBgDr4w0AFy9exH333YfHH38ckydPRnBwMC5fvoy7774bpaWlmDNnDkJDQ/HRRx9h+vTpqKuru+YPo+19P5lMJjz44IM4ePAgHn/8cbz44otobGzE/v378d1332HChAnX/Jx2ZMaMGcjOzsajjz6KRYsW4fPPP0dGRgZOnDhh9yNHez6LW8XFxWHnzp0deaiJiOhKIhG5XH19vQhAfOihh9p9TUhIiDht2jTr30uXLhUdvWXffvttEYBYVlYmiqIofvzxxyIA8Ysvvmiz7JqaGhGAuHTpUrtz99xzjxgTEyO2tLRY08xmszhmzBgxPDzcrt5x48aJRqPxmm1qvR8AYn5+vjXt/PnzolQqFRctWmRNmzt3rigIgvj1119b0y5evCgGBgbalenIyJEjxaCgIPHixYvWtG+++UaUSCTi1KlTrWmHDx8WAYgfffTRNcsTRVH87rvvRJlMJgIQR44cKb744ovizp07xebmZpt8JpNJHDRokPib3/zGJv21114TBUEQz5w5Y/N4XPn8trbn8OHD1jStVmvXloyMDFEQBLGiosKaNm3aNBGAOHfuXGua2WwWH3jgAdHT01Osqamxpl/9vD/77LNi//79xQsXLtjU8/jjj4t+fn4O23Cl1ufV0ZGRkWHXxtWrV1vTLl26JMpkMlEQBPGDDz6wphcXF9u1s/U1FRcXJ+r1emv6q6++KgIQ//nPf4qiKIqNjY2iv7+/+Nxzz9m0s7q6WvTz87NJHzlypNi/f3+xrq7OmrZv3z4RgBgSEmJN27lzpwhAfPXVV61pRqNR/NnPfiYCEN9++21ruqP3KQDR09NTLC0ttaZ98803IgAxMzPTmpacnCx6e3uLZ8+etaaVlJSI7u7uNmX+6U9/EgHYPK/tYTAYREEQbN5vrVJTUx1+vrSlurpaVCqVIgBx2LBh4syZM8X333/f5rFsNW3aNFEul4uiKIrZ2dkiAHHHjh3W8wDE1NRU699lZWVtvqYAiJ999tk129b6HJw8eVKsqakRy8rKxM2bN4tSqVQMDg62vm/Hjx8vAhA3bdpkc/2f//xnEYC4detWa5perxcTEhJEHx8fsaGhwabtnXk//e1vfxMBiK+99ppd+81msyiK1/6cvvp1ptFoRADijBkzbPItXrxYBCAeOnTImtbez+JWq1evFgGIP/30k905IiK6Pg4pJ+oCrSvjdsWiM61zknft2uVwaPO11NbW4tChQ/j1r3+NxsZGXLhwARcuXMDFixcxceJElJSU4OzZszbXPPfcc+2erx0VFYWf/exn1r+VSiVuu+02nDlzxpqWl5eHhIQEjBw50poWGBiIp5566rrlV1VVQaPRYPr06QgMDLSmx8bGIikpCbt3725XO68WHR0NjUaDyZMno7y8HK+//joefvhhBAcHY8uWLdZ8EokETz31FP71r3+hsbHRmv7ee+9hzJgxdr2113Nl73BzczMuXLiAMWPGQBRFfP3113b5r1zpubWHTa/X48CBAw7LF0UROTk5SE5OhiiK1uf7woULmDhxIurr61FYWHjddt55553Yv3+/3fHEE0/Y5b1ygTp/f3/cdtttkMvlNnOKb7vtNvj7+9u8Llo9//zz8PDwsP49a9YsuLu7W5/b/fv3o66uDk888YTN/bi5ueHOO+/E4cOHAfzvtTJt2jT4+flZy0tKSkJUVJRNnbt374a7uztmzZplTXNzc8PcuXOv+9i0mjBhgs0w7tjYWCgUCus9mkwmHDhwAA8//LDNYnNhYWG47777bMpqfY//85//hNlsbncbamtrIYoiAgIC2n1NW4KDg/HNN99g5syZuHTpEjZt2oQnn3wSQUFBWLlyZZtTGZ566imEh4e3a7rD888/7/B1dfXz05bbbrsNSqUSoaGheOGFFxAWFobc3FybOdpSqRRPP/20zXW7d+9Gv379bF6/Hh4emDdvHpqamvDvf//bYX0deT/l5OSgb9++Dl9DnVnQrfX1v3DhQpv01sUMr14fpD2fxa1aXy+ORhsQEdH1cUg5URdQKBQAYBOEucr48eORkpKC5cuX409/+hPuvvtuPPzww3jyySevOyy7tLQUoihiyZIlWLJkicM858+fx8CBA61/dySIHDx4sF1aQECAzRzciooKhwsahYWFXbf8iooKAHA4VzoyMhJ79+61LuzWUREREXj33XdhMplQVFSEXbt24dVXX8Xzzz+P0NBQTJgwAQAwdepUrFmzBh9//DGmTp2KkydP4quvvsKmTZs6XGdlZSX+8Ic/4F//+pfdPOX6+nqbvyUSCdRqtV2bAbQ5772mpgZ1dXV4880321wdvz2LwvXt29d6/9fi5eVlNzzaz88PgwYNsgsy/Pz8HM7NDg8Pt/nbx8cH/fv3t95jSUkJALQ59L/1vdj6Wrm6PMDy+rnyh4aKigr079/fbtXtjszJv95r//z587h8+bLD1/nVab/5zW/w17/+FTNmzEBaWhruuece/OpXv8Kjjz4KieT6v6NfL9C9Uk1NDUwmk/VvHx8f6+PQv39/bNy4ERs2bEBJSQn27t2LNWvW4A9/+AP69+/vcPV/Nzc3vPLKK5g2bRp27txpN0XlSuHh4e16XbUlJycHCoUCHh4eGDRokM0PHq0GDhxot8BbRUUFwsPD7R7LyMhI63lHOvJ+On36NG677TanLXxWUVEBiURi91rp168f/P397drcns/iVq2vl56+vzwRUU/FgJuoCygUCgwYMADfffddp8to68vOlV+GW/Nt374dR48exf/7f/8Pe/fuxTPPPIP169fj6NGjdkHDlVp7yxYvXoyJEyc6zHP1F7ore2Gvp62e8I4EAN3Nzc0NMTExiImJQUJCAhITE/Hee+9ZA4OoqCjExcVh69atmDp1KrZu3QpPT88OrwptMpmQlJSE2tpa/N///R+GDRsGuVyOs2fPYvr06R3q2WxLaxmTJ09ucx7uteaMdlRbz78zXxet9/Tuu++iX79+due7awVxZ96jTCZDfn4+Dh8+jNzcXOTl5eHDDz/EL37xC+zbt6/NugIDAyEIgsOgqi133HGHTbC2dOlSu0XCBEFAREQEIiIi8MADDyA8PBzvvfdem9vtPfXUU9a53A8//HC729JRP//5z62rlLelI59f19PV7ydH2hsUd+T12Pp6ud5jSUREjjHgJuoiDz74IN5880189tlnbW5Jcy2tw/rq6upstrJqq7flrrvuwl133YX09HS8//77eOqpp/DBBx9gxowZbX4pa+0h9fDwuKGepRsREhKC0tJSu3RHaY6uBYCTJ0/anSsuLkbfvn071bvdlvj4eACW4clXmjp1KhYuXIiqqiq8//77eOCBBzo8jPf48eM4deoUsrOzMXXqVGv6/v37HeY3m804c+aMtVcbAE6dOgUA1kXCrqZUKuHr6wuTydRtz3dHlZSUIDEx0fp3U1MTqqqqrPs1t/ZiBgUFXfOeWl8rrT3iV7r69RMSEoKDBw+iqanJ5gcrR6+zzgoKCoKXl1e7X/sSiQT33HMP7rnnHrz22mtYvXo1Xn75ZRw+fLjN+3Z3d8fQoUOtq3Rfqa3PhPfee89mVfyrR1FcTa1WIyAgwO49caXWXu7p06f3yNWvQ0JC8O2338JsNtv0chcXF1vPO9KR99PQoUPx+eefw2Aw2EyRuFJHepRDQkJgNptRUlJi7YkHLIu41dXVtdnm9igrK0Pfvn3tRqcQEVH7cA43URf53e9+B7lcjhkzZuCnn36yO3/69Gm8/vrrbV7fGkjk5+db05qbm+22qrl06ZJdL0XrfOjW7WFa5zBeva1RUFAQ7r77bmzevNnhF+aampo22+csEydOxGeffQaNRmNNq62txXvvvXfda/v374+RI0ciOzvb5t6+++477Nu3zxqUddR//vMfh/PhW+dNXj20+IknnoAgCHjxxRdx5syZTu0l3NoDdeVzKYriNV8jf/nLX2zy/uUvf4GHhwfuueeeNutISUlBTk6Ow9EXXfF8d9Sbb75p81xs3LgRRqPROs954sSJUCgUWL16tcPnrPWernytXDk8f//+/SgqKrK55v7774fRaMTGjRutaSaTCZmZmU67Lzc3N0yYMAE7d+7EuXPnrOmlpaXYs2ePTd7a2lq7669+j7clISEBX375pV166w9RV38mjB07FhMmTLAerQH3559/7nBXgmPHjuHixYvXHW4/efJkhIWFYfny5dfM1x3uv/9+VFdX48MPP7SmGY1GZGZmwsfHx7ql4NU68n5KSUnBhQsXbN6zrVrf8219TrfVZgD485//bJP+2muvAQAeeOCB65bRlq+++qpTPxITEZEFe7iJusjQoUPx/vvv4ze/+Q0iIyMxdepUDB8+HHq9Hp9++ql125m23HvvvRg8eDCeffZZ/Pa3v4Wbmxv+9re/QalUorKy0povOzsbGzZswCOPPIKhQ4eisbERW7ZsgUKhsH4pk8lkiIqKwocffoiIiAgEBgZi+PDhGD58OLKysjBu3DjExMTgueeeg1qtxk8//YTPPvsMP/74I7755huXPk6/+93vsHXrViQlJWHu3LnWbcEGDx6M2tra6/b6rF27Fvfddx8SEhLw7LPPWrcF8/PzsxsK215r1qzBV199hV/96lfWIaGFhYV45513EBgYaLd1llKpxKRJk/DRRx/B39+/U192hw0bhqFDh2Lx4sU4e/YsFAoFcnJy2hwO7OXlhby8PEybNg133nkn9uzZg9zcXPz+97+/Zs/UH//4Rxw+fBh33nknnnvuOURFRaG2thaFhYU4cOCAw+DuamfPnsXWrVvt0n18fJw+ZFiv1+Oee+7Br3/9a5w8eRIbNmzAuHHj8Mtf/hKAZfrGxo0bMWXKFNx+++14/PHHre+R3NxcjB071hrkZGRk4IEHHsC4cePwzDPPoLa2FpmZmYiOjkZTU5O1zuTkZIwdOxZpaWkoLy9HVFQUduzYYTeP/kYtW7YM+/btw9ixYzFr1iyYTCb85S9/wfDhw21+gFqxYgXy8/PxwAMPICQkBOfPn8eGDRswaNAgjBs37pp1PPTQQ3j33Xdx6tQpm9EQcXFxAIB58+Zh4sSJcHNzw+OPP95mOe+++y7ee+89PPLII4iLi4OnpydOnDiBv/3tb/Dy8rrunuBubm54+eWX7RYsu1JhYaHD19XQoUNdGgA+//zz2Lx5M6ZPn46vvvoKQ4YMwfbt2/HJJ5/gz3/+8zUXv2zv+2nq1Kl45513sHDhQhw7dgw/+9nP0NzcjAMHDmD27Nl46KGHrvk5fbURI0Zg2rRpePPNN1FXV4fx48fj2LFjyM7OxsMPP2wzKqQjzp8/j2+//Rapqamdup6IiMBtwYi62qlTp8TnnntOHDJkiOjp6Sn6+vqKY8eOFTMzM2224rp62yhRFMWvvvpKvPPOO0VPT09x8ODB4muvvWa3BVdhYaH4xBNPiIMHDxalUqkYFBQkPvjgg+KXX35pU9ann34qxsXFiZ6ennZbz5w+fVqcOnWq2K9fP9HDw0McOHCg+OCDD4rbt2+35mmt19H2Y21tC/bAAw/Y5R0/frw4fvx4m7Svv/5a/NnPfiZKpVJx0KBBYkZGhvjGG2+IAMTq6urrPMKieODAAXHs2LGiTCYTFQqFmJycLBYVFdnk6ci2YJ988omYmpoqDh8+XPTz8xM9PDzEwYMHi9OnTxdPnz7t8Jp//OMfIgDx+eefd3i+PduCFRUViRMmTBB9fHzEvn37is8995x1O6krt6Jq3Xbp9OnT4r333it6e3uLwcHB4tKlS0WTyWRT79XPtSiK4k8//SSmpqaKKpVK9PDwEPv16yfec8894ptvvnndx+Za24JdubXWlVtDXWn8+PFidHS0w3KvfL20vqb+/e9/i88//7wYEBAg+vj4iE899ZTNFnCtDh8+LE6cOFH08/MTvby8xKFDh4rTp0+3ex/k5OSIkZGRolQqFaOiosQdO3aI06ZNs2m7KFq2ppsyZYqoUChEPz8/ccqUKeLXX3/d7m3Brtz26sp7vPo9fvDgQXHUqFGip6enOHToUPGvf/2ruGjRItHLy8smz0MPPSQOGDBA9PT0FAcMGCA+8cQT4qlTp+zquJpOpxP79u0rrly50ibdaDSKc+fOFZVKpSgIwnW3CPv222/F3/72t+Ltt98uBgYGiu7u7mL//v3Fxx57TCwsLLTJ29ZzbzAYxKFDh3Z4W7CrH7OrtT4H19s2ra3Xniha3hNPP/202LdvX9HT01OMiYmxeZ5b3cj7SavVii+//LIYGhpqzffoo4/afKa09Tnt6HVmMBjE5cuXW8tTqVTiSy+9ZPP/FVHs2Gfxxo0bRW9vb5ut0IiIqGMEUexFqxUR0S1r/vz52Lx5M5qamtq9DVl3+uc//4mHH34Y+fn5NtvvEHXUww8/jO+//97hfPPOWLlyJd5++22UlJT0ivdST2UymeDu7o6VK1filVde6e7muMSoUaNw9913409/+lN3N4WIqNfiHG4i6nGuXKQJAC5evIh3330X48aN6zUBwpYtW6BWq687xJfoSle/9ktKSrB7927cfffdTqtjwYIFaGpqwgcffOC0Mm9Fretc3Kyrd+fl5aGkpAQvvfRSdzeFiKhX4xxuIupxEhIScPfddyMyMhI//fQT3nrrLTQ0NLS5N3hP8sEHH+Dbb79Fbm4uXn/9de5dSx2iVqsxffp0qNVqVFRUYOPGjfD09MTvfvc7p9Xh4+PTrv3VqW3bt2/HO++8A0EQOj0/uqebNGmSzVoGRETUORxSTkQ9zu9//3ts374dP/74IwRBwO23346lS5f2iq2rBEGAj48PfvOb32DTpk3dtu8z9U5PP/00Dh8+jOrqakilUiQkJGD16tW4/fbbu7tpdAW1Wg1BEPDKK69cc+E3IiIiBtxERERERERELsA53EREREREREQuwICbiIiIiIiIyAVuqcmFZrMZ586dg6+vLxcyIiIiIiLqBURRRGNjIwYMGACJhP2F1LvcUgH3uXPnoFKpursZRERERETUQT/88AMGDRrU3c0g6pBbKuD29fUFYHmzKhSKbm4NERERERFdT0NDA1QqlfW7PFFvcksF3K3DyBUKBQNuIiIiIqJehFNCqTfiJAgiIiIiIiIiF2DATUREREREROQCDLiJiIiIiIiIXOCWmsNNRERERETUVcxmM/R6fXc3g5zMw8MDbm5u7crLgJuIiIiIiMjJ9Ho9ysrKYDabu7sp5AL+/v7o16/fdRfzY8BNRERERETkRKIooqqqCm5ublCpVJBIOJP3ZiGKIrRaLc6fPw8A6N+//zXzM+AmIiIiIiJyIqPRCK1WiwEDBsDb27u7m0NOJpPJAADnz59HUFDQNYeX86cWIiIiIiIiJzKZTAAAT0/Pbm4JuUrrDykGg+Ga+RhwExERERERucD15vdS79Xe55YBNxEREREREZELMOAmIiIiIiKidhMEATt37uzuZvQKDLiJiIiIiIgIAFBdXY25c+dCrVZDKpVCpVIhOTkZBw8edEl9R44cgSAIqKurc0n5AFBbW4unnnoKCoUC/v7+ePbZZ9HU1OSy+q7EVcqJiIiIiIh6ILNZxKnzjajXGuDn7YGIIF9IJK6bF15eXo6xY8fC398fa9euRUxMDAwGA/bu3YvU1FQUFxe7rO4bJYoiTCYT3N3tQ9ynnnoKVVVV2L9/PwwGA55++mk8//zzeP/9913eLvZwExERERER9TBfVdRi/ocaLPzwG7z88XEs/PAbzP9Qg68qal1W5+zZsyEIAo4dO4aUlBREREQgOjoaCxcuxNGjRx1e46iHWqPRQBAElJeXAwAqKiqQnJyMgIAAyOVyREdHY/fu3SgvL0diYiIAICAgAIIgYPr06QAAs9mMjIwMhIaGQiaTYcSIEdi+fbtdvXv27EFcXBykUikKCgrs2nfixAnk5eXhr3/9K+68806MGzcOmZmZ+OCDD3Du3DnnPHDXwB5uIiIiIiKiHuSrilqk555AndaAIF8pvDykaDGY8P25eqTnnsDLD0QiLiTQqXXW1tYiLy8P6enpkMvlduf9/f07XXZqair0ej3y8/Mhl8tRVFQEHx8fqFQq5OTkICUlBSdPnoRCobDucZ2RkYGtW7di06ZNCA8PR35+PiZPngylUonx48dby05LS8O6deugVqsREBBgV/dnn30Gf39/xMfHW9MmTJgAiUSCzz//HI888kin76s9GHATERERERH1EGaziOxPK1CnNWBIH2/r9lNyqTu8Pd1QUavFO59WYJQqwKnDy0tLSyGKIoYNG+a0MltVVlYiJSUFMTExAAC1Wm09Fxho+eEgKCjIGtTrdDqsXr0aBw4cQEJCgvWagoICbN682SbgXrFiBZKSktqsu7q6GkFBQTZp7u7uCAwMRHV1tVPu71oYcBMREREREfUQp843ovR8E4J8pXZ7PQuCAKWPFCXnm3DqfCOG9VM4rV5RFJ1W1tXmzZuHWbNmYd++fZgwYQJSUlIQGxvbZv7S0lJotVq7QFqv12PUqFE2aVf2XPdEnMNNRERERETUQ9RrDdAbTfDycHN43svDDXqjCfVag1PrDQ8PhyAIHV4YTSKxhJRXBuwGg23bZsyYgTNnzmDKlCk4fvw44uPjkZmZ2WaZrSuI5+bmQqPRWI+ioiKbedwAHA5/v1K/fv1w/vx5mzSj0Yja2lr069fv+jd4gxhwExERERER9RB+3h7wdHdDi8Hk8HyLwQRPdzf4eXs4td7AwEBMnDgRWVlZaG5utjvf1rZdSqUSAFBVVWVN02g0dvlUKhVmzpyJHTt2YNGiRdiyZQsAwNPTEwBgMv3vfqOioiCVSlFZWYmwsDCbQ6VSdei+EhISUFdXh6+++sqadujQIZjNZtx5550dKqszGHATERERERH1EBFBvggL8kFNk85umLcoiqhp0iE8yAcRQb5OrzsrKwsmkwmjR49GTk4OSkpKcOLECbzxxhvWudRXaw2Cly1bhpKSEuTm5mL9+vU2eebPn4+9e/eirKwMhYWFOHz4MCIjIwEAISEhEAQBu3btQk1NDZqamuDr64vFixdjwYIFyM7OxunTp1FYWIjMzExkZ2d36J4iIyMxadIkPPfcczh27Bg++eQTzJkzB48//jgGDBjQuQeqAxhwExERERER9RASiYBpY0LgJ/NARa0WzTojTGYRzTojKmq18JN5YOqYEJfsx61Wq1FYWIjExEQsWrQIw4cPR1JSEg4ePIiNGzc6vMbDwwPbtm1DcXExYmNjsWbNGqxatcomj8lkQmpqqjX4jYiIwIYNGwAAAwcOxPLly5GWlobg4GDMmTMHALBy5UosWbIEGRkZ1utyc3MRGhra4ft67733MGzYMNxzzz24//77MW7cOLz55psdLqczBNGVs+N7mIaGBvj5+aG+vh4KhfMWGCAiIiIiItfojd/hW1paUFZWhtDQUHh5eXWqjK8qapH9aQVKzzdBb7QMIw8P8sHUMSFO3xKMOq69zzFXKSciIiIiIuph4kICMUoVgFPnG1GvNcDP2wMRQb4u6dkm12HATURERERE1ANJJIJTt/6irseAm4iI6BZiNovsLSEiIuoiDLiJiIhuEY7mA4YF+WAa5wMSERG5BFcpJyIiugV8VVGL9NwT+O5sPRRe7hgU4A2Flzu+P1eP9NwT+KqitrubSEREdNNhwE1ERHSTM5tFZH9agTqtAUP6eEMudYebRIBc6o6QQG/UXzbgnU8rYDbfMhuXEBERdQkG3ERERDe5U+cbUXq+CUG+UgiC7XxtQRCg9JGi5HwTTp1v7KYWEhER3ZwYcBMREd3k6rUG6I0meHm4OTzv5eEGvdGEeq2hi1tGRER0c+OiaURERDc5P28PeLq7ocVgglxq/7/+FoNlATU/b49Olc+Vz4mIiBxjwE1ERHSTiwjyRViQD74/Vw9vTzebYeWiKKKmSYfhA/wQEeTb4bK58jkR0a1HEAR8/PHHePjhh7u7KT0eh5QTERHd5CQSAdPGhMBP5oGKWi2adUaYzCKadUZU1GrhJ/PA1DEhHe6V5srnREQ3n+rqasydOxdqtRpSqRQqlQrJyck4ePCgS+o7cuQIBEFAXV2dS8oHgPT0dIwZMwbe3t7w9/d3WT2OMOAmIiK6BcSFBOLlByIRPcAPDS1G/HhJi4YWI4YP8MPLD0Q67I02m0UUVzfg8zMXUVzdYLOKOVc+JyJyPVEUUa814EKDHvVaA0TRtZ+p5eXliIuLw6FDh7B27VocP34ceXl5SExMRGpqqkvrvlGiKMJoNDo8p9fr8dhjj2HWrFld3CoOKSciIrplxIUEYpQqoF3zra83VLwjK58P66foqlskIrppXGzU40y1Fo2XTTCZRbhJBPjK3KDu540+vp4uqXP27NkQBAHHjh2DXC63pkdHR+OZZ55xeM2RI0eQmJiIS5cuWXuPNRoNRo0ahbKyMgwZMgQVFRWYM2cOCgoKoNfrMWTIEKxduxZRUVFITEwEAAQEBAAApk2bhr///e8wm81Ys2YN3nzzTVRXVyMiIgJLlizBo48+alPv7t278corr+D48ePYt28f7r77brs2Ll++HADw97//3UmPVPsx4CYiIrqFSCTCdQPg1qHidVoDgnyl8PKQosVgsg4Vf/mBSBhN4n9XPpc6LMPLww0XmnRc+ZyIqBMuNupxvKIRBqMILw8JvDwEmMxAXbMRxysaERPi6/Sgu7a2Fnl5eUhPT7cJtlvdyFDs1NRU6PV65OfnQy6Xo6ioCD4+PlCpVMjJyUFKSgpOnjwJhUIBmUwGAMjIyMDWrVuxadMmhIeHIz8/H5MnT4ZSqcT48eOtZaelpWHdunVQq9XWoL0nYcBNREREVlcPFW/tvZZL3eHt6YaKWi3e+bQCL4xXu3TlcyKiW5UoijhTrYXBKEIulVg/h93dALlEgmadGWeqtQj08bAbYXQjSktLIYoihg0b5rQyW1VWViIlJQUxMTEAALVabT0XGGiZ0hQUFGQN6nU6HVavXo0DBw4gISHBek1BQQE2b95sE3CvWLECSUlJTm+zszDgJiIi6iW6Yvut9g4VhwCXrXxORHQra7hsRONlE7w8JA4/h708JGi8bELDZaNTf9R05fzwefPmYdasWdi3bx8mTJiAlJQUxMbGtpm/tLQUWq3WLpDW6/UYNWqUTVp8fLxL2uwsDLiJiIh6ga7afqtea2jXUPHGy0ZMGxOC9NwTqKjVQukjhZeHpce7pknX6ZXPiYhudQajCJNZhJeH489PNwnQYhBhMDo3QA4PD4cgCCguLu7QdRKJZR3uKwN2g8F2OtGMGTMwceJE5ObmYt++fcjIyMD69esxd+5ch2U2NTUBAHJzczFw4ECbc1Kp7f+fHA1/70m4SjkREVEP15Xbb/l5e1iHijty5VDxzqx8TkRE1+bhLsBNYpmz7YjJDLhJBHi4O/cHzcDAQEycOBFZWVlobm62O9/Wtl1KpRIAUFVVZU3TaDR2+VQqFWbOnIkdO3Zg0aJF2LJlCwDA09MyF91k+t//d6KioiCVSlFZWYmwsDCbQ6VSdfYWuwV7uImIiHqw9s6pHqUKcEpvckSQb4eGindk5XMiIro+hcwdvjI31DUbIZdI7D6HWwxm+MvdoZA5P5TLysrC2LFjMXr0aKxYsQKxsbEwGo3Yv38/Nm7ciBMnTthd0xoEL1u2DOnp6Th16hTWr19vk2f+/Pm47777EBERgUuXLuHw4cOIjIwEAISEhEAQBOzatQv3338/ZDIZfH19sXjxYixYsABmsxnjxo1DfX09PvnkEygUCkybNq1D91VZWYna2lpUVlbCZDJZfxAICwuDj49P5x6sdmIPNxERUQ/Wke23nEEiETBtTAj8ZB6oqNWiWWeEySyiWWdERa3W4VDx1pXP71T3wbB+CgbbREQ3QBAEqPt5w8NdQLPODKNJtOwxbRLRrDPD010CdT9vpy6Y1kqtVqOwsBCJiYlYtGgRhg8fjqSkJBw8eBAbN250eI2Hhwe2bduG4uJixMbGYs2aNVi1apVNHpPJhNTUVERGRmLSpEmIiIjAhg0bAAADBw7E8uXLkZaWhuDgYMyZMwcAsHLlSixZsgQZGRnW63JzcxEaGtrh+/rDH/6AUaNGYenSpWhqasKoUaMwatQofPnllx0uq6ME0dW7p/cgDQ0N8PPzQ319PRQK7glKREQ93+dnLuLlj49jUIA33BwEsiaziB8vaZH+SAzuVPdxWr2O5oyHB/lgqpPnjBMRXU9v/A7f0tKCsrIyhIaGwsvLq1NldMc+3NR+7X2OOaSciIioB7tyTnVXbr/FoeJERN2rj68nAn080HDZCINRhIe7AIXM3SU92+Q6DLiJiIh6sI7OqXam1qHiRETUPQRBcPoPqtS1OIebiIioB2trTnVTiwGnzjdBIggYf1vf7m4mEREROcCAm4iIqIe7evutkvONOFHdiIbLBmj1Rvz1P+WY/6HGqduDERER0Y1jwE1ERNQLxIUE4s+/GYkZPwuFt6c7FDIPRPZXIDzI12V7chMREdGNYcBNRETUixw5WQOTWUREkA98pO5wkwiQS90REuiN+ssGvPNpBczmW2YDEiIioh6NATcREVEv0dV7chMREdGN4SrlREREvUS91gC90QQvD6nD814ebrjQpEO91tDpOsxmkVuBEREROQkDbiIiol7C1Xtyf1VRi+xPK1B6vgl6o6WssCAfTBsTgriQwBttPhER0S2HQ8qJiIh6idY9uWuadBBF23narXtyhynlMJtFfH7mIoqrG9o9n/urilqk557Ad2frofByx6AAby7GRkREDgmCgJ07d3Z3M3oFBtxERES9RFt7cjfrjKio1cJNIqBWq8fij77Fyx8fx8IPv2nXdmFms4jsTytQpzVgSB9vyLkYGxHRLau6uhpz586FWq2GVCqFSqVCcnIyDh486JL6jhw5AkEQUFdX55Lyy8vL8eyzzyI0NBQymQxDhw7F0qVLodfrXVLf1TiknIiIqBdp3ZO7dej3hSYdPN3dMMBPhp8aW3CurgVBvlJ4eUjRYjBZe6hffiCyzWHhHVmMbVg/RVfcJhERAYDZDNScAC7XATJ/QBkJSFzXZ1peXo6xY8fC398fa9euRUxMDAwGA/bu3YvU1FQUFxe7rO4bJYoiTCYT3N1tQ9zi4mKYzWZs3rwZYWFh+O677/Dcc8+hubkZ69atc3m72MNNRETUy7Tuyf3ab0Yg/ZEYrPt1LPy9PWA0iZ3qof7fYmxuDs97ebhBbzTd0GJsRETUQZWfAzueAz5+Adg13/Lvjucs6S4ye/ZsCIKAY8eOISUlBREREYiOjsbChQtx9OhRh9c46qHWaDQQBAHl5eUAgIqKCiQnJyMgIAByuRzR0dHYvXs3ysvLkZiYCAAICAiAIAiYPn06AMBsNiMjI8PaMz1ixAhs377drt49e/YgLi4OUqkUBQUFdu2bNGkS3n77bdx7771Qq9X45S9/icWLF2PHjh3OedCugz3cREREvZBEIlh7m4urG3C6prnTPdSuXoyNiIg6qPJzYN/Llp5t32DAXQYYLwNV31rS700HBt/p1Cpra2uRl5eH9PR0yOVyu/P+/v6dLjs1NRV6vR75+fmQy+UoKiqCj48PVCoVcnJykJKSgpMnT0KhUEAmkwEAMjIysHXrVmzatAnh4eHIz8/H5MmToVQqMX78eGvZaWlpWLduHdRqNQICAtrVnvr6egQGds1ioAy4iYiIerkb3S6sdTG278/Vw9vTzSZob12MbfgAP0QE+bqk/UREdAWzGTj2piXYDlQDrZ/Jnj5AoByoLQO+2AIMusOpw8tLS0shiiKGDRvmtDJbVVZWIiUlBTExMQAAtVptPdca+AYFBVmDep1Oh9WrV+PAgQNISEiwXlNQUIDNmzfbBNwrVqxAUlJSu9tSWlqKzMzMLhlODnBIORERUa93ZQ+1I9frob7eYmx+Mg9MHRPC/biJiLpCzQngwklLz/ZVo5YgCIBvEFBTbMnnRFfvfuFM8+bNw6pVqzB27FgsXboU33777TXzl5aWQqvVIikpCT4+PtbjnXfewenTp23yxsfHt7sdZ8+exaRJk/DYY4/hueee69S9dBQDbiIiol6uPduFhQf5XLOHunUxtugBfmhoMeLHS1o0tBgxfIDfNRdcIyIiJ7tcBxh1lmHkjrjLLOcv1zm12vDwcAiC0OGF0ST/7WW/8v8/BoPtiKoZM2bgzJkzmDJlCo4fP474+HhkZma2WWZTUxMAIDc3FxqNxnoUFRXZzOMG4HD4uyPnzp1DYmIixowZgzfffLNd1zgDh5QTERH1cq091Om5J1BRq4XSRwovD0uPd02Trt091HEhgRilCsCp842o1xrg5+2BiCBf9mwTEXUlmT/gLrXM2fb0sT9vvGw5L/N3arWBgYGYOHEisrKyMG/ePLtAtq6uzuE8bqVSCQCoqqqyzqHWaDR2+VQqFWbOnImZM2fipZdewpYtWzB37lx4enoCAEym/43SioqKglQqRWVlpc3w8c46e/YsEhMTERcXh7ffftv6I0FXYA83ERHRTcBZPdSti7Hdqe6DYf0UDLaJiLqaMhLoexvQeB64epi3KFrSlcMs+ZwsKysLJpMJo0ePRk5ODkpKSnDixAm88cYb1rnUVwsLC4NKpcKyZctQUlKC3NxcrF+/3ibP/PnzsXfvXpSVlaGwsBCHDx9GZKSl/SEhIRAEAbt27UJNTQ2amprg6+uLxYsXY8GCBcjOzsbp06dRWFiIzMxMZGdnd+iezp49i7vvvhuDBw/GunXrUFNTg+rqalRXV3fuQeog9nATERHdJNhDTUR0E5BIgNHPW1Yjry2zzNluXaW88bylZ/uO51yyH7darUZhYSHS09OxaNEiVFVVQalUIi4uDhs3bnR4jYeHB7Zt24ZZs2YhNjYWd9xxB1atWoXHHnvMmsdkMiE1NRU//vgjFAoFJk2ahD/96U8AgIEDB2L58uVIS0vD008/jalTp+Lvf/87Vq5cCaVSiYyMDJw5cwb+/v64/fbb8fvf/75D97R//36UlpaitLQUgwYNsjnnynnrrQSxK2rpIRoaGuDn54f6+nooFPbbohARERERUc/SG7/Dt7S0oKysDKGhofDy8upcIZWfW1Yrv3Dyv3O6pZae7Tuec/qWYNRx7X2O2cNNRERERETU0wy+07L1V80JywJpMn/LMPIunH9MN44BNxERERERUU8kkQDB0d3dCroBvfbnkT/+8Y8QBAHz58/v7qYQERERERER2emVAfcXX3yBzZs3IzY2trubQkRERERERORQrwu4m5qa8NRTT2HLli3Wfd6IiIiIiIiIeppeF3CnpqbigQcewIQJE66bV6fToaGhweYgIiIiIiIi6gq9atG0Dz74AIWFhfjiiy/alT8jIwPLly93cauIiIiIiIiI7PWaHu4ffvgBL774It57771272X30ksvob6+3nr88MMPLm4lERERERERkUWv6eH+6quvcP78edx+++3WNJPJhPz8fPzlL3+BTqeDm5ubzTVSqRRSqbSrm0pERERERETUe3q477nnHhw/fhwajcZ6xMfH46mnnoJGo7ELtomIiIiIiMj5BEHAzp07u7sZvUKvCbh9fX0xfPhwm0Mul6NPnz4YPnx4dzePiIiIiIio16uursbcuXOhVqshlUqhUqmQnJyMgwcPuqS+I0eOQBAE1NXVuaR8APjlL3+JwYMHw8vLC/3798eUKVNw7tw5l9V3pV4TcBMREREREd1SRDPQVA1cKrP8K5pdWl15eTni4uJw6NAhrF27FsePH0deXh4SExORmprq0rpvlCiKMBqNDs8lJibiH//4B06ePImcnBycPn0ajz76aJe0q1cH3EeOHMGf//zn7m4GERERERGRc9VVAN/9w3Kc+Ph//11X4bIqZ8+eDUEQcOzYMaSkpCAiIgLR0dFYuHAhjh496vAaRz3UGo0GgiCgvLwcAFBRUYHk5GQEBARALpcjOjoau3fvRnl5ORITEwEAAQEBEAQB06dPBwCYzWZkZGQgNDQUMpkMI0aMwPbt2+3q3bNnD+Li4iCVSlFQUOCwjQsWLMBdd92FkJAQjBkzBmlpaTh69CgMBsONP2jX0WsWTSMiIiIiIrol1FUAJbsBgxbw9AXcPACTAWg8Z0kPvx/wD3FqlbW1tcjLy0N6ejrkcrndeX9//06XnZqaCr1ej/z8fMjlchQVFcHHxwcqlQo5OTlISUnByZMnoVAoIJPJAFi2eN66dSs2bdqE8PBw5OfnY/LkyVAqlRg/fry17LS0NKxbtw5qtRoBAQHtus/33nsPY8aMgYeHR6fvqb0YcBMREREREfUUohn44TNLsC3rAwiCJd1dCrh5ApdrLef9VIDgvAHLpaWlEEURw4YNc1qZrSorK5GSkoKYmBgAgFqttp4LDAwEAAQFBVmDep1Oh9WrV+PAgQNISEiwXlNQUIDNmzfbBNwrVqxAUlLSddvwf//3f/jLX/4CrVaLu+66C7t27XLW7V1Trx5STkREREREdFNpPm85PH3/F2y3EgTA0+d/eZxIFEWnlnelefPmYdWqVRg7diyWLl2Kb7/99pr5S0tLodVqkZSUBB8fH+vxzjvv4PTp0zZ54+Pj29WG3/72t/j666+xb98+uLm5YerUqS6951bs4SYiIiIiIuopDJcBs9EyjNwRNw9A32TJ50Th4eEQBAHFxcUduk4isfThXhm8Xj03esaMGZg4cSJyc3Oxb98+ZGRkYP369Zg7d67DMpuamgAAubm5GDhwoM05qVRq87ej4e+O9O3bF3379kVERAQiIyOhUqlw9OhRaw+6q7CHm4iIiIiIqKfwkAESd8ucbUdMBst5D5lTqw0MDMTEiRORlZWF5uZmu/NtbdulVCoBAFVVVdY0jUZjl0+lUmHmzJnYsWMHFi1ahC1btgAAPD09AQAmk8maNyoqClKpFJWVlQgLC7M5VCpVZ2/Rymy2rPau0+luuKzrYcBNRERERETUU8iDLIe+Cbh6yLMoWtJb8zhZVlYWTCYTRo8ejZycHJSUlODEiRN444032uwJbg2Cly1bhpKSEuTm5mL9+vU2eebPn4+9e/eirKwMhYWFOHz4MCIjIwEAISEhEAQBu3btQk1NDZqamuDr64vFixdjwYIFyM7OxunTp1FYWIjMzExkZ2d36J4+//xz/OUvf4FGo0FFRQUOHTqEJ554AkOHDnV57zbAgJuIiIiIiKjnECSAKsHSg325FjDqLAupGXWWvz1klvNOXDCtlVqtRmFhIRITE7Fo0SIMHz4cSUlJOHjwIDZu3OjwGg8PD2zbtg3FxcWIjY3FmjVrsGrVKps8JpMJqampiIyMxKRJkxAREYENGzYAAAYOHIjly5cjLS0NwcHBmDNnDgBg5cqVWLJkCTIyMqzX5ebmIjQ0tEP35O3tjR07duCee+7BbbfdhmeffRaxsbH497//bTc83RUEsStmivcQDQ0N8PPzQ319PRQKRXc3h4iIiIiIrqM3fodvaWlBWVkZQkND4eXl1blC6iosq5E3n7fM6Za4W3q1VQlO3xKMOq69zzEXTSMiIiIiIupp/EMsW381n7cskOYhswTcLujZJtdhwE1ERERERNQTCRLAp193t4JuAH8eISIiIiIiInIBBtxERERERERELsCAm4iIiIiIiMgFGHATERERERERuQADbiIiIiIiIiIXYMBNRERERERE5AIMuImIiIiIiIhcgAE3ERERERERtZsgCNi5c2d3N6NXYMBNREREREREAIDq6mrMnTsXarUaUqkUKpUKycnJOHjwoEvqO3LkCARBQF1dnUvKv5JOp8PIkSMhCAI0Go3L6wMA9y6phYiIiIiIiDrELJpRWleKBl0DFFIFwvzDIBFc12daXl6OsWPHwt/fH2vXrkVMTAwMBgP27t2L1NRUFBcXu6zuGyWKIkwmE9zd2w5xf/e732HAgAH45ptvuqxd7OEmIiIiIiLqYTTnNUj7Txpe/s/LWHl0JV7+z8tI+08aNOc1Lqtz9uzZEAQBx44dQ0pKCiIiIhAdHY2FCxfi6NGjDq9x1EOt0WggCALKy8sBABUVFUhOTkZAQADkcjmio6Oxe/dulJeXIzExEQAQEBAAQRAwffp0AIDZbEZGRgZCQ0Mhk8kwYsQIbN++3a7ePXv2IC4uDlKpFAUFBW3e2549e7Bv3z6sW7fuxh6kDmIPNxERERERUQ+iOa/Bui/XoV5XD6VMCS93L7QYW1B8sRjrvlyHxfGLMTJopFPrrK2tRV5eHtLT0yGXy+3O+/v7d7rs1NRU6PV65OfnQy6Xo6ioCD4+PlCpVMjJyUFKSgpOnjwJhUIBmUwGAMjIyMDWrVuxadMmhIeHIz8/H5MnT4ZSqcT48eOtZaelpWHdunVQq9UICAhwWP9PP/2E5557Djt37oS3t3en76MzGHATERERERH1EGbRjPeL30e9rh6DfQdDEAQAgLeHN1TuKvzQ+AO2FW9DrDLWqcPLS0tLIYoihg0b5rQyW1VWViIlJQUxMTEAALVabT0XGBgIAAgKCrIG9TqdDqtXr8aBAweQkJBgvaagoACbN2+2CbhXrFiBpKSkNusWRRHTp0/HzJkzER8fb+117yoMuIlcqKvn3RARERFR71ZaV4qyujIoZUprsN1KEAT0lfXFmbozKK0rRURAhNPqFUXRaWVdbd68eZg1axb27duHCRMmICUlBbGxsW3mLy0thVartQuk9Xo9Ro0aZZMWHx9/zbozMzPR2NiIl156qfM3cAMYcBO5iOa8Bu8Xv4+yujLozXp4SjwR6h+KJ4c96fQhQERERER0c2jQNUBv1sPL3cvheam7FBdbLqJB1+DUesPDwyEIQocXRpNILJ1JVwbsBoPBJs+MGTMwceJE5ObmYt++fcjIyMD69esxd+5ch2U2NTUBAHJzczFw4ECbc1Kp1OZvR8Pfr3To0CF89tlndtfFx8fjqaeeQnZ29jWvv1HsaiNygdZ5NycunoCvpy8G+gyEr6evdd6NKxe7ICIiIqLeSyFVwFPiiRZji8PzOqMOnhJPKKQKp9YbGBiIiRMnIisrC83NzXbn29q2S6lUAgCqqqqsaY623FKpVJg5cyZ27NiBRYsWYcuWLQAAT09PAIDJZLLmjYqKglQqRWVlJcLCwmwOlUrVoft644038M0330Cj0UCj0WD37t0AgA8//BDp6ekdKqsz2MNN5GTdNe+GiIiIiHq/MP8whPqHovhiMVTuKpth5aIo4sLlC4jsE4kw/zCn152VlYWxY8di9OjRWLFiBWJjY2E0GrF//35s3LgRJ06csG/vf4PgZcuWIT09HadOncL69ett8syfPx/33XcfIiIicOnSJRw+fBiRkZEAgJCQEAiCgF27duH++++HTCaDr68vFi9ejAULFsBsNmPcuHGor6/HJ598AoVCgWnTprX7ngYPHmzzt4+PDwBg6NChGDRoUEcfog7jt30iJ+vIvBsiIiIioitJBAmeHPYkFFIFfmj8AVqDFibRBK1Bix8af4BCqsATw55wSceNWq1GYWEhEhMTsWjRIgwfPhxJSUk4ePAgNm7c6PAaDw8PbNu2DcXFxYiNjcWaNWuwatUqmzwmkwmpqamIjIzEpEmTEBERgQ0bNgAABg4ciOXLlyMtLQ3BwcGYM2cOAGDlypVYsmQJMjIyrNfl5uYiNDTU6fftSoLoytnxPUxDQwP8/PxQX18PhcK5QzCIWn1Z/SVWHl2JgT4DHX4QmkQTzjWdw5K7liC+37UXeSAiIiK61fXG7/AtLS0oKytDaGgovLwcz8W+HkfrAan91Xhi2BNcD6gHaO9zzCHlRE525bwbbw/7ff5cNe+GiIiIiG4eI4NGIlYZyx1vejkG3ERO1p3zboiIiIjo5iERJE7d+ou6Hn8eIXKy7px3Q0REREREPQe/8dMtzSyacerSKXxZ/SVOXToFs2h2Srkjg0ZicfxiDOszDI36RpxrOodGfSMi+0RicfxizrshIiIiIroFcEg53bIcLUQR6h+KJ4c96ZSAmPNuiIiIiIhubQy46ZakOa/Bui/XoV5XD6VMCS93L7QYW1B8sRjrvlzntF5ozrshIiIiIrp1sauNbjlm0Yz3i99Hva4eg30Hw9vDGxJBAm8Pb6h8VWjQNWBb8TanDS8nIiIiIqJbEwNuuuWU1pWirK4MSpnSZgVxABAEAX1lfXGm7gxK60q7qYVERERERHQzYMBNt5wGXQP0Zj283B1vUC91l0Jv1qNB19DFLSMiIiIiopsJA2665SikCnhKPNFibHF4XmfUwVPiCYVU0cUtIyIiIiLq+QRBwM6dO7u7Gb0CA2665YT5hyHUPxQXLl+AKIo250RRxIXLF6D2VyPMP8yp9bpqCzIiIiIiImeprq7G3LlzoVarIZVKoVKpkJycjIMHD7qkviNHjkAQBNTV1bmkfAAYMmQIBEGwOf74xz+6rL4rcZVyuqWYRTNK60oxSjkKp+tO44fGH9BX1hdSdyl0Rh0uXL4AhVSBJ4Y94dTtu1y9BRkRERER3XxEUUSt4QJ05hZIJV4I9OhrtwaRM5WXl2Ps2LHw9/fH2rVrERMTA4PBgL179yI1NRXFxcUuq/tGiaIIk8kEd3fHIe6KFSvw3HPPWf/29fXtknaxh5tuGZrzGqT9Jw0v/+dlfHDyA7QYW9BiasFP2p9wrukcGvWNiOwT6bQtwa6sd92X63Di4gn4evpioM9A+Hr6Wrcg05zXOK0uIiIiIro5VLecxcELu3C4Zjf+c3EfDtfsxsELu1DdctZldc6ePRuCIODYsWNISUlBREQEoqOjsXDhQhw9etThNY56qDUaDQRBQHl5OQCgoqICycnJCAgIgFwuR3R0NHbv3o3y8nIkJiYCAAICAiAIAqZPnw4AMJvNyMjIQGhoKGQyGUaMGIHt27fb1btnzx7ExcVBKpWioKCgzXvz9fVFv379rIdcLr+xB6ud2MNNt4S29t2u0dbA090Tj0U8htuDb0eYf5hTe7av3oKs9RdJbw9vqNxV+KHxB2wr3oZYZaxT6yUiIiKi3qu65Sw+qz0CnbkF3m5yuAtyGEUjLurO4zPDESQE3o1+XgOdWmdtbS3y8vKQnp7uMBj19/fvdNmpqanQ6/XIz8+HXC5HUVERfHx8oFKpkJOTg5SUFJw8eRIKhQIymQwAkJGRga1bt2LTpk0IDw9Hfn4+Jk+eDKVSifHjx1vLTktLw7p166BWqxEQENBmG/74xz9i5cqVGDx4MJ588kksWLCgzd5wZ2LATTe9awW9gxWD8UPjD/j6/Nd4NOJRlNaVokHXAIVU4ZTguyNbkEUERNxQXURERETU+4miiO8aC6Ezt0Dh7m/9DukheMJd8ECjsQ7fN36NYOkApw4vLy0thSiKGDZsmNPKbFVZWYmUlBTExMQAANRqtfVcYGAgACAoKMga1Ot0OqxevRoHDhxAQkKC9ZqCggJs3rzZJuBesWIFkpKSrln/vHnzcPvttyMwMBCffvopXnrpJVRVVeG1115z5m06xICbbnrtCXq/u/Ad5h6aiwvaC06dY92eLcgutlzkFmREREREBACoNVxAnb4W3m5yh99dZW5yXNJfRK3hAvp4Kp1W79WLCTvTvHnzMGvWLOzbtw8TJkxASkoKYmNj28xfWloKrVZrF0jr9XqMGjXKJi0+Pv669S9cuND637GxsfD09MQLL7yAjIwMSKXSDt5Nx3AMK930rhf06kw6/KT9CaWXSp0+x5pbkBERERFRR+jMLTDBCHfBcd+om+AOE4zQmR1/v+ys8PBwCILQ4YXRJBJLSHllwG4wGGzyzJgxA2fOnMGUKVNw/PhxxMfHIzMzs80ym5qaAAC5ubnQaDTWo6ioyGYeN4BOzcW+8847YTQarXPMXYkBN930rhX0ihDxQ+MPMItmDPQZCG8Pb0gEiWWOta8KDboGbCve1uktvLprCzIiIiIi6p2kEi+4wR1G0ejwvEk0wg3ukEocdyZ1VmBgICZOnIisrCw0NzfbnW9r2y6l0tLLXlVVZU3TaDR2+VQqFWbOnIkdO3Zg0aJF2LJlCwDA09MTAGAymax5o6KiIJVKUVlZibCwMJtDpVJ19hZt2ieRSBAUFHTDZV0PA2666V0r6G3SN6HZ0AxfD1/IPWx/Hbt6jnVnSAQJnhz2JBRSBX5o/AFagxYm0QStQYsfGn9wyRZkRERERNR7BXr0hb9nIC6bmh122Fw2NSPAsw8CPfo6ve6srCyYTCaMHj0aOTk5KCkpwYkTJ/DGG29Y51JfrTUIXrZsGUpKSpCbm4v169fb5Jk/fz727t2LsrIyFBYW4vDhw4iMjAQAhISEQBAE7Nq1CzU1NWhqaoKvry8WL16MBQsWIDs7G6dPn0ZhYSEyMzORnZ3doXv67LPP8Oc//xnffPMNzpw5g/feew8LFizA5MmTr7nImrPwWz7d9K4V9FY1V0EQBKh8VQ4XnZC6S6E3629ojvXIoJFYHL8Yw/oMQ6O+0aVbkBERERFR7yYIAob73g5PiRcajXUwmPUwi2YYzHo0GusglXgh2neUS/bjVqvVKCwsRGJiIhYtWoThw4cjKSkJBw8exMaNGx1e4+HhgW3btqG4uBixsbFYs2YNVq1aZZPHZDIhNTUVkZGRmDRpEiIiIrBhwwYAwMCBA7F8+XKkpaUhODgYc+bMAQCsXLkSS5YsQUZGhvW63NxchIaGduiepFIpPvjgA4wfPx7R0dFIT0/HggUL8Oabb3biEeo4QXTl7PgepqGhAX5+fqivr4dCwTmztxrNeQ3eL34fZXVl1oXRlN5KVDRUINg7GN4e3nbXaA1aNOobkf6z9BteRdwsmp2+CjoRERHRza43fodvaWlBWVkZQkND4eXVuaHf1S1n8V1jIer0tTDBMow8wLMPon1HOX1LMOq49j7HXKWcbhkjg0YiVhlrE/Sq/dT4fcHvUXyxGCp3217u1jnWkX0inTLHWiJIuPUXEREREbVLP6+BCJYOQK3hAnTmFkglXgj06OuSnm1yHQbcdEtxFPQ+OexJrPtyHX5o/AF9ZX0hdZdCZ9ThwuULnGNNRERERN1GEASnbv1FXY9RBN3yOMeaiIiIiIhcgT3cRHA83JxzrImIiIiI6EYw4Cb6L86xJiIiIiIiZ2L3HREREREREZELMOAmIiIiIiIicgEG3EREREREREQuwICbiIiIiIiI2k0QBOzcubO7m9ErMOCmXsssmnHq0il8Wf0lTl06BbNo7u4mERERERH1atXV1Zg7dy7UajWkUilUKhWSk5Nx8OBBl9R35MgRCIKAuro6l5TfKjc3F3feeSdkMhkCAgLw8MMPu7S+VlylnHolzXkN3i9+H2V1ZdCb9fCUeCLUPxRPDnuy2/fNNotmbi9GRERERL1OeXk5xo4dC39/f6xduxYxMTEwGAzYu3cvUlNTUVxc3N1NbJMoijCZTHB3tw9xc3Jy8Nxzz2H16tX4xS9+AaPRiO+++65L2sUogHodzXkN1n25DicunoCvpy8G+gyEr6cvii8WY92X66A5r+nWtqX9Jw0v/+dlrDy6Ei//52Wk/SetW9tERERERL2TaDaj5eQpaL/4Ai0nT0E0u3ZE5+zZsyEIAo4dO4aUlBREREQgOjoaCxcuxNGjRx1e46iHWqPRQBAElJeXAwAqKiqQnJyMgIAAyOVyREdHY/fu3SgvL0diYiIAICAgAIIgYPr06QAAs9mMjIwMhIaGQiaTYcSIEdi+fbtdvXv27EFcXBykUikKCgrs2mc0GvHiiy9i7dq1mDlzJiIiIhAVFYVf//rXznnQroM93NSrmEUz3i9+H/W6egz2HQxBEAAA3h7eULmr8EPjD9hWvA2xytgu71Vu/SGgXlcPpUwJL3cvtBhbrD8ELI5f3O2970RERETUO2gLv8al996D7sxpiHo9BE9PSNVDEfDUU/C+fZTT66utrUVeXh7S09Mhl8vtzvv7+3e67NTUVOj1euTn50Mul6OoqAg+Pj5QqVTIyclBSkoKTp48CYVCAZlMBgDIyMjA1q1bsWnTJoSHhyM/Px+TJ0+GUqnE+PHjrWWnpaVh3bp1UKvVCAgIsKu7sLAQZ8+ehUQiwahRo1BdXY2RI0di7dq1GD58eKfvqb0YcFOvUlpXirK6MihlSmuw3UoQBPSV9cWZujMorStFREBEl7WrJ/8QQERERES9i7bwa5x/9VWY6uvhrlRCkEoh6nRoOXEC5199FUG/+53Tg+7S0lKIoohhw4Y5tVwAqKysREpKCmJiYgAAarXaei4wMBAAEBQUZA3qdTodVq9ejQMHDiAhIcF6TUFBATZv3mwTcK9YsQJJSUlt1n3mzBkAwLJly/Daa69hyJAhWL9+Pe6++26cOnXKWr+r8Js/9SoNugbozXp4uXs5PC91l0Jv1qNB19Cl7erIDwFERERERG0RzWZceu89mOrr4TF4MCTe3hDc3CDx9oaHSgVTQwMuvf+e04eXi6Lo1PKuNG/ePKxatQpjx47F0qVL8e23314zf2lpKbRaLZKSkuDj42M93nnnHZw+fdomb3x8/DXLMv/3cXr55ZeRkpKCuLg4vP322xAEAR999NGN3Vg7MOCmXkUhVcBT4okWY4vD8zqjDp4STyikii5tV0/9IYCIiIiIehddSSl0Z05berYddOS49+kD3enT0JU4tyMnPDwcgiB0eGE0icQSUl4ZsBsMBps8M2bMwJkzZzBlyhQcP34c8fHxyMzMbLPMpqYmAJaVxTUajfUoKiqymccNwOHw9yv1798fABAVFWVNk0qlUKvVqKysbMcd3hgG3NSrhPmHIdQ/FBcuX7D7FU4URVy4fAFqfzXC/MO6tF099YcAIiIiIupdzA31ljnbUqnD84KXF0S9HuaGeqfWGxgYiIkTJyIrKwvNzc1259vatkupVAIAqqqqrGkajcYun0qlwsyZM7Fjxw4sWrQIW7ZsAQB4enoCAEwmkzVvVFQUpFIpKisrERYWZnOoVKoO3VfrgmonT560phkMBpSXlyMkJKRDZXUGA27qVSSCBE8OexIKqQI/NP4ArUELk2iC1qDFD40/QCFV4IlhT3T5POme+kMAEREREfUuEoUfBE9PiDqdw/NiSwsET09IFH5OrzsrKwsmkwmjR49GTk4OSkpKcOLECbzxxhvWudRXaw2Cly1bhpKSEuTm5mL9+vU2eebPn4+9e/eirKwMhYWFOHz4MCIjIwEAISEhEAQBu3btQk1NDZqamuDr64vFixdjwYIFyM7OxunTp1FYWIjMzExkZ2d36J4UCgVmzpyJpUuXYt++fTh58iRmzZoFAHjsscc68Sh1DANu6nVGBo3E4vjFGNZnGBr1jTjXdA6N+kZE9onstpXAe+oPAURERETUu0jDwyBVD4XxguOOHOPFi5AOHQppuPM7ctRqNQoLC5GYmIhFixZh+PDhSEpKwsGDB7Fx40aH13h4eGDbtm0oLi5GbGws1qxZg1WrVtnkMZlMSE1NRWRkJCZNmoSIiAhs2LABADBw4EAsX74caWlpCA4Oxpw5cwAAK1euxJIlS5CRkWG9Ljc3F6GhoR2+r7Vr1+Lxxx/HlClTcMcdd6CiogKHDh1yuKq5swmiK2fH9zANDQ3w8/NDfX09FAoO7e3tzKIZpXWlaNA1QCFVIMw/rNsDWs15Dd4vfh9ldWXQm/XwlHhC7a/GE8Oe4JZgRERERJ3QG7/Dt7S0oKysDKGhofDycrzGz7VYVylvaIB7nz6WYeQtLTBevAg3hcIlq5RTx7T3Oea2YNRrSQRJl2791R4jg0YiVhnb434IICIiIqLew/v2UQj63e/+tw937UUInp7wiopEwJOu2YebXIMBN5GT9cQfAoiIiIiod/G+fRRkI0dAV1IKc0M9JAo/SMPDIEjYkdObMOAmIiIiIiLqgQSJBF63sSOnN+PPI0REREREREQuwB5uIiIiop5AFIHaWkCnA6RSIDAQEITubhUREd0ABtxERERE3a26GvjuOFB3CTCZADc3wD8AGB4D9OvX3a0jIqJO4pByIiIiou5UXQ189ilw4QLgKQV8FZZ/L16wpFdXd3cLiYiokxhwExEREXUXUbT0bOt0gEIBeHhYhpF7eFgCb70O+P47Sz4iIup1GHATERERdZfaWsswcm9v+/naggDIvIFLtZZ8RETU6zDgJiIiIuouOp1lzrZ7G8vquLlZzut0XdsuIqJrEAQBO3fu7O5m9AoMuImIiIi6i1RqCaqNRsfnWxdQk0o7XrYoAhcvAufOWf7lsHQiaofq6mrMnTsXarUaUqkUKpUKycnJOHjwoEvqO3LkCARBQF1dnUvLd3R88cUXLqnzSlylnIiIiKi7BAZaViO/eAFwV9gOKxdF4LIW6Ku05GuP1q3FqqqAinJA2wyYzVz1nIjapby8HGPHjoW/vz/Wrl2LmJgYGAwG7N27F6mpqSguLu7uJrZJFEWYTCa4XzViaMyYMaiqqrJJW7JkCQ4ePIj4+HiXt4s93ERERETdoTU47tcfkEiAhnrAYLAEyAYD0Nhg6dmOHt6+/birq4GDB4B9e4GjnwLnzgJarWXFc656TtQ7dfFIldmzZ0MQBBw7dgwpKSmIiIhAdHQ0Fi5ciKNHjzq8xlEPtUajgSAIKC8vBwBUVFQgOTkZAQEBkMvliI6Oxu7du1FeXo7ExEQAQEBAAARBwPTp0wEAZrMZGRkZCA0NhUwmw4gRI7B9+3a7evfs2YO4uDhIpVIUFBTYtc/T0xP9+vWzHn369ME///lPPP300xDa89l6g9jDTURERNTVrt532yxa/m1usgTfbm6Wnu3o4e3rkW7dWkyns6xsDljmhRuNljoCAiyrnjc2WFY9Dw5uXxBPRN3n6s8JF49Uqa2tRV5eHtLT0yGXy+3O+/v7d7rs1NRU6PV65OfnQy6Xo6ioCD4+PlCpVMjJyUFKSgpOnjwJhUIBmUwGAMjIyMDWrVuxadMmhIeHIz8/H5MnT4ZSqcT48eOtZaelpWHdunVQq9UICAi4blv+9a9/4eLFi3j66ac7fT8dwYCbiIiIqCtdGRx7e/8vMNZqLV+oI6OA/v0tw8jbExRfubWYzMsyjNzdHRAklsNkBBobgT5S21XP+/Rx/b0SUee09TnROlIlYYzTg+7S0lKIoohhw4Y5tVwAqKysREpKCmJiYgAAarXaei7wv1NmgoKCrEG9TqfD6tWrceDAASQkJFivKSgowObNm20C7hUrViApKandbXnrrbcwceJEDBo06EZvq10YcBMRERF1lav33W4NqD08LH83NgA/VQPR0e3vgb5yazGzyVJH67WCAEjcLEPUDQbLl3auek7Us13rc8LddSNVRBcOV583bx5mzZqFffv2YcKECUhJSUFsbGyb+UtLS6HVau0Cab1ej1GjRtmkdWQe9o8//oi9e/fiH//4R8du4AZwDjcRERFRV3HFvttXbi0mkVjKufKLc+vfZvONrXpORF3DFZ8T7RAeHg5BEDq8MJpEYgkprwzYDQaDTZ4ZM2bgzJkzmDJlCo4fP474+HhkZma2WWZTUxMAIDc3FxqNxnoUFRXZzOMG4HD4e1vefvtt9OnTB7/85S/bfc2NYsBNRERE1FVcse/2lVuLeXhaesFMJgD//fLb2uMtCJZVzwMC27/qORF1PVd8TrRDYGAgJk6ciKysLDQ3N9udb2vbLqVSCQA2K4FrNBq7fCqVCjNnzsSOHTuwaNEibNmyBYBlUTMAMJlM1rxRUVGQSqWorKxEWFiYzaFSqTp1f6Io4u2338bUqVPh4eHRqTI6gwE3ERERUVdxxb7brVuLXdZagmtfhaWn22i0DDE3/7fMlssdW/WciLqHKz4n2ikrKwsmkwmjR49GTk4OSkpKcOLECbzxxhvWudRXaw2Cly1bhpKSEuTm5mL9+vU2eebPn4+9e/eirKwMhYWFOHz4MCIjIwEAISEhEAQBu3btQk1NDZqamuDr64vFixdjwYIFyM7OxunTp1FYWIjMzExkZ2d36t4OHTqEsrIyzJgxo1PXdxYDbiIiIqKucnVw3MqgtwTETY2W8x3pgRYEy6rFnlLL3E6JxFJG6yJLogh4egLKIOAu5y+0RERO1tbnBGD524UjVdRqNQoLC5GYmIhFixZh+PDhSEpKwsGDB7Fx40aH13h4eGDbtm0oLi5GbGws1qxZg1WrVtnkMZlMSE1NRWRkJCZNmoSIiAhs2LABADBw4EAsX74caWlpCA4Oxpw5cwAAK1euxJIlS5CRkWG9Ljc3F6GhoZ26t7feegtjxoxxyaJw1yKIrpwd38M0NDTAz88P9fX1UCgU3d0cIiIiuhW1rj6s1wFu7pZVxVv335ZILAshxY/ueGDsaAshbzkwOKRjq54T9TC98Tt8S0sLysrKEBoaCi8vr44XcOXnhMz7f8PIL2stPdv88azbtfc55irlRERERF2pXz/Llj5ffmH5Ui3+N9D28rJ8sW5u7ty2P/36WYL12lrL3E6plEE2UW/V+jlx9Y9ofZWWaSEMtnsNBtxEREREXS042BIQy2SWw83NsthZ64rind32RxC4vzbRzYI/ot0UGHATERERdbXaWqC+DvD1tQTaV7p62x8G0ES3Lv6I1utx0TQiIiKirtZN2/4QEVHXYsBNRERE1NW6cdsfIiLqOgy4iYiIiLpaN277Q0REXYcBNxEREVFXu3rv7NZtwQwGy99SqWUlYi6ORETUq3HRNCIiIqIbJYodX0mY2/4QEd30GHATERER3Yjqavug2T/A0oN9vaCZ2/4QEd3UOKSciIiIqLOqq4HPPgUuXLAMD/dVWP69eMGSXl19/TJat/0ZMMDyL4NtIurhBEHAzp07u7sZvQIDbiIiIqLOEEVLz7ZOBygUlv20BcHyr68C0OuA77+zXxSNiKgHq66uxty5c6FWqyGVSqFSqZCcnIyDBw+6pL4jR45AEATU1dW5pHwAOHXqFB566CH07dsXCoUC48aNw+HDh11W35UYcBMRERF1Rm2tZRi5t7d9r7QgADJv4FKtJR8RUS9QXl6OuLg4HDp0CGvXrsXx48eRl5eHxMREpKamdnfzrkkURRjb2GrxwQcfhNFoxKFDh/DVV19hxIgRePDBB1HdnlFIN4gBNxEREVFn6HSWOdvubSyJ4+ZmOa/TdW27iOimIZpFXDzbhHMll3DxbBNEs2tHzMyePRuCIODYsWNISUlBREQEoqOjsXDhQhw9etThNY56qDUaDQRBQHl5OQCgoqICycnJCAgIgFwuR3R0NHbv3o3y8nIkJiYCAAICAiAIAqZPnw4AMJvNyMjIQGhoKGQyGUaMGIHt27fb1btnzx7ExcVBKpWioKDArn0XLlxASUkJ0tLSEBsbi/DwcPzxj3+EVqvFd99955wH7hq4aBoRERFRZ0illqDaaLQMI79a6wJqUmnXt42Ier2q0/U4fuRHXKpqhslohpu7BAH95Yi5exD6D/Vzen21tbXIy8tDeno65HK53Xl/f/9Ol52amgq9Xo/8/HzI5XIUFRXBx8cHKpUKOTk5SElJwcmTJ6FQKCCTyQAAGRkZ2Lp1KzZt2oTw8HDk5+dj8uTJUCqVGD9+vLXstLQ0rFu3Dmq1GgEBAXZ19+nTB7fddhveeecd3H777ZBKpdi8eTOCgoIQFxfX6XtqLwbcRERERJ0RGGhZjfziBcBdYTusXBSBy1rLFl+Bgd3XRiLqlapO1+OTnBLomo2Q+3nCzUMCk8GMmh8a8UlOCcamhDs96C4tLYUoihg2bJhTywWAyspKpKSkICYmBgCgVqut5wL/+xkZFBRkDep1Oh1Wr16NAwcOICEhwXpNQUEBNm/ebBNwr1ixAklJSW3WLQgCDhw4gIcffhi+vr6QSCQICgpCXl6ewwDd2RhwExEREXWGIFi2/vrsU6CxwTJnu3UY+WWtpWc7ejhXHSeiDhHNIo4f+RG6ZiP8lF4Q/vsZIpG6wa+vF+ovtOD4kR/RL1QBQeK8zxfRhQs8zps3D7NmzcK+ffswYcIEpKSkIDY2ts38paWl0Gq1doG0Xq/HqFGjbNLi4+OvWbcoikhNTUVQUBD+85//QCaT4a9//SuSk5PxxRdfoH///p2/sXbgHG4iIiKizurXD0gYA/Tpa1mVvKnR8m9fJXDXmOvvw01EdJXaqmZcqmqG3M/TGmy3EgQB3r6euFTVjNqqZqfWGx4eDkEQUFxc3KHrJBJLSHllwG4wGGzyzJgxA2fOnMGUKVNw/PhxxMfHIzMzs80ym5qaAAC5ubnQaDTWo6ioyGYeNwCHw9+vdOjQIezatQsffPABxo4di9tvvx0bNmyATCZDdnZ2h+61M9jDTURERHQj+vUDgoMtq5HrdJae7cBA9mwTUafotAbLnG0Px32j7p4SXG40Q6c1ODzfWYGBgZg4cSKysrIwb948u0C2rq7O4TxupVIJAKiqqrIO0dZoNHb5VCoVZs6ciZkzZ+Kll17Cli1bMHfuXHh6egIATCaTNW9UVBSkUikqKyttho93hlarBfC/HwZaSSQSmM3mGyq7PdjDTURERHSjBAHo0wcYMMDyL4NtIuokqbcH3Nwtc7YdMeotC6hJvR0s1niDsrKyYDKZMHr0aOTk5KCkpAQnTpzAG2+8YZ1LfbWwsDCoVCosW7YMJSUlyM3Nxfr1623yzJ8/H3v37kVZWRkKCwtx+PBhREZGAgBCQkIgCAJ27dqFmpoaNDU1wdfXF4sXL8aCBQuQnZ2N06dPo7CwEJmZmR3ulU5ISEBAQACmTZuGb775BqdOncJvf/tblJWV4YEHHujcA9UBDLiJiIiIiIh6iMD+cgT0l6O5QW83r1oURWgb9QjoL0dg/2sPpe4MtVqNwsJCJCYmYtGiRRg+fDiSkpJw8OBBbNy40eE1Hh4e2LZtG4qLixEbG4s1a9Zg1apVNnlMJhNSU1MRGRmJSZMmISIiAhs2bAAADBw4EMuXL0daWhqCg4MxZ84cAMDKlSuxZMkSZGRkWK/Lzc1FaGhoh+6pb9++yMvLQ1NTE37xi18gPj4eBQUF+Oc//4kRI0Z04lHqGEF05ez4HqahoQF+fn6or6+HQqHo7uYQEREREdF19Mbv8C0tLSgrK0NoaCi8vLw6fL11lXKtEd6+nnD3lMCoN0PbqIeXtzvGuGCVcuqY9j7H7OEmIiIiIiLqQfoP9cPYlHAoVb7QXzai8WIL9JeNUKp8GWz3Mlw0jYiIiIiIqIfpP9QP/UIVqK1qhk5rgNTbA4H95U7dCoxcjwE3ERERERFRDyRIBPQZ6NPdzaAbwCHlRERERERERC7AgJuIiIiIiIjIBRhwExEREREREbkAA24iIiIiIiIiF+g1AXdGRgbuuOMO+Pr6IigoCA8//DBOnjzZ3c0iIiIiIiIicqjXBNz//ve/kZqaiqNHj2L//v0wGAy499570dzc3N1NIyIiIiIiIrLTa7YFy8vLs/n773//O4KCgvDVV1/h5z//eTe1ioiIiIiI6NYiCAI+/vhjPPzww93dlB6v1/RwX62+vh4AEBgY2GYenU6HhoYGm4OIiIiIiIgcq66uxty5c6FWqyGVSqFSqZCcnIyDBw+6pL4jR45AEATU1dW5pHwAKCwsRFJSEvz9/dGnTx88//zzaGpqcll9V+qVAbfZbMb8+fMxduxYDB8+vM18GRkZ8PPzsx4qlaoLW0lERERERNR7lJeXIy4uDocOHcLatWtx/Phx5OXlITExEampqd3dvGsSRRFGo9Eu/dy5c5gwYQLCwsLw+eefIy8vD99//z2mT5/eJe3qlQF3amoqvvvuO3zwwQfXzPfSSy+hvr7eevzwww9d1EIiIiIiIqIbI4oimhtaUH+hGc0NLRBF0aX1zZ49G4Ig4NixY0hJSUFERASio6OxcOFCHD161OE1jnqoNRoNBEFAeXk5AKCiogLJyckICAiAXC5HdHQ0du/ejfLyciQmJgIAAgICIAiCNRA2m83IyMhAaGgoZDIZRowYge3bt9vVu2fPHsTFxUEqlaKgoMCufbt27YKHhweysrJw22234Y477sCmTZuQk5OD0tJS5zxw19Br5nC3mjNnDnbt2oX8/HwMGjTomnmlUimkUmkXtYyIiIiIiMg5Gmq1qD5TC22jDqJZhCAR4O0rRT91IBSB3k6vr7a2Fnl5eUhPT4dcLrc77+/v3+myU1NTodfrkZ+fD7lcjqKiIvj4+EClUiEnJwcpKSk4efIkFAoFZDIZAMto5a1bt2LTpk0IDw9Hfn4+Jk+eDKVSifHjx1vLTktLw7p166BWqxEQEGBXt06ng6enJySS//U1t9ZRUFCAsLCwTt9Xe/SagFsURcydOxcff/wxjhw5gtDQ0O5uEhERERERkdM11GpR/t1PMBpM8JS6Q+ImwGwS0VzfgvLvfsKQ4cFOD7pLS0shiiKGDRvm1HIBoLKyEikpKYiJiQEAqNVq67nWNbmCgoKsQb1Op8Pq1atx4MABJCQkWK8pKCjA5s2bbQLuFStWICkpqc26f/GLX2DhwoVYu3YtXnzxRTQ3NyMtLQ0AUFVV5dT7dKTXDClPTU3F1q1b8f7778PX1xfV1dWorq7G5cuXu7tpRERERERETiGKIqrP1MJoMMHL2wNu7hIIggA3dwmk3h4wGUyoPlPr9OHlrhyuPm/ePKxatQpjx47F0qVL8e23314zf2lpKbRaLZKSkuDj42M93nnnHZw+fdomb3x8/DXLio6ORnZ2NtavXw9vb2/069cPoaGhCA4Otun1dpVeE3Bv3LgR9fX1uPvuu9G/f3/r8eGHH3Z304iIiIiIiJxC26iDtlEHT6k7BEGwOScIAjyk7tY8zhQeHg5BEFBcXNyh61qD1isDdoPBYJNnxowZOHPmDKZMmYLjx48jPj4emZmZbZbZuoJ4bm4uNBqN9SgqKrKZxw3A4fD3qz355JOorq7G2bNncfHiRSxbtgw1NTU2Pe2u0msCblEUHR5dtbocERERERGRqxn1JohmERI3weF5iZsA0SzCqDc5td7AwEBMnDgRWVlZaG5utjvf1rZdSqUSgO3wbI1GY5dPpVJh5syZ2LFjBxYtWoQtW7YAADw9PQEAJtP/7icqKgpSqRSVlZUICwuzOW5k56ng4GD4+Pjgww8/hJeX1zWHojtLrwm4iYiIiIiIbnbunm4QJJY5246YTZYF1Nw93Zxed1ZWFkwmE0aPHo2cnByUlJTgxIkTeOONN6xzqa/WGgQvW7YMJSUlyM3Nxfr1623yzJ8/H3v37kVZWRkKCwtx+PBhREZGAgBCQkIgCAJ27dqFmpoaNDU1wdfXF4sXL8aCBQuQnZ2N06dPo7CwEJmZmcjOzu7wff3lL39BYWEhTp06haysLMyZMwcZGRk3tBBcezHgJiIiIiIi6iG8faXw9pXCoDPazasWRREGndGax9nUajUKCwuRmJiIRYsWYfjw4UhKSsLBgwexceNGh9d4eHhg27ZtKC4uRmxsLNasWYNVq1bZ5DGZTEhNTUVkZCQmTZqEiIgIbNiwAQAwcOBALF++HGlpaQgODsacOXMAACtXrsSSJUuQkZFhvS43N7dTi2cfO3YMSUlJiImJwZtvvonNmzdj3rx5HS6nMwTR1Zu59SANDQ3w8/NDfX09FApFdzeHiIiIiIiuozd+h29paUFZWRlCQ0Ph5eXV4etbVyk3GUzwuGKVcoPOCHcPN4S4YJVy6pj2Psfs4SYiIiIiIupBFIHeGDI8GHI/LxgNJui0BhgNJsj9vBhs9zK9Zh9uIiIiIiKiW4Ui0Bu+ATJoG3Uw6k1w93SDt6/UbuVy6tkYcBMREREREfVAgiBAruj4kHTqOTiknIiIiIiIiMgFGHATERERERERuQADbiIiIiIiIiIXYMBNRERERERE5AIMuImIiIiIiIhcgAE3ERERERERkQsw4CYiIiIiIqJ2EwQBO3fu7O5m9AoMuImIiIiIiAgAUF1djblz50KtVkMqlUKlUiE5ORkHDx50SX1HjhyBIAioq6tzSfkAkJ6ejjFjxsDb2xv+/v4O81RWVuKBBx6At7c3goKC8Nvf/hZGo/GG63a/4RKIiIiIiIio1ysvL8fYsWPh7++PtWvXIiYmBgaDAXv37kVqaiqKi4u7u4ltEkURJpMJ7u72Ia5er8djjz2GhIQEvPXWW3bnTSYTHnjgAfTr1w+ffvopqqqqMHXqVHh4eGD16tU31C72cBMREREREfVAotmMmspy/Fj0HWoqyyGazS6tb/bs2RAEAceOHUNKSgoiIiIQHR2NhQsX4ujRow6vcdRDrdFoIAgCysvLAQAVFRVITk5GQEAA5HI5oqOjsXv3bpSXlyMxMREAEBAQAEEQMH36dACA2WxGRkYGQkNDIZPJMGLECGzfvt2u3j179iAuLg5SqRQFBQUO27h8+XIsWLAAMTExDs/v27cPRUVF2Lp1K0aOHIn77rsPK1euRFZWFvR6fQcfRVvs4SYiIqIOM5vNqKmpweXLlyGTyaBUKiGR8Hd8IiJnOXvyBDR7/x8unv0RJoMBbh4e6DNwEEZOTMbA2yKdXl9tbS3y8vKQnp4OuVxud76todjtkZqaCr1ej/z8fMjlchQVFcHHxwcqlQo5OTlISUnByZMnoVAoIJPJAAAZGRnYunUrNm3ahPDwcOTn52Py5MlQKpUYP368tey0tDSsW7cOarUaAQEBnWrfZ599hpiYGAQHB1vTJk6ciFmzZuH777/HqFGjOn3vDLiJiIioQyorK3Hs2DFcuHABRqMR7u7u6Nu3L0aPHo3Bgwd3d/OIiHq9sydP4N/vvoWW5ibI/f3h7imFUa/DT2Vn8O9338L4Kc86PeguLS2FKIoYNmyYU8sFLP/fSElJsfYwq9Vq67nAwEAAQFBQkDWo1+l0WL16NQ4cOICEhATrNQUFBdi8ebNNwL1ixQokJSXdUPuqq6ttgm0A1r+rq6tvqGwG3ERERNRulZWV2LdvHy5fvgxfX1+4u7vDaDSiqqoK+/btw7333sugm4joBohmMzR7/x9ampvgH9wPgiAAADy9ZPAI9kLdT9XQ7N2FAeG3QXDiyCJRFJ1W1tXmzZuHWbNmYd++fZgwYQJSUlIQGxvbZv7S0lJotVq7QFqv19v1NsfHx7ukzc7CsV9ERETULmazGceOHcPly5cRGBgIT09PSCQSeHp6IjAwEJcvX8YXX3wBs4vnGBIR3cwu/FiJi2d/hNzf3xpstxIEAXJ/f1w8+wMu/Fjp1HrDw8MhCEKHF0ZrnU50ZcBuMBhs8syYMQNnzpzBlClTcPz4ccTHxyMzM7PNMpuamgAAubm50Gg01qOoqMhmHjcAh8PfO6pfv3746aefbNJa/+7Xr98Nlc2Am4iIiNqlpqYGFy5cgK+vr8Mvgb6+vqipqUFNTU03tZCIqPfTNTXBZDDA3VPq8Ly7pxQmgwG6/walzhIYGIiJEyciKysLzc3Ndufb2rZLqVQCAKqqqqxpGo3GLp9KpcLMmTOxY8cOLFq0CFu2bAEAeHp6ArCsFN4qKioKUqkUlZWVCAsLszlUKlVnb7FNCQkJOH78OM6fP29N279/PxQKBaKiom6obAbcRERE1C6XL1+2ztl2pHV4+eXLl7u4ZURENw+pjw/cPDxg1OscnjfqdXDz8IDUx8fpdWdlZcFkMmH06NHIyclBSUkJTpw4gTfeeMM6l/pqrUHwsmXLUFJSgtzcXKxfv94mz/z587F3716UlZWhsLAQhw8fRmSkZQ56SEgIBEHArl27UFNTg6amJvj6+mLx4sVYsGABsrOzcfr0aRQWFiIzMxPZ2dkdvq/KykpoNBpUVlbCZDJZe8xbe9LvvfdeREVFYcqUKfjmm2+wd+9evPLKK0hNTYVU6viHj/ZiwE1ERETtIpPJrEG1I63BeOsKs0RE1HF9Bw1Gn4GD0FxXZzevWhRFNNfVoc9AFfoOcv56GWq1GoWFhUhMTMSiRYswfPhwJCUl4eDBg9i4caPDazw8PLBt2zYUFxcjNjYWa9aswapVq2zymEwmpKamIjIyEpMmTUJERAQ2bNgAABg4cCCWL1+OtLQ0BAcHY86cOQCAlStXYsmSJcjIyLBel5ubi9DQ0A7f1x/+8AeMGjUKS5cuRVNTE0aNGoVRo0bhyy+/BAC4ublh165dcHNzQ0JCAiZPnoypU6dixYoVHa7raoLoytnxPUxDQwP8/PxQX18PhULR3c0hIiLqVcxmM3bs2IGqqioEBgbaDCsXRRG1tbUYMGAAHnnkEW4RRkRO0xu/w7e0tKCsrAyhoaHw8vLq8PVtrVLeXFcHL7mPS1Ypp45p73PM/xsSERFRu0gkEowePRoymQy1tbXQ6/Uwm83Q6/Wora2FTCbDHXfcwWCbiOgGDbwtEuOnPIvgUDV0Wi0aas5Dp9UiOHQog+1ehtuCERERUbsNHjwY9957r90+3AMGDMAdd9zBLcGIiJxk4G2RGBB+Gy78WAldUxOkPj7oO2iwU7cCI9djwE1EREQdMnjwYAwaNAg1NTW4fPkyZDIZlEole7aJiJxMkEigHDyku5tBN4ABNxEREXWYRCJBcHBwdzeDiIioR+NP0UREREREREQuwICbiIiIiIiIyAUYcBMRERERERG5AANuIiIiIiIiIhdgwE1ERERERETkAgy4iYiIiIiIqN0EQcDOnTu7uxm9AgNuIiIiIiIiAgBUV1dj7ty5UKvVkEqlUKlUSE5OxsGDB11S35EjRyAIAurq6lxSPgCkp6djzJgx8Pb2hr+/v8M88+bNQ1xcHKRSKUaOHOm0uhlwExEREREREcrLyxEXF4dDhw5h7dq1OH78OPLy8pCYmIjU1NTubt41iaIIo9Ho8Jxer8djjz2GWbNmXbOMZ555Br/5zW+c2i4G3ERERERERD2QKIporr2I+qpzaK69CFEUXVrf7NmzIQgCjh07hpSUFERERCA6OhoLFy7E0aNHHV7jqIdao9FAEASUl5cDACoqKpCcnIyAgADI5XJER0dj9+7dKC8vR2JiIgAgICAAgiBg+vTpAACz2YyMjAyEhoZCJpNhxIgR2L59u129e/bssfZMFxQUOGzj8uXLsWDBAsTExLR572+88QZSU1OhVqs78Ihdn7tTSyMiIiIiIqIb1vBTFaqKjuNy3SWYTSZI3Nwg8w9A/6gYKIL7O72+2tpa5OXlIT09HXK53O58W0Ox2yM1NRV6vR75+fmQy+UoKiqCj48PVCoVcnJykJKSgpMnT0KhUEAmkwEAMjIysHXrVmzatAnh4eHIz8/H5MmToVQqMX78eGvZaWlpWLduHdRqNQICAjrdRldhwE1ERERERNSDNPxUhfJjn8Co08FDJoeHuxvMRhOaL9ag/NgnGDJ6rNOD7tLSUoiiiGHDhjm1XACorKxESkqKtYf5yl7kwMBAAEBQUJA1qNfpdFi9ejUOHDiAhIQE6zUFBQXYvHmzTcC9YsUKJCUlOb3NzsKAm4iIiIiIqIcQRRFVRcdh1Okg9fWDIAgAADcPCSTuftA11qPqxHH4BvWznnNWva4yb948zJo1C/v27cOECROQkpKC2NjYNvOXlpZCq9XaBdJ6vR6jRo2ySYuPj3dJm52Fc7iJiIiIiIh6CO2lWlyuuwQPmdwuoBYEAR4yb1y+dAnaS7VOrTc8PByCIKC4uLhD10kklpDyyoDdYDDY5JkxYwbOnDmDKVOm4Pjx44iPj0dmZmabZTY1NQEAcnNzodForEdRUZHNPG4ADoe/9yQMuImIiIiIiHoIo05nmbPt7ubwvMTNHWaTCUadzqn1BgYGYuLEicjKykJzc7Pd+ba27VIqlQCAqqoqa5pGo7HLp1KpMHPmTOzYsQOLFi3Cli1bAACenp4AAJPJZM0bFRUFqVSKyspKhIWF2Rwqlaqzt9gtOKSciIiIiIioh3CXSiFxs8zZdvOw7x81m4yQuLnBXSp1et1ZWVkYO3YsRo8ejRUrViA2NhZGoxH79+/Hxo0bceLECbtrWoPgZcuWIT09HadOncL69ett8syfPx/33XcfIiIicOnSJRw+fBiRkZEAgJCQEAiCgF27duH++++HTCaDr68vFi9ejAULFsBsNmPcuHGor6/HJ598AoVCgWnTpnXoviorK1FbW4vKykqYTCbrDwJhYWHw8fEBYBnG3tTUhOrqaly+fNmaJyoqyvqjQGcw4CYiIiIiIuohvAMCIfMPQPPFGkjc/WyGlYuiCMNlLeR9lfAOCHR63Wq1GoWFhUhPT8eiRYtQVVUFpVKJuLg4bNy40eE1Hh4e2LZtG2bNmoXY2FjccccdWLVqFR577DFrHpPJhNTUVPz4449QKBSYNGkS/vSnPwEABg4ciOXLlyMtLQ1PP/00pk6dir///e9YuXIllEolMjIycObMGfj7++P222/H73//+w7f1x/+8AdkZ2db/26dB3748GHcfffdACzD3v/973/b5SkrK8OQIUM6XGcrQXT1Zm49SENDA/z8/FBfXw+FQtHdzSEiIiIiouvojd/hW1paUFZWhtDQUHh5eXX4ettVyr3/O4zcCMNlLdylXhgyeoxLtgaj9mvvc8w53ERERERERD2IIrg/howeC3kfJUx6PXRNjTDp9ZD3VTLY7mU4pJyIiIiIiKiHUQT3h29QP2gv1cKo08FdKoV3QKBTtwIj12PATURERERE1AMJggB5YJ/ubgbdAA4pJyIiIiIiInIBBtxERERERERELsCAm4iIiIiIiMgFGHATERERERERuQADbiIiIiIiIiIXYMBNRERERERE5AIMuImIiIiIiKjdBEHAzp07u7sZvQIDbiIiIiIiIgIAVFdXY+7cuVCr1ZBKpVCpVEhOTsbBgwddUt+RI0cgCALq6upcUj4ApKenY8yYMfD29oa/v7/d+W+++QZPPPEEVCoVZDIZIiMj8frrrzulbnenlEJERERERES9Wnl5OcaOHQt/f3+sXbsWMTExMBgM2Lt3L1JTU1FcXNzdTWyTKIowmUxwd7cPcfV6PR577DEkJCTgrbfesjv/1VdfISgoCFu3boVKpcKnn36K559/Hm5ubpgzZ84NtYs93ERERERERD2QaBZhqG6G7kw9DNXNEM2iS+ubPXs2BEHAsWPHkJKSgoiICERHR2PhwoU4evSow2sc9VBrNBoIgoDy8nIAQEVFBZKTkxEQEAC5XI7o6Gjs3r0b5eXlSExMBAAEBARAEARMnz4dAGA2m5GRkYHQ0FDIZDKMGDEC27dvt6t3z549iIuLg1QqRUFBgcM2Ll++HAsWLEBMTIzD88888wxef/11jB8/Hmq1GpMnT8bTTz+NHTt2dPARtMcebiIiIiIioh5GV9GApk/PwXheC9FohuAugXuQN3zGDIA0ROH0+mpra5GXl4f09HTI5XK7846GYrdXamoq9Ho98vPzIZfLUVRUBB8fH6hUKuTk5CAlJQUnT56EQqGATCYDAGRkZGDr1q3YtGkTwsPDkZ+fj8mTJ0OpVGL8+PHWstPS0rBu3Tqo1WoEBAR0uo1Xq6+vR2Bg4A2Xw4CbiIiIiIioB9FVNKA+9wzMWiMkvp6QeEggGswwnGtCfe4Z+D2gdnrQXVpaClEUMWzYMKeWCwCVlZVISUmx9jCr1WrrudagNigoyBrU63Q6rF69GgcOHEBCQoL1moKCAmzevNkm4F6xYgWSkpKc2t5PP/0UH374IXJzc2+4LAbcREREREREPYRoFtH06TmYtUa49fGCIAgAAEHqBsHTC6baFjR9eg6eKl8IEsF59YquG64+b948zJo1C/v27cOECROQkpKC2NjYNvOXlpZCq9XaBdJ6vR6jRo2ySYuPj3dqW7/77js89NBDWLp0Ke69994bLo9zuImIiIiIiHoI43ktjOe1kPh6WoPtVoIgQOLjac3jTOHh4RAEocMLo0kklpDyyoDdYDDY5JkxYwbOnDmDKVOm4Pjx44iPj0dmZmabZTY1NQEAcnNzodForEdRUZHNPG4ADoe/d1ZRURHuuecePP/883jllVecUiYDbiIiIiIioh7CrDVa5mx7OA7VBA8JRKMZZq3RqfUGBgZi4sSJyMrKQnNzs935trbtUiqVAICqqiprmkajscunUqkwc+ZM7NixA4sWLcKWLVsAAJ6engAAk8lkzRsVFQWpVIrKykqEhYXZHCqVqrO3eE3ff/89EhMTMW3aNKSnpzutXA4pJyIiIiIi6iEk3u4Q3C1ztgWpm9150WBZQE3i7fxQLisrC2PHjsXo0aOxYsUKxMbGwmg0Yv/+/di4cSNOnDhhd01rELxs2TKkp6fj1KlTWL9+vU2e+fPn47777kNERAQuXbqEw4cPIzIyEgAQEhICQRCwa9cu3H///ZDJZPD19cXixYuxYMECmM1mjBs3DvX19fjkk0+gUCgwbdq0Dt1XZWUlamtrUVlZCZPJZP1BICwsDD4+Pvjuu+/wi1/8AhP/P3t3Hh5lfe///3Xfs2Uy2SEQlhgIiyyCRlyK2KMcRbCW1l+p7emp1qX2e9CoX1HbYk8VQTG1SFvrF9Fa63JsbXvE2lZcUBQptlYspkUBBSQJSwKB7Mlktvv+/TFmJCRgtslkeT6uay7M3PfM/R5GYF7zWd5z5+qWW25RRUWFJMnhcMS+UOgqRrh7mW1Zqiwr0b5t76uyrES2ZSW6JAAAAAB9hHNYspzDkmU1BNusq7ZtW1ZDMHZOT8vPz9eWLVs0e/Zs3XrrrTrllFM0Z84crV+/XqtXr273MS6XS88884x27Nih6dOn67777tM999zT6pxIJKLCwkJNnjxZ8+bN08SJE/XQQw9JkkaNGqWlS5dq8eLFGj58eKzv9d1336077rhDRUVFscetXbtWY8eO7fTruvPOO1VQUKAlS5aooaFBBQUFKigo0LvvvitJevbZZ1VZWamnn35aI0aMiN3OPPPMTl/rWIYdz9XxfUxdXZ3S09NVW1urtLSe30r/s+z/cLuKX/mzjuzfp0goJIfLpSGjRuu0ufM16uTJvV4PAAAA0Ncl+jN8VzQ3N2vPnj0aO3askpKSOv342C7l/rDMFHd0GnnIktUQlOl1xmWXcnROR99jRrh7yf4Pt+vN/3lMB/d8LE9ystKyh8mTnKyDez7Wm//zmPZ/2HZ6BgAAAIDBx5OXpvRL8uUamSK7OaxIdbPs5rBcI1MI2/0Ma7h7gW1ZKn7lz2pubFDG8JzYboPuJK9cw5NUc7BCxa+8oJETTpZh8h0IAAAAMNh58tLkzk1V+FBTtB93slPOYck92goM8Ufg7gWH95XpyP598mVktLu1vy8jQ0f279XhfWXKPmlMYooEAAAA0KcYpiFXTs+1vULvI3D3gkBDgyKhkJxuT7vHnW6PmmprFfik35xtWTq8r0yBhgZ5UlI0dPRJjHwDAAAAQD9D4O4FnpQUOVwuhYMBuZO8sm1boWa/IuGIHE6HbEkOl0uelBQ2VgMAAACAAYLA3QuGjj5JQ0aN1sE9HyviC6vu8CGFmv3Rbf4NQ7KlERNOVqCxURt//biaGxvky8iQ0+1ROBiIbax23hXfJnQDAAAAQD/BPOVeYJimTps7X6Zp6lDJxwo0NsgwHTIdTtnhiKxIRI3VVfrbmt/ENlZzJ3llmqbcSV5lDM9Rc2ODil95gb7dAAAAANBPELh7ycgJJ8uXmSWH0yHDdMiKhGVbljwpKRo2Jl+hQEAHP96l5PT0z9xYDQAAAADQ9zGlvJcc3lcmf32dho0dJ9mSFQnLdDjl9nolSeFQQI01VbItu93HH7uxGgAAAACgb2OEu5ccvVO52+tVUkpqLGxLktsT/e9Qs7/dx4eDgdjGagAAAACQKIZh6Pnnn090Gf0CgbuXHL1TeXsMhymn261AU1N0M7Wj2LatxpoaDRmVq6GjT+qNcgEAAAAMQhUVFbrxxhuVn58vj8ej3NxczZ8/X+vXr4/L9TZs2CDDMFRTUxOX55ek5cuX65xzzlFycrIyMjLaHD9y5IjmzZunkSNHxl7zDTfcoLq6um5fm8DdS1p2Km+sqTluoM7Jn6jk9AzVHKxQsNkvy7IUbPar5mCFknwpOm3uF+nHDQAAAAwStm3Lqm+WVdUoq765TY7oaSUlJZoxY4Zef/11rVixQlu3btXLL7+s2bNnq7CwMK7X7i7bthUOh9s9FgwGddlll+m6665r97hpmvryl7+sP/3pT/roo4/0xBNP6LXXXtPChQu7XRfprZe07FSe5Es5bqCeedl/6vxvXavhY/MVaGpSXeUhBZqaNHzsOFqCAQAAAIOIVd2o0D/3Rm/v74/9t1XdGLdrXn/99TIMQ++8844WLFigiRMnaurUqbrlllv09ttvt/uY9kaoi4uLZRiGSkpKJEmlpaWaP3++MjMz5fP5NHXqVL344osqKSnR7NmzJUmZmZkyDENXXXVV9PVbloqKijR27Fh5vV6deuqpevbZZ9tc96WXXtKMGTPk8Xi0adOmdmtcunSpFi1apGnTprV7PDMzU9ddd53OOOMM5eXl6YILLtD111+vv/zlL538HWyLTdN60aiTJ+u8K76t4lf+rCP796mptlYOl0vDx47TaXO/GAvUIyecrMP7yhRoaJAnJUVDR5/EyDYAAAAwSFjVjQrtqJBCYcntkhyGFLFl1zUrtKNCrkk5MjN9PXrNqqoqvfzyy1q+fLl8vrbP3d5U7I4qLCxUMBjUxo0b5fP5tG3bNqWkpCg3N1dr1qzRggUL9OGHHyotLU3eT/a5Kioq0tNPP62HH35YEyZM0MaNG3X55ZcrOztb5513Xuy5Fy9erPvvv1/5+fnKzMzsco1HO3DggJ577rlW1+mqDgfuUCik//7v/9Zzzz2nrKwsLVy4UNdcc03s+MGDBzVy5EhFIpFuFzWQjTp58mcGasM0lX3SmMQVCQAAACAhbNtWuPRINGx73Z+2DHYash0uyR9SuPSIXBnJbdoJd8euXbtk27YmTZrUY8/ZoqysTAsWLIiNMOfn58eOZWVlSZKGDRsWC/WBQED33nuvXnvtNc2cOTP2mE2bNumRRx5pFYSXLVumOXPm9Eid3/jGN/THP/5Rfr9f8+fP1y9/+ctuP2eHh02XL1+up556SgsXLtRFF12kW265Rf/1X//V6px4rykYKFoC9egppyj7pDGMXgMAAACQJNkNAdkNAcntahOoDcOQ3I5Pz+nJ68Yxy91000265557NGvWLC1ZskT/+te/Tnj+rl271NTUpDlz5iglJSV2e+qpp7R79+5W555xxhk9VudPf/pTbdmyRX/84x+1e/du3XLLLd1+zg6PcP/617/WL3/5S33xi1+UJF111VW6+OKLdfXVV+tXv/qVJPXoNywAAAAAMOiEIpJlR6eRt8dhSsFI9LweNGHCBBmGoR07dnTqceYng4dHB/ZQKNTqnGuvvVZz587V2rVrtW7dOhUVFWnlypW68cYb233OhoYGSdLatWs1atSoVsc8Hk+rn9ub/t5VOTk5ysnJ0aRJk5SVlaXPf/7zuuOOOzRixIguP2eHh1b379+vU045Jfbz+PHjtWHDBv31r3/VFVdcwVRyAAAAAOgul0Myo2u22xWxosddjh69bFZWlubOnatVq1apsbHtxmzHa9uVnZ0tSSovL4/dV1xc3Oa83NxcLVy4UM8995xuvfVWPfroo5Ikt9stSa3y5JQpU+TxeFRWVqbx48e3uuXm5nb1JXaKZVmSotPbu6PDI9w5OTnavXu3xowZE7tv1KhReuONNzR79uzYbnIAAAAAgK4xUjwyUjyy65plO1pPK7dtWwpGZKQlyUjxnOBZumbVqlWaNWuWzjrrLC1btkzTp09XOBzWq6++qtWrV2v79u1tHtMSgu+66y4tX75cH330kVauXNnqnJtvvlkXX3yxJk6cqOrqar3xxhuaPDm6YXReXp4Mw9ALL7ygL3zhC/J6vUpNTdVtt92mRYsWybIsnXvuuaqtrdVbb72ltLQ0XXnllZ16XWVlZaqqqlJZWZkikUjsC4Hx48crJSVFL774og4ePKgzzzxTKSkp+uCDD/Td735Xs2bNapV/u6LDI9z//u//rt/85jdt7h85cqRef/117dmzp1uFAAAAAMBgZxiGnHlDoiPY/pDscES2bcsORyR/SHI55MwbEpflvPn5+dqyZYtmz56tW2+9VaeccormzJmj9evXa/Xq1e0+xuVy6ZlnntGOHTs0ffp03XfffbrnnntanROJRFRYWKjJkydr3rx5mjhxoh566CFJ0UHcpUuXavHixRo+fLhuuOEGSdLdd9+tO+64Q0VFRbHHrV27VmPHju3067rzzjtVUFCgJUuWqKGhQQUFBSooKNC7774rSfJ6vXr00Ud17rnnavLkyVq0aJG+9KUv6YUXXuj0tY5l2B1cHV9aWqodO3Zo7ty57R4/cOCAXn311U5/29Cb6urqlJ6ertraWqWlpSW6HAAAAACfoT9+hm9ubtaePXs0duxYJSUldek5rOpGhUuPRDdHs2zJNGSkeOTMG9LjLcHQeR19jzs8pTwvL095eXnHPT5y5Mg+HbYBAAAAoL8wM31yZSRHA3coIrkc0enmbFTdr3Q4cAMAAAAAeo9hGDJSuzZCjr6BwA0AAAB0g23bqg6GFbQsuU1TmW4no5AAJBG4AQAAgC476A9qe02DaoNhRT5pnZzudmpyRoqGe92JLg9AgnV4l3IAAAAAnzroD2rz4VodCYTldhhKdZlyOwxVBcLafLhWB/3BRJcIIMG6PMIdDAZ16NChWEPwFieddFK3iwIAAAD6Mtu2tb2mQYGIrTSXGZtC7jIMOV226kOWdtQ0aFhSJtPLgUGs04F7586duuaaa/TXv/611f22bcswDEUikR4rDgAAAOiLqoNh1QbDSnYabQK1YRjyOg3VBMOqDoaV5XElqEoAidbpwH3VVVfJ6XTqhRde0IgRI/jGDgAAAINO0LIUsSXncT4LOwxDEdtW8JjZoAAGl04H7uLiYv3jH//QpEmT4lEPAAAAEFc9sau42zTlMKSwbcvVzmMjti2HET0PwODV6cA9ZcoUHT58OB61AAAAAHHVU7uKZ7qdSnc7VRUIy+myWwV227blD9sa4nEq001TIAw8hmHoD3/4gy699NJEl9Lndfort/vuu0/f+973tGHDBh05ckR1dXWtbgAAAEBf1JO7ihuGockZKXI7DNWHLIUsS5ZtK2RZqg9ZcjsMTcpIYfkl+p2KigrdeOONys/Pl8fjUW5urubPn6/169fH5XobNmyQYRiqqamJy/NL0vLly3XOOecoOTlZGRkZJzz3yJEjGj16dI/V1Omv3C688EJJ0gUXXNDqfjZNAwAAQF8Vj13Fh3vdOnNo+lEj5tFp5EM8Tk36ZMS8J6avY/CyLEuVlZXy+/3yer3Kzs6WGcdlCiUlJZo1a5YyMjK0YsUKTZs2TaFQSK+88ooKCwu1Y8eOuF27u2zbViQSkdPZNuIGg0Fddtllmjlzph577LETPs+3v/1tTZ8+Xfv37++RujoduN94440euTAAAADQW+K1q/hwr1vDkjLbDdU9NX0dg1NZWZneeecdHT58WOFwWE6nU0OHDtVZZ50Vt1bM119/vQzD0DvvvCOfzxe7f+rUqbrmmmvafcyGDRs0e/ZsVVdXx0aPi4uLVVBQoD179mjMmDEqLS3VDTfcoE2bNikYDGrMmDFasWKFpkyZotmzZ0uSMjMzJUlXXnmlnnjiCVmWpfvuu0+/+MUvVFFRoYkTJ+qOO+7QV7/61VbXffHFF/XDH/5QW7du1bp163T++ee3qXHp0qWSpCeeeOKEr3/16tWqqanRnXfeqZdeeqkzv3XH1enAfd555/XIhQEAAIDeEs9dxQ3DaBPSW6avByK2kp2GnIahsG3Hpq+fOTSd0I3jKisr07p16+T3+5Wamiqn06lwOKzy8nKtW7dOF110UY+H7qqqKr388stavnx5q7Dd4rOmYp9IYWGhgsGgNm7cKJ/Pp23btiklJUW5ublas2aNFixYoA8//FBpaWnyer2SpKKiIj399NN6+OGHNWHCBG3cuFGXX365srOzW2XSxYsX6/7771d+fn4stHfFtm3btGzZMv3973/Xxx9/3OXnOVaXdnGoqanRY489pu3bt0v69BuP9PT0HisMAAAA6Cm9uat4PKavY/CwLEvvvPOO/H6/srKyYv+PuN1uZWVlqaqqSps3b9bo0aN7dHr5rl27ZNt2XLpRlZWVacGCBZo2bZokKT8/P3YsKytLkjRs2LBYqA8EArr33nv12muvaebMmbHHbNq0SY888kirwL1s2TLNmTOnW/UFAgF94xvf0IoVK3TSSSf1aODu9Dv07rvvaty4cfrpT3+qqqoqVVVV6Sc/+YnGjRunLVu29FhhAAAAQE9p2VXcH7Zl23arYy27ime4e2ZX8c5MXweOVVlZqcOHDys1NbXd/39SU1NVWVmpysrKHr3usX8uetJNN92ke+65R7NmzdKSJUv0r3/964Tn79q1S01NTZozZ45SUlJit6eeekq7d+9ude4ZZ5zR7fpuv/12TZ48WZdffnm3n+tYnQ7cixYt0pe+9CWVlJToueee03PPPac9e/boi1/8om6++eYeLxAAAADort7cVbxj09fVpenrGPj8fn9szXZ7WqaX+/3+Hr3uhAkTZBhGpzdGaxllPzqwh0KhVudce+21+vjjj3XFFVdo69atOuOMM/Tggw8e9zkbGhokSWvXrlVxcXHstm3bNj377LOtzm1v+ntnvf766/rf//1fOZ1OOZ3O2AbhQ4cO1ZIlS7r13F0a4f7+97/f6n8Ap9Op733ve3r33Xe7VQwAAAAQLy27imd5nApGbDWELAUj0X7ZPbmm+ujp6+3pyenrGHi8Xm8sVLenJYy3rHXuKVlZWZo7d65WrVqlxsbGNseP1yIrOztbklReXh67r7i4uM15ubm5WrhwoZ577jndeuutevTRRyVFp8pLatXtasqUKfJ4PCorK9P48eNb3XJzc7v6Eo9rzZo1+uc//xkL9r/85S8lSX/5y19UWFjYrefu9JyZtLQ0lZWVtZnbv3fvXqWmpnarGAAAACCeTrSreE9pmb5eFQjL6bJbPXfL9PUhnp6Zvo6BJzs7W0OHDlV5eXmrNdxS9P+f+vp6jRw5MhZ0e9KqVas0a9YsnXXWWVq2bJmmT5+ucDisV199VatXr47t4XW0lhB81113afny5froo4+0cuXKVufcfPPNuvjiizVx4kRVV1frjTfe0OTJkyVJeXl5MgxDL7zwgr7whS/I6/UqNTVVt912mxYtWiTLsnTuueeqtrZWb731ltLS0nTllVd26nWVlZWpqqpKZWVlikQisS8Exo8fr5SUFI0bN67V+YcPH5YkTZ48uVubxUldGOH++te/rm9/+9v63e9+p71792rv3r367W9/q2uvvVbf+MY3ulUMAAAAEG8tu4rneD3K8rh6fOOy3py+joHHNE2dddZZ8nq9qqqqUjAYlGVZCgaDqqqqktfr1ZlnnhmXftz5+fnasmWLZs+erVtvvVWnnHKK5syZo/Xr12v16tXtPsblcumZZ57Rjh07NH36dN1333265557Wp0TiURUWFioyZMna968eZo4caIeeughSdKoUaO0dOlSLV68WMOHD9cNN9wgSbr77rt1xx13qKioKPa4tWvXauzYsZ1+XXfeeacKCgq0ZMkSNTQ0qKCgQAUFBb0yQ9uwO7k6PhgM6rvf/a4efvjh2DQHl8ul6667Tj/60Y/k8XjiUmhPqKurU3p6umpra5WWlpbocgAAADCAtdeHO8Pt1CT6cHdKf/wM39zcrD179mjs2LFKSkrq0nO014c7OztbZ555Ztz6cKPjOvoedzpwt2hqaortEDdu3DglJyd3rdJe1B//sAIAAKD/sm2709PXu/KYgaw/fobvicAtRVuEVVZWyu/3y+v1Kjs7Oy4j2+i8jr7HXV44kpycHOujBgAAAKCtlunrHdXeqHi626nJjIoPSqZpavjw4YkuA93QocD9la98RU888YTS0tL0la985YTnPvfccz1SGAAAAAYI25IaD0khv+TySr5hksEo3bEO+oPafLhWgYitZKchp2EobNuqCoS1+XBtj+6kDqB3dChwp6enx6axpKenx7UgAAAADCA1pdLev0UDtxWWTGc0cOfOlDLyEl1dn2HbtrbXNCgQsZXmMmOfvV2GIafLVn3I0o6aBg1LyhzU08uB/qZDgfvxxx9v978BAACA46oplXa+KIWaJHeq5HBJkZBUfyB6/4QvELo/UR0MqzYYVrLTaBOoDcOQ12moJhhWdTDcqSnqABKr03N5/H6/mpqaYj+XlpbqZz/7mdatW9ejhQEAAKAfs63oyHaoSfIOkZye6DRyp0fyZkWnl+/9W/Q8KGhZitiS8zij1w7DUMSOngeg/+h04P7yl7+sp556SpJUU1Ojs846SytXrtSXv/zl4/ZmAwAAwCDTeCh6c6dKx4ZIw5DcKZ+eA7lNUw5DCh+ngVDEtuUwoucB6D86/Sd2y5Yt+vznPy9JevbZZ5WTk6PS0lI99dRT+vnPf97jBQIAAKAfCvmja7Ydx5n+7HBFj4f8vVtXH5Xpdird7ZQ/bOvYrr22bcsftpXhdirT3eUmQwASoNOBu6mpSampqZKkdevW6Stf+YpM09TnPvc5lZaW9niBAAAA6Idc3ugGaZFQ+8cjoehxl7d36+qjDMPQ5IwUuR2G6kOWQpYly7YVsizVhyy5HYYmZaSwYRrQz3Q6cI8fP17PP/+89u7dq1deeUUXXXSRJOnQoUP9phE9gE/Ztq1AoFJ+/z4FApVtvlUHAKBLfMOit2CDdOy/LbYdvb/lHEiShnvdOnNourI8TgUjthpCloIRW0M8TlqCoU8xDEPPP/98osvoFzoduO+8807ddtttGjNmjM4++2zNnDlTUnS0u6CgoMcLPNaqVas0ZswYJSUl6eyzz9Y777wT92sCA5Xff0AHD76ogwdf1qFDr+ngwZd18OCL8vsPJLo0AEB/Z5jR1l8ur+SvksKB6AZp4UD0Z5c3epx+3K0M97p1Xk6mPp+TqXOGp+vzOZn6t5xMwjZ6TUVFhW688Ubl5+fL4/EoNzdX8+fP1/r16+NyvQ0bNsgwDNXU1MTl+SVp+fLlOuecc5ScnKyMjIx2zzEMo83tt7/9bbev3elFIF/96ld17rnnqry8XKeeemrs/gsuuED/3//3/3W7oBP53e9+p1tuuUUPP/ywzj77bP3sZz/T3Llz9eGHH2rYML4dBTrD7z+gw4fflGU1y+HwyTCcsu2wAoFDOnz4TQ0dep683pGJLhMA0J9l5EVbf7X04Q42RKeRp44ckH24bdtWdTCsQCSiQMSWx2HI43Ao0+3s1FRwwzBo/QVJn6zf9/sVDofldDrl9XrjuqygpKREs2bNUkZGhlasWKFp06YpFArplVdeUWFhoXbs2BG3a3eXbduKRCJyOttG3GAwqMsuu0wzZ87UY489dtznePzxxzVv3rzYz8cL553Rpa8Uc3JyVFBQIPOoXRLPOussTZo0qdsFnchPfvITfec739HVV1+tKVOm6OGHH1ZycrJ+9atfxfW6wEBj27Zqa9+TZTXL6cyQabplGKZM0y2nM0OW1aza2mKmlwMAui8jTzrla9Hb5P/v0/8eYGH7oD+oNyuq9fqBKq0/UK2NFdVaf6Bab5RX6c2Kah30BxNdIvqZ+vp67dq1S7t27dLHH38c++/6+vq4XfP666+XYRh65513tGDBAk2cOFFTp07VLbfcorfffrvdx7Q3Ql1cXCzDMFRSUiIp2kp6/vz5yszMlM/n09SpU/Xiiy+qpKREs2fPliRlZmbKMAxdddVVkiTLslRUVKSxY8fK6/Xq1FNP1bPPPtvmui+99JJmzJghj8ejTZs2tVvj0qVLtWjRIk2bNu2Erz8jI0M5OTmxW1JSUgd/546v0yPcjY2N+tGPfqT169fr0KFDso7pBfjxxx93u6j2BINB/eMf/9Dtt98eu880TV144YX629/+FpdrAgNVMHhYwWDVJyPbrb8lNQxDDodPweARBYOH5fFkJ6hKAEDC2FZ0RDrkj0799g3r3tRvw5RScnquvj7moD+ozYdr1RiyFLIisiWZhmTZtvzhiA7ZUsPhWtZho8Pq6+tVWlqqcDgsl8sll8sly7LU2Nio0tJS5eXlxTay7ilVVVV6+eWXtXz5cvl8vjbHuzPaW1hYqGAwqI0bN8rn82nbtm1KSUlRbm6u1qxZowULFujDDz9UWlqavN7oRopFRUV6+umn9fDDD2vChAnauHGjLr/8cmVnZ+u8886LPffixYt1//33Kz8/X5mZmV2usaXOa6+9Vvn5+Vq4cKGuvvrqbs8o6HTgvvbaa/Xmm2/qiiuu0IgRI3ptp8TDhw8rEolo+PDhre4fPnz4cac2BAIBBQKB2M91dXVxrRHoLywrINuOyDDa/ysgOr28SZYVaPc4usa2bdkNASkUkVwOGSkedpsF0PfUlH46BdwKR6eA+4YNyCngPcG2bW2vaVBz2JItS7YMOYxPvsC2bYXt6EhdICLtqGnQsKRM/u7HCdm2rYqKCoXDYXk8n35WcDgcMk1TgUBAFRUVSknp2V3rd+3aJdu24zJruaysTAsWLIiNMOfn58eOZWVlSZKGDRsWC/WBQED33nuvXnvttdieYfn5+dq0aZMeeeSRVoF72bJlmjNnTrdrXLZsmf793/9dycnJWrduna6//no1NDTopptu6tbzdjpwv/TSS1q7dq1mzZrVrQv3hqKiIi1dujTRZQB9jml6ZBgO2XZYhtH2m/bo/Q6ZpicB1Q1MVnWjwqVHooHbsiXTkJHikTNviMzMtt8iA0BC1JRKO1+UQk2SOzXaKzsSkuoPRO+f8AVC9zGqg2HVBsNyOww1B6Mj27EQZBhyKBq6kw1TNcGwqoNh1mfjhPx+v/x+v1wuV7szEV0uV+yc5OTkHrtuPJcS3nTTTbruuuu0bt06XXjhhVqwYIGmT59+3PN37dqlpqamNkE6GAy22aj7jDPO6JEa77jjjth/FxQUqLGxUStWrOh24O703KDMzMzYtxC9aejQoXI4HDp48GCr+w8ePKicnPanKN1+++2qra2N3fbu3dsbpQJ9nts9VG53liKRxjZ/uUY3nGiU2z1EbvfQBFU4sFjVjQrtqJBd55ecDsnrkpwO2XXNCu2okFXdmOgSASA6jXzv36Jh2ztEcnqiU8GdHsmbFZ1evvdv0fMQE7QsRezoh2pbto4dbzQk2ZIMw1bEjp4PnEg4HJZlWa32yzqaaZqyLEvhcLhHrzthwgQZhtHpjdFa6jz6M2UoFGp1zrXXXquPP/5YV1xxhbZu3aozzjhDDz744HGfs6GhQZK0du1aFRcXx27btm1rtY5bUrvT33vC2WefrX379rWaMd0VnQ7cd999t+688041NTV168Kd5Xa7NWPGjFbb0VuWpfXr18emGRzL4/EoLS2t1Q1A9NvR9PQCmWaSwuEaWVZQtm3JsoIKh2tkmklKTz+NKW89wLZthUuPSKGw5HXLcJrRVhNOMxq8Q5HoyDcb1AFItMZD0Zs7VTr273/DkNwpn56DGLdpymFIliRDho7929zWJ6Hbjk41dx8nRAEtnE5nLFS3pyWMt7cbd3dkZWVp7ty5WrVqlRob2w4GHK9tV3Z2dL+f8vLy2H3FxcVtzsvNzdXChQv13HPP6dZbb9Wjjz4qKZrzJCkSicTOnTJlijwej8rKyjR+/PhWt9zc3K6+xE4pLi5WZmamPJ7uzfjs9Lu0cuVK7d69W8OHD9eYMWPkcrWeErNly5ZuFXQit9xyi6688kqdccYZOuuss/Szn/1MjY2Nuvrqq+N2TWCg8npHaujQ81Rb+56CwSrZdpMMwyGPZ7jS00+jJVgPsRsC0Wnk7vanhdluR+wcI7X7O2ECQJeF/NE1247jTHd2uKJtvUL+3q2rj8t0O5XudupIc0hOUwpZkiE7+ne+HR3VdhlSyLY01ONSprtnQxIGHq/XK6/Xq8bGRpmm2erzg23bCoVC8vl8sc3FetKqVas0a9YsnXXWWVq2bJmmT5+ucDisV199VatXr9b27dvbPKYlBN91111avny5PvroI61cubLVOTfffLMuvvhiTZw4UdXV1XrjjTc0efJkSVJeXp4Mw9ALL7ygL3zhC/J6vUpNTdVtt92mRYsWybIsnXvuuaqtrdVbb72ltLQ0XXnllZ16XWVlZaqqqlJZWZkikUjsC4Hx48crJSVFf/7zn3Xw4EF97nOfU1JSkl599VXde++9uu2227r2G3mUTv+Jv/TSS7t90a76+te/rsrKSt15552qqKjQaaedppdffrnNRmoAOsbrHamkpBEKBg/LsgIyTY/c7qGMbPekUCS6ZttxnN9ThykFI9HzACCRXN7oBmmRUHQa+bEioehxV89/yO/PDMPQ5IwUbT5cq0hIMhRRxJYM227ZskOm6ZDHYWpSRs9ucoWByTAM5eTkqLS0VIFAQC6XKzbiHQqF5HQ6lZOTE5f/l/Lz87VlyxYtX75ct956q8rLy5Wdna0ZM2Zo9erV7T7G5XLpmWee0XXXXafp06frzDPP1D333KPLLrssdk4kElFhYaH27duntLQ0zZs3Tz/96U8lSaNGjdLSpUu1ePFiXX311frWt76lJ554Qnfffbeys7NVVFSkjz/+WBkZGTr99NP1gx/8oNOv684779STTz4Z+7llHfgbb7yh888/Xy6XS6tWrdKiRYtk27bGjx8fa0ndXYY9iOYx1tXVKT09XbW1tUwvB/oI27YHdOC36psV+udeyemITiM/hh2OSGFLrlNzZTLCDSCRbEt6//fRDdK8Wa2nldu25K+SUkdGe2h3p0XYAHXQH9T2mgYdbg6pOWLJsm2ZhiGv09RQj0uTMlJoCdZF/fEzfHNzs/bs2aOxY8d2uZdzfX29Kioq5Pf7Y9PIvV6vcnJyerwlGDqvo+9xl+a01NTU6Nlnn9Xu3bv13e9+V1lZWdqyZYuGDx+uUaNGdbloAIOL33/gqCntERmGQ253ltLTCwbMlHYjxSMjxSO7rlm2w9VmWpiCERlpSTJS2BEeQIIZZrT1184Xo+HanfLpLuXBhujIdu5MwvZxDPe6NSwpU9XBsAKRiAIRWx6HIY/DoUy3c0B9mYzekZqaqpSUFPn9foXDYTmdTnm9Xv5f6mc6Hbj/9a9/6cILL1R6erpKSkr0ne98R1lZWXruuedUVlamp556Kh51Ahhg/P4DOnz4TVlWsxwO3ye9v8MKBA7p8OE3NXToeQMidBuGIWfeEIV2VEj+kGy3IzqNPGJFp5K7HHLmDeEfTwB9Q0ZetPVXSx/uYEN0GnnqSPpwd4BhGJ+0/KLtF3qGYRg92voLva/TgfuWW27RVVddpR//+MetpjJ84Qtf0H/+53/2aHEABibbtlVb+54sq1lOZ0YsbBqGW4bhUjhco9raYiUljRgQQdTM9Mk1KefTPtzBSLQPd1oSfbgB9D0ZeVJ6bjRwh/zRkW3fMEa2AaALOh24N2/erEceeaTN/aNGjVJFRUWPFAVgYAsGDysYrPpkZLvtzt0Oh0/B4BEFg4fl8WQnqMqeZWb65MpIjgbuUHRk20jxDIgvFAAMQIYppeQkugoA6Pc6Hbg9Ho/q6ura3P/RRx/FerABwIlYVuCTNdvt/xUUnV7eJMsK9HJl8WUYBq2/AAAABpFOzw360pe+pGXLlikUCkmKfoAsKyvT97//fS1YsKDHCwQw8JimR4bhkG2H2z1u22EZhkOmyUZiAAAA6L86HbhXrlyphoYGDRs2TH6/X+edd57Gjx+v1NRULV++PB41Ahhg3O6hcruzFIk06tjOhLZtKxJplNs9RG730ARVCAAAAHRfp6eUp6en69VXX9WmTZv0r3/9Sw0NDTr99NN14YUXxqM+AAOQYRhKTy/Q4cNvKhyuabVLeSTSKNNMUnr6aaxvBgAAQL/WpT7cknTuuefq3HPP7claAAwiXu9IDR163lF9uJtkGA55PMOVnn7agGgJBgAAgMGtS4F78+bNeuONN3To0CFZltXq2E9+8pMeKQzAwOf1jlRS0ggFg4dlWQGZpkdu91BGtgEAAPowwzD0hz/8QZdeemmiS+nzOr2G+95779XZZ5+txx9/XO+++67ee++92K24uDgOJQIYyAzDkMeTLa93tDyebMI2AABAAlVUVOjGG29Ufn6+PB6PcnNzNX/+fK1fvz4u19uwYYMMw1BNTU1cnl+Sli9frnPOOUfJycnKyMg47nlPPPGEpk+frqSkJA0bNkyFhYXdvnanR7gfeOAB/epXv9JVV13V7YsDAE7Mti01Nu5UKFQrlytdPt8EGUanvysFAAD9UG9/DigpKdGsWbOUkZGhFStWaNq0aQqFQnrllVdUWFioHTt2xO3a3RXdeDcip7NtxA0Gg7rssss0c+ZMPfbYY+0+/ic/+YlWrlypFStW6Oyzz1ZjY6NKSkq6XVen3y3TNDVr1qxuXxgAcGI1tf/QB9tu0QfbbtWOD3+oD7bdqg+23aKa2n8kujQAABBnifgccP3118swDL3zzjtasGCBJk6cqKlTp+qWW27R22+/3e5j2huhLi4ulmEYscBaWlqq+fPnKzMzUz6fT1OnTtWLL76okpISzZ49W5KUmZkpwzBiA7uWZamoqEhjx46V1+vVqaeeqmeffbbNdV966SXNmDFDHo9HmzZtarfGpUuXatGiRZo2bVq7x6urq/XDH/5QTz31lP7zP/9T48aN0/Tp0/WlL32pk7+DbXU6cC9atEirVq3q9oUBAMdXU/sP7dxZpPr6D+R0psmbNFpOZ5rq6z/Qzp1FhG4AAAawRHwOqKqq0ssvv6zCwkL5fL42x080FfuzFBYWKhAIaOPGjdq6davuu+8+paSkKDc3V2vWrJEkffjhhyovL9cDDzwgSSoqKtJTTz2lhx9+WB988IEWLVqkyy+/XG+++War5168eLF+9KMfafv27Zo+fXqX6nv11VdlWZb279+vyZMna/To0fra176mvXv3dvk1t+j0lPLbbrtNl1xyicaNG6cpU6bI5XK1Ov7cc891uygAGMxs29K+ff+jcLhW3qS82Lp2p8MnR1Ky/M2l2rfvaaWnFTC9HACAASZRnwN27dol27Y1adKkHnvOFmVlZVqwYEFshDk/Pz92LCsrS5I0bNiwWKgPBAK699579dprr2nmzJmxx2zatEmPPPKIzjvvvNjjly1bpjlz5nSrvo8//liWZenee+/VAw88oPT0dP3whz/UnDlz9K9//Utut7vLz93pwH3TTTfpjTfe0OzZszVkyBA2OAKAHtbYuFONjbvkdrfdRM4wDLnd2Z+cs1MpKScnqEoAABAPifocYNt2jz3XsW666SZdd911WrdunS688EItWLDghKPRu3btUlNTU5sgHQwGVVBQ0Oq+M844o9v1WZalUCikn//857roooskSc8884xycnL0xhtvaO7cuV1+7k4H7ieffFJr1qzRJZdc0uWLAgCOLxSqlWUF5TCT2j3uMJMUtA4rFKrt5coAAEC8JepzwIQJE2QYRqc3RjPN6Cj70YE9FAq1Oufaa6/V3LlztXbtWq1bt05FRUVauXKlbrzxxnafs6GhQZK0du1ajRo1qtUxj8fT6uf2pr931ogRIyRJU6ZMid2XnZ2toUOHqqysrFvP3ek5CFlZWRo3bly3LgoAOD6XK12m6VbEam73eMRqlmm65XKl93JlAAAg3hL1OSArK0tz587VqlWr1NjY2Ob48dp2ZWdnS5LKy8tj97XXLjo3N1cLFy7Uc889p1tvvVWPPvqoJMWma0cikdi5U6ZMkcfjUVlZmcaPH9/qlpub29WXeFwtm4J/+OGHsfuqqqp0+PBh5eXldeu5Ox2477rrLi1ZskRNTU3dujAAoH0+3wT5fOMVDFa2md5l27aCwcpPzpmQoAoBAEC8JPJzwKpVqxSJRHTWWWdpzZo12rlzp7Zv366f//znsbXUx2oJwXfddZd27typtWvXauXKla3Oufnmm/XKK69oz5492rJli9544w1NnjxZkpSXF12n/sILL6iyslINDQ1KTU3VbbfdpkWLFunJJ5/U7t27tWXLFj344IN68sknO/26ysrKVFxcrLKyMkUiERUXF6u4uDg2kj5x4kR9+ctf1v/9v/9Xf/3rX/X+++/ryiuv1KRJk2K7qHdVp6eU//znP9fu3bs1fPhwjRkzps2maVu2bOlWQQAw2BmGqdGjr9DOnUXyN5fK7c6Ww0xSxGpWMFgppzNdo0dfzoZpAAAMQIn8HJCfn68tW7Zo+fLluvXWW1VeXq7s7GzNmDFDq1evbvcxLpdLzzzzjK677jpNnz5dZ555pu655x5ddtllsXMikYgKCwu1b98+paWlad68efrpT38qSRo1apSWLl2qxYsX6+qrr9a3vvUtPfHEE7r77ruVnZ2toqIiffzxx8rIyNDpp5+uH/zgB51+XXfeeWeroN6yDvyNN97Q+eefL0l66qmntGjRIl1yySUyTVPnnXeeXn755TZ5t7MMu5Or45cuXXrC40uWLOlWQfFUV1en9PR01dbWKi0tLdHlAMAJ1dT+Q/v2/Y8aG3fJsoIyTbd8vgkaPfpyZaTPSHR5AAD0iv74Gb65uVl79uzR2LFjlZTU/lrsz8LngL6to+9xp0e4+3KgBtD32bYtv9+vcDgsp9Mpr9dLt4PjyEifofS0AjU27lQoVCuXK10+3wRGtgEAGAT4HDAwdDpwA0BX1dfXq6KiQn6/X5ZlyTRNeb1e5eTkKDU1NdHl9UmGYdL6CwCAQYrPAf1fhwJ3VlaWPvroIw0dOlSZmZknHI2qqqrqseIADBz19fUqLS1VOByWy+WSy+WSZVlqbGxUaWmp8vLyCN0AAAAYUDoUuH/605/GPgj/7Gc/i2c9AAYg27ZVUVGhcDgsj8cT+9LO4XDINE0FAgFVVFQoJSWF6eUAAAAYMDoUuK+88sp2/xsAOsLv98vv98vlcrUJ1IZhyOVyxc5JTk5OUJUAAABAz+r0Gu7a2lq9+uqrKikpkWEYys/P1wUXXNBvdgwE0PvC4bAsyzpuWwXTNBUKhRQOh3u5ssHNti02YgEAAIijTgXup59+WjfccIPq6upa3Z+enq6HH35YX//613u0OAADg9PplGmasixLDoejzfGWDdScTvZx7C3ttxoZr9Gjr6DVCAAAQA/p8FDGli1bdPXVV+vSSy/Ve++9J7/fr6amJr377ruaP3++rrjiCv3zn/+MZ60A+imv1yuv16tQKCTbtlsds21boVAodg7ir6b2H9q5s0j19R/I6UyTN2m0nM401dd/oJ07i1RT+49ElwgAADAgdDhwP/jgg7r00kv1xBNP6NRTT5XH41FSUpJOP/10PfXUU/rSl76kBx54IJ61AuinDMNQTk6OnE6nAoGAIpGIbNtWJBJRIBCQ0+lUTk4OG6b1Atu2tG/f/ygcrpU3KU9Oh0+G4ZDT4ZM3KU/hcK327Xtatm0lulQAAIB+r8OB+6233tJ//dd/Hff4woULtWnTph4pCsDAk5qaqry8PPl8vljQjkQi8vl8tATrRY2NO9XYuEtud3a7G9i53dmfnLMzQRUCAIC+zjAMPf/884kuo1/ocOA+cOCAJk6ceNzjEydO1P79+3ukKAADU2pqqsaPH6/x48crPz8/9t+E7d4TCtXKsoJymEntHneYSbKsoEKh2l6uDAAA9AUVFRW68cYblZ+fL4/Ho9zcXM2fP1/r16+Py/U2bNggwzBUU1MTl+eXpOXLl+ucc85RcnKyMjIy2hx/4oknZBhGu7dDhw5169od3qGoqalJSUntf0CTJI/Ho+bm5m4VA2DgMwyD1l8J5HKlyzTdiljNcjp8bY5HrGaZplsuV3oCqgMAAEezbVvB4GFZVkCm6ZHbPTSuS/BKSko0a9YsZWRkaMWKFZo2bZpCoZBeeeUVFRYWaseOHXG7dne1LFdsbxPeYDCoyy67TDNnztRjjz3W5vjXv/51zZs3r9V9V111lZqbmzVs2LBu1dWpLYFfeeUVpae3/yEsnt9IAAB6hs83QT7feNXXfyBHUnKrf7Sj/6hXKjX1FPl8ExJYJQAA8PsPqLb2PQWDVbLtiAzDIbc7S+npBfJ6R8blmtdff70Mw9A777wjn+/TL+anTp2qa665pt3HbNiwQbNnz1Z1dXVs9Li4uFgFBQXas2ePxowZo9LSUt1www3atGmTgsGgxowZoxUrVmjKlCmaPXu2JCkzM1OSdOWVV+qJJ56QZVm677779Itf/EIVFRWaOHGi7rjjDn31q19tdd0XX3xRP/zhD7V161atW7dO559/fpsaly5dKik6kt2eYzfvrays1Ouvv95uOO+sTgXuK6+88oTH2fAIR7MtW1XljQo0heRJdilrhE+Gyf8jQCIZhqnRo6/Qzp1F8jeXyu3OlsNMUsRqVjBYKaczXaNHX04/bgAAEsjvP6DDh9+UZTXL4fDJMJyy7bACgUM6fPhNDR16Xo+H7qqqKr388stavnx5q7Ddor2p2B1VWFioYDCojRs3yufzadu2bUpJSVFubq7WrFmjBQsW6MMPP1RaWlos+BYVFenpp5/Www8/rAkTJmjjxo26/PLLlZ2drfPOOy/23IsXL9b999+v/Pz8WGjvrqeeekrJycmxcN8dHQ7clsWOtei48t212rphn6rLGxUJW3I4TWWO8Gna+aM1YhxTVYGusm1bdkNACkUkl0NGiqfTX3ZmpM/QhAm3x/pwB63DMk23UlNP0ejRl9OHGwCABLJtW7W178mymuV0ZsT+nTcMtwzDpXC4RrW1xUpKGtGjA567du2SbduaNGlSjz1ni7KyMi1YsEDTpk2TJOXn58eOZWVlSZKGDRsWC/WBQED33nuvXnvtNc2cOTP2mE2bNumRRx5pFbiXLVumOXPm9Gi9jz32mP7zP/+zR1rWdmqEG+iI8t21emvNTgUaw/Klu+VwmYqELFXurddba3Zq1oIJhG60y7Zt+f1+hcNhOZ1Oeb1eZs4cxapuVLj0SDRwW7ZkGjJSPHLmDZGZ2fab6BPJSJ+h9LQCNTbuVChUK5crXT7fBEa2AQBIsGDwsILBqk9Gttt2FHE4fAoGjygYPCyPJ7vHrmvbdo8917FuuukmXXfddVq3bp0uvPBCLViwQNOnTz/u+bt27VJTU1ObIB0MBlVQUNDqvjPOOKNHa/3b3/6m7du363/+53965PkI3OhRtmVr64Z9CjSGlZ6dFPtLwvQ4lD40SbWHm7V1wz7ljE1jejlaqa+vV0VFhfx+vyzLkmma8nq9ysnJYRdzRcN2aEeFFApLbpfkMKSILbuuWaEdFXJNyul06DYMUykpJ8epYgAA0BWWFfhkzXb7US06vbxJlhXo0etOmDBBhmF0emM004x+WX90YA+FQq3OufbaazV37lytXbtW69atU1FRkVauXKkbb7yx3edsaGiQJK1du1ajRo1qdczj8bT6ub3p793xy1/+UqeddppmzOiZGX8MZaBHVZU3qrq8Ub50tyQp2BxWc2NIweawJCk51a3q8kZVlTcmskz0MfX19SotLVVjY6McDoc8Ho8cDocaGxtVWlqq+vr6RJeYULZtK1x6JBq2vW4ZTjPaqsJpSl6XFIpER77j+M00AADoHabpkWE4ZNvhdo/bdliG4ZBpeto93lVZWVmaO3euVq1apcbGtp/Vj7dJdnZ2dJS9vLw8dl9xcXGb83Jzc7Vw4UI999xzuvXWW/Xoo49KktzuaG6IRCKxc6dMmSKPx6OysrJYG9mWW25ubldf4mdqaGjQ73//e33729/usedkhBs9KtAUUiRsKRyKqOZgk4KBiGTbkmHI7XEoJcujSNhSoCn02U+GQcG2bVVUVCgcDsvj+XQ9ssPhkGmaCgQCqqioUEpKyqCdXm43BKLTyN2udqeW2W5H7Bwj9fjtGwEAQN/ndg+V252lQOCQDMPVpqNIJNIoj2e43O6hPX7tVatWadasWTrrrLO0bNkyTZ8+XeFwWK+++qpWr16t7du3t3lMSwi+6667tHz5cn300UdauXJlq3NuvvlmXXzxxZo4caKqq6v1xhtvaPLkyZKkvLw8GYahF154QV/4whfk9XqVmpqq2267TYsWLZJlWTr33HNVW1urt956S2lpaZ+5mfexysrKVFVVpbKyMkUikdgXAuPHj1dKSkrsvN/97ncKh8O6/PLLO/k7d3yMcKNHeZJdsiK2qsqbFPCHZToMOVymTIehQHNYVeVNsiK2PMmuRJeKPsLv98vv98vlaj9Mulyu2DmDVigSXbPtOM4XDg4zejwUaf84AADoNwzDUHp6gUwzSeFwjSwrKNu2ZFlBhcM1Ms0kpaefFpeBiPz8fG3ZskWzZ8/WrbfeqlNOOUVz5szR+vXrtXr16nYf43K59Mwzz2jHjh2aPn267rvvPt1zzz2tzolEIiosLNTkyZM1b948TZw4UQ899JAkadSoUVq6dKkWL16s4cOH64YbbpAk3X333brjjjtUVFQUe9zatWs1duzYTr+uO++8UwUFBVqyZIkaGhpUUFCggoICvfvuu63Oe+yxx/SVr3ylWzuyH8uwOzgH8Z133tGMGTPkcDjaPR4IBPTHP/5RX/va13qsuJ5WV1en9PR01dbWKi0tLdHlDEhW2NL/3Pk3NdUG5fY62vb49UeUnO7WFctmynTyfc9gdfTmaM3Nzdq/f7+SkpLa/YfDtm0FAgHl5+cP2j+3Vn2zQv/cKzkd0Wnkx7DDESlsyXVqrkxGuAEAA0x//Azf3NysPXv2aOzYsUpK6tq/ze334R6i9PTT4taHGx3X0fe4w1PKZ86cqfLycg0bNkySlJaWpuLi4tiW7jU1NfrGN77RpwM34q/6YJMcTlMOl6lwKNoOzDCM6PSXsCWHy5TDaar6YJOGjEr57CfEgHPs5miSFA6HFQqFYmt4jtaygZrTOXhXwBgpHhkpHtl1zbIdbaeWKRiRkZYkI6Vn13IBAIDE8XpHKilphILBw7KsgEzTI7d76KBdYtdfdfgT7LED4e0NjLNhDwJNIZmmoawRPjVUNX+yhtuSDEOeJKdSspIU9IdZwz1ItWyOFg6H5XK55HK5ZFmWgsGg/H5/bAp5C9u2FQqF5PP5eqQPYn9lGIaceUOiu5T7Q7Ldjug08oglBaP9uJ15Q/gHGACAAcYwjB5t/YXe16NDRnzYgyfZJYfTlNNlamhuqkKBiKyIJdNhyuVxKBSIyOE0WcM9CJ1oczSv16umpqZY6HY4HLIsS6FQSE6nUzk5OYP+7xcz0yfXpJxP+3AHI9E+3GlJXerDDQAAgPgbvHM0ERdZI3zKHOFT5d56pQ9NkjvJISm67t+2bTXVB5Wdm6qsEYSDweZEm6O5XC55vV4FAgGFw2GFw2GZpimfz0cf7qOYmT65MpKjgTsUHdk2UjyD/ssIAACAvqpTgXvbtm2qqKiQFA1PO3bsiDUlP3z4cM9Xh37HMA1NO3+03lqzU7WHm5Wc6pbTbSoctNRUH1RSslPTzh8twyQgDDbhcFiWZbWaMn60lunlI0eOVFJSkpxOp7xeL2HyGIZh0PoLAACgn+hU4L7gggtardP+4he/KEmxTbH4YAxJGjEuXbMWTNDWDftUXd4of31087Ts3FRNO3+0RoxLT3SJSACn0ynTNGVZVrvdDlo2R0tJSVFycnICKgQAAAB6VocD9549e+JZBwaYEePSlTM2TVXljQo0heRJdilrhI+R7UHM6/XK6/WqsbFRpmm22WmbzdEAAAAw0HQ4cOfl5cWzDiSIbdlxC8WGadD6CzGGYSgnJ0elpaUKBAJyuVyxEW82RwMAAMBA1OHAXVZW1qHzTjrppC4Xg95Vvrs2Nu07Eo5O+84c4WPaN+ImNTVVeXl5sT7coVCIzdEAAAAwYHU4cI8ZM6bdkaej124bhqFwONxz1SFuynfX6q01OxVoDMuX7pbDZSoSslS5t15vrdmpWQsmELoRF6mpqUpJSZHf71c4HGZzNAAAgH7GMAz94Q9/0KWXXproUvo8s6Mnvvfee9qyZUu7t+9+97vyeDzKysqKZ63oIbZla+uGfQo0hpWenSSXxyHTNOTyOJQ+NEmBprC2btgn27I/+8mALjAMQ8nJyUpLS1NycjJhGwAAoI+oqKjQjTfeqPz8fHk8HuXm5mr+/Plav359XK63YcMGGYahmpqauDy/JC1fvlznnHOOkpOTlZGR0e45mzdv1gUXXKCMjAxlZmZq7ty5+uc//9nta3c4cJ966qltbpWVlbr22mv10EMP6Xvf+552797d7YIQf1Xljaoub5Qv3d0m6BiGoeRUt6rLG1VV3pigCgEAAABYtq3tDX79raZB2xv8suz4DoiVlJRoxowZev3117VixQpt3bpVL7/8smbPnq3CwsK4Xru7bNs+7mzrYDCoyy67TNddd127xxsaGjRv3jyddNJJ+vvf/65NmzYpNTVVc+fOVSgU6lZdHQ7cR9uyZYvmzJmjL37xi/rc5z6nXbt26a677mL9ZT8RaApF12y72n/7nW5TkbClQFP3/ucCAAAA0DWbaxtVuK1UN24v1fc+3Ksbt5eqcFupNtfGb1Ds+uuvl2EYeuedd7RgwQJNnDhRU6dO1S233KK333673ce0N0JdXFwswzBUUlIiSSotLdX8+fOVmZkpn8+nqVOn6sUXX1RJSYlmz54tScrMzJRhGLrqqqskRVvGFhUVaezYsfJ6vTr11FP17LPPtrnuSy+9pBkzZsjj8WjTpk3t1rh06VItWrRI06ZNa/f4jh07VFVVpWXLlunkk0/W1KlTtWTJEh08eFClpaWd/F1srVN9uHfv3q0f/OAHWrNmjb72ta9p27Ztys/P71YB6H2eZJcczuiabdPTth9yOBjdQM2T7EpAdQAAAMDgtrm2UXft2q/acETD3E4lmaaaLUtbG/y6a9d+3TV+lM5M9/XoNauqqvTyyy9r+fLl8vnaPvfxpmJ3RGFhoYLBoDZu3Cifz6dt27YpJSVFubm5WrNmjRYsWKAPP/xQaWlpsRaxRUVFevrpp/Xwww9rwoQJ2rhxoy6//HJlZ2frvPPOiz334sWLdf/99ys/P1+ZmZldqu/kk0/WkCFD9Nhjj+kHP/iBIpGIHnvsMU2ePFljxozp8uuWOhG4r7/+ej322GOaPXu23n33XZ122mndujASJ2uET5kjfKrcW6/0oUlt+iE31QeVnZuqrBGt/6DFs4UYAAAAgOg08l/tq1RtOKIxSZ8uAfU5HBqTZKqkOajH91VqRlqyzB7cB2fXrl2ybVuTJk3qsedsUVZWpgULFsRGmI8etG3ZB2zYsGGxUB8IBHTvvffqtdde08yZM2OP2bRpkx555JFWgXvZsmWaM2dOt+pLTU3Vhg0bdOmll+ruu++WJE2YMEGvvPKKnM5OjVG30eFHP/zww0pKStKhQ4d0zTXXHPe8LVu2dKsgxJ9hGpp2/mi9tWanag83KznVLafbVDhoqak+qKRkp6adP7pVmKaFGNBzbNtWnT+sUNiWy2kozetk4zgAACBJ+rCxWTubmjXM3fbzgWEYGuZ26qOmZn3Y2KzJKd4eu64dx/XhN910k6677jqtW7dOF154oRYsWKDp06cf9/xdu3apqampTZAOBoMqKChodd8ZZ5zR7fr8fr++/e1va9asWXrmmWcUiUR0//3365JLLtHmzZtjo+5d0eHAvWTJki5fBH3PiHHpmrVgQixE++ujITo7N7VNiKaFGNBzjtQH9XFFk+r9EUUsWw7TUKrXofycZA1JdSe6PAAAkGA14YgClq0ks/39lpJMU5VWWDXhSI9ed8KECTIMQzt27OjU48xP6jw6sB+70di1116ruXPnau3atVq3bp2Kioq0cuVK3Xjjje0+Z0NDgyRp7dq1GjVqVKtjHo+n1c/tTX/vrN/85jcqKSnR3/72t9jr+c1vfqPMzEz98Y9/1H/8x390+bkJ3IPYiHHpyhmbdsJp4se2EGv5ls38pIVY7eFmbd2wTzlj05heDnyGI/VBbS2tVyhsK8llKsllKGJJNY1hbS2t17S8VEI3AACDXIbTIY9pqNmy5HO03W+p2bLkMQ1lONse646srCzNnTtXq1at0k033dQmyNbU1LS7jjs7O1uSVF5eHltDXVxc3Oa83NxcLVy4UAsXLtTtt9+uRx99VDfeeKPc7uhnn0jk0y8QpkyZIo/Ho7KyslbTx+OlqalJpmm2mlHQ8rNlWd167i7tUn60N998Uy+++KKqq6u7+1RIAMM0NGRUikZOyNSQUSltQjMtxICeYdu2Pq5oUihsy+cx5XQYMgxDTochn8dUKBw9Hs/pXAAAoO872ZekCclJOhQMt/lcYNu2DgXDmpicpJN9ST1+7VWrVikSieiss87SmjVrtHPnTm3fvl0///nPY2upjzV+/Hjl5ubqrrvu0s6dO7V27VqtXLmy1Tk333yzXnnlFe3Zs0dbtmzRG2+8ocmTJ0uS8vLyZBiGXnjhBVVWVqqhoUGpqam67bbbtGjRIj355JPavXu3tmzZogcffFBPPvlkp19XWVmZiouLVVZWpkgkouLiYhUXF8dG0ufMmaPq6moVFhZq+/bt+uCDD3T11VfL6XTGdlHvqg4H7vvuu0933HFH7GfbtjVv3jzNnj1bX/ziFzV58mR98MEH3SoGfQ8txNCf2batpqYm1dXVqakpsWG2zh9WvT+iJJfZ7pdXSS5T9f6I6vzt948EAACDg2kYumZ0ttKdDpU0B9UYiShi22qMRFTSHFSG06GrR2f36IZpLfLz87VlyxbNnj1bt956q0455RTNmTNH69ev1+rVq9t9jMvl0jPPPKMdO3Zo+vTpuu+++3TPPfe0OicSiaiwsFCTJ0/WvHnzNHHiRD300EOSpFGjRmnp0qVavHixhg8frhtuuEGSdPfdd+uOO+5QUVFR7HFr167V2LFjO/267rzzThUUFGjJkiVqaGhQQUGBCgoK9O6770qSJk2apD//+c/617/+pZkzZ+rzn/+8Dhw4oJdfflkjRozo9PWOZtgd/AR6+umn6/vf/76+/vWvS5L+93//V1deeaVeffVVTZ48Wd/61reUnJys3//+990qKJ7q6uqUnp6u2tpapaWlJbqcfuHI/ga99vg2eZKdcrXTQizYHFHQH9aFV0/RkFEpCagQaF99fb0qKirk9/tlWZZM05TX61VOTo5SU1N7vZ7DdUG993GdfJ62gVuKfjnQGLBUkJ+moWlMKwcAoEV//Azf3NysPXv2aOzYsUpK6tpI9ObaRv1qX6V2NjUrYNnymIYmJifp6tHZPd4SDJ3X0fe4w2u49+zZ02onuRdffFFf/epXNWvWLEnSD3/4Q1122WXdKBl9UVdbiAGJVF9fr9LSUoXDYblcLrlcLlmWpcbGRpWWliovL6/XQ7fLachhRtdst7fkKmJJDtOQy8leCAAAQDoz3acZacn6sLFZNeGIMpwOnexLisvINuKnw1PKw+Fwqx3h/va3v+mcc86J/Txy5EgdPny4Z6tDwrW0EPMkO1V7uFnB5ogsy1awOaLaw83tthADEsm2bVVUVMT+znI4HDIMQw6HQx6PR+FwWBUVFb0+vTzN61Sq16HmkNXueqzmkKVUr0Np3u71egQAAAOHaRianOLVzIwUTU7xErb7oQ4H7nHjxmnjxo2SoovOP/roI/3bv/1b7Pi+ffs0ZMiQnq8QCdfSQiw7N1VBf1j1R5oV9IeVnZuqc2gJhj7G7/fL7/fL5XK1u1ba5XLFzulNhmEoPydZLqehxoClcMSWbdsKR6JTyd1OU/k5yfTjBgAAGEA6PJRSWFioG264QX/5y1/09ttva+bMmZoyZUrs+Ouvv96mCTkGjo60EAP6gnA4LMuy5HK52j1umqZCoZDC4d7fnGxIqlvT8lJjfbibQ9E+3Bk+J324AQAABqAOB+7vfOc7cjgc+vOf/6x/+7d/a9OX+8CBA7rmmmt6vED0HS0txIC+zOl0yjRNWZYlRzu9K1s2UHM6EzN1e0iqW1kpLtX5wwqFbbmchtK8Tka2AQAABqBOfeK85pprjhuqW7Z1R99hWzYj0hh0vF6vvF6vGhsbZZpmm43+QqGQfD6fvF5vwmo0DEPpye2PwAMAAGDg6NYQzyWXXKJf/vKX3e5Nhp5XvrtWWzfsU3V5Y7SPttNU5gifpp0/mjXXGNAMw1BOTo5KS0sVCATkcrliI96hUEhOp1M5OTmMKAMAACDuOrxpWns2btzY6xsP4bOV767VW2t2qrKsXp5kp1KHJMmT7FTl3nq9tWanynfXJrpEIK5SU1OVl5cnn8+nSCSiQCCgSCQin8+XkJZgAAAAGJzoPzPA2JatrRv2KdAYVnr2p32zTY9D6UOTVHu4WVs37FPO2DSml2NAS01NVUpKivx+v8LhsJxOp7xeLyPbAAAA6DXdGuHOy8s77k7ASIyq8kZVlzfKl+5utyVScqpb1eWNqipvTFCFQO8xDEPJyclKS0tTcjIttwAAAHqCYRh6/vnnE11Gv9DpwF1WVibbtiVJ77//vnJzcyVFNyMqKyvr2erQaYGmUHTNtqv9t9bpNhUJWwo0hXq5MgAAAAB9XUVFhW688Ubl5+fL4/EoNzdX8+fP1/r16+NyvQ0bNsgwDNXU1MTl+SVp+fLlOuecc5ScnKyMjIx2z1m/fr3OOeccpaamKicnR9///vd7pI1spwP32LFjVVlZ2eb+qqoqjR07ttsFoXs8yS45nKYiIavd4+FgdAM1DzskAwAAAH2abduqCoRU4Q+oKhCKDXzGS0lJiWbMmKHXX39dK1as0NatW/Xyyy9r9uzZKiwsjOu1u8u27eMG5GAwqMsuu0zXXXddu8f/+c9/6gtf+ILmzZun9957T7/73e/0pz/9SYsXL+52XZ0O3LZttzsts6GhQUlJSd0uCN2TNcKnzBE+NdYF2/yBtG1bTfVBZY7wKWuEL0EVAgAAAPgsB/1BvVlRrb9UVOuvB2v1l4pqvVlRrYP+YNyuef3118swDL3zzjtasGCBJk6cqKlTp+qWW27R22+/3e5j2huhLi4ulmEYKikpkSSVlpZq/vz5yszMlM/n09SpU/Xiiy+qpKREs2fPliRlZmbKMAxdddVVkiTLslRUVKSxY8fK6/Xq1FNP1bPPPtvmui+99JJmzJghj8ejTZs2tVvj0qVLtWjRIk2bNq3d47/73e80ffp03XnnnRo/frzOO+88/fjHP9aqVatUX1/fyd/F1jq8adott9wiKTpf/4477lBycnLsWCQS0d///neddtpp3SoG3WeYhqadP1pvrdmp2sPNSk51y+k2FQ5aaqoPKinZqWnnjx6UG6bRlxwAAAD9wUF/UJsP1yoQsZXsNOQ0DIVtW1WBsDYfrtWZQ9M13Ovu0WtWVVXp5Zdf1vLly+XztR2cO95U7I4oLCxUMBjUxo0b5fP5tG3bNqWkpCg3N1dr1qzRggUL9OGHHyotLU1er1eSVFRUpKeffloPP/ywJkyYoI0bN+ryyy9Xdna2zjvvvNhzL168WPfff7/y8/OVmZnZpfoCgUCbwWOv16vm5mb94x//0Pnnn9/l197hwP3ee+9Jio6Sbt26VW73p2+w2+3Wqaeeqttuu63LhaDnjBiXrlkLJsT6cPvro9PIs3NTB20fbvqSAwAAoD+wbVvbaxoUiNhKc5mx2cUuw5DTZas+ZGlHTYOGJWX26Iawu3btkm3bmjRpUo89Z4uysjItWLAgNsKcn58fO5aVlSVJGjZsWCzUBwIB3XvvvXrttdc0c+bM2GM2bdqkRx55pFXgXrZsmebMmdOt+ubOnauf/exneuaZZ/S1r31NFRUVWrZsmSSpvLy8W8/d4cD9xhtvSJKuvvpqPfDAA0pLS+vWhRFfI8alK2dsWo+N6Pbn0eGWvuSBxrB86W45XNE17i19yWctmEDoBgAAQJ9QHQyrNhhWstNot+uQ12moJhhWdTCsLE/P7csUz/XhN910k6677jqtW7dOF154oRYsWKDp06cf9/xdu3apqampTZAOBoMqKChodd8ZZ5zR7fouuugirVixQgsXLtQVV1whj8ejO+64Q3/5y19kmt1q7NX5PtyPP/54ty6I3mOYhoaMSun28/Tn0WH6kgMAAKA/CVqWIrbkPM7otcMwFLFtBa32N0nuqgkTJsgwDO3YsaNTj2sJpEcH9lCodUeka6+9VnPnztXatWu1bt06FRUVaeXKlbrxxhvbfc6GhgZJ0tq1azVq1KhWxzweT6uf25v+3hW33HKLFi1apPLycmVmZqqkpES33357q9H4ruheXMeA1zI6XFlWL0+yU6lDkuRJdsZGh8t31ya6xBOiLzkAAAD6E7dpymFI4eOMOEdsWw4jel5PysrK0ty5c7Vq1So1Nrb9bHy8tl3Z2dmSWk+9Li4ubnNebm6uFi5cqOeee0633nqrHn30UUmKLVWORCKxc6dMmSKPx6OysjKNHz++1a2lLXU8GIahkSNHyuv16plnnlFubq5OP/30bj1np0e4MXgMhNHhjvQl99fTlxwAAAB9Q6bbqXS3U1WBsJyu1h2ibNuWP2xriMepTHfPR7lVq1Zp1qxZOuuss7Rs2TJNnz5d4XBYr776qlavXq3t27e3eUxLCL7rrru0fPlyffTRR1q5cmWrc26++WZdfPHFmjhxoqqrq/XGG29o8uTJkqS8vDwZhqEXXnhBX/jCF+T1epWamqrbbrtNixYtkmVZOvfcc1VbW6u33npLaWlpuvLKKzv1usrKylRVVaWysjJFIpHYFwLjx49XSkp0RvCKFSs0b948maap5557Tj/60Y/0+9//Xg6Howu/k59ihBvHNRBGh+lLDgAAgP7EMAxNzkiR22GoPmQpZFmybFshy1J9yJLbYWhSRkqPbpjWIj8/X1u2bNHs2bN166236pRTTtGcOXO0fv16rV69ut3HuFwuPfPMM9qxY4emT5+u++67T/fcc0+rcyKRiAoLCzV58mTNmzdPEydO1EMPPSRJGjVqlJYuXarFixdr+PDhuuGGGyRJd999t+644w4VFRXFHrd27VqNHTu206/rzjvvVEFBgZYsWaKGhgYVFBSooKBA7777buycl156SZ///Od1xhlnaO3atfrjH/+oSy+9tNPXOpZhx7t7eh9SV1en9PR01dbWsulbBxzYWa0Nv/5QqUOSZLYzgm1ZtuqPNOv8b56skRO6tgV/vNmWrVcf36bKvfVKH5rU5hvC2sPNys5N1Zyrp/TZUXoAAIDBrD9+hm9ubtaePXs0duzYNu2mOuqgP6jtNQ2qDYYVsSWHIWW4nZqUkdLjLcHQeR19j5lSjuM6enTY9LSdStEfRofpSw4AAID+aLjXrWFJmaoOhhW0LLlNU5luZ1xGthE/TCnHcWWN8ClzhE+NdcE2bQJs21ZTfVCZI3zKGtEzOwPGS0tf8uzcVAX9YdUfaVbQH1Z2bqrO6UJLMNuydWR/gw7srNaR/Q2yrUEzSQQAAAC9yDAMZXlcyvF6lOVxEbb7IUa4cVwDaXS4p/qS9+cWaQAAAAB6FyPcOKGeHh1OpJa+5CMnZGrIqJQuhe3+3CINAAAAQO9ihBufqadGh/uzgdAiDQAAAEDvInCjQ1pGhwerzrRIG8y/TwAAAPjUIGoINehYVvtth49F4AY6INAUiq7ZdrW/CsPpNuWvtxRoCvVyZQAAAOhrXK7oBmeVlZXKzs5ms7MBxLZtBYNBVVZWyjRNud0nbtFG4AY6YCC0SAMAAEDvcDgcGj16tPbt26eSkpJEl4M4SE5O1kknnSTTPPG2aARuoANaWqRV7q1X+tCkVt9StrRIy85N7fMt0gAAANA7UlJSNGHCBIVCzIAcaBwOh5zOjvVEJ3ADHTCQWqQBAACgdzgcDjkcbWdHYvCgLRjQQQOpRRoAAACA+GOEG+gEWqQBAAAA6CgCN9BJg71FGgAAAICOYUo5AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4qBfBO6SkhJ9+9vf1tixY+X1ejVu3DgtWbJEwWAw0aUBAAAAANAuZ6IL6IgdO3bIsiw98sgjGj9+vN5//3195zvfUWNjo+6///5ElwcAAAAAQBuGbdt2oovoihUrVmj16tX6+OOPO/yYuro6paenq7a2VmlpaXGsDgAAAEBP4DM8+rN+MaW8PbW1tcrKykp0GQAAAAAAtKtfTCk/1q5du/Tggw9+5nTyQCCgQCAQ+7muri7epQEAAAAAICnBI9yLFy+WYRgnvO3YsaPVY/bv36958+bpsssu03e+850TPn9RUZHS09Njt9zc3Hi+HAAAAAAAYhK6hruyslJHjhw54Tn5+flyu92SpAMHDuj888/X5z73OT3xxBMyzRN/X9DeCHdubi7rPwAAAIB+gjXc6M8SOqU8Oztb2dnZHTp3//79mj17tmbMmKHHH3/8M8O2JHk8Hnk8nu6WCQAAAABAp/WLNdz79+/X+eefr7y8PN1///2qrKyMHcvJyUlgZQAAAAAAtK9fBO5XX31Vu3bt0q5duzR69OhWx/ppVzMAAAAAwADXL9qCXXXVVbJtu90bAAAAAAB9Ub8I3AAAAAAA9DcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4sCZ6AIAAOgLrHBYDetfV/jQQTmHDVfKBf8u08k/kwAAoOv4JAEAGPSqfvs7HfnFLxQ5ckS2ZckwTTmGDNGQ//N/lPUfX090eQAAoJ8icAMABhzbshTYuUtWXa3MtHR5JoyXpDb3Gaapqt/+Tod+/GPZoZCMpCSZTqfscFjhykod+vGPJYnQDQAAuoTADQAYUJq2vKfqX/9agY93yw4GZbjdcmRkSoahSHVV7D5P/jil/8fXdeQXv5AdCslMSZFhRrc2Mdxu2U6nrIYGHfnFL5Tx1QVMLwcAAJ3GpmkAgAGjact7OvTjH6t52zY5UtPkGjlKkiH/li3y/+Mfkgy5Ro6SIzVNzdu3q+KOOxWurJSRlBQL2y0M05Th8Shy5Iga1r+ekNcDAAD6NwI3AGBAsC1L1b/+tSK1tXKddJLM5GTJNBU+ckQyTdmmqciRI5LDlJmcLFdurqy6OikclhyOdp/TcLlkW5bChw728qsBAAADAfPjAAADQmDnLgU+3i1ndrYMw5AkWY2NshsbZXg8MmxbkcYGhSsPy3S5JKdTRmamVF0thULthm47FJJhmnIOG97bLwcAAAwABG4AwIBg1dVG12d7PJ/eGQ7Ltu3odPFQSHZzQMHduyXTlGEYMrxeyTBkNzfLdrtbTSu3LUt2ICBndrZSLvj3BLwiAADQ3xG4AQADgpmWHt3sLBCQkZwcvdPplGEYsoNB2X6/ZFnR+9xuybJkNTRIpimZpqyGhuhIuMslOxSKPo/LpSH/5/+wYRoAAOgS1nADAAYEz4Tx8uSPU/jwYdm2LUkyfT4pOblV2DY97ujotmnKNgyZHo88EyfKMXSoFApFQ3goJGd2toZ973u0BAMAAF3GV/YAgAHBME1lfvObOvTjHyu0d6+cQ4bISEqSw+eTdfiwJMn0eCRbsiPhaCswl0uuUaOkSFi5q/6fQnv3KXzooJzDhivlgn9nZBsAAHQLnyQAAANG8ukFGva9733ah7vqiGRbMpI8kidJCodkNTfLMA05UlLkGjVKZkqKQgf2y25sVNrcixL9EgAAwABC4AYADCjJpxfIe9qpCuzcJauuVuGqah1evVpmWqpkK7ojucsl05csQ4asxkYZbrfMtPRElw4AAAYYAjcAYMAxTFNJJ0+UFN1tvH7dOjVv3y5Xbm6sZZgk2bat8JEjSpoyWZ4J4xNVLgAAGKDYNA0AMKC1rO12pKUptHdvtDd3JCKrsVGhvXvlSEtT5n9+s1VLMAAAgJ7ApwsAwIDXsrY7afJkRRrqFTqwX5GGeiVNmaxh3/uekk8vSHSJAABgAGJKOQBgUDh2bbeZli7PhPGMbAMAgLghcAMABo2j13YDAADEG1/rAwAAAAAQBwRuAAAAAADigMANAAAAAEAcELgBAAAAAIgDAjcAAAAAAHFA4AYAAAAAIA4I3AAAAAAAxAF9uAEAAIBjWJalqooGBf0hub0uZeWkyDQZqwLQOQRuAAAA4CgVJVXa99FhhQIRybYlw5DL49DoiUOVMyYr0eUB6EcI3AAAAMAnKkqqVPL+QdmWLdNhSIYh25KCzWHtef+gbNvWiLFDEl0mgH6CeTEAAACAotPI9310OBq2naZkGLIitizLlmzJjtgq+eCQag83JLpUAP0EgRsAAACQVFXRoFAgEh3ZlhQJW7ItyZBkRO+SHbG1u7hcdVVNiSsUQL9B4AYAAAAkBf2hT9ZsS1bEkuxPg/bRwiFLFR9Xybbt3i8SQL9C4AYAAAAkub2u2Jpt+zhhW5JcHoea6gNqqg/0boEA+h0CNwAAACApKydFLo8jtmb7aC2D2YZpyJ3klG3ZCgcjvV8kgH6FwA0AAABIMk1ToycOlWFGh7Zt+9NbC6/PFV3XbRpyuh0JqhRAf0HgBgAAAD6RMyZLY6YOk+FoPZ/cMA0lp7rl9roUCoSVnOpRcqonQVUC6C/oww0AAAAcZcTYIUpO9Wh3cbnCIUsuj+OTaeRSoCkkp8uhnPwsGcdb5A0An2CEGwAAADhG+tAUjT99lDKyfTIMQ0F/WOFQRL70JOWdMlxpWcmJLhFAP8AINwAAANCOtKxkpWZ61VQfUDgYkdPtUHKqh5FtAB1G4AYAAACOwzAM+dKSEl0GgH6KKeUAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQB9uoA+zbVtN1VUKBwJyejxKzsySYRiJLgsAAABABxC4gT6q7mC5yrdtlb+mWlYkItPhkDcjUyOmTFPa8BGJLg8AAADAZ2BKOdAH1R0sV8k7b6nxSKUcbo88qalyuD1qPFKp3W+9qfJtW9VYdUS2bSe6VAAAAADHwQg30MfYtq3ybVsVDgTkSU2PTSG3raBCwaAigYBK3/2bPClpSs7MYsQbAAAA6KMY4Qb6mKbqKvlrquXy+mJhOxxoVlNNlaxQUKbDIVuSYTrUeKRSJe+8pbqD5YktGgAAAEAbBG6gjwkHAtE1207HJ/fYam6ol21ZMp0uGaYp2ZJhmvKkpiscCKh8+1amlwMAAAB9DIEb6GOcHo9Mh0NWOCJJioRCn4xsO2VIkm1LhmSapgzDkMubLH91tZqqqxJaNwAAAIDWCNxAH5OcmSVvRqZC/kbZti3bsmTbkozo+m7Lisjhcst0uSRJpsMpKxJROBBIbOEAAAAAWiFwA32MYRgaMWWanB6PAvW1ikTCsm1LViisSDgkGYaSUlJj67utSFimwyGnx5PgygEAAAAcjcAN9EFpw0dozFmz5PalKFBfJzsSkRUJy7YsGTIUDgYVCQZlWZZC/iZ5MzOVnJmV6LIBAAAAHIW2YEAfFgmF5HR75EryKtDYIDsSUSQUlL+mSs0Oh0zTlCclVSMmT4uNeAMAAADoGxjhBvqgll7ckWBA3oxMOZyu6IZpR4Xq6NpudiYHAAAA+ioCN9AHtfTiNh1ONR45rMbqI7Ii0V3LDdOU4XTKdDjkGzJUtmXRFgwAAADogwjcQB8UDgQUDgbkr69VJPjJ7uOGIcMwojuXh8OSbcu2RFswAAAAoI8icAN9kMPtVjgYiG6S5nBIUmyNdmx3csuSaRq0BQMAAAD6KAI30IcZMqSWmeKf/Grbdmwtty3aggEAAAB9FYEb6IMiwaAcLo8MhynbtiQZsu3Wm6QZpimbtmAAAABAn0XgBvogp8cjl8cjT0qaHG6PTPOTP6q2LUOGTIdDhmEo5G+S05NEWzAAAACgD6IPN9AHJWdmyZuRqcYjlUrOHCI7HFYo0KxQs1+RcEi2Zcl0OpU6LEcjpkxT2vARiS4ZAAAAwDEI3EAfZBiGRkyZppJ33lKwoU4ub7I8vhQ5PR4FGxtkOl0aPb1A2eNPZmQbAAAA6KMI3EAflTZ8hMacNUvl27bKX1OtUMQv0+FQ6vAcjZjMqDYAAADQ1xG4gT4sbfgIpQ7LUVN1lcKBgJwej5IzsxjVBgAAAPoBAjfQxxmGIV/WkESXAQAAAKCT2KUcAAAAAIA4IHADAAAAABAHBG4AAAAAAOKAwA0AAAAAQBwQuAEAAAAAiAMCNwAAAAAAcUDgBgAAAAAgDgjcAAAAAADEAYEbAAAAAIA4IHADAAAAABAHBG4AAAAAAOKAwA0AAAAAQBwQuAEAAAAAiAMCNwAAAAAAcUDgBgAAAAAgDgjcAAAAAADEAYEbAAAAAIA4IHADAAAAABAHBG4AAAAAAOKAwA0AAAAAQBwQuAEAAAAAiAMCNwAAAAAAcUDgBgAAAAAgDgjcAAAAAADEQb8L3IFAQKeddpoMw1BxcXGiywEAAAAAoF39LnB/73vf08iRIxNdBgAAAAAAJ9SvAvdLL72kdevW6f777090KQAAAAAAnJAz0QV01MGDB/Wd73xHzz//vJKTkzv0mEAgoEAgEPu5rq4uXuUBAAAAANBKvxjhtm1bV111lRYuXKgzzjijw48rKipSenp67JabmxvHKgEAAAAA+FRCA/fixYtlGMYJbzt27NCDDz6o+vp63X777Z16/ttvv121tbWx2969e+P0SgAAAAAAaM2wbdtO1MUrKyt15MiRE56Tn5+vr33ta/rzn/8swzBi90ciETkcDn3zm9/Uk08+2aHr1dXVKT09XbW1tUpLS+tW7QAAAADij8/w6M8SGrg7qqysrNX66wMHDmju3Ll69tlndfbZZ2v06NEdeh7+sAIAAAD9C5/h0Z/1i03TTjrppFY/p6SkSJLGjRvX4bANAAAAAEBv6hebpgEAAAAA0N/0ixHuY40ZM0b9YCY8AAAAAGAQY4QbAAAAAIA4IHADAAAAABAHBG4AAAAAAOKAwA0AAAAAQBwQuAEAAAAAiAMCNwAAAAAAcUDgBgAAAAAgDvplH26gJ9iWrfChJllNYZnJTjmHJcswjUSXBQAAAGCAIHBjUAqU1qnhrwcUPtQkO2zJcJpyDktWyjkj5clLS3R5AAAAAAYAppRj0AmU1ql27ccK7W+QkeSUIzNJRpJToQMNql37sQKldYkuEQAAAMAAQODGoGJbthr+ekBWU1iOIUkyPQ4ZpiHT45AjK0mWP6yGvx6QbdmJLhUAAABAP0fgxqASPtSk8KEmmaluGUbr9dqGYchMccfOAQAAAIDuIHBjULGawtE12672/9c3XKbssCWrKdzLlQEAAAAYaAjcGFTMZKcMpyk7ZLV73A5FN1Azk9lPEAAAAED3ELgxqDiHJcs5LFlWQ1C23Xqdtm3bshqCsXMAAAAAoDsI3BhUDNNQyjkjZXqdilQ1ywpEZFu2rEBEkapmmV6nUs4ZST9uAAAAAN1G4Mag48lLU/ol+XKNTJHdHFakull2c1iukSlKvySfPtwAAAAAegQLVTEoefLS5M5NVfhQk6ymsMxkp5zDkhnZBgAAANBjCNwYtAzTkCvHl+gyAAAAAAxQTCkHAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4oDADQAAAABAHBC4AQAAAACIAwI3AAAAAABxQOAGAAAAACAOCNwAAAAAAMQBgRsAAAAAgDggcAMAAAAAEAcEbgAAAAAA4sCZ6AIAAAAAdI5l2/qwsVk14YgynA6d7EuSaRiJLgvAMQjcAAAAQD+yubZRv9pXqZ1NzQpYtjymoQnJSbpmdLbOTPclujwAR2FKOQAAANBPbK5t1F279mtrg19pTodyk9xKczq0tcGvu3bt1+baxkSXCOAoBG4AABLNsqSDH0glb0V/taxEVwSgD7JsW7/aV6nacERjktzyORxyGIZ8DofGJLlVG47o8X2Vsmw70aUC+ARTygEASKSyv0vv/EI6/KEUDkhOjzT0ZOms/yOddHaiqwPQh3zY2KydTc0a5nbKOGa9tmEYGuZ26qOmZn3Y2KzJKd4EVQngaIxwAwCQKGV/l9b9t1T+TykpXcrIi/5a/q/o/WV/T3SFXceoPdDjasIRBSxbSWb7H+GTTFMBy1ZNONLLlQE4Hka4AQBIBMuKjmz7a6SsfKlltMqdImX5pKo90uZHpdFnSsf5cN1nMWoPxEWG0yGPaajZsuRzONocb7YseUxDGc62xwAkRj/7FxwAgAGicns0kKYO/zRstzAMKXWYVLkjel5/MpBH7YEEO9mXpAnJSToUDMs+Zp22bds6FAxrYnKSTvYlJahCAMcicAMAkAj+mk9Gf4+zztLpjR731/RmVd1z7Ki9O0UyHZ+M2o+V/LXRUXumlwNdYhqGrhmdrXSnQyXNQTVGIorYthojEZU0B5XhdOjq0dn04wb6EAI3AACJ4M2ITrUO+9s/HvZHj3szerOq7hmoo/ZAH3Jmuk93jR+laSle1YUj2tscVF04oukpXi0ZP4o+3EAfwxpuAAASIXtydF1z+b+ia7aPDqi2LdUfkkaeGj2vv+jQqP2h/jVqD/RBZ6b7NCMtWR82NqsmHFGG06GTfUk9OrJt27aqQocVsJrlMZOU5RraZmd0AJ+NwA0AQCKYZnQTsXX/Hd0gLXXYJ4HUHw3b3gzpzO/0rw3Tjh61d6e0Pd4fR+2BPso0jLi1/qpo3q/367eoJliliMJyyKkMd5ZOST1dOUmj4nJNYKDqR/+KAwAwwJx0tnTRcmnEdKm5Vqopjf468lTponv6347eLaP29Yeio/RHaxm1z57Uv0btgUHEtm19VP+BNlS+rAP+vbJsSx7DK5fh1pHAIf2taoMqmvcnukygX2GEGwCARDrp7Gjrr8rt0anW3oxoIO1rI9uW9dk1DsRRe2CQqGjer/frtqjMv1shOyRJ8ltNcsiUx0xSijNdQatZH9S/p+GekUwvBzqIwA0AQKKZpjR8aqKrOL7O9NVuGbWPnX8oev7IU6Nhu7+N2gODQEXzfv2taoPqI7WxsB1lK6KImqxGBUNBpTszVR08oqrQYQ1xZyesXqA/IXADAIDja+mr7a+J7j7eMmLd0lf7ouXth+7+MGoPQLZt6/36LQpYfoUiweOeF7ZDagjXKcnhVcBq7sUKgf6NwA0AANp3bF/tlimk7pTozupVe6J9tUef2f708r48ag9AklQVOqyaYJXchlt1Cp/w3KAdkMf2ymMm9VJ1QP/HV80AAKB99NUGBryA1ayIwgrbEdmyT3iuLVseh0dZrqG9VB3Q/xG4AQBA+zrUVztAX22gH/OYSXLIKUsRSZKhE2+GNippDBumAZ1A4AYAAO07uq92e+irDfR7Wa6hynBnKWJHJBmydfzQ7TRcGuc7uVfrA/o7AjcAAGgffbWBAc8wDJ2SerqSHT45DYeiE8ejsVufBG9TDjnk0HDPSHYnBzqJwA0AANrX0lfbmx7dIC3YIFmR6K9Ve+irDQwQOUmjNDNrtoZ7RsmU45N7bRlSNGobDqU5M3VGxiymkwOdZNj2sV9ZD1x1dXVKT09XbW2t0tLSEl0OAAD9Q3t9uLMn0VcbGGBs29bOhm3a0bBV9eFa2bYlp+nWUPcwnZJ2unKSRiWkLj7Doz+jLRgAADix3uqrbVn07gYSyDAMTUydqgkpU1QVOqyA1SyPmaQs11BGtoEuInADAIDPFu++2u2Nog89OTqlnVF0oFcZhsFabaCH8LUxAABIrLK/S+v+Wyr/p5SULmXkRX8t/1f0/rK/J7pCAAC6hMANAAASx7KiI9v+GikrX3KnSKYj+mvWWMlfK21+NHoeAAD9DIEbAIDBwLKkgx9IJW9Ff+0rAbZye3Qaeepw6dg1ooYhpQ6TKndEzwMAoJ9hDTcAAANdX14f7a/5pCZv+8edXil8KHoeAAD9DCPcAAAMZH19fbQ3I/oFQNjf/vGwP3rcm9GbVQEA0CMI3AAADFQ9vT46HtPSsydHR9vrD0m23fqYbUfvz54UPQ8AgH6GKeUAAAxUnVkf/Vktv+I1Ld00o8+x7r+lqj3Rmpze6Mh2/aHoyPaZ36EfNwCgX+JfLwAABqoOrY8OfPb66HhPSz/pbOmi5dKI6VJzrVRTGv115KnSRfckfp05AABdxAg3AAAD1dHro90pbY93ZH30sdPSW0bK3SlSli86Kr35UWn0md0bhT7p7OhzVG6PXsubEZ1Gzsg2AKAfI3ADADBQtayPLv9XNBwfPa28ZX30yFNPvD66J6elfxbT7P5zAADQh/C1MQAAA1XL+mhvenQkOtggWZHor1V7OrY+uqempQMAMAgRuAEAGMi6uz6atl0AAHQZU8oBABjourM+uiempUvRteCszwYADDIEbgAABoOuro/uibZd8WopBgBAH8dXywAA4MS6My093i3FAADowxjhBgAAn60r09J7q6UYAAB9FIEbAAB0TGenpfdmSzEAAPogvk4GAADxQUsxAMAgR+AGAADxQUsxAMAgR+AGAADx0dJSrP5QtIXY0VpaimVP+uyWYgAA9FMEbgAAEB8tLcW86dEN0oINkhWJ/lq1p2MtxQAA6Mf4Fw4AAMRPd1qKAQDQz7FLOQAAiK+utBQDAGAAIHADAID462xLMQAABgC+WgYAAAAAIA4I3AAAAAAAxAGBGwAAAACAOCBwAwAAAAAQBwRuAAAAAADigMANAAAAAEAcELgBAAAAAIgDAjcAAAAAAHFA4AYAAAAAIA4I3AAAAAAAxAGBGwAAAACAOCBwAwAAAAAQBwRuAAAAAADigMANAAAAAEAcELgBAAAAAIgDAjcAAAAAAHFA4AYAAAAAIA4I3AAAAAAAxAGBGwAAAACAOCBwAwAAAAAQBwRuAAAAAADigMANAAAAAEAcELgBAAAAAIgDAjcAAAAAAHHgTHQBvcm2bUlSXV1dgisBAAAA0BEtn91bPssD/cmgCtz19fWSpNzc3ARXAgAAAKAz6uvrlZ6enugygE4x7EH0VZFlWTpw4IBSU1NlGEaiy+myuro65ebmau/evUpLS0t0OfgE70vfxPvS9/Ce9E28L30T70vfxPvSu2zbVn19vUaOHCnTZEUs+pdBNcJtmqZGjx6d6DJ6TFpaGn/J90G8L30T70vfw3vSN/G+9E28L30T70vvYWQb/RVfEQEAAAAAEAcEbgAAAAAA4oDA3Q95PB4tWbJEHo8n0aXgKLwvfRPvS9/De9I38b70TbwvfRPvC4COGlSbpgEAAAAA0FsY4QYAAAAAIA4I3AAAAAAAxAGBGwAAAACAOCBw9yNFRUU688wzlZqaqmHDhunSSy/Vhx9+mOiyBrXVq1dr+vTpsT6cM2fO1EsvvZTosnCMH/3oRzIMQzfffHOiSxnU7rrrLhmG0eo2adKkRJcFSfv379fll1+uIUOGyOv1atq0aXr33XcTXdagNmbMmDZ/XgzDUGFhYaJLG7QikYjuuOMOjR07Vl6vV+PGjdPdd98ttkMCcCLORBeAjnvzzTdVWFioM888U+FwWD/4wQ900UUXadu2bfL5fIkub1AaPXq0fvSjH2nChAmybVtPPvmkvvzlL+u9997T1KlTE10eJG3evFmPPPKIpk+fnuhSIGnq1Kl67bXXYj87nfwzlGjV1dWaNWuWZs+erZdeeknZ2dnauXOnMjMzE13aoLZ582ZFIpHYz++//77mzJmjyy67LIFVDW733XefVq9erSeffFJTp07Vu+++q6uvvlrp6em66aabEl0egD6KXcr7scrKSg0bNkxvvvmm/u3f/i3R5eATWVlZWrFihb797W8nupRBr6GhQaeffroeeugh3XPPPTrttNP0s5/9LNFlDVp33XWXnn/+eRUXFye6FBxl8eLFeuutt/SXv/wl0aXgBG6++Wa98MIL2rlzpwzDSHQ5g9IXv/hFDR8+XI899ljsvgULFsjr9erpp59OYGUA+jKmlPdjtbW1kqIBD4kXiUT029/+Vo2NjZo5c2aiy4GkwsJCXXLJJbrwwgsTXQo+sXPnTo0cOVL5+fn65je/qbKyskSXNOj96U9/0hlnnKHLLrtMw4YNU0FBgR599NFEl4WjBINBPf3007rmmmsI2wl0zjnnaP369froo48kSf/85z+1adMmXXzxxQmuDEBfxly+fsqyLN18882aNWuWTjnllESXM6ht3bpVM2fOVHNzs1JSUvSHP/xBU6ZMSXRZg95vf/tbbdmyRZs3b050KfjE2WefrSeeeEInn3yyysvLtXTpUn3+85/X+++/r9TU1ESXN2h9/PHHWr16tW655Rb94Ac/0ObNm3XTTTfJ7XbryiuvTHR5kPT888+rpqZGV111VaJLGdQWL16suro6TZo0SQ6HQ5FIRMuXL9c3v/nNRJcGoA8jcPdThYWFev/997Vp06ZElzLonXzyySouLlZtba2effZZXXnllXrzzTcJ3Qm0d+9e/d//+3/16quvKikpKdHl4BNHjwJNnz5dZ599tvLy8vT73/+eJRgJZFmWzjjjDN17772SpIKCAr3//vt6+OGHCdx9xGOPPaaLL75YI0eOTHQpg9rvf/97/frXv9ZvfvMbTZ06VcXFxbr55ps1cuRI/qwAOC4Cdz90ww036IUXXtDGjRs1evToRJcz6Lndbo0fP16SNGPGDG3evFkPPPCAHnnkkQRXNnj94x//0KFDh3T66afH7otEItq4caP+3//7fwoEAnI4HAmsEJKUkZGhiRMnateuXYkuZVAbMWJEmy8IJ0+erDVr1iSoIhyttLRUr732mp577rlElzLoffe739XixYv1H//xH5KkadOmqbS0VEVFRQRuAMdF4O5HbNvWjTfeqD/84Q/asGGDxo4dm+iS0A7LshQIBBJdxqB2wQUXaOvWra3uu/rqqzVp0iR9//vfJ2z3EQ0NDdq9e7euuOKKRJcyqM2aNatNi8mPPvpIeXl5CaoIR3v88cc1bNgwXXLJJYkuZdBramqSabbe/sjhcMiyrARVBKA/IHD3I4WFhfrNb36jP/7xj0pNTVVFRYUkKT09XV6vN8HVDU633367Lr74Yp100kmqr6/Xb37zG23YsEGvvPJKoksb1FJTU9vsbeDz+TRkyBD2PEig2267TfPnz1deXp4OHDigJUuWyOFw6Bvf+EaiSxvUFi1apHPOOUf33nuvvva1r+mdd97RL37xC/3iF79IdGmDnmVZevzxx3XllVfSQq8PmD9/vpYvX66TTjpJU6dO1Xvvvaef/OQnuuaaaxJdGoA+jL+9+5HVq1dLks4///xW9z/++ONspJIghw4d0re+9S2Vl5crPT1d06dP1yuvvKI5c+YkujSgz9m3b5++8Y1v6MiRI8rOzta5556rt99+W9nZ2YkubVA788wz9Yc//EG33367li1bprFjx+pnP/sZG0H1Aa+99prKysoIdH3Egw8+qDvuuEPXX3+9Dh06pJEjR+q//uu/dOeddya6NAB9GH24AQAAAACIA/pwAwAAAAAQBwRuAAAAAADigMANAAAAAEAcELgBAAAAAIgDAjcAAAAAAHFA4AYAAAAAIA4I3AAAAAAAxAGBGwAAAACAOCBwAwD6tCeeeEIZGRmJLuMzXXXVVbr00ksTXQYAAOhDCNwAMICcf/75uvnmmzt07qOPPqpTTz1VKSkpysjIUEFBgYqKimLH77rrLhmGoYULF7Z6XHFxsQzDUElJiSSppKREhmG0e3v77bePe/2jz/P5fJowYYKuuuoq/eMf/2h13te//nV99NFHHfsNSKAHHnhATzzxRNyvs3z5cp1zzjlKTk7uF19EAAAwmBG4AWAQ+tWvfqWbb75ZN910k4qLi/XWW2/pe9/7nhoaGlqdl5SUpMcee0w7d+78zOd87bXXVF5e3uo2Y8aMEz7m8ccfV3l5uT744AOtWrVKDQ0NOvvss/XUU0/FzvF6vRo2bFjXXmgvSk9P75UAHAwGddlll+m6666L+7UAAED3ELgBYIC46qqr9Oabb+qBBx6IjRy3jEIf609/+pO+9rWv6dvf/rbGjx+vqVOn6hvf+IaWL1/e6ryTTz5Zs2fP1n//939/5vWHDBminJycVjeXy3XCx2RkZCgnJ+f/b+9eQ5r84jiAf/fXhpqpeZtm6hOuzLV1GSOYl0odE4mgpMIIMS0hJQKji2BYgoFpgtELiVKyXthlki9MTRleKGtqFyEQBakkUiaapGZirv+LcLS2Wf90/Eu+H9iLnee333POeffbOed5IAgCtFotdDodDh48iGPHjuHDhw8ArLeUnz9/Hps3b0ZFRQVCQkLg7u6OrKwszM7OoqioCAEBAfD397cay9jYGI4cOQI/Pz94eHggLi4O3d3dVnlv3boFQRDg6emJ5ORkjI+Pm2N0Oh0UCgVcXV3h4+MDjUaDyclJ8/x/v6V8enoax48fh7+/P1xcXBAdHY3Ozk7z9ZaWFohEIuj1eqhUKri5uSEyMhK9vb3zzll+fj6ys7OhUCjmjSMiIqL/HwtuIqIl4vLly1Cr1cjIyDCvMAcHB9uMDQgIwNOnT/H27duf5i0sLER1dTW6uroWu8s2ZWdnY3x8HE1NTXZj+vv7UV9fj4aGBlRVVaG8vBw7d+7Eu3fv0NraiosXL+Ls2bMwGAzm3+zbtw9GoxH19fV49uwZlEol4uPjMTo6apG3pqYGtbW1qK2tRWtrKwoLCwEAg4ODOHDgANLT09HT04OWlhYkJSXh69evNvt4+vRpVFdXo7KyEs+fP4dUKkVCQoLF/QAgNzcXJSUl6OrqgrOzM9LT0xcyfURERPQHYcFNRLREeHp6QiwWw83NzbzC7OTkZDP23Llz8PLygiAICA8Px6FDh3D37l2YTCarWKVSif379+PMmTPz3j8yMhLu7u4Wn9+xfv16ALC7Og8AJpMJFRUVkMlk2LVrF2JjY9Hb24vS0lKEh4cjLS0N4eHhaG5uBgA8evQIHR0duHfvHlQqFdauXYtLly7By8sLOp3OIu+NGzcgl8sRExODlJQU6PV6AN8K7i9fviApKQmCIEChUCArK8vmOCcnJ1FWVobi4mIkJiZCJpPh2rVrcHV1RXl5uUXshQsXsH37dshkMuTk5KC9vR2fP3/+rbkjIiKiP4vz/90BIiJyrA0bNphXsmNiYlBfX4/AwEA8efIEr169QltbG9rb25Gamorr16+joaEB//xj+X9sQUEBIiIi0NjYaPc89Z07dxAREbHg/s6tGItEIrsxgiBgxYoV5u8SiQROTk4W/ZZIJDAajQCA7u5uTExMwMfHxyLP1NQU+vv77eYNDAw059i0aRPi4+OhUCiQkJAArVaLvXv3YuXKlVb96+/vx8zMDKKiosxty5Ytw9atW9HT02MRu3HjRov7AYDRaERISIjd8RMREdHfgQU3EdESV1dXh5mZGQDfHkD2PblcDrlcjqysLBw9ehQxMTFobW1FbGysRVxYWBgyMjKQk5NjtUI7Jzg4GFKpdMH9nStI16xZ+hYqaQAAAxJJREFUYzfmx7PhIpHIZtvciv3ExAQCAwPR0tJilev78+Hz5XByckJTUxPa29vR2NiIK1euIDc3FwaDYd6+/sz395z7k8HWTgMiIiL6+3BLORHREiIWizE7O2vRFhoaCqlUCqlUiqCgILu/lclkAGB+CNiP8vLy0NfXh9u3by9eh20oLS2Fh4cHNBrNouVUKpUYGhqCs7OzeS7mPr6+vr+cRyQSISoqCvn5+Xjx4gXEYjHu379vFRcWFgaxWIzHjx+b22ZmZtDZ2WmeZyIiIlr6uMJNRLSECIIAg8GAN2/ewN3dHd7e3lbbwwEgMzMTq1atQlxcHFavXo3BwUEUFBTAz88ParXaZm6JRIITJ06guLjY5vWRkREMDQ1ZtHl5ecHFxcVuf8fGxjA0NITp6Wn09fXh6tWrqKmpwc2bNxf1FVsajQZqtRq7d+9GUVER1q1bh/fv3+PBgwfYs2cPVCrVT3MYDAbo9XpotVr4+/vDYDBgeHjY5jb65cuXIzMzE6dOnYK3tzdCQkJQVFSET58+4fDhwwsay8DAAEZHRzEwMIDZ2Vm8fPkSACCVSn/73DwRERE5BgtuIqIl5OTJk0hNTYVMJsPU1BRev34NQRCs4jQaDSoqKlBWVoaRkRH4+vpCrVZDr9dbnXP+MX9ZWZnNh3rZWpGuqqpCcnKy3XxpaWkAvr3vOygoCNHR0ejo6IBSqfyF0f46kUiEuro65ObmIi0tDcPDwwgICMC2bdsgkUh+KYeHhwfa2tpQWlqKjx8/IjQ0FCUlJUhMTLQZX1hYCJPJhJSUFIyPj0OlUuHhw4c2z3z/F3l5eaisrDR/37JlCwCgubkZO3bsWFBuIiIiWlyir/beZ0JEREREREREv41nuImIiIiIiIgcgAU3ERERERERkQOw4CYiIiIiIiJyABbcRERERERERA7AgpuIiIiIiIjIAVhwExERERERETkAC24iIiIiIiIiB2DBTUREREREROQALLiJiIiIiIiIHIAFNxEREREREZEDsOAmIiIiIiIicgAW3EREREREREQO8C9cM4LL12daSwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#PLOT CLUSTERS PRINTED IN PREVIOUS CELL\n",
    "embedding_array = np.array(embedding_list)  #shape: (num_samples, embedding_dim)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embedding_2d = tsne.fit_transform(embedding_array)\n",
    "\n",
    "colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_id in range(num_clusters):\n",
    "    indices = [i for i, c in enumerate(clusters) if c == cluster_id]\n",
    "    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],\n",
    "                color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Clustering of Syllable Embeddings (t-SNE Projection)\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Jzl6SUAGJitw"
   },
   "outputs": [],
   "source": [
    "#Save embeddings on file with one-hot-encoded mapping\n",
    "syllable_autoencoder.save_embeddings(f\"syllable_embeddings_one-hot-encoded_{EMBEDDINGS_DIM}.npy\", syllables, tokenList=input_tokens)\n",
    "syllable_autoencoder.save_embeddings(f\"syllable_embeddings_{EMBEDDINGS_DIM}.npy\", syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_nZv6xMFPtCf"
   },
   "outputs": [],
   "source": [
    "#Load embeddings for further use\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(f\"syllable_embeddings_one-hot-encoded_{EMBEDDINGS_DIM}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9svVyxHZ8MRi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Frequences and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DrBdFVlW8U_M"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_tf_idf(sequences):\n",
    "    # Step 1: Initialize data structures\n",
    "    eps = 1e-10\n",
    "    words_tf_idf = defaultdict(lambda: {\"tf\": 0, \"df\": 0, \"tf_idf\": eps, \"unigram_freq\": eps})\n",
    "    tokens_tf_idf = defaultdict(lambda: {\"tf\": 0, \"df\": 0, \"tf_idf\": eps, \"unigram_freq\": eps})\n",
    "\n",
    "    total_sequences = len(sequences)  # Total number of sequences for IDF calculation\n",
    "    total_words = 0  # Total word count across all sequences\n",
    "    total_tokens = 0  # Total token count across all sequences\n",
    "\n",
    "    # Step 2: Count TF and DF\n",
    "    for sequence in sequences:\n",
    "        words = sequence.split()\n",
    "        seen_words = set()  # Track words seen in a single sequence\n",
    "        seen_tokens = set()  # Track tokens seen in a single sequence\n",
    "\n",
    "        for w in words:\n",
    "            words_tf_idf[w][\"tf\"] += 1  # Increment term frequency\n",
    "            total_words += 1  # Count total words\n",
    "\n",
    "            if w not in seen_words:\n",
    "                words_tf_idf[w][\"df\"] += 1  # Increment document frequency once per sequence\n",
    "                seen_words.add(w)\n",
    "\n",
    "            # Token-level processing\n",
    "            tokens = w.split(\"-\")\n",
    "            for tok in tokens:\n",
    "                tokens_tf_idf[tok][\"tf\"] += 1  # Increment token frequency\n",
    "                total_tokens += 1  # Count total tokens\n",
    "\n",
    "                if tok not in seen_tokens:\n",
    "                    tokens_tf_idf[tok][\"df\"] += 1  # Increment token document frequency once per sequence\n",
    "                    seen_tokens.add(tok)\n",
    "\n",
    "    # Step 3: Compute TF-IDF and Unigram Frequency\n",
    "    for w in words_tf_idf:\n",
    "        df = words_tf_idf[w][\"df\"]\n",
    "        idf = math.log((total_sequences + 1) / (df + 1))  # Smoothed IDF\n",
    "        words_tf_idf[w][\"tf_idf\"] = words_tf_idf[w][\"tf\"] * idf  # Compute TF-IDF\n",
    "        words_tf_idf[w][\"unigram_freq\"] = words_tf_idf[w][\"tf\"] / total_words  # Compute unigram probability\n",
    "\n",
    "    for tok in tokens_tf_idf:\n",
    "        df = tokens_tf_idf[tok][\"df\"]\n",
    "        idf = math.log((total_sequences + 1) / (df + 1))  # Smoothed IDF\n",
    "        tokens_tf_idf[tok][\"tf_idf\"] = tokens_tf_idf[tok][\"tf\"] * idf  # Compute TF-IDF\n",
    "        tokens_tf_idf[tok][\"unigram_freq\"] = tokens_tf_idf[tok][\"tf\"] / total_tokens  # Compute unigram probability\n",
    "\n",
    "    return words_tf_idf, tokens_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F1UaI-7pCsyF"
   },
   "outputs": [],
   "source": [
    "sequences = train_y + test_y\n",
    "word_results, token_results = compute_tf_idf(sequences)\n",
    "\n",
    "# Print results\n",
    "#print(\"Word TF-IDF Scores:\")\n",
    "#for word, values in word_results.items():\n",
    "#    print(f\"{word}: {values}\")\n",
    "\n",
    "#print(\"\\nToken TF-IDF Scores:\")\n",
    "#for token, values in token_results.items():\n",
    "#    print(f\"{token}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWT6GrV--8Rh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PJHEKgzKuQT7"
   },
   "outputs": [],
   "source": [
    "def inverse_mapping(tokens=output_tokens):\n",
    "    inv_map = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        inv_map[i] = t\n",
    "    return inv_map\n",
    "\n",
    "inv_map = inverse_mapping(output_tokens)\n",
    "inv_map_input = inverse_mapping(input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8MXG-0zCP929"
   },
   "outputs": [],
   "source": [
    "k = 70\n",
    "top_k_frequent = sorted(token_results.keys(), key=lambda x: token_results[x][\"unigram_freq\"], reverse=True)[:k]\n",
    "\n",
    "def one_hot_encode_sequences(batch, vocab_size):\n",
    "    #takes lists of integer tokens and outputs batches of shape: (batch_size, seq_len)\n",
    "    tensor_batch = torch.tensor(batch, dtype=torch.long)\n",
    "    return torch.nn.functional.one_hot(tensor_batch, num_classes=vocab_size).float()\n",
    "\n",
    "\n",
    "# replace symbols with numerical labels to prepare one_hot_encoding\n",
    "def tokenize_sequences(dataset, mapping=input_mapping, use_SOS=True, use_EOS=False, distortion_prob=1, top_k_frequent=top_k_frequent):\n",
    "    assert distortion_prob >= 0 and distortion_prob <= 1, \"Distortion probability must be between 0 and 1\"\n",
    "    result = []\n",
    "    for i, subset in enumerate(dataset):\n",
    "        res = []\n",
    "        for seq in subset:\n",
    "            distorted_sequence = random.random() < distortion_prob\n",
    "            one_hot_encoded = [mapping[\"SOS\"]] if use_SOS else []\n",
    "            seq = seq.split(\" \")\n",
    "            for j, w in enumerate(seq):\n",
    "                if \"-\" in w:\n",
    "                    w = w.split(\"-\")\n",
    "                    for sign in w:\n",
    "                        if distorted_sequence and sign not in top_k_frequent and sign != \"?\":\n",
    "                            one_hot_encoded.append(mapping[\"SYL\"])\n",
    "                        else:\n",
    "                            one_hot_encoded.append(mapping[sign])\n",
    "                elif distorted_sequence and w not in top_k_frequent and w != \"?\":\n",
    "                    if w == \"1\" or w == \"2\" or w == \"NUM\":\n",
    "                        one_hot_encoded.append(mapping[\"NUMERAL\"])\n",
    "                    else:\n",
    "                        one_hot_encoded.append(mapping[\"LOG\"])\n",
    "                else:\n",
    "                    one_hot_encoded.append(mapping[w])\n",
    "\n",
    "                if j != len(seq) - 1:\n",
    "                    one_hot_encoded.append(mapping[\" \"])\n",
    "                elif use_EOS:\n",
    "                    one_hot_encoded.append(mapping[\"EOS\"])\n",
    "            res.append(one_hot_encoded)\n",
    "        result.append(res)\n",
    "    return result\n",
    "\n",
    "\n",
    "#Get labelsfor each sequence to get encoded dataset\n",
    "input_dim = len(input_tokens)\n",
    "output_dim = len(output_tokens)\n",
    "\n",
    "# lazy modification to avoid losing sequences\n",
    "seq_train_x, seq_train_y, seq_test_x, seq_test_y= train_x, train_y, test_x, test_y\n",
    "dataset = [seq_train_x, seq_test_x]\n",
    "# For BRNN: src starts with SOS\n",
    "train_x, test_x = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_X, use_EOS=USE_EOS_IN_X, distortion_prob=0)\n",
    "distorted_train_x, distorted_test_x = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_X, use_EOS=USE_EOS_IN_X, distortion_prob=1)\n",
    "dataset = [seq_train_y, seq_test_y]\n",
    "\n",
    "\n",
    "train_y, test_y = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_Y, use_EOS=USE_EOS_IN_Y, distortion_prob=0)\n",
    "\n",
    "#Compute lengths before padding\n",
    "train_lengths = torch.tensor([len(seq) for seq in train_x])\n",
    "test_lengths = torch.tensor([len(seq) for seq in test_x])\n",
    "\n",
    "#Pad the dataset for the RNN (Transformers need sequences padded to same length, GPT says)\n",
    "train_x = pad_sequences(train_x)\n",
    "train_y = pad_sequences(train_y)\n",
    "test_x = pad_sequences(test_x)\n",
    "test_y = pad_sequences(test_y)\n",
    "distorted_train_x = pad_sequences(distorted_train_x)\n",
    "distorted_test_x = pad_sequences(distorted_test_x)\n",
    "\n",
    "if USE_ONE_HOT_ENCODING:\n",
    "    #Convert to one-hot encoded sequences\n",
    "    train_x = one_hot_encode_sequences(train_x, input_dim)\n",
    "    test_x = one_hot_encode_sequences(test_x, input_dim)\n",
    "    distorted_train_x = one_hot_encode_sequences(distorted_train_x, input_dim)\n",
    "    distorted_test_x = one_hot_encode_sequences(distorted_test_x, input_dim)\n",
    "    train_y = one_hot_encode_sequences(train_y, output_dim)\n",
    "    test_y = one_hot_encode_sequences(test_y, output_dim)\n",
    "\n",
    "p_unigram = np.zeros(len(output_tokens))\n",
    "tf_idf = np.zeros(len(output_tokens))\n",
    "for tok in token_results:\n",
    "    p_unigram[output_mapping[tok]] = token_results[tok][\"unigram_freq\"]\n",
    "    tf_idf[output_mapping[tok]] = token_results[tok][\"tf_idf\"]\n",
    "p_unigram = torch.tensor(p_unigram)\n",
    "tf_idf = torch.tensor(tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PbyMYI9vKaM_"
   },
   "outputs": [],
   "source": [
    "#seq_train_x[1], train_x[1], train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "S8OhIqILttVi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNMMKBIcIZwf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqTNdrIyIfUP"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sentences, split_sequences=False, remove_hyphen=True):\n",
    "    \"\"\"\n",
    "    Process Linear B text, handling numerals/logograms.\n",
    "\n",
    "    :param sentences: List of Linear B sentences (e.g., [\"we-we-si-jo OVISm 3 OVISf\"])\n",
    "    :return: Tokenized and normalized sentences\n",
    "        \"\"\"\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize by spaces (since Linear B syllabograms are typically space-separated)\n",
    "        if split_sequences:\n",
    "            tokens = sentence.split()\n",
    "            if remove_hyphen:\n",
    "                tokens = [token.replace(\"-\", \"\") for token in tokens]\n",
    "            processed_sentences.append(tokens)\n",
    "        else:\n",
    "            if remove_hyphen:\n",
    "                processed_sentences.append(sentence.replace(\"-\", \"\"))\n",
    "            else:\n",
    "                processed_sentences.append(sentence)\n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "sequences = preprocess_text(seq_train_y)\n",
    "test_sequences = preprocess_text(seq_test_y)\n",
    "sequences.extend(test_sequences)\n",
    "\n",
    "#convert the list of sequences into a single string with spaces\n",
    "text_data = \"\\n\".join(sequences)\n",
    "\n",
    "sp_file_name = os.path.join(prefix_path, \"linear_b_spm.model\")\n",
    "sp_input_dim = 500 #vocab size of sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COBurZ8xgPT9"
   },
   "outputs": [],
   "source": [
    "new_tokens = [\"?\", \"?\"]\n",
    "\n",
    "train_spm = True\n",
    "if train_spm:\n",
    "    seq_temp_file = \"temp_linear_b_text.txt\"\n",
    "    # Write the text data to a temporary file\n",
    "    with open(seq_temp_file, \"w\") as f:\n",
    "        f.write(text_data)\n",
    "\n",
    "    # Now train the SentencePiece model using this temporary file\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=seq_temp_file,  # Temporary file\n",
    "        model_prefix=sp_file_name[:-6],         # Output model name\n",
    "        vocab_size=sp_input_dim,   # Adjust based on your dataset size\n",
    "        model_type='unigram',             # Use 'unigram' or 'bpe'\n",
    "        pad_id=0,\n",
    "        unk_id=2,\n",
    "        bos_id=1, # we are forced to initialize this params\n",
    "        eos_id=3, # we are forced to initialize this params\n",
    "        pad_piece='<pad>',\n",
    "        unk_piece='<unk>',\n",
    "        bos_piece='<s>',\n",
    "        eos_piece='</s>',\n",
    "        user_defined_symbols=new_tokens\n",
    "    )\n",
    "\n",
    "    print(\"Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eEVmYWuI_gL"
   },
   "outputs": [],
   "source": [
    "def tokenize_sequences(sequences, sentence_piece_model):\n",
    "    tokenized_sequences = []\n",
    "    for seq in sequences:\n",
    "        tokens = sp.encode(seq)\n",
    "        tokenized_sequences.append(tokens)\n",
    "    return tokenized_sequences\n",
    "\n",
    "def tokenize_spaces(input):\n",
    "    res = []\n",
    "    for tok_list in input:\n",
    "        our_tok_list = [1] # SOS\n",
    "        for i, tok in enumerate(tok_list):\n",
    "            tok_str = sp.id_to_piece(tok)\n",
    "            if tok_str.startswith(\"\") and i != 0:\n",
    "                our_tok_list.append(3) # add spaces with unused token for eos!\n",
    "            our_tok_list.append(tok)\n",
    "        res.append(our_tok_list)\n",
    "    return res\n",
    "#load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=sp_file_name)\n",
    "\n",
    "sequences = preprocess_text(seq_train_x)\n",
    "test_sequences = preprocess_text(seq_test_x)\n",
    "\n",
    "spm_train_x = tokenize_sequences(sequences, sp)\n",
    "spm_test_x = tokenize_sequences(test_sequences, sp)\n",
    "\n",
    "spm_train_x = tokenize_spaces(spm_train_x)\n",
    "spm_test_x = tokenize_spaces(spm_test_x)\n",
    "spm_lengths_train = torch.tensor([len(seq) for seq in spm_train_x])\n",
    "spm_lengths_test = torch.tensor([len(seq) for seq in spm_test_x])\n",
    "spm_train_x = pad_sequences(spm_train_x)\n",
    "spm_test_x = pad_sequences(spm_test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seo1_O3hf7VN"
   },
   "source": [
    "#### Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3hG9lpgZJCn"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "tokens = [tok.item() for tok in spm_train_x[idx] if tok != 0]\n",
    "tokens = [sp.id_to_piece(tok) for tok in tokens]\n",
    "print(sp.encode(\"OVISf\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHUtiVPPZeSo"
   },
   "outputs": [],
   "source": [
    "print(sp.id_to_piece(6))  # Get token corresponding to ID 5\n",
    "print(sp.id_to_piece(1))  # Get token corresponding to ID 0\n",
    "# Decode token IDs back into text\n",
    "decoded_text = sp.decode([286, 7, 4, 31, 4, 6, 1, 160, 10, 162, 38, 7, 9, 21, 7, 4])\n",
    "\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p3GM8JLqaQy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What the hellish graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLc8WQfJzx2X"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define the BRNN Model\n",
    "class BRNNGraphEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNGraphEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-3)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "    def tok_embeddings(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def rnn_embeddings(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhG9FTzZ0fhq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, train_x, train_y, test_x, test_y, num_epochs=10, batch_size=32, lr=0.001, device=device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.NLLLoss(ignore_index=0)  # Ignore padding index\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            lengths = (x_batch != 0).sum(dim=1)  # Compute sequence lengths\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch, lengths)\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                lengths = (x_batch != 0).sum(dim=1)\n",
    "\n",
    "                output = model(x_batch, lengths)\n",
    "                predictions = output.argmax(dim=-1)\n",
    "\n",
    "                mask = y_batch != 0  # Ignore padding\n",
    "                correct += (predictions[mask] == y_batch[mask]).sum().item()\n",
    "                total += mask.sum().item()\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfLjBg-n2mnH"
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, method='t-SNE'):\n",
    "    \"\"\"\n",
    "    Visualizes the embeddings using either PCA or t-SNE, and optionally adds labels from `input_tokens`.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to visualize.\n",
    "        labels: The labels for coloring the embeddings (optional).\n",
    "        method: The dimensionality reduction method ('PCA' or 't-SNE').\n",
    "        input_tokens: A list of token labels (words) to display next to the points.\n",
    "    \"\"\"\n",
    "    # Apply dimensionality reduction (PCA or t-SNE)\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    elif method == 't-SNE':\n",
    "        tsne = TSNE(n_components=2)\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the reduced embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=30)\n",
    "\n",
    "    plt.title(f\"Token Embeddings Visualization ({method})\")\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gupBNQoI16jQ"
   },
   "outputs": [],
   "source": [
    "model = BRNNGraphEmbedding(len(input_tokens)-1, EMBEDDINGS_DIM, 2*EMBEDDINGS_DIM, 8, 0.2)\n",
    "train_model(model, train_x, train_y, test_x, test_y, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYPETcvI4zCg"
   },
   "outputs": [],
   "source": [
    "seq = 'a-e-i-o-u-da-de-di-do-du-ja-je-jo-ka-ke-ki-ko-ku-ma-me-mi-mo-mu-na-ne-ni-no-nu-pa-pe-pi-po-pu-qa-qe-qi-qo-ra-re-ri-ro-ru-sa-se-si-so-su-ta-te-ti-to-tu-wa-we-wi-wo-za-ze-zo-ha-ai-au-dwo-nwa-phu-pte-rya-rai-ryo-tya'\n",
    "seq = (tokenize_sequences([[seq]], use_SOS=False, use_EOS=False))[0][0]\n",
    "length = torch.tensor([len(seq)]).to(device)\n",
    "seq += [0] * (train_x.shape[1] - len(seq))\n",
    "seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "emb = model.tok_embeddings(seq).cpu().detach()\n",
    "print(emb.shape)\n",
    "\n",
    "rnn_emb = model.rnn_embeddings(seq, length).cpu().detach()\n",
    "print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMHqPkbAAVIV"
   },
   "outputs": [],
   "source": [
    "emb = emb.squeeze(0)\n",
    "emb = emb[:length[0].item(), :]\n",
    "emb = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwftR5l_Bu42"
   },
   "outputs": [],
   "source": [
    "rnn_emb = rnn_emb.squeeze(0)\n",
    "rnn_emb = rnn_emb[:length[0].item(), :]\n",
    "rnn_emb = rnn_emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPmE7Rwe_kIf"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RH6b6kmBwia"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(rnn_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeqnih8Oqq8c"
   },
   "outputs": [],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK88rcyNqhxN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define node features (5 nodes, 3 features each)\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Define edge index (4 edges, where each column is a pair of (source, target) nodes)\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3],  # source nodes\n",
    "    [1, 2, 3, 4]   # target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Define edge attributes (optional)\n",
    "edge_attr = torch.tensor([\n",
    "    [0.5],  # weight of edge (0 -> 1)\n",
    "    [0.6],  # weight of edge (1 -> 2)\n",
    "    [0.7],  # weight of edge (2 -> 3)\n",
    "    [0.8]   # weight of edge (3 -> 4)\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Define node labels (optional, for node classification)\n",
    "y = torch.tensor([0, 1, 0, 1, 0], dtype=torch.long)\n",
    "\n",
    "# Create Data object\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMl4id-vGs_A"
   },
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er8DJ-hIxCZg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LmTusRDhJp4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in ./.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from fasttext) (1.23.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.local/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.local/lib/python3.10/site-packages (from fasttext) (80.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DO9WUvlb2JcP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'konoso': [(0.6594921350479126, 'kumonoso'), (0.6537270545959473, 'konosode'), (0.6365762948989868, 'konosija'), (0.5780808329582214, 'konosijo'), (0.5686375498771667, 'kononipi'), (0.5430461764335632, 'nosiro'), (0.49234628677368164, 'kono'), (0.4802187979221344, 'diso'), (0.47665509581565857, 'konija'), (0.47106119990348816, 'koso')]\n",
      "Nearest neighbors for 'potinija': [(0.7025516033172607, 'potinijawe'), (0.6799739003181458, 'potinijaweja'), (0.6756334900856018, 'potinijawijo'), (0.6643137335777283, 'sitopotinija'), (0.6455649137496948, 'upojopotinija'), (0.644006609916687, 'erewijopotinija'), (0.6003291010856628, 'potinijaweijo'), (0.5768953561782837, 'atanapotinija'), (0.5700084567070007, 'poti'), (0.5374864935874939, 'potinajo')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "class LinearBWordEmbedding:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext/\"), f\"linearb_fasttext_model_{EMBEDDINGS_DIM}.bin\"),\n",
    "                 train_data_path=\"linearb_train.txt\",\n",
    "                 model='skipgram',\n",
    "                 dim=EMBEDDINGS_DIM,\n",
    "                 ws=3,\n",
    "                 min_count=1,\n",
    "                 epoch=500,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=4,\n",
    "                 neg=5,\n",
    "                 loss='softmax',\n",
    "                 bucket=2000000,\n",
    "                 thread=4,\n",
    "                 t=0.0001,\n",
    "                 word_ngrams=1,\n",
    "                 verbose=2\n",
    "                ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'model': model,\n",
    "            'dim': dim,\n",
    "            'ws': ws,\n",
    "            'minCount': min_count,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'neg': neg,\n",
    "            'loss': loss,\n",
    "            'bucket': bucket,\n",
    "            'thread': thread,\n",
    "            't': t,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, sentences):\n",
    "        processed = []\n",
    "        uppercase_pattern = re.compile(r'[A-Z]')\n",
    "        digits_only_pattern = re.compile(r'^\\d+$')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            filtered_tokens = []\n",
    "            for token in tokens:\n",
    "                token_no_hyphen = token.replace('-', '')\n",
    "                if uppercase_pattern.search(token_no_hyphen):\n",
    "                    continue\n",
    "                if digits_only_pattern.match(token_no_hyphen):\n",
    "                    continue\n",
    "                filtered_tokens.append(token_no_hyphen)\n",
    "            processed.append(filtered_tokens)\n",
    "        return processed\n",
    "\n",
    "    def _write_train_data(self, sentences):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for tokens in sentences:\n",
    "                f.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "    def train(self, sentences):\n",
    "        tokenized = self.preprocess_text(sentences)\n",
    "        self._write_train_data(tokenized)\n",
    "        self.model = fasttext.train_unsupervised(\n",
    "            input=self.train_data_path,\n",
    "            model=self.params['model'],\n",
    "            dim=self.params['dim'],\n",
    "            ws=self.params['ws'],\n",
    "            minCount=self.params['minCount'],\n",
    "            epoch=self.params['epoch'],\n",
    "            lr=self.params['lr'],\n",
    "            minn=self.params['minn'],\n",
    "            maxn=self.params['maxn'],\n",
    "            neg=self.params['neg'],\n",
    "            loss=self.params['loss'],\n",
    "            bucket=self.params['bucket'],\n",
    "            thread=self.params['thread'],\n",
    "            t=self.params['t'],\n",
    "            wordNgrams=self.params['wordNgrams'],\n",
    "            verbose=self.params['verbose']\n",
    "        )\n",
    "        # Optional: remove training data file after training\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):  #saves to default path 'linearb_fasttext_model.bin'\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):  #loads from default path 'linearb_fasttext_model.bin'\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if self.model:\n",
    "            return self.model.get_word_vector(word)\n",
    "        return None\n",
    "\n",
    "    def find_nearest_neighbors(self, word, k=5):\n",
    "        if self.model:\n",
    "            return self.model.get_nearest_neighbors(word, k)\n",
    "        return None\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "\n",
    "#traininig the model\n",
    "#embedding_model.train(sequences)\n",
    "#embedding_model.save_model()\n",
    "embedding_model.load_model() \n",
    "\n",
    "#get some nearest neighbors\n",
    "neighbors = embedding_model.find_nearest_neighbors(\"konoso\", k=10)\n",
    "print(f\"Nearest neighbors for 'konoso': {neighbors}\")\n",
    "\n",
    "neighbors = embedding_model.find_nearest_neighbors(\"potinija\", k=10) #get words similar to \"mistress\"\n",
    "print(f\"Nearest neighbors for 'potinija': {neighbors}\\n\")\n",
    "\n",
    "\n",
    "def nearest_neighbors_from_vector(model, vector, k=10):\n",
    "    all_words = model.model.get_words()\n",
    "    similarities = []\n",
    "    for w in all_words:\n",
    "        w_vec = model.get_vector(w)\n",
    "        sim = np.dot(vector, w_vec) / (np.linalg.norm(vector) * np.linalg.norm(w_vec))\n",
    "        similarities.append((sim, w))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words nearest to this v5 are:  [(0.65686536, 'wanaka'), (0.6448369, 'pawoko'), (0.6283691, 'dekutuwoko'), (0.6058625, 'tokosowoko'), (0.5709508, 'damoko'), (0.5689939, 'watuoko'), (0.56247073, 'ijerowoko'), (0.55798674, 'wanakate'), (0.55643743, 'kowirowoko'), (0.5383182, 'wanakatera')]\n"
     ]
    }
   ],
   "source": [
    "v2 = embedding_model.get_vector(\"wanaka\")\n",
    "v3 = embedding_model.get_vector(\"woko\")\n",
    "v4 = embedding_model.get_vector(\"qasireu\")\n",
    "v5 = v2 + v3\n",
    "\n",
    "def nearest_neighbors_from_vector(model, vector, k=10):\n",
    "    all_words = model.model.get_words()\n",
    "    similarities = []\n",
    "    for w in all_words:\n",
    "        w_vec = model.get_vector(w)\n",
    "        sim = np.dot(vector, w_vec) / (np.linalg.norm(vector) * np.linalg.norm(w_vec))\n",
    "        similarities.append((sim, w))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "print(\"\\n Words nearest to this v5 are: \",nearest_neighbors_from_vector(embedding_model, v5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearB2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx=0, max_seq_length=456):\n",
    "        super(LinearB2Vec, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.encoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # decoder\n",
    "\n",
    "        self.fc_enc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.encoder(packed)\n",
    "        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        latent = self.fc_enc(hidden_cat)\n",
    "\n",
    "        # decoder\n",
    "        decoder_init = latent.unsqueeze(0)\n",
    "        dec_out, _ = self.decoder(embedded, decoder_init)\n",
    "        logits = self.fc_out(dec_out)\n",
    "\n",
    "        # pad logits\n",
    "        seq_len = logits.size(1)\n",
    "        if seq_len < self.max_seq_length:\n",
    "            pad_size = self.max_seq_length - seq_len\n",
    "            padding = torch.zeros(logits.size(0), pad_size, logits.size(2), device=logits.device)\n",
    "            logits = torch.cat([logits, padding], dim=1)\n",
    "        else:\n",
    "            logits = logits[:, :self.max_seq_length, :]\n",
    "\n",
    "        return logits, latent\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.encoder(packed)\n",
    "        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        latent = self.fc_enc(hidden_cat)\n",
    "        return latent\n",
    "\n",
    "    def top_k_similar(self, query_embedding, all_embeddings, k, emb2word, metric='cosine'):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        query_embedding = query_embedding.to(device)\n",
    "        all_embeddings = all_embeddings.to(device)\n",
    "\n",
    "        if metric == 'cosine':\n",
    "            query_norm = query_embedding / query_embedding.norm(p=2)\n",
    "            all_norm = all_embeddings / all_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "            similarities = torch.matmul(all_norm, query_norm.t()).squeeze(1)\n",
    "        elif metric == 'euclidean':\n",
    "            diff = all_embeddings - query_embedding.unsqueeze(0)\n",
    "            similarities = -torch.norm(diff, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "        topk_scores, topk_indices = torch.topk(similarities, k)\n",
    "        topk_indices = topk_indices.cpu()\n",
    "        topk_words = [emb2word[i.item()] for i in topk_indices]\n",
    "\n",
    "        return topk_words, topk_scores.tolist()\n",
    "\n",
    "    def get_all_embeddings(self, word_list, max_seq_length):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        all_embeddings = []\n",
    "        mapping = {}\n",
    "\n",
    "        for i, word in enumerate(word_list):\n",
    "            tokenized = fromWord2Vec(word, max_seq_length).to(device)\n",
    "            length = (tokenized != 0).sum().item()\n",
    "            tokenized = tokenized.unsqueeze(0)\n",
    "            lengths = torch.tensor([length], dtype=torch.long)\n",
    "            embedding = self.encode(tokenized, lengths).squeeze(0).cpu()\n",
    "            all_embeddings.append(embedding)\n",
    "            mapping[i] = word\n",
    "\n",
    "        return torch.stack(all_embeddings, dim=0), mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j0dVzaywr_2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a-ko-ra-jo', 'ma-ra', 'au-to', 'pi-ri-ja-o', 'o-du-we', 'po-ki-ro-nu-ka', 'tu-we-ta', 'sa-zo', 'da-i-wo-wo', 'we-je-ke-ha']\n"
     ]
    }
   ],
   "source": [
    "#Prepare data for LB2VEC\n",
    "def tokenize_sequences_LB2VEC(sequences, mapping=input_mapping, use_SOS=False, use_EOS=False):\n",
    "\n",
    "    tokenized_dataset = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        tokens = []\n",
    "        if use_SOS:\n",
    "            tokens.append(mapping[\"SOS\"])\n",
    "\n",
    "        words = seq.split(\" \")\n",
    "        for i, word in enumerate(words):\n",
    "            if \"-\" in word:\n",
    "                sub_words = word.split(\"-\")\n",
    "                for sub in sub_words:\n",
    "                    if sub not in mapping:\n",
    "                        raise KeyError(f\"Token '{sub}' not found in mapping!\")\n",
    "                    tokens.append(mapping[sub])\n",
    "            else:\n",
    "                if word not in mapping:\n",
    "                    raise KeyError(f\"Token '{word}' not found in mapping!\")\n",
    "                tokens.append(mapping[word])\n",
    "\n",
    "            # Add a space token between words (if not the last word)\n",
    "            if i != len(words) - 1:\n",
    "                tokens.append(mapping[\" \"])\n",
    "\n",
    "        if use_EOS:\n",
    "            tokens.append(mapping[\"EOS\"])\n",
    "\n",
    "        # Convert the list of token IDs to a PyTorch tensor.\n",
    "        tokenized_dataset.append(torch.tensor(tokens, dtype=torch.long))\n",
    "\n",
    "    return tokenized_dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences  # This is your list of tokenized tensors.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Custom collate_fn that pads sequences in a batch.\n",
    "def collate_fn_LB2VEC(max_seq_length, pad_token=0):\n",
    "    def collate_fn(batch):\n",
    "        padded_batch = []\n",
    "        lengths = []\n",
    "        for seq in batch:\n",
    "            lengths.append(seq.size(0))\n",
    "            # If the sequence is shorter than max_seq_length, pad it on the right.\n",
    "            pad_size = max_seq_length - seq.size(0)\n",
    "            if pad_size > 0:\n",
    "                padding = torch.full((pad_size,), pad_token, dtype=seq.dtype)\n",
    "                padded_seq = torch.cat([seq, padding], dim=0)\n",
    "            else:\n",
    "                padded_seq = seq\n",
    "            padded_batch.append(padded_seq)\n",
    "            lengths_t = torch.tensor(lengths)\n",
    "        return torch.stack(padded_batch, dim=0), lengths_t\n",
    "    return collate_fn\n",
    "\n",
    "def fromWord2Vec(word, max_seq_length):\n",
    "    res = tokenize_sequences_LB2VEC([word])\n",
    "    sequence = res[0]  # This is a tensor representing the tokenized word.\n",
    "\n",
    "    seq_len = sequence.size(0)\n",
    "    if seq_len < max_seq_length:\n",
    "        pad_size = max_seq_length - seq_len\n",
    "        padding = torch.zeros(pad_size, dtype=sequence.dtype)\n",
    "        sequence = torch.cat([sequence, padding], dim=0)\n",
    "    elif seq_len > max_seq_length:\n",
    "        sequence = sequence[:max_seq_length]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def getAllWords(sequences):\n",
    "    words = set()\n",
    "    for seq in sequences:\n",
    "        seq = seq.split(\" \")\n",
    "        for w in seq:\n",
    "            words.add(w)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "tokenized = tokenize_sequences_LB2VEC(sequences)\n",
    "max_seq_length = max(seq.size(0) for seq in tokenized)\n",
    "dataset = SequenceDataset(tokenized)\n",
    "collate_fn = collate_fn_LB2VEC(max_seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "words = getAllWords(sequences)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xxnMF-STTvHI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5441\n",
      "Epoch 2/10, Loss: 0.0320\n",
      "Epoch 3/10, Loss: 0.0139\n",
      "Epoch 4/10, Loss: 0.0061\n",
      "Epoch 5/10, Loss: 0.0032\n",
      "Epoch 6/10, Loss: 0.0017\n",
      "Epoch 7/10, Loss: 0.0009\n",
      "Epoch 8/10, Loss: 0.0006\n",
      "Epoch 9/10, Loss: 0.0004\n",
      "Epoch 10/10, Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "LB2vec_model = LinearB2Vec(len(input_tokens), EMBEDDINGS_DIM, EMBEDDINGS_DIM)\n",
    "optimizer = optim.Adam(LB2vec_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LB2vec_model.to(device)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch, lengths in data_loader:\n",
    "        # Move data to the GPU\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = LB2vec_model(batch, lengths)\n",
    "        logits_flat = logits.view(-1, len(input_tokens))\n",
    "        targets_flat = batch.view(-1)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po-ti-ni-ja 1.0000001192092896\n",
      "po-ti-ni-ja-we-ja 0.9680911302566528\n",
      "po-si-da-e-ja 0.9603016376495361\n",
      "po-so-re-ja 0.9535226225852966\n",
      "po-ni-ke-ja 0.9515949487686157\n",
      "po-pu-re-ja 0.9459639191627502\n",
      "po-re-no-zo-te-ri-ja 0.9432565569877625\n",
      "po-ro-e-ke-te-ri-ja 0.9404106140136719\n",
      "po-ni-ki-ja 0.9383034706115723\n",
      "po-qe-wi-ja 0.9265150427818298\n",
      "po-ni-ja-ja 0.916567862033844\n",
      "ko-no-so 0.9999999403953552\n",
      "ko-me-no 0.9324777126312256\n",
      "ko-to-na-no-no 0.930557131767273\n",
      "ko-to-no 0.9266769289970398\n",
      "ko-ro-tu-no 0.9254539608955383\n",
      "ko-i-no 0.9241381287574768\n",
      "ko-tu-ryo 0.9233684539794922\n",
      "ko-no-si-jo 0.9208793640136719\n",
      "ko-no-so-de 0.9206832051277161\n",
      "ko-to-i-na 0.9180614948272705\n",
      "ko-ri-jo 0.9174559116363525\n"
     ]
    }
   ],
   "source": [
    "# Use embeddings\n",
    "all_embeddings, emb2word = LB2vec_model.get_all_embeddings(words, max_seq_length)\n",
    "\n",
    "query = \"po-ti-ni-ja\"\n",
    "query_seq = fromWord2Vec(query, max_seq_length).unsqueeze(0).to(device)  #vectorize the word through tokenization\n",
    "query_len = torch.tensor([(query_seq != 0).sum().item()], dtype=torch.long).to(device)\n",
    "query_embedding = LB2vec_model.encode(query_seq, query_len) \n",
    "neighbors, scores = LB2vec_model.top_k_similar(query_embedding, all_embeddings, k=11, emb2word=emb2word, metric=\"cosine\")\n",
    "for n, s in zip(neighbors, scores):\n",
    "    print(n,s)\n",
    "\n",
    "query = \"ko-no-so\"\n",
    "query_seq = fromWord2Vec(query, max_seq_length).unsqueeze(0).to(device)  #vectorize the word through tokenization\n",
    "query_len = torch.tensor([(query_seq != 0).sum().item()], dtype=torch.long).to(device)\n",
    "query_embedding = LB2vec_model.encode(query_seq, query_len)\n",
    "neighbors, scores = LB2vec_model.top_k_similar(query_embedding, all_embeddings, k=11, emb2word=emb2word, metric=\"cosine\")\n",
    "for n, s in zip(neighbors, scores):\n",
    "    print(n,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPt6iYZyjy6I"
   },
   "source": [
    "#### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ddRqPnXNLbxC"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LinearB2Vec.get_all_embeddings() missing 1 required positional argument: 'max_seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TEST4LB2VEC\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_embeddings, emb2word \u001b[38;5;241m=\u001b[39m \u001b[43mLB2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m456\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m aminisijo \u001b[38;5;241m=\u001b[39m fromWord2Vec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-mi-ni-si-jo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m456\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m aminisija \u001b[38;5;241m=\u001b[39m fromWord2Vec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-mi-ni-si-ja\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m456\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: LinearB2Vec.get_all_embeddings() missing 1 required positional argument: 'max_seq_length'"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='linear_b.model')\n",
    "#TEST4LB2VEC\n",
    "all_embeddings, emb2word = LB2vec_model.get_all_embeddings(words, 456)\n",
    "aminisijo = fromWord2Vec(\"a-mi-ni-si-jo\", 456).unsqueeze(0)\n",
    "aminisija = fromWord2Vec(\"a-mi-ni-si-ja\", 456).unsqueeze(0)\n",
    "korisijo = fromWord2Vec(\"ko-ri-si-jo\", 456).unsqueeze(0)\n",
    "aminisijo = aminisijo.to(device)\n",
    "aminisija = aminisija.to(device)\n",
    "korisijo = korisijo.to(device)\n",
    "length = torch.tensor([aminisijo.size(0)])\n",
    "\n",
    "male_name = LB2vec_model.encode(aminisijo, length)\n",
    "female_name = LB2vec_model.encode(aminisija, length)\n",
    "king = LB2vec_model.encode(korisijo, length)\n",
    "\n",
    "hopefully_queen = male_name + king - female_name\n",
    "print(LB2vec_model.top_k_similar(king, all_embeddings, 150, emb2word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F3I_cwsVTTl"
   },
   "outputs": [],
   "source": [
    "embedding_model.find_nearest_neighbors(\"a-ta-ra-?-jo\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N82ZkEb51yFu"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"linear_b.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear_b.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m aminisijo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(embedding_model\u001b[38;5;241m.\u001b[39mget_vector(sp\u001b[38;5;241m.\u001b[39mencode_as_pieces(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maminisijo\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      4\u001b[0m aminisija \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(embedding_model\u001b[38;5;241m.\u001b[39mget_vector(sp\u001b[38;5;241m.\u001b[39mencode_as_pieces(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maminisija\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:468\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Init\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_threads \u001b[38;5;241m=\u001b[39m num_threads\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mor\u001b[39;00m model_proto:\n\u001b[0;32m--> 468\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_proto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"linear_b.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "aminisijo = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"aminisijo\")))\n",
    "aminisija = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"aminisija\")))\n",
    "korisijo = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"korisijo\")))\n",
    "\n",
    "print(korisijo, aminisijo.shape, aminisija.shape)\n",
    "\n",
    "hopefully_korisija = korisijo + aminisijo - aminisija\n",
    "embedding_model.find_nearest_neighbors_to_embedding(hopefully_queen, top_n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXZRUM3G8lpn"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_model.check_meaningful_embeddings(\"_korisijo\", \"_aminisijo\", \"_aminisija\", top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuzMK1v6GCCN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def analyze_embedding_clusters(embedding_model, num_clusters=5, method=\"pca\"):\n",
    "    \"\"\"\n",
    "    Cluster and visualize Linear B word embeddings.\n",
    "\n",
    "    :param embedding_model: Trained Word2Vec/FastText model\n",
    "    :param num_clusters: Number of clusters for K-Means\n",
    "    :param method: \"pca\" or \"tsne\" for dimensionality reduction\n",
    "    \"\"\"\n",
    "    # Extract words and corresponding vectors\n",
    "    words = list(embedding_model.model.wv.index_to_key)\n",
    "    vectors = np.array([embedding_model.get_vector(word) for word in words if embedding_model.get_vector(word) is not None])\n",
    "\n",
    "    # Reduce dimensions (PCA or t-SNE)\n",
    "    if method == \"pca\":\n",
    "        reduced_vectors = PCA(n_components=2).fit_transform(vectors)\n",
    "    elif method == \"tsne\":\n",
    "        reduced_vectors = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(vectors)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Use 'pca' or 'tsne'.\")\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(vectors)\n",
    "\n",
    "    # Use a color map with as many colors as clusters\n",
    "    colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        indices = [i for i, c in enumerate(labels) if c == cluster_id]\n",
    "        plt.scatter(reduced_vectors[indices, 0], reduced_vectors[indices, 1],\n",
    "                    color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "\n",
    "    # Move the legend outside of the plot and remove the colorbar\n",
    "    plt.legend(title=\"Clusters\", bbox_to_anchor=(1.05, 0.5), loc='center left', fontsize=9)\n",
    "\n",
    "    # Annotate some points (adjust the number of labels)\n",
    "    for i, word in enumerate(words):\n",
    "        #if i % 10 == 0:  # Annotate every 10th word\n",
    "            plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=9)\n",
    "\n",
    "    # Add title and show plot\n",
    "    plt.title(f\"Word Embedding Clusters using {method.upper()}\")\n",
    "    plt.tight_layout()  # Adjust layout to accommodate the legend\n",
    "    plt.show()\n",
    "\n",
    "    # Print the words in each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        indices = [i for i, c in enumerate(labels) if c == cluster_id]\n",
    "        cluster_words = [words[i] for i in indices]\n",
    "        print(cluster_words)\n",
    "\n",
    "# Run the clustering visualization with desired number of clusters\n",
    "analyze_embedding_clusters(embedding_model, num_clusters=15, method=\"tsne\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12nNn5MGQkXY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IEfz2pEhMovx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SqueezeExcitationFusion(nn.Module):\n",
    "    def __init__(self, d_model, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)  #avg pooling (batch, d_model, 1)\n",
    "\n",
    "        #connected layers to model interactions\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // reduction_ratio),  # Reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // reduction_ratio, d_model),  # Restore dimensionality\n",
    "            nn.Sigmoid()  # Normalize importance scores between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, token_emb, word_emb, phon_emb):\n",
    "        #Concatenate embeddings along the feature dimension\n",
    "        fusion_emb = token_emb + phon_emb #(batch, seq, d_model)\n",
    "        fusion_emb = fusion_emb + word_emb if USE_WORD_EMB else fusion_emb\n",
    "\n",
    "        #Squeeze: Compute global context (reduce across sequence dimension)\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1))  # (batch, d_model, 1)\n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  # Flatten to (batch, d_model)\n",
    "\n",
    "        #Excitation: Generate importance weights\n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "\n",
    "        #Scale embeddings by learned importance scores\n",
    "        fused_emb = fusion_emb * attention_weights  # Adaptive weighting\n",
    "\n",
    "        return fused_emb\n",
    "\n",
    "class TFIDF_SequeezeExcitationFusion(SqueezeExcitationFusion): #change forward function to use tf-idf and unigrams\n",
    "    def forward(self, token_emb, tfidf, unigram):\n",
    "        fusion_emb = token_emb + tfidf + unigram\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1)) \n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  \n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  \n",
    "        fused_emb = fusion_emb * attention_weights  \n",
    "        return fused_emb\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BRNN Allowing also for one-hot encoding\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.use_one_hot = USE_ONE_HOT_ENCODING\n",
    "\n",
    "        input_size = vocab_size if self.use_one_hot else embed_size\n",
    "\n",
    "        if not self.use_one_hot:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size - 4)  # output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        if not self.use_one_hot:\n",
    "            x = self.embedding(x)  # Shape: (batch, seq_len, embed_size)\n",
    "        # else: x is already one-hot: shape (batch, seq_len, vocab_size)\n",
    "\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Uzb5zvgof-3Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define the BRNN Model\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-4)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "FRFNCY0tvTmx"
   },
   "outputs": [],
   "source": [
    "#SE FUSION\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-4)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "        packed_embedded = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "-Vq-GTgnOxVq"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model: Hidden dimension of the embeddings.\n",
    "            max_seq_len: Maximum sequence length supported.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a matrix (max_seq_len, d_model) with sinusoidal values\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)  # Shape: (max_seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Compute sinusoidal values\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
    "\n",
    "        # Register buffer so it's not a model parameter (no gradient updates needed)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # Shape: (1, max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_len=100, eos_token=2):\n",
    "        super(TransformerTextInfiller, self).__init__()\n",
    "        self.eos_token = eos_token  # EOS token value\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important! Now input shape is (batch, seq, features)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size - 4)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, tgt_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        - src: The input sequence with missing tokens (batch, seq_len).\n",
    "        - tgt: The sequence to decode (batch, seq_len).\n",
    "        - src_lengths: Lengths of the source sequences (for padding mask).\n",
    "        - tgt_lengths: Lengths of the target sequences (for padding mask).\n",
    "        \"\"\"\n",
    "\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        tgt_key_padding_mask = self.create_padding_mask(tgt)\n",
    "        tgt_mask = self.create_causal_mask(tgt, self.transformer.nhead)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_embedded,\n",
    "            tgt_embedded,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True\n",
    "        )\n",
    "\n",
    "        return self.softmax(self.fc(out))\n",
    "\n",
    "    def generate(self, src, src_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Autoregressive generation for inference.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence, shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence, shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        src_key_padding_mask = self.create_padding_mask(src)\n",
    "        tgt_key_padding_mask = self.tgt_key_padding_mask_from_src(src_key_padding_mask)\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        pos_indices = torch.arange(seq_length + 1, device=src.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=src.device)  # [SOS]\n",
    "        temp = torch.cat([src.clone(), torch.zeros((src.shape[0], 1), dtype=torch.int, device=src.device)], dim=1)\n",
    "        tgt_mask = self.create_causal_mask(temp)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        memory = self.transformer.encoder(src_embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "        result = []\n",
    "\n",
    "        for i in range(seq_length + 1):  # Generate up to input length\n",
    "            tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "            tgt_mask_cur = tgt_mask[:, :i + 1, :i + 1]  # Shape: [batch_size*num_heads, i+1, i+1]\n",
    "            tgt_key_padding_mask_cur = tgt_key_padding_mask[:, :i + 1]  # Shape: [batch_size, i+1]\n",
    "\n",
    "            out = self.transformer.decoder(\n",
    "                tgt_embedded,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask_cur,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask_cur,\n",
    "                tgt_is_causal=True\n",
    "            )\n",
    "\n",
    "            out = self.softmax(self.fc(out))[:, -1, :]\n",
    "            result.append(out)\n",
    "            next_token = out.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        result = torch.stack(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    # masks padding\n",
    "    def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "        \"\"\"\n",
    "        Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "            padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                          True = \"ignore this token\", False = \"keep it\".\n",
    "        \"\"\"\n",
    "        if unknown_token is None and mask_unknown:\n",
    "            unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "        return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "    # causal mask\n",
    "    def create_causal_mask(self, tgt, num_heads=1):\n",
    "        \"\"\"\n",
    "        Create a causal mask (upper triangular) for the target sequence.\n",
    "        - seq_len: The length of the target sequence.\n",
    "        - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "        - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tgt.shape\n",
    "        # Create the upper triangular mask (causal mask)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "        # Expand the mask for batch size and number of heads\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "        mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "        mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "        return mask.bool()\n",
    "\n",
    "    def tgt_key_padding_mask_from_src(self, src_key_padding_mask):\n",
    "        # Step 1: Find first True and change it to False\n",
    "        modified_mask = src_key_padding_mask.clone()\n",
    "        for i in range(modified_mask.shape[0]):\n",
    "            first_true = (modified_mask[i] == True).nonzero(as_tuple=True)[0]\n",
    "            if first_true.numel() > 0:  # If there is at least one True\n",
    "                modified_mask[i, first_true[0]] = False\n",
    "\n",
    "        # Step 2: Append False at the end of each sequence\n",
    "        modified_mask = torch.cat([modified_mask, torch.full((modified_mask.shape[0], 1), True, dtype=torch.bool, device=modified_mask.device)], dim=1)\n",
    "        return modified_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ilGHn-oZOAlR"
   },
   "outputs": [],
   "source": [
    "# SQUEEZE EXCITATION FUSION\n",
    "class TransformerTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_len=100, eos_token=2):\n",
    "        super(TransformerTextInfiller, self).__init__()\n",
    "        self.eos_token = eos_token  # EOS token value\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.se_fusion = SqueezeExcitationFusion(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important! Now input shape is (batch, seq, features)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size - 1)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, tgt_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        - src: The input sequence with missing tokens (batch, seq_len).\n",
    "        - tgt: The sequence to decode (batch, seq_len).\n",
    "        - src_lengths: Lengths of the source sequences (for padding mask).\n",
    "        - tgt_lengths: Lengths of the target sequences (for padding mask).\n",
    "        \"\"\"\n",
    "\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        tgt_key_padding_mask = self.create_padding_mask(tgt)\n",
    "        tgt_mask = self.create_causal_mask(tgt, self.transformer.nhead)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "        src_embedded = self.se_fusion(src_embedded, word_emb, phon_emb)\n",
    "        tgt_word_emb = torch.cat([torch.zeros((word_emb.size(0), 1, word_emb.size(2)), device=word_emb.device), word_emb], dim=1)\n",
    "        tgt_phon_emb = torch.cat([torch.zeros((phon_emb.size(0), 1, phon_emb.size(2)), device=phon_emb.device), phon_emb], dim=1)\n",
    "        tgt_embedded = self.se_fusion(tgt_embedded, tgt_word_emb, tgt_phon_emb)\n",
    "\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_embedded,\n",
    "            tgt_embedded,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True\n",
    "        )\n",
    "\n",
    "        return self.softmax(self.fc(out))\n",
    "\n",
    "    def generate(self, src, src_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Autoregressive generation for inference.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence, shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence, shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        src_key_padding_mask = self.create_padding_mask(src)\n",
    "        tgt_key_padding_mask = self.tgt_key_padding_mask_from_src(src_key_padding_mask)\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        pos_indices = torch.arange(seq_length + 1, device=src.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=src.device)  # [SOS]\n",
    "        temp = torch.cat([src.clone(), torch.zeros((src.shape[0], 1), dtype=torch.int, device=src.device)], dim=1)\n",
    "        tgt_mask = self.create_causal_mask(temp)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        memory = self.transformer.encoder(src_embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "        result = []\n",
    "\n",
    "        src_embedded = self.se_fusion(src_embedded, word_emb, phon_emb)\n",
    "        tgt_word_emb = torch.cat([torch.zeros((word_emb.size(0), 1, word_emb.size(2)), device=word_emb.device), word_emb], dim=1)\n",
    "        tgt_phon_emb = torch.cat([torch.zeros((phon_emb.size(0), 1, phon_emb.size(2)), device=phon_emb.device), phon_emb], dim=1)\n",
    "\n",
    "\n",
    "        for i in range(seq_length + 1):  # Generate up to input length\n",
    "            tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "            tgt_word_emb_cur = tgt_word_emb[:, :i+1, :]\n",
    "            tgt_phon_emb_cur = tgt_phon_emb[:, :i+1, :]\n",
    "            tgt_embedded = self.se_fusion(tgt_embedded, tgt_word_emb_cur, tgt_phon_emb_cur)\n",
    "\n",
    "            tgt_mask_cur = tgt_mask[:, :i + 1, :i + 1]  # Shape: [batch_size*num_heads, i+1, i+1]\n",
    "            tgt_key_padding_mask_cur = tgt_key_padding_mask[:, :i + 1]  # Shape: [batch_size, i+1]\n",
    "\n",
    "            out = self.transformer.decoder(\n",
    "                tgt_embedded,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask_cur,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask_cur,\n",
    "                tgt_is_causal=True\n",
    "            )\n",
    "\n",
    "            out = self.softmax(self.fc(out))[:, -1, :]\n",
    "            result.append(out)\n",
    "            next_token = out.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        result = torch.stack(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    # masks padding\n",
    "    def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "        \"\"\"\n",
    "        Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "            padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                          True = \"ignore this token\", False = \"keep it\".\n",
    "        \"\"\"\n",
    "        if unknown_token is None and mask_unknown:\n",
    "            unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "        return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "    # causal mask\n",
    "    def create_causal_mask(self, tgt, num_heads=1):\n",
    "        \"\"\"\n",
    "        Create a causal mask (upper triangular) for the target sequence.\n",
    "        - seq_len: The length of the target sequence.\n",
    "        - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "        - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tgt.shape\n",
    "        # Create the upper triangular mask (causal mask)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "        # Expand the mask for batch size and number of heads\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "        mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "        mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "        return mask.bool()\n",
    "\n",
    "    def tgt_key_padding_mask_from_src(self, src_key_padding_mask):\n",
    "        # Step 1: Find first True and change it to False\n",
    "        modified_mask = src_key_padding_mask.clone()\n",
    "        for i in range(modified_mask.shape[0]):\n",
    "            first_true = (modified_mask[i] == True).nonzero(as_tuple=True)[0]\n",
    "            if first_true.numel() > 0:  # If there is at least one True\n",
    "                modified_mask[i, first_true[0]] = False\n",
    "\n",
    "        # Step 2: Append False at the end of each sequence\n",
    "        modified_mask = torch.cat([modified_mask, torch.full((modified_mask.shape[0], 1), True, dtype=torch.bool, device=modified_mask.device)], dim=1)\n",
    "        return modified_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTRI TEXTINFILLER \n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-1)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "        packed_embedded = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFEKd7wnGIGp"
   },
   "source": [
    "#### MASK DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "pQwFuwJSaZd4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#POSSIBILI INCUSIONI NEL CODICE: formalizzato un po' meglio il discorso delle maschere\n",
    "def compute_src_mask(src, unknown_token=None):\n",
    "    \"\"\"\n",
    "    Compute a boolean mask for the source sequence.\n",
    "    Masks positions where the token equals the unknown token (default: max value in src).\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): The input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be masked. Defaults to max value in src.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    if unknown_token is None:\n",
    "        unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "    return src == unknown_token  # True where src equals unknown token\n",
    "\n",
    "# masks unknown token\n",
    "def create_attention_mask(src, unknown_token=None):\n",
    "    \"\"\"\n",
    "    Create a seq_len x seq_len mask for attention while ignoring unknown tokens.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be ignored.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len, seq_len).\n",
    "                      True means \"ignore\", False means \"attend\".\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = src.shape\n",
    "\n",
    "    # Compute unknown token mask\n",
    "    src_mask = compute_src_mask(src, unknown_token)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Create base attention mask (1s in upper triangle to mask future tokens in causal models)\n",
    "    attention_mask = torch.ones((seq_len, seq_len), dtype=torch.bool).triu(1)\n",
    "\n",
    "    # Expand unknown token mask to (batch_size, seq_len, seq_len)\n",
    "    src_mask = src_mask.unsqueeze(1).expand(-1, seq_len, -1)  # Broadcast across seq_len\n",
    "\n",
    "    # Combine masks: ignore future tokens and unknown tokens\n",
    "    final_mask = attention_mask.unsqueeze(0).expand(batch_size, -1, -1) | src_mask\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "# masks padding\n",
    "def create_padding_mask(sequence, pad_token=0):\n",
    "    \"\"\"\n",
    "    Create a padding mask for the given sequence and lengths.\n",
    "    - sequence: The sequence of input tokens (batch_size, seq_len).\n",
    "    - lengths: The lengths of the sequences (batch_size).\n",
    "    \"\"\"\n",
    "    mask = sequence == pad_token\n",
    "    return mask\n",
    "\n",
    "# causal mask\n",
    "def create_causal_mask(tgt, num_heads=1):\n",
    "    \"\"\"\n",
    "    Create a causal mask (upper triangular) for the target sequence.\n",
    "    - seq_len: The length of the target sequence.\n",
    "    - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "    - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = tgt.shape\n",
    "    # Create the upper triangular mask (causal mask)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "    # Expand the mask for batch size and number of heads\n",
    "    mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "    mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "    mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "    return mask.bool()\n",
    "import torch\n",
    "\n",
    "def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "    \"\"\"\n",
    "    Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "        padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                      True = \"ignore this token\", False = \"keep it\".\n",
    "    \"\"\"\n",
    "    if unknown_token is None and mask_unknown:\n",
    "        unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "    return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "# Example input\n",
    "src = torch.tensor([\n",
    "    [1, 2, 99, 0, 0],  # 99 and 0 should be ignored\n",
    "    [4, 99, 5, 6, 0]   # 99 and 0 should be ignored\n",
    "])\n",
    "\n",
    "# Example usage\n",
    "src = torch.tensor([[1, 2, 99, 0, 0], [4, 99, 5, 6, 0]])  # Assume 99 is the unknown token\n",
    "#mask = create_src_key_padding_mask(src, mask_unknown=True)\n",
    "#print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiDssS3AtLjc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "TTeJqPwwuY-E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A single fully connected block with activation, dropout, and optional normalization.\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, normalize):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = normalize\n",
    "        if self.normalize:\n",
    "            self.layer_norm = nn.LayerNorm(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class FCNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, normalize=False):\n",
    "        super(FCNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(FCBlock(embed_size, hidden_size, dropout, normalize))  # First layer\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.fc_layers.append(FCBlock(hidden_size, hidden_size, dropout, normalize))  # Hidden layers\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size - 4)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        out = embedded\n",
    "        for layer in self.fc_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.output_layer(out)  # (batch_size, seq_len, vocab_size - 1)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8f-1ZUN5wH4d"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv1dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv1dLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x))\n",
    "\n",
    "\n",
    "class ConvTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, num_layers, dropout, normalize=False):\n",
    "        super(ConvTextInfiller, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers: each kernel size gets its own Conv1D layer\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = [FCBlock(num_filters * len(kernel_sizes), num_filters * len(kernel_sizes), dropout, normalize) for _ in range(num_layers-1)]\n",
    "        self.fc_layers.append(FCBlock(num_filters * len(kernel_sizes), num_filters, dropout, normalize))\n",
    "        self.fc_layers = nn.ModuleList(self.fc_layers)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters, vocab_size - 4)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Fully connected layers with normalization\n",
    "        for fc in self.fc_layers:\n",
    "            conv_out = fc(conv_out)\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(conv_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0fvN_Fv-TpZO"
   },
   "outputs": [],
   "source": [
    "class ConvTextInfillerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, hidden_size, num_layers, dropout):\n",
    "        super(ConvTextInfillerRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # RNN Layer (replaces FC layers)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=num_filters * len(kernel_sizes),\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(hidden_size * 2, vocab_size - 4)  # *2 for bidirectional\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Pack the sequence to ignore padding in RNN\n",
    "        packed_input = pack_padded_sequence(conv_out, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(rnn_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xKQ3W9oJqTzy"
   },
   "outputs": [],
   "source": [
    "class ConvTextInfillerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, hidden_size, num_layers, dropout):\n",
    "        super(ConvTextInfillerRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers (after RNN)\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(hidden_size * 2, num_filters, k) for k in kernel_sizes])  # Hidden size * 2 for bidirectional RNN\n",
    "\n",
    "        # RNN Layer (before convolutional layers)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters * len(kernel_sizes), vocab_size - 4)  # *2 for bidirectional\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "\n",
    "        # Pack the sequence to ignore padding in RNN\n",
    "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        # Now apply convolutions after RNN\n",
    "        rnn_out = rnn_out.permute(0, 2, 1)  # (B, H, L) -> (B, H, L) for Conv1D (H=hidden size)\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(rnn_out) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(conv_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiConvTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, kernel_sizes=[1, 3, 5, 7], dropout=0.3):\n",
    "        super(BiConvTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        #multiple forward and backward conv layers (one per kernel size)\n",
    "        self.conv_fwd = nn.ModuleList([\n",
    "            nn.Conv1d(embed_size, hidden_size, k, padding=k // 2) for k in kernel_sizes\n",
    "        ])\n",
    "        self.conv_bwd = nn.ModuleList([\n",
    "            nn.Conv1d(embed_size, hidden_size, k, padding=k // 2) for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        total_hidden = hidden_size * len(kernel_sizes) * 2  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(total_hidden, vocab_size - 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)  # (B, E, L)\n",
    "\n",
    "        # Forward convolutions\n",
    "        out_fwd = [F.relu(conv(embedded)) for conv in self.conv_fwd]\n",
    "\n",
    "        # Backward convolutions (flip input)\n",
    "        reversed_emb = torch.flip(embedded, dims=[2])\n",
    "        out_bwd = [F.relu(conv(reversed_emb)) for conv in self.conv_bwd]\n",
    "        out_bwd = [torch.flip(o, dims=[2]) for o in out_bwd]\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        conv_outputs = out_fwd + out_bwd  # list of tensors: (B, H, L)\n",
    "        out = torch.cat(conv_outputs, dim=1)  # (B, H * 2 * K, L)\n",
    "\n",
    "        # Project back to vocabulary\n",
    "        out = self.dropout(out.permute(0, 2, 1))  # (B, L, H')\n",
    "        return self.softmax(self.fc(out))  # (B, L, V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdENsy1li13g",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Architectures for composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CNd3TxqJjAV2"
   },
   "outputs": [],
   "source": [
    "#Define the BRNN Model for both our tokenized input and for the distorted input\n",
    "class BRNNTokenizedInput(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTokenizedInput, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FzH5lmnulomS"
   },
   "outputs": [],
   "source": [
    "#Define the BRNN Model for spm\n",
    "class BRNNSentencePiece(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNSentencePiece, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, y):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=y.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "otMZcHPkLzSg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Composition(nn.Module):\n",
    "    def __init__(self, hyperparams, device):\n",
    "        super(Composition, self).__init__()\n",
    "\n",
    "        hyperparams[\"model\"] = \"BRNNTokenizedInput\"\n",
    "        self.original = initialize_model(hyperparams, device)\n",
    "        self.distorted = initialize_model(hyperparams, device)\n",
    "\n",
    "        hyperparams[\"model\"] = \"BRNNSentencePiece\"\n",
    "        self.sentence_piece = initialize_model(hyperparams, device)\n",
    "\n",
    "        #make weights learnable by converting them to torch.nn.Parameter\n",
    "        self.w_original = nn.Parameter(torch.tensor(0.8))  #learnable weight for original model\n",
    "        self.w_distorted = nn.Parameter(torch.tensor(0.1))  #learnable weight for distorted model\n",
    "        self.w_sentence_piece = nn.Parameter(torch.tensor(0.1))  #learnable weight for sentence piece model\n",
    "\n",
    "        #ensure that the weights sum up to 1 manually (soft constraint)\n",
    "        assert abs(self.w_original.item() + self.w_distorted.item() + self.w_sentence_piece.item() - 1.0) < 1e-5, \"Weights should sum to 1 initially.\"\n",
    "\n",
    "\n",
    "    def forward(self, x, y, lengths, x_distorted, x_sp, x_sp_length):\n",
    "        # Normalize weights (soft constraint)\n",
    "        with torch.no_grad():\n",
    "            total_weight = self.w_original + self.w_distorted + self.w_sentence_piece\n",
    "            self.w_original.data /= total_weight\n",
    "            self.w_distorted.data /= total_weight\n",
    "            self.w_sentence_piece.data /= total_weight\n",
    "    \n",
    "        #debugging\n",
    "        #print(self.w_original.item(), self.w_distorted.item(), self.w_sentence_piece.item())\n",
    "    \n",
    "        #get predictions (log-probabilities from each model)\n",
    "        orig_logp = self.original(x, lengths)  #[batch, seq, vocab]\n",
    "        distorted_logp = self.distorted(x_distorted, lengths)\n",
    "        sp_logp = self.sentence_piece(x_sp, x_sp_length, y)\n",
    "    \n",
    "        #exponentiate to convert to probs and use the weights\n",
    "        orig_p = torch.exp(orig_logp)\n",
    "        distorted_p = torch.exp(distorted_logp)\n",
    "        sp_p = torch.exp(sp_logp)\n",
    "    \n",
    "        #weighted sum in probability space\n",
    "        combined_p = (\n",
    "            self.w_original * orig_p +\n",
    "            self.w_distorted * distorted_p +\n",
    "            self.w_sentence_piece * sp_p\n",
    "        )\n",
    "    \n",
    "        #convert back to log-probabilities for the loss\n",
    "        combined_logp = torch.log(combined_p + 1e-12)  #use an epsilon to avoid log(0)\n",
    "    \n",
    "        return combined_logp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0guTkDZ7nI2C",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Recover Word and Phonetic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "SLpd49b-ntEZ"
   },
   "outputs": [],
   "source": [
    "file_name = f\"./fasttext/linearb_fasttext_model_{EMBEDDINGS_DIM}.bin\"\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "embedding_model.load_model(os.path.join(prefix_path, file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_word_embeddings(embedding_model, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_embeddings = [] \n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  #split the sequence into words\n",
    "        seq_embeddings = []  #to store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            word_len = word.count(\"-\") + 1  #compute the length of the word (based on '-' count)\n",
    "            emb = torch.tensor(embedding_model.get_vector(word))  #get the word embedding\n",
    "            # Unsqueeze the embedding to match (word_len, embedding_dim)\n",
    "            emb_unsqueezed = emb.unsqueeze(0)\n",
    "\n",
    "            emb_expanded = emb_unsqueezed.expand(word_len, -1)  # Unsqueeze and expand\n",
    "            if i != len(seq) - 1:\n",
    "                space_embed = torch.zeros_like(emb_unsqueezed)\n",
    "                seq_embeddings.extend([emb_expanded, space_embed])  # Add to the sequence's embeddings\n",
    "            else:\n",
    "                seq_embeddings.append(emb_expanded)  # Add to the sequence's embeddings\n",
    "\n",
    "        # adding zeros for SOS at the beginning of the sequence\n",
    "        if use_sos_in_x:\n",
    "            seq_embeddings = [torch.zeros_like(emb_unsqueezed)] + seq_embeddings\n",
    "\n",
    "        # Concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = torch.cat(seq_embeddings, dim=0)\n",
    "        padding = torch.zeros((final_length - seq_embedding.shape[0], seq_embedding.shape[1]))\n",
    "        seq_embedding = torch.cat([seq_embedding, padding], dim=0)  # Add padding to the end\n",
    "\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.stack(all_embeddings)  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "## Example usage:\n",
    "word_embeddings_train_x = recover_word_embeddings(embedding_model, seq_train_x, train_x.shape[1])\n",
    "word_embeddings_test_x = recover_word_embeddings(embedding_model, seq_test_x, test_x.shape[1])\n",
    "\n",
    "# Free my ram please\n",
    "del embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5hcgTbxhcHmh"
   },
   "outputs": [],
   "source": [
    "syllable_autoencoder = SyllableAutoencoder()\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(f\"syllable_embeddings_{EMBEDDINGS_DIM}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "52FluTPRZ0RV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([419, 256])\n",
      "419\n",
      "torch.Size([457, 256])\n",
      "457\n"
     ]
    }
   ],
   "source": [
    "def recover_phonetic_embeddings(embedding_dict, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_embeddings = []  # To store all sequence embeddings\n",
    "    embed_size = len(list(embedding_dict.values())[0])  # Assuming all embeddings have the same size\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_embeddings = []  # To store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            for syllable in word.split(\"-\"):\n",
    "                if syllable in embedding_dict:\n",
    "                    emb = torch.from_numpy(embedding_dict[syllable])  # Get the word embedding\n",
    "                    seq_embeddings.append(emb)  # Add to the sequence's embeddings\n",
    "                else:\n",
    "                    seq_embeddings.append(torch.zeros(embed_size))\n",
    "            if i != len(seq) - 1:\n",
    "                space_embed = torch.zeros(embed_size)\n",
    "                seq_embeddings.append(space_embed)  # Add to the sequence's embeddings\n",
    "\n",
    "        #adding zeros for SOS at the beginning of the sequence\n",
    "        if use_sos_in_x:\n",
    "            seq_embeddings = [torch.zeros_like(seq_embeddings[0])] + seq_embeddings\n",
    "\n",
    "        #concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = torch.stack(seq_embeddings)\n",
    "        if seq_embedding.shape[0] > 400:\n",
    "            print(seq_embedding.shape)\n",
    "            print(len(seq_embeddings))\n",
    "        padding = torch.zeros((final_length - seq_embedding.shape[0], seq_embedding.shape[1]))\n",
    "        seq_embedding = torch.cat([seq_embedding, padding], dim=0)  # Add padding to the end\n",
    "\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.stack(all_embeddings)  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "phonetic_embeddings_train_x = recover_phonetic_embeddings(loaded_embeddings, seq_train_x, train_x.shape[1])\n",
    "phonetic_embeddings_test_x = recover_phonetic_embeddings(loaded_embeddings, seq_test_x, test_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VcCrJUmbpjsA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3318, 419, 256]), torch.Size([3318, 419, 256]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_train_x.shape, phonetic_embeddings_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_lchCPdO3lK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Recover Tf-idf and Unigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dW2_51ANO2a8"
   },
   "outputs": [],
   "source": [
    "def recover_words_features(word_info, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_tf_idf= []  # To store all sequence embeddings\n",
    "    all_unigram= []\n",
    "    lookup = word_info.copy()\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_tf_idf = [-2] if use_sos_in_x else []  # To store embeddings for the current sequence\n",
    "        seq_unigram = [-2] if use_sos_in_x else []\n",
    "        for i, word in enumerate(seq):\n",
    "            seq_tf_idf.extend([lookup[word][\"tf_idf\"]]*(word.count(\"-\") + 1))\n",
    "            seq_unigram.extend([lookup[word][\"unigram_freq\"]]*(word.count(\"-\") + 1))\n",
    "            if i != len(seq) - 1:\n",
    "                seq_tf_idf.append(-1)  # Add to the sequence's embeddings\n",
    "                seq_unigram.append(-1)\n",
    "        seq_tf_idf.extend([0]*(final_length - len(seq_tf_idf)))\n",
    "        seq_unigram.extend([0]*(final_length - len(seq_unigram)))\n",
    "\n",
    "        all_tf_idf.append(seq_tf_idf)  # Add to the overall embeddings list\n",
    "        all_unigram.append(seq_unigram)\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.tensor(all_tf_idf), torch.tensor(all_unigram),  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "tf_idf_words_train_x, unigram_words_train_x = recover_words_features(word_results, seq_train_x, train_x.shape[1])\n",
    "tf_idf_words_test_x, unigram_words_test_x = recover_words_features(word_results, seq_test_x, test_x.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CCksMGI8Vjc5"
   },
   "outputs": [],
   "source": [
    "def recover_tokens_features(tokens_info, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_tf_idf= []  # To store all sequence embeddings\n",
    "    all_unigram= []\n",
    "    lookup = tokens_info.copy()\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_tf_idf = [-2] if use_sos_in_x else []  # To store embeddings for the current sequence\n",
    "        seq_unigram = [-2] if use_sos_in_x else []\n",
    "        for i, word in enumerate(seq):\n",
    "            for tok in word.split(\"-\"):\n",
    "                seq_tf_idf.append(lookup[tok][\"tf_idf\"])\n",
    "                seq_unigram.append(lookup[tok][\"unigram_freq\"])\n",
    "            if i != len(seq) - 1:\n",
    "                seq_tf_idf.append(-1)  # Add to the sequence's embeddings\n",
    "                seq_unigram.append(-1)\n",
    "        seq_tf_idf.extend([0]*(final_length - len(seq_tf_idf)))\n",
    "        seq_unigram.extend([0]*(final_length - len(seq_unigram)))\n",
    "\n",
    "        all_tf_idf.append(seq_tf_idf)  # Add to the overall embeddings list\n",
    "        all_unigram.append(seq_unigram)\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.tensor(all_tf_idf), torch.tensor(all_unigram),  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "tf_idf_tokens_train_x, unigram_tokens_train_x = recover_tokens_features(token_results, seq_train_x, train_x.shape[1])\n",
    "tf_idf_tokens_test_x, unigram_tokens_test_x = recover_tokens_features(token_results, seq_test_x, test_x.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Kz3MMtgj5K-y"
   },
   "outputs": [],
   "source": [
    "#Tf-Idf BRNN\n",
    "if USE_TFIDF:\n",
    "    print(\"Chaning to tf-idf BRNNTextInfiller\")\n",
    "    class BRNNTextInfiller(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, unigram_probs, tfidf_scores):\n",
    "            super(BRNNTextInfiller, self).__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "            self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_size * 2, vocab_size - 4)  # *2 for bidirectional\n",
    "            self.softmax = nn.LogSoftmax(dim=-1)\n",
    "            self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "    \n",
    "            #convert unigram and TF-IDF scores to log-space\n",
    "            self.unigram_log_probs = torch.log(unigram_probs + 1e-10)\n",
    "            self.tfidf_scores = tfidf_scores\n",
    "    \n",
    "            self.mask = torch.ones(vocab_size-4)\n",
    "            self.mask[0:4] = 0  #zero out first 4 tokens\n",
    "            self.mask = self.mask\n",
    "            self.se_out = TFIDF_SequeezeExcitationFusion(vocab_size-4)\n",
    "    \n",
    "        def forward(self, x, lengths, word_emb, phon_emb):\n",
    "            embedded = self.embedding(x)\n",
    "            fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "            packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            out, _ = self.brnn(packed_embedded)\n",
    "            out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "            out = self.fc(out)\n",
    "    \n",
    "            # Integrate unigram bias + TF-IDF weighting\n",
    "            #print(out.shape, self.unigram_log_probs.shape, self.tfidf_scores.shape, (self.unigram_log_probs.to(x.device) * self.mask.to(x.device)).unsqueeze(0).unsqueeze(1).expand_as(out).shape)\n",
    "            out = self.se_out(out, self.tfidf_scores.to(x.device).float(), (self.unigram_log_probs.to(x.device).float() * self.mask.to(x.device)).unsqueeze(0).unsqueeze(1).expand_as(out).float())\n",
    "            #out = out + self.unigram_log_probs.to(x.device) * self.mask.to(x.device)\n",
    "            return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItxZ4t0HgO0n",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions to reconstruct sequences, create DataLoaders and collect metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "EEH6yNGfoMdE"
   },
   "outputs": [],
   "source": [
    "class NLLWithKL(nn.Module):\n",
    "    def __init__(self, unigram_probs, kl_excluded_tokens=[i for i in range(4)], pad_idx=0, lambda_kld=1):\n",
    "        \"\"\"\n",
    "        Computes Negative Log-Likelihood (NLL) loss + KL-Divergence regularization.\n",
    "\n",
    "        Parameters:\n",
    "        - unigram_probs: Tensor of shape (vocab_size,) with unigram probabilities\n",
    "        - kl_excluded_tokens: List of token indices to exclude from KL-divergence computation\n",
    "        - lambda_kld: Weight for KL-divergence term\n",
    "        \"\"\"\n",
    "        super(NLLWithKL, self).__init__()\n",
    "        self.register_buffer(\"unigram_probs\", unigram_probs)\n",
    "        self.excluded_tokens = kl_excluded_tokens\n",
    "        self.lambda_kld = lambda_kld\n",
    "\n",
    "        #define loss functions\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=0, reduction=\"mean\")\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        Compute the combined NLL + KL loss.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Log-softmax outputs (batch_size, seq_len, vocab_size)\n",
    "        - targets: Ground-truth token indices (batch_size, seq_len)\n",
    "        - lengths: Sequence lengths for each batch element (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "        - Total loss (scalar)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = predictions.shape\n",
    "\n",
    "        # Compute Negative Log-Likelihood (NLL) Loss\n",
    "        ce_loss = self.nll_loss(predictions.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Expand ungram probabilities to match batch size and sequence length\n",
    "        p_unigram = self.unigram_probs.unsqueeze(0).unsqueeze(0).expand_as(predictions).to(predictions.device)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Compute KL-divergence (token-wise)\n",
    "        kl_div = self.kl_div_loss(predictions, p_unigram)  # KL divergence loss\n",
    "\n",
    "        # Mask out special tokens in KL computation\n",
    "        mask = torch.ones(vocab_size, dtype=torch.bool, device=predictions.device)\n",
    "        mask[self.excluded_tokens] = False\n",
    "\n",
    "        # Normalize by sequence length\n",
    "        kl_div = (kl_div * mask.float().unsqueeze(0).unsqueeze(1)).sum(dim=1) / lengths.float().unsqueeze(1)\n",
    "        kl_div = kl_div.sum(dim=-1)\n",
    "        kl_div = kl_div.mean()  # Mean over batch\n",
    "\n",
    "        # Final loss\n",
    "        return ce_loss + self.lambda_kld * kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5lVL9KqI6P4a"
   },
   "outputs": [],
   "source": [
    "def extract_label(t):\n",
    "    if t.ndim == 0:\n",
    "        return t.item()\n",
    "    elif t.ndim == 1:\n",
    "        return t.argmax().item()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label shape: expected scalar or 1D vector\")\n",
    "\n",
    "#def reconstruct_sequences(model_output, test_y, test_x, inv_map=inv_map, inv_map_input=inv_map_input):\n",
    "#    input_seq = []\n",
    "#    gt_seq = []\n",
    "#    out_seq = []\n",
    "\n",
    "#    for i in range(test_y.shape[0]):\n",
    "#        test_seq = []\n",
    "  #      sequence = []\n",
    "#        result = []\n",
    "#        for j in range(test_y.shape[1]):\n",
    "#            if test_y[i][j] == 0:\n",
    "#                break\n",
    "#            sequence.append(inv_map[test_y[i][j].item()])\n",
    "#            result.append(inv_map[model_output[i][j].argmax().item()])\n",
    "\n",
    "            # src is one character shorter!\n",
    "#            if j != test_y.shape[1] - 1:\n",
    "#                test_seq.append(inv_map_input[test_x[i][j].item()])\n",
    "#\n",
    "#        input_seq.append(\" \".join(test_seq))\n",
    "#        gt_seq.append(\" \".join(sequence))\n",
    "#        out_seq.append(\" \".join(result))\n",
    "#    return input_seq, gt_seq, out_seq\n",
    "\n",
    "def reconstruct_sequences(model_output, test_y, test_x, inv_map=inv_map, inv_map_input=inv_map_input):\n",
    "    input_seq = []\n",
    "    gt_seq = []\n",
    "    out_seq = []\n",
    "\n",
    "    for i in range(test_y.shape[0]):\n",
    "        test_seq = []\n",
    "        sequence = []\n",
    "        result = []\n",
    "        for j in range(test_y.shape[1]):\n",
    "            y_idx = extract_label(test_y[i][j])\n",
    "            if y_idx == 0: \n",
    "                break\n",
    "\n",
    "            sequence.append(inv_map[y_idx])\n",
    "            result.append(inv_map[model_output[i][j].argmax().item()])\n",
    "\n",
    "            # src is one character shorter!\n",
    "            if j != test_y.shape[1] - 1:\n",
    "                test_seq.append(inv_map_input[extract_label(test_x[i][j])])\n",
    "\n",
    "        input_seq.append(\" \".join(test_seq))\n",
    "        gt_seq.append(\" \".join(sequence))\n",
    "        out_seq.append(\" \".join(result))\n",
    "\n",
    "    return input_seq, gt_seq, out_seq\n",
    "\n",
    "def collect_batch_metrics(model_output, test_y, test_x, top_k=(20, 15, 10, 5, 1), use_sos_in_x=USE_SOS_IN_X):\n",
    "    \"\"\"\n",
    "    Collects batch-level metrics without computing final scores.\n",
    "    This allows aggregation over multiple batches.\n",
    "\n",
    "    Args:\n",
    "        model_output (Tensor): Model's output log probabilities (batch_size, seq_len, num_classes).\n",
    "        test_x (Tensor): Input sequence (batch_size, seq_len), contains '?' locations.\n",
    "        test_y (Tensor): Ground truth labels (batch_size, seq_len).\n",
    "        top_k (tuple): List of top-k values to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with raw counts for aggregation.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, num_classes = model_output.shape\n",
    "    question_mark_id = input_mapping[\"?\"] #test_x.max().item()  # '?' is the highest label in test_x\n",
    "\n",
    "    batch_metrics = {\n",
    "        \"correct_sequences\": 0,\n",
    "        \"total_sequences\": batch_size,\n",
    "        \"total_question_marks\": 0,\n",
    "        \"top_k_correct\": {k: 0 for k in top_k},\n",
    "        \"mrr_sum\": 0.0,  # Mean Reciprocal Rank numerator\n",
    "    }\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        correct_sequence = True\n",
    "\n",
    "        for j in range(1 if use_sos_in_x else 0, seq_len if use_sos_in_x else seq_len-1):  # Start from 1 to check i-1\n",
    "            index_to_check = j - 1 if use_sos_in_x else j\n",
    "\n",
    "            # Check full sequence accuracy\n",
    "            \n",
    "            true_label = extract_label(test_y[i, index_to_check])\n",
    "            predicted_label = model_output[i, index_to_check].argmax().item()\n",
    "\n",
    "            if true_label == 0:  # Padding reached\n",
    "                break\n",
    "\n",
    "            if (use_sos_in_x or j != seq_len - 1) and extract_label(test_x[i, j]) == question_mark_id:  # '?' in input\n",
    "                batch_metrics[\"total_question_marks\"] += 1\n",
    "\n",
    "                sorted_indices = model_output[i, index_to_check].argsort(descending=True)\n",
    "\n",
    "                # Rank of the correct answer (1-based index)\n",
    "                rank = (sorted_indices == true_label).nonzero(as_tuple=True)[0].item() + 1\n",
    "                batch_metrics[\"mrr_sum\"] += 1 / rank  # Add reciprocal rank\n",
    "\n",
    "                # Check top-K\n",
    "                for k in top_k:\n",
    "                    if rank <= k:\n",
    "                        batch_metrics[\"top_k_correct\"][k] += 1\n",
    "\n",
    "            if predicted_label != true_label:\n",
    "                correct_sequence = False\n",
    "\n",
    "        if correct_sequence:\n",
    "            batch_metrics[\"correct_sequences\"] += 1\n",
    "\n",
    "    return batch_metrics\n",
    "\n",
    "def aggregate_metrics(metrics, batch):\n",
    "    if metrics is None:\n",
    "        return batch\n",
    "    for key, value in batch.items():\n",
    "        if key != \"top_k_correct\":\n",
    "            metrics[key] += value\n",
    "        else:\n",
    "            for k, v in metrics[\"top_k_correct\"].items():\n",
    "                metrics[\"top_k_correct\"][k] += value[k]\n",
    "    return metrics\n",
    "\n",
    "def compute_final_metrics(aggregated):\n",
    "    \"\"\"\n",
    "    Computes final metrics from aggregated batch metrics.\n",
    "\n",
    "    Args:\n",
    "        aggregated (dict): Aggregated counts from all batches.\n",
    "\n",
    "    Returns:\n",
    "        dict: Final computed metric values.\n",
    "    \"\"\"\n",
    "    final_metrics = {}\n",
    "\n",
    "    # Sequence accuracy\n",
    "    final_metrics[\"sequence_accuracy\"] = (\n",
    "        aggregated[\"correct_sequences\"] / aggregated[\"total_sequences\"]\n",
    "        if aggregated[\"total_sequences\"] > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Mean Reciprocal Rank (MRR)\n",
    "    final_metrics[\"mrr\"] = (\n",
    "        aggregated[\"mrr_sum\"] / aggregated[\"total_question_marks\"]\n",
    "        if aggregated[\"total_question_marks\"] > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Top-k accuracies\n",
    "    final_metrics[\"top_k_accuracy\"] = {\n",
    "        k: (aggregated[\"top_k_correct\"][k] / aggregated[\"total_question_marks\"])\n",
    "        if aggregated[\"total_question_marks\"] > 0 else 0\n",
    "        for k in aggregated[\"top_k_correct\"]\n",
    "    }\n",
    "\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "#Takes self.history with is < k_numfold : (avg_loss, test_metrics) > dictionary, every tuple is an epoch\n",
    "#Returns couple [avg_metric_epoch_1, avg_metric_epoch_2, ...] [num_folds_epoch_1, num_folds_epoch_2, ...]\n",
    "def aggregate_k_fold_metrics(history):\n",
    "    \"\"\"\n",
    "    Aggregates the metrics from k-fold cross-validation, accounting for different early stopping points.\n",
    "    \"\"\"\n",
    "    max_epochs = max(len(history[k]) for k in range(len(history)))  # Longest training\n",
    "    avg_metrics_per_epoch = []\n",
    "    num_folds_used = []\n",
    "\n",
    "    for epoch in range(max_epochs): #for each epoch get averages of parameters between the different folds\n",
    "        losses = []\n",
    "        seq_accs = []\n",
    "        mrrs = []\n",
    "        top_k_accs = {20: [], 15: [], 10: [], 5: [], 1: []}\n",
    "        folds_used = 0\n",
    "\n",
    "        for k in range(len(history)): #iterate through every fold\n",
    "            if epoch < len(history[k]):  # If fold has this epoch\n",
    "                loss, metrics = history[k][epoch] #extract avg loss and val metrics\n",
    "                losses.append(loss)\n",
    "                seq_accs.append(metrics[\"sequence_accuracy\"])\n",
    "                mrrs.append(metrics[\"mrr\"])\n",
    "                for k_top in top_k_accs.keys():\n",
    "                    top_k_accs[k_top].append(metrics[\"top_k_accuracy\"][k_top]) #populate also top_k parameters\n",
    "                folds_used += 1\n",
    "\n",
    "        if folds_used > 0: #compute final avg metrics\n",
    "            # same format as the input: i.e. a tuple with loss as first element and metrics dict as the second\n",
    "            # this ensures compatibility of the unpack function with this format as well\n",
    "            avg_metrics = (\n",
    "                np.mean(losses),\n",
    "                {\n",
    "                    \"sequence_accuracy\": np.mean(seq_accs),\n",
    "                    \"mrr\": np.mean(mrrs),\n",
    "                    \"top_k_accuracy\": {k: np.mean(v) for k, v in top_k_accs.items()}\n",
    "                }\n",
    "            )\n",
    "            avg_metrics_per_epoch.append(avg_metrics)\n",
    "            num_folds_used.append(folds_used)\n",
    "\n",
    "    return avg_metrics_per_epoch, num_folds_used\n",
    "\n",
    "#Takes input [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2...]\n",
    "def unpack_history(history):\n",
    "    epochs = []\n",
    "    losses = []\n",
    "    top_k_accuracies = defaultdict(lambda: [])\n",
    "    sequence_accuracies = []\n",
    "    mrrs = []\n",
    "    #Unpacks for every epoch its aggregated metrics and returns them as sorted lists\n",
    "    for i, (loss, metrics) in enumerate(history):\n",
    "       epoch = i + 1\n",
    "       epochs.append(epoch)\n",
    "       losses.append(loss)\n",
    "\n",
    "       sequence_accuracies.append(metrics[\"sequence_accuracy\"])\n",
    "       mrrs.append(metrics[\"mrr\"])\n",
    "\n",
    "       for k in metrics[\"top_k_accuracy\"].keys():\n",
    "           top_k_accuracies[k].append(metrics[\"top_k_accuracy\"][k])\n",
    "\n",
    "    return epochs, losses, sequence_accuracies, mrrs, top_k_accuracies\n",
    "\n",
    "def create_loader(x, y, lengths, word_emb, phon_emb, batch_size):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    dataset = TensorDataset(x, y, lengths, word_emb, phon_emb)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    return loader\n",
    "\n",
    "def create_loader_with_sp(x, y, lengths, x_distorted, x_sp, lengths_sp, batch_size):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    dataset = TensorDataset(x, y, lengths, x_distorted, x_sp, lengths_sp)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    return loader\n",
    "\n",
    "#\n",
    "def replace_eos_with_sos(batch_y, sos_token = 1, eos_token = 2, pad_token = 0):\n",
    "    inp = batch_y.clone()\n",
    "    for i in range(inp.shape[0]):\n",
    "        for j in range(inp.shape[1]):\n",
    "            if inp[i][j] == eos_token and (j == inp.shape[1] - 1 or inp[i][j+1] == pad_token):\n",
    "                inp[i][j] = pad_token # replace EOS with padding\n",
    "                break\n",
    "    return torch.cat([torch.ones((inp.shape[0], 1), dtype=torch.long, device=batch_y.device) * sos_token, inp[:, :-1]], dim=1)\n",
    "\n",
    "def convert_keys(d):\n",
    "    \"\"\"Recursively converts numeric string keys to integers in a dictionary.\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: convert_keys(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_keys(i) for i in d]\n",
    "    else:\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8tKyaEpgZlB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initialization of Model, Optimizer, Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JYHgre_7adfD"
   },
   "outputs": [],
   "source": [
    "CLASS_REGISTRY = {\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"NLLLoss\": nn.NLLLoss,\n",
    "    \"BRNNTextInfiller\": BRNNTextInfiller,\n",
    "    \"TransformerTextInfiller\": TransformerTextInfiller,\n",
    "    \"FCNTextInfiller\": FCNTextInfiller,\n",
    "    \"ConvTextInfiller\": ConvTextInfiller,\n",
    "    \"ConvTextInfillerRNN\": ConvTextInfillerRNN,\n",
    "    \"BRNNTokenizedInput\": BRNNTokenizedInput,\n",
    "    \"BRNNSentencePiece\": BRNNSentencePiece,\n",
    "    \"BiConvTextInfiller\": BiConvTextInfiller,\n",
    "    \"NLLWithKL\": NLLWithKL,\n",
    "    \"KLDivLoss\" : nn.KLDivLoss,  \n",
    "}\n",
    "\n",
    "def get_class(class_name):\n",
    "    return CLASS_REGISTRY.get(class_name, None)\n",
    "\n",
    "# General hyperparameters (shared across models)\n",
    "general_hyperparams = {\n",
    "    # general\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 200,\n",
    "    \"num_folds\": 7,\n",
    "\n",
    "    # optimizer\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.0001,\n",
    "\n",
    "    # criterion\n",
    "    \"criterion\": \"NLLWithKL\" if USE_NLLWithKL else \"NLLLoss\",\n",
    "    \"pad_idx\": 0,\n",
    "\n",
    "    \"ranked_based_loss_contribution\": USE_RANKED_LOSS_CONTRIB,\n",
    "    \"ranked_based_loss_contribution_weight\": 100,\n",
    "    \"kl_loss_contribution_weight\": 0.1\n",
    "\n",
    "}\n",
    "\n",
    "# Model-specific hyperparameters\n",
    "if USE_COMPOSITION:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"Composition\",\n",
    "        \"input_dim\": input_dim,\n",
    "        \"sp_input_dim\": sp_input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_BRNN and USE_CONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"ConvTextInfillerRNN\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"num_filters\": EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1, 3, 5, 7],  # Example kernel sizes\n",
    "        \"hidden_size\": 2 * EMBEDDINGS_DIM,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_BRNN:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"BRNNTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        #\"norms_or_ratios\":  [1.0, 0.8],\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_TRANSFORMER:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"TransformerTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"d_model\": EMBEDDINGS_DIM,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 5,\n",
    "        \"dim_feedforward\": 512,\n",
    "        \"max_seq_len\": max([train_lengths.max().item(), test_lengths.max().item()]) + 1,\n",
    "        \"dropout\": 0.1\n",
    "    }\n",
    "elif USE_FCN:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"FCNTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "        \"normalize\": False\n",
    "    }\n",
    "elif USE_CONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"ConvTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"num_filters\": 2*EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1, 3, 5, 7], \n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "        \"normalize\": False\n",
    "    }\n",
    "elif USE_BICONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"BiConvTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1,3,5,7],\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "def initialize_model(hyperparams, device):\n",
    "    model_name = hyperparams[\"model\"]\n",
    "\n",
    "    # Initialize the BRNNTextInfiller model\n",
    "    if model_name == \"BRNNTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            #norms_or_ratios=hyperparams[\"norms_or_ratios\"], #if SE\n",
    "            # DEBUG\n",
    "            #unigram_probs=p_unigram,\n",
    "            #tfidf_scores=tf_idf\n",
    "        )\n",
    "\n",
    "    elif model_name == \"BRNNTokenizedInput\":\n",
    "        model = get_class(model_name)(\n",
    "            input_dim=hyperparams[\"input_dim\"],\n",
    "            output_dim=hyperparams[\"output_dim\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "\n",
    "    elif model_name == \"BRNNSentencePiece\":\n",
    "        model = get_class(model_name)(\n",
    "            input_dim=hyperparams[\"sp_input_dim\"],\n",
    "            output_dim=hyperparams[\"output_dim\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the TransformerTextInfiller model\n",
    "    elif model_name == \"TransformerTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            d_model=hyperparams[\"d_model\"],\n",
    "            num_heads=hyperparams[\"num_heads\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dim_feedforward=hyperparams[\"dim_feedforward\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            max_seq_len=hyperparams[\"max_seq_len\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the FCNTextInfiller model\n",
    "    elif model_name == \"FCNTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            normalize=hyperparams[\"normalize\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the ConvTextInfiller model\n",
    "    elif model_name == \"ConvTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            normalize=hyperparams[\"normalize\"],\n",
    "        )\n",
    "    elif model_name == \"ConvTextInfillerRNN\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "    elif model_name == \"BiConvTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "def initialize_optimizer(hyperparams, model):\n",
    "    optimizer = get_class(hyperparams[\"optimizer\"])(model.parameters(), lr=hyperparams[\"lr\"])\n",
    "    return optimizer\n",
    "\n",
    "def initialize_criterion(hyperparams):\n",
    "    if hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(p_unigram, kl_excluded_tokens=[0,1,2,3], pad_idx=hyperparams[\"pad_idx\"], lambda_kld=hyperparams[\"kl_loss_contribution_weight\"])\n",
    "    elif hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(ignore_index=hyperparams[\"pad_idx\"])\n",
    "    elif hyperparams[\"criterion\"] == \"KLDivLoss\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(reduction='batchmean')\n",
    "    return criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TA036xcf-Gjq"
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, hyperparams, save_dir=prefix_path, experiment_name=None, verbose=True, use_gpu=True):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.hyperparams = hyperparams\n",
    "        if not self.hyperparams is None:\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "        if not experiment_name:\n",
    "            self.experiment_name = \"experiment\"\n",
    "        else:\n",
    "            self.experiment_name = experiment_name\n",
    "\n",
    "        results_dir = os.path.join(save_dir, \"Models Results\")\n",
    "        #create directory if it doesn't exist\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        self.save_dir = os.path.join(results_dir, self.experiment_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.model_path = os.path.join(self.save_dir, \"model.pt\")\n",
    "        self.optimizer_path = os.path.join(self.save_dir, \"optimizer.pt\")\n",
    "        self.hypers_path = os.path.join(self.save_dir, \"hypers.json\")\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def log_specifics(self):\n",
    "\n",
    "        with open(os.path.join(self.save_dir, \"model_specifics.txt\"), 'w') as f:\n",
    "\n",
    "            f.write(\"\\n=== Initialized Model ===\")\n",
    "            f.write(f\"{self.model}\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"\\n=== Optimizer ===\")\n",
    "            f.write(f\"{self.optimizer}\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"\\n=== Loss Function (Criterion) ===\")\n",
    "            f.write(f\"{self.criterion}\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def ping(self, epoch):\n",
    "        with open(os.path.join(self.save_dir, \"HeartBeat.txt\"), 'a') as f:\n",
    "            f.write(f\"PING: Current hour = {datetime.now()}, epoch {epoch}\\n\")\n",
    "            \n",
    "\n",
    "    def initialize_from_hyperparams(self):\n",
    "        self.batch_size = self.hyperparams[\"batch_size\"]\n",
    "        self.model = initialize_model(self.hyperparams, self.device)\n",
    "        self.optimizer = initialize_optimizer(self.hyperparams, self.model)\n",
    "        self.criterion = initialize_criterion(self.hyperparams)\n",
    "\n",
    "    #If validating, it saves all aggregated validation metrics and loss per epoch, else it saves only testing metrics\n",
    "    def save_experiment(self, history, folds_involved=None):\n",
    "\n",
    "        is_validation = folds_involved is not None\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_{'val' if is_validation else 'test'}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        # create dictionary to save JSON file\n",
    "        for epoch, (loss, metrics_dictionary) in enumerate(history):\n",
    "            metadata[f\"epoch_{epoch+1}\"] = {\n",
    "                \"loss\": loss,\n",
    "                \"extra_info\": metrics_dictionary\n",
    "            }\n",
    "            # add also number of folds from which the information derives\n",
    "            if is_validation:\n",
    "                metadata[f\"epoch_{epoch+1}\"][\"folds aggregated\"] = folds_involved[epoch]\n",
    "\n",
    "\n",
    "        if is_validation:\n",
    "            # this code has been put here to avoid saving the metadata twice\n",
    "            with open(self.hypers_path, \"w\") as f:\n",
    "                json.dump(self.hyperparams, f, indent=4)\n",
    "\n",
    "        else:\n",
    "            #save model only when training on whole dataset and testing on test set\n",
    "            torch.save(self.model.state_dict(), self.model_path)\n",
    "            torch.save(self.optimizer.state_dict(), self.optimizer_path)\n",
    "\n",
    "        #Dump metadata (test metrics or validation aggregated data)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metrics(self, epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Plots and saves loss, sequence accuracy, MRR, and Top-K accuracy metrics.\n",
    "        Ensures points are connected by a default-colored line, while points themselves\n",
    "        are color-coded based on the number of folds involved in training (if available).\n",
    "        \"\"\"\n",
    "        is_validation = folds_involved is not None\n",
    "\n",
    "        if is_validation:\n",
    "            # If folds are provided, create a colormap\n",
    "            unique_folds = sorted(set(folds_involved))\n",
    "            colormap = plt.colormaps[\"tab20\"]  # Switch to 'tab10' for distinct colors\n",
    "            fold_color_map = {fold: colormap(i+2) for i, fold in enumerate(unique_folds)} # i+2 skips too bluish colors for better visualization\n",
    "\n",
    "\n",
    "        def plot_metric(metric_values, ylabel, title, filename):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "            # Plot a continuous gray line through all points\n",
    "            plt.plot(epochs, metric_values, zorder=1)\n",
    "\n",
    "            # Overlay scatter points: Use fold colors if available, otherwise default color\n",
    "            if is_validation:\n",
    "                for fold in unique_folds[::-1]:\n",
    "                    fold_epochs = [e for e, f in zip(epochs, folds_involved) if f == fold]\n",
    "                    fold_values = [v for v, f in zip(metric_values, folds_involved) if f == fold]\n",
    "                    plt.scatter(fold_epochs, fold_values, color=fold_color_map[fold], label=f\"{fold} Folds\", s=50, zorder=2)\n",
    "            else:\n",
    "                plt.scatter(epochs, metric_values, s=50, zorder=2)\n",
    "\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"{title} ({'Validation' if is_validation else 'Test'})\")\n",
    "            if is_validation:\n",
    "                plt.legend(title=\"Folds Used\", loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(self.save_dir, filename))\n",
    "            if verbose:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "        # Plot Loss\n",
    "        plot_metric(losses, \"Loss\", \"Loss Over Epochs\", f\"loss_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Sequence Accuracy\n",
    "        plot_metric(sequence_accuracies, \"Sequence Accuracy\", \"Sequence Accuracy Over Epochs\",\n",
    "                    f\"sequence_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot MRR\n",
    "        plot_metric(mrrs, \"MRR\", \"Mean Reciprocal Rank Over Epochs\",\n",
    "                    f\"mrr_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Top-K Accuracy\n",
    "        for k, values in top_k_accuracies.items():\n",
    "            plot_metric(values, \"Accuracy\", f\"Top-{k} Accuracy Over Epochs\",\n",
    "                        f\"top_{k}_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "    def load_experiment(self):\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_test.json\")\n",
    "        assert os.path.exists(self.hypers_path) and os.path.exists(self.model_path) and os.path.exists(self.optimizer_path) and os.path.exists(metadata_path), \" No saved experiment found!\"\n",
    "\n",
    "        with open(self.hypers_path, \"r\") as f:\n",
    "            self.hyperparams = json.load(f)\n",
    "\n",
    "        #load model with hyperparams and optimizer\n",
    "        self.initialize_from_hyperparams()\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))\n",
    "        self.optimizer.load_state_dict(torch.load(self.optimizer_path, map_location=self.device, weights_only=True))\n",
    "\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        i = 0\n",
    "        # recover history of past iterations\n",
    "        while f\"epoch_{i+1}\" in metadata:\n",
    "            loss = metadata[f\"epoch_{i+1}\"][\"loss\"]\n",
    "            extra_info = metadata[f\"epoch_{i+1}\"][\"extra_info\"]\n",
    "            # converts integers key of top_k accuracies back to int (by default they become strings once in the JSON file)\n",
    "            extra_info = convert_keys(extra_info)\n",
    "            self.history.append((loss, extra_info))\n",
    "            i += 1\n",
    "\n",
    "    def test(self, test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, verbose=True):\n",
    "        if verbose:\n",
    "            print(\"TESTING MODEL!\")\n",
    "            num_sequences = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        test_loader = create_loader(test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_corrects = 0\n",
    "            metrics = None\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, word_emb, phon_emb in test_loader:\n",
    "                batch_x, batch_y, batch_lengths, word_emb, phon_emb = batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), word_emb.to(self.device), phon_emb.to(self.device)\n",
    "\n",
    "                if self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfillerRNN\" or self.hyperparams[\"model\"] == \"BiConvTextInfiller\":\n",
    "                    # For BRNNTextInfiller, use the model's forward pass\n",
    "                    model_output = self.model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "                elif self.hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                    # Generate the sequence autoregressively using the model's generate function\n",
    "                    model_output = self.model.generate(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "                if verbose:\n",
    "                    # Reconstruct sequences (input, ground truth, generated output)\n",
    "                    inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "\n",
    "                    for (input, gt_seq, out_seq) in zip(inp, gt, out):\n",
    "                        print(f\"TEST n. {num_sequences + 1}\")\n",
    "                        print(f\"Input: {input}\")\n",
    "                        print(f\"Ground Truth: {gt_seq}\")\n",
    "                        print(f\"Output: {out_seq}\")\n",
    "                        num_sequences += 1\n",
    "\n",
    "                # Collect batch metrics (e.g., accuracy, loss, etc.)\n",
    "                batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "                metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "            # Compute final metrics\n",
    "            metrics = compute_final_metrics(metrics)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"TEST FINISHED\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    #Train model, returns history if early stopping is true, else it returns empty tuple\n",
    "    def train(self, train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x, val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x,\n",
    "              num_epochs=10, verbose=False, early_stop=False, max_mrr_init=None, max_top1_init=None, max_seq_acc_init=None, best_epoch_init=None):\n",
    "\n",
    "        train_loader = create_loader(train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x, self.batch_size)\n",
    "\n",
    "        if early_stop:\n",
    "            max_mrr = max_mrr_init\n",
    "            max_top1 = max_top1_init\n",
    "            max_seq_acc = max_seq_acc_init\n",
    "            best_epoch = best_epoch_init\n",
    "            patience = 50\n",
    "            counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            self.ping(epoch)\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, word_emb, phon_emb in train_loader:  # batch processing to not have RAM usage explode\n",
    "                batch_x, batch_y, batch_lengths, word_emb, phon_emb = batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), word_emb.to(self.device), phon_emb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfillerRNN\" or self.hyperparams[\"model\"] == \"BiConvTextInfiller\":\n",
    "                    # accounts for SOS beggining the sequence\n",
    "                    model_output = self.model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "                    #reshape for loss calculation\n",
    "                    output = model_output.view(-1, model_output.shape[-1])\n",
    "                    batch_y_reshaped = batch_y.view(-1)\n",
    "                    if USE_ONE_HOT_ENCODING:\n",
    "                        output = output.view(-1)\n",
    "                        \n",
    "                elif self.hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                    tgt_input = replace_eos_with_sos(batch_y)\n",
    "                    #accounting for the fact that target sequence is always one token longer (SOS)\n",
    "                    tgt_lengths = batch_lengths + torch.ones_like(batch_lengths, device=batch_lengths.device)\n",
    "                    model_output = self.model(batch_x, batch_lengths, tgt_input, tgt_lengths, word_emb, phon_emb)\n",
    "                    output = model_output.view(-1, model_output.shape[-1])\n",
    "                    batch_y_reshaped = batch_y.view(-1)\n",
    "\n",
    "                if self.hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "                elif self.hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "                    loss = self.criterion(model_output, batch_y, batch_lengths)\n",
    "                elif self.hyperparams[\"criterion\"] == \"KLDivLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "\n",
    "                if self.hyperparams[\"ranked_based_loss_contribution\"]:\n",
    "                    # Step 1: Find the maximum indices along the sequence dimension for each batch (along axis 1)\n",
    "                    max_indices = batch_x.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                    # Step 2: Compute the previous index for the comparison (i, j-1)\n",
    "                    # Ensure that j-1 is not less than 0 (i.e., for the first token in the sequence)\n",
    "                    prev_indices = max_indices - 1  if (self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\") else max_indices #clamp to ensure no negative indices\n",
    "                    # Step 3: Select the corresponding predictions and ground truth for the prev indices (i, j-1)\n",
    "                    # Get the log probabilities from model_output at (i, j-1)\n",
    "                    predictions_at_prev_idx = model_output[torch.arange(model_output.size(0)), prev_indices, :]  # (batch_size, vocab_size)\n",
    "                    # Turn log probabilities into probabilities\n",
    "                    predictions_at_prev_idx = torch.exp(predictions_at_prev_idx)\n",
    "                    # gt values\n",
    "                    target_at_prev_idx = batch_y[torch.arange(batch_y.size(0)), prev_indices] # (batch_size)\n",
    "                    # Step 2: Extract log probability of the correct token\n",
    "                    correct_probs = predictions_at_prev_idx.gather(dim=-1, index=target_at_prev_idx.unsqueeze(-1)).squeeze(-1)  # (batch_size,)\n",
    "                    # Step 3: Find the max predicted token probability\n",
    "                    max_predicted_probs, _ = predictions_at_prev_idx.max(dim=-1)  # (batch_size,)\n",
    "                    # Step 4: Compute the loss term comparing max-predicted probability vs ground-truth probability\n",
    "                    prob_diff_loss = (max_predicted_probs - correct_probs).mean()  # Penalize if gt prob is lower than max prob\n",
    "                    loss_contrib = prob_diff_loss * self.hyperparams[\"ranked_based_loss_contribution_weight\"]\n",
    "                    loss += loss_contrib\n",
    "\n",
    "                # Compute loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            metrics = self.test(val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x, verbose=False)  # validation\n",
    "            history.append((avg_loss, metrics))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{len(self.history) + epoch+1}/{len(self.history) + num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "                print(\"Validation metrics are: \", metrics)\n",
    "\n",
    "            # Save validation after each epoch\n",
    "            if early_stop:\n",
    "                if not max_mrr or metrics[\"mrr\"] > max_mrr or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] > max_top1) or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] == max_top1 and max_seq_acc > metrics[\"sequence_accuracy\"]):\n",
    "                    max_mrr = metrics[\"mrr\"]\n",
    "                    max_top1 = metrics[\"top_k_accuracy\"][1]\n",
    "                    max_seq_acc = metrics[\"sequence_accuracy\"]\n",
    "                    best_epoch = epoch\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience and epoch >= 150: #avoids \"too\" early stopping\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "        if early_stop:\n",
    "            return history, best_epoch\n",
    "        else:\n",
    "            # when training on the whole dataset, or when continuing train after loading the model, I want to store the new updated history always\n",
    "            self.history.extend(history)\n",
    "            self.save_experiment(self.history)\n",
    "            epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(self.history)\n",
    "\n",
    "            self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies)\n",
    "\n",
    "            return\n",
    "\n",
    "    def k_fold_cross_validation(self, train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x,\n",
    "                                test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, num_epochs, num_folds=2, verbose=True):\n",
    "        assert num_folds >= 2, \"Cannot use less than 2 folds\"\n",
    "\n",
    "        self.log_specifics()\n",
    "        folds_history = []\n",
    "        best_epochs = []\n",
    "        if verbose:\n",
    "            print(f\"Performing {num_folds}-fold cross validation\")\n",
    "\n",
    "        for k in range(num_folds):\n",
    "            # define validation set indices\n",
    "            val_start = int(k * (train_x.shape[0] // num_folds))\n",
    "            val_end = min(int((k+1) * (train_x.shape[0] // num_folds)), train_x.shape[0])\n",
    "\n",
    "            # validation set\n",
    "            val_x = train_x[val_start:val_end]\n",
    "            val_y = train_y[val_start:val_end]\n",
    "            val_lengths = train_lengths[val_start:val_end]\n",
    "            word_emb_val_x = word_emb_train_x[val_start:val_end]\n",
    "            phon_emb_val_x = phon_emb_train_x[val_start:val_end]\n",
    "\n",
    "            # train set\n",
    "            train_x_fold = torch.cat([train_x[:val_start], train_x[val_end:]])\n",
    "            train_y_fold = torch.cat([train_y[:val_start], train_y[val_end:]])\n",
    "            train_lengths_fold = torch.cat([train_lengths[:val_start], train_lengths[val_end:]])\n",
    "            word_emb_train_x_fold = torch.cat([word_emb_train_x[:val_start], word_emb_train_x[val_end:]])\n",
    "            phon_emb_train_x_fold = torch.cat([phon_emb_train_x[:val_start], phon_emb_train_x[val_end:]])\n",
    "\n",
    "            # reinitialize the model, criterion, optimizer\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "            # training returns history as (avg_loss, val_metrics) pair\n",
    "            history, best_epoch = self.train(train_x_fold, train_y_fold, train_lengths_fold, word_emb_train_x_fold, phon_emb_train_x_fold,\n",
    "                                             val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x, num_epochs=num_epochs, early_stop=True)\n",
    "\n",
    "            folds_history.append(history) #self.history is dictioanry < num_kfold : (avg_loss, val_metrics) >\n",
    "            best_epochs.append(best_epoch)\n",
    "            if verbose:\n",
    "                print(f\"Trained fold {k+1} for {len(history)} epochs\")\n",
    "                print(f\"Best results at epoch {best_epoch+1}: {history[best_epoch]}\")\n",
    "\n",
    "        aggregated_history, folds_considered = aggregate_k_fold_metrics(folds_history) #aggregated history is [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2, ...]\n",
    "        epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(aggregated_history) #sorted lists of different aggregated metrics per epoch\n",
    "\n",
    "        self.save_experiment(aggregated_history, folds_involved=folds_considered)\n",
    "        self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=folds_considered)\n",
    "\n",
    "        avg_epochs = int(round(np.mean(best_epochs))) + 1 #get average best number of epochs to train the model on the whole dataset\n",
    "        if verbose:\n",
    "            print(f\"Average best number of epochs: {avg_epochs} (max: {np.max(best_epochs) + 1}, min: {np.min(best_epochs) + 1})\")\n",
    "\n",
    "        self.initialize_from_hyperparams()\n",
    "        #We do training on the whole training set and test data directly on the test set\n",
    "        self.train(train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x,\n",
    "                   test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, num_epochs=avg_epochs, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i_N_Z-Mghu7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment class for composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Z7spxWUnWTtT"
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, hyperparams, save_dir=prefix_path, experiment_name=None, verbose=True, use_gpu=True):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.hyperparams = hyperparams\n",
    "        if not self.hyperparams is None:\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "        if not experiment_name:\n",
    "            self.experiment_name = \"experiment\"\n",
    "        else:\n",
    "            self.experiment_name = experiment_name\n",
    "\n",
    "        results_dir = os.path.join(save_dir, \"Models Results\")\n",
    "        #create directory if it doesn't exist\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        self.save_dir = os.path.join(results_dir, self.experiment_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.model_path = os.path.join(self.save_dir, \"model.pt\")\n",
    "        self.optimizer_path = os.path.join(self.save_dir, \"optimizer.pt\")\n",
    "        self.hypers_path = os.path.join(self.save_dir, \"hypers.json\")\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def initialize_from_hyperparams(self):\n",
    "        self.batch_size = self.hyperparams[\"batch_size\"]\n",
    "        self.model = Composition(self.hyperparams, self.device)\n",
    "        self.optimizer = initialize_optimizer(self.hyperparams, self.model)\n",
    "        self.criterion = initialize_criterion(self.hyperparams)\n",
    "\n",
    "    #If validating, it saves all aggregated validation metrics and loss per epoch, else it saves only testing metrics\n",
    "    def save_experiment(self, history, folds_involved=None):\n",
    "\n",
    "        is_validation = folds_involved is not None\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_{'val' if is_validation else 'test'}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        # create dictionary to save JSON file\n",
    "        for epoch, (loss, metrics_dictionary) in enumerate(history):\n",
    "            metadata[f\"epoch_{epoch+1}\"] = {\n",
    "                \"loss\": loss,\n",
    "                \"extra_info\": metrics_dictionary\n",
    "            }\n",
    "            # add also number of folds from which the information derives\n",
    "            if is_validation:\n",
    "                metadata[f\"epoch_{epoch+1}\"][\"folds aggregated\"] = folds_involved[epoch]\n",
    "\n",
    "\n",
    "        if is_validation:\n",
    "            # this code has been put here to avoid saving the metadata twice\n",
    "            with open(self.hypers_path, \"w\") as f:\n",
    "                json.dump(self.hyperparams, f, indent=4)\n",
    "\n",
    "        else:\n",
    "            #save model only when training on whole dataset and testing on test set\n",
    "            torch.save(self.model.state_dict(), self.model_path)\n",
    "            torch.save(self.optimizer.state_dict(), self.optimizer_path)\n",
    "\n",
    "        #Dump metadata (test metrics or validation aggregated data)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metrics(self, epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Plots and saves loss, sequence accuracy, MRR, and Top-K accuracy metrics.\n",
    "        Ensures points are connected by a default-colored line, while points themselves\n",
    "        are color-coded based on the number of folds involved in training (if available).\n",
    "        \"\"\"\n",
    "        is_validation = folds_involved is not None\n",
    "\n",
    "        if is_validation:\n",
    "            # If folds are provided, create a colormap\n",
    "            unique_folds = sorted(set(folds_involved))\n",
    "            colormap = plt.colormaps[\"tab20\"]  # Switch to 'tab10' for distinct colors\n",
    "            fold_color_map = {fold: colormap(i+2) for i, fold in enumerate(unique_folds)} # i+2 skips too bluish colors for better visualization\n",
    "\n",
    "\n",
    "        def plot_metric(metric_values, ylabel, title, filename):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "            # Plot a continuous gray line through all points\n",
    "            plt.plot(epochs, metric_values, zorder=1)\n",
    "\n",
    "            # Overlay scatter points: Use fold colors if available, otherwise default color\n",
    "            if is_validation:\n",
    "                for fold in unique_folds[::-1]:\n",
    "                    fold_epochs = [e for e, f in zip(epochs, folds_involved) if f == fold]\n",
    "                    fold_values = [v for v, f in zip(metric_values, folds_involved) if f == fold]\n",
    "                    plt.scatter(fold_epochs, fold_values, color=fold_color_map[fold], label=f\"{fold} Folds\", s=50, zorder=2)\n",
    "            else:\n",
    "                plt.scatter(epochs, metric_values, s=50, zorder=2)\n",
    "\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"{title} ({'Validation' if is_validation else 'Test'})\")\n",
    "            if is_validation:\n",
    "                plt.legend(title=\"Folds Used\", loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(self.save_dir, filename))\n",
    "            if verbose:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "        # Plot Loss\n",
    "        plot_metric(losses, \"Loss\", \"Loss Over Epochs\", f\"loss_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Sequence Accuracy\n",
    "        plot_metric(sequence_accuracies, \"Sequence Accuracy\", \"Sequence Accuracy Over Epochs\",\n",
    "                    f\"sequence_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot MRR\n",
    "        plot_metric(mrrs, \"MRR\", \"Mean Reciprocal Rank Over Epochs\",\n",
    "                    f\"mrr_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Top-K Accuracy\n",
    "        for k, values in top_k_accuracies.items():\n",
    "            plot_metric(values, \"Accuracy\", f\"Top-{k} Accuracy Over Epochs\",\n",
    "                        f\"top_{k}_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "    def load_experiment(self):\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_test.json\")\n",
    "        assert os.path.exists(self.hypers_path) and os.path.exists(self.model_path) and os.path.exists(self.optimizer_path) and os.path.exists(metadata_path), \" No saved experiment found!\"\n",
    "\n",
    "        with open(self.hypers_path, \"r\") as f:\n",
    "            self.hyperparams = json.load(f)\n",
    "\n",
    "        #load model with hyperparams and optimizer\n",
    "        self.initialize_from_hyperparams()\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))\n",
    "        self.optimizer.load_state_dict(torch.load(self.optimizer_path, map_location=self.device, weights_only=True))\n",
    "\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        i = 0\n",
    "        # recover history of past iterations\n",
    "        while f\"epoch_{i+1}\" in metadata:\n",
    "            loss = metadata[f\"epoch_{i+1}\"][\"loss\"]\n",
    "            extra_info = metadata[f\"epoch_{i+1}\"][\"extra_info\"]\n",
    "            # converts integers key of top_k accuracies back to int (by default they become strings once in the JSON file)\n",
    "            extra_info = convert_keys(extra_info)\n",
    "            self.history.append((loss, extra_info))\n",
    "            i += 1\n",
    "\n",
    "    def test(self, test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, verbose=True):\n",
    "        if verbose:\n",
    "            print(\"TESTING MODEL!\")\n",
    "            num_sequences = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        test_loader = create_loader_with_sp(test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_corrects = 0\n",
    "            metrics = None\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp in test_loader:\n",
    "                batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp = \\\n",
    "                batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), batch_x_distorted.to(self.device), \\\n",
    "                batch_x_sp.to(self.device), batch_lengths_sp.to(self.device)\n",
    "\n",
    "                model_output = self.model(batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp)\n",
    "\n",
    "                if verbose:\n",
    "                    # Reconstruct sequences (input, ground truth, generated output)\n",
    "                    inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "\n",
    "                    for (input, gt_seq, out_seq) in zip(inp, gt, out):\n",
    "                        print(f\"TEST n. {num_sequences + 1}\")\n",
    "                        print(f\"Input: {input}\")\n",
    "                        print(f\"Ground Truth: {gt_seq}\")\n",
    "                        print(f\"Output: {out_seq}\")\n",
    "                        num_sequences += 1\n",
    "\n",
    "                # Collect batch metrics (e.g., accuracy, loss, etc.)\n",
    "                batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "                metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "            # Compute final metrics\n",
    "            metrics = compute_final_metrics(metrics)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"TEST FINISHED\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    #Train model, returns history if early stopping is true, else it returns empty tuple\n",
    "    def train(self, train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "              val_x, val_y, val_lengths, x_distorted_val_x, x_sp_val_x, batch_lengths_sp_val_x,\n",
    "              num_epochs=10, verbose=False, early_stop=False, max_mrr_init=None, max_top1_init=None, max_seq_acc_init=None, best_epoch_init=None):\n",
    "\n",
    "        train_loader = create_loader_with_sp(train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x, self.batch_size)\n",
    "\n",
    "        if early_stop:\n",
    "            max_mrr = max_mrr_init\n",
    "            max_top1 = max_top1_init\n",
    "            max_seq_acc = max_seq_acc_init\n",
    "            best_epoch = best_epoch_init\n",
    "            patience = 50\n",
    "            counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp in train_loader:\n",
    "                batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp = \\\n",
    "                batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), batch_x_distorted.to(self.device), \\\n",
    "                batch_x_sp.to(self.device), batch_lengths_sp.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                model_output = self.model(batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp)\n",
    "                output = model_output.view(-1, model_output.shape[-1])\n",
    "                batch_y_reshaped = batch_y.view(-1)\n",
    "\n",
    "\n",
    "                if self.hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "\n",
    "                elif self.hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "                    loss = self.criterion(model_output, batch_y, batch_lengths)\n",
    "\n",
    "                if self.hyperparams[\"ranked_based_loss_contribution\"]:\n",
    "                    # Step 1: Find the maximum indices along the sequence dimension for each batch (along axis 1)\n",
    "                    max_indices = batch_x.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                    # Step 2: Compute the previous index for the comparison (i, j-1)\n",
    "                    # Ensure that j-1 is not less than 0 (i.e., for the first token in the sequence)\n",
    "                    prev_indices = max_indices - 1  if (self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\") else max_indices # Clamp to ensure no negative indices\n",
    "                    # Step 3: Select the corresponding predictions and ground truth for the prev indices (i, j-1)\n",
    "                    # Get the log probabilities from model_output at (i, j-1)\n",
    "                    predictions_at_prev_idx = model_output[torch.arange(model_output.size(0)), prev_indices, :]  # (batch_size, vocab_size)\n",
    "                    # Turn log probabilities into probabilities\n",
    "                    predictions_at_prev_idx = torch.exp(predictions_at_prev_idx)\n",
    "                    # gt values\n",
    "                    target_at_prev_idx = batch_y[torch.arange(batch_y.size(0)), prev_indices] # (batch_size)\n",
    "                    # Step 2: Extract log probability of the correct token\n",
    "                    correct_probs = predictions_at_prev_idx.gather(dim=-1, index=target_at_prev_idx.unsqueeze(-1)).squeeze(-1)  # (batch_size,)\n",
    "                    # Step 3: Find the max predicted token probability\n",
    "                    max_predicted_probs, _ = predictions_at_prev_idx.max(dim=-1)  # (batch_size,)\n",
    "\n",
    "                    # Step 4: Compute the loss term comparing max-predicted probability vs ground-truth probability\n",
    "                    prob_diff_loss = (max_predicted_probs - correct_probs).mean()  # Penalize if gt prob is lower than max prob\n",
    "\n",
    "                    loss_contrib = prob_diff_loss * self.hyperparams[\"ranked_based_loss_contribution_weight\"]\n",
    "\n",
    "                    loss += loss_contrib\n",
    "\n",
    "                # Compute loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            metrics = self.test(val_x, val_y, val_lengths, x_distorted_val_x, x_sp_val_x, batch_lengths_sp_val_x, verbose=False)  # validation\n",
    "            history.append((avg_loss, metrics))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{len(self.history) + epoch+1}/{len(self.history) + num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "                print(\"Validation metrics are: \", metrics)\n",
    "\n",
    "            # Save validation after each epoch\n",
    "            if early_stop:\n",
    "                if not max_mrr or metrics[\"mrr\"] > max_mrr or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] > max_top1) or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] == max_top1 and max_seq_acc > metrics[\"sequence_accuracy\"]):\n",
    "                    max_mrr = metrics[\"mrr\"]\n",
    "                    max_top1 = metrics[\"top_k_accuracy\"][1]\n",
    "                    max_seq_acc = metrics[\"sequence_accuracy\"]\n",
    "                    best_epoch = epoch\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience and epoch >= 150: #avoids \"too\" early stopping\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "        if early_stop:\n",
    "            return history, best_epoch\n",
    "        else:\n",
    "            # when training on the whole dataset, or when continuing train after loading the model, I want to store the new updated history always\n",
    "            self.history.extend(history)\n",
    "            self.save_experiment(self.history)\n",
    "            epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(self.history)\n",
    "\n",
    "            self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies)\n",
    "\n",
    "            return\n",
    "\n",
    "    def k_fold_cross_validation(self, train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "                                test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, num_epochs, num_folds=2, verbose=True):\n",
    "        assert num_folds >= 2, \"Cannot use less than 2 folds\"\n",
    "\n",
    "        folds_history = []\n",
    "        best_epochs = []\n",
    "        if verbose:\n",
    "            print(f\"Performing {num_folds}-fold cross validation\")\n",
    "\n",
    "        for k in range(num_folds):\n",
    "            # define validation set indices\n",
    "            val_start = int(k * (train_x.shape[0] // num_folds))\n",
    "            val_end = min(int((k+1) * (train_x.shape[0] // num_folds)), train_x.shape[0])\n",
    "\n",
    "            # validation set\n",
    "            val_x = train_x[val_start:val_end]\n",
    "            val_y = train_y[val_start:val_end]\n",
    "            val_lengths = train_lengths[val_start:val_end]\n",
    "            val_x_distorted = x_distorted_train_x[val_start:val_end]\n",
    "            val_x_sp = x_sp_train_x[val_start:val_end]\n",
    "            val_lengths_sp = lengths_sp_train_x[val_start:val_end]\n",
    "\n",
    "            # train set\n",
    "            train_x_fold = torch.cat([train_x[:val_start], train_x[val_end:]])\n",
    "            train_y_fold = torch.cat([train_y[:val_start], train_y[val_end:]])\n",
    "            train_lengths_fold = torch.cat([train_lengths[:val_start], train_lengths[val_end:]])\n",
    "            x_distorted_train_x_fold = torch.cat([x_distorted_train_x[:val_start], x_distorted_train_x[val_end:]])\n",
    "            x_sp_train_x_fold = torch.cat([x_sp_train_x[:val_start], x_sp_train_x[val_end:]])\n",
    "            lengths_sp_train_x_fold = torch.cat([lengths_sp_train_x[:val_start], lengths_sp_train_x[val_end:]])\n",
    "\n",
    "            # reinitialize the model, criterion, optimizer\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "            # training returns history as (avg_loss, val_metrics) pair\n",
    "            history, best_epoch = self.train(train_x_fold, train_y_fold, train_lengths_fold, x_distorted_train_x_fold, x_sp_train_x_fold, lengths_sp_train_x_fold,\n",
    "                                             val_x, val_y, val_lengths, val_x_distorted, val_x_sp, val_lengths_sp, num_epochs=num_epochs, early_stop=True)\n",
    "\n",
    "            folds_history.append(history) #self.history is dictioanry < num_kfold : (avg_loss, val_metrics) >\n",
    "            best_epochs.append(best_epoch)\n",
    "            if verbose:\n",
    "                print(f\"Trained fold {k+1} for {len(history)} epochs\")\n",
    "                print(f\"Best results at epoch {best_epoch+1}: {history[best_epoch]}\")\n",
    "\n",
    "        aggregated_history, folds_considered = aggregate_k_fold_metrics(folds_history) #aggregated history is [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2, ...]\n",
    "        epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(aggregated_history) #sorted lists of different aggregated metrics per epoch\n",
    "\n",
    "        self.save_experiment(aggregated_history, folds_involved=folds_considered)\n",
    "        self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=folds_considered)\n",
    "\n",
    "        avg_epochs = int(round(np.mean(best_epochs))) + 1 #get average best number of epochs to train the model on the whole dataset\n",
    "        if verbose:\n",
    "            print(f\"Average best number of epochs: {avg_epochs} (max: {np.max(best_epochs) + 1}, min: {np.min(best_epochs) + 1})\")\n",
    "\n",
    "        self.initialize_from_hyperparams()\n",
    "        #We do training on the whole training set and test data directly on the test set\n",
    "        self.train(train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "                   test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, num_epochs=avg_epochs, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D32xz_Oz_SU0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnG6AVEGyLcp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 3.0989\n",
      "Validation metrics are:  {'sequence_accuracy': 0.0, 'mrr': 0.22803581962258992, 'top_k_accuracy': {20: 0.6856368563685636, 15: 0.5772357723577236, 10: 0.4444444444444444, 5: 0.3062330623306233, 1: 0.12466124661246612}}\n",
      "Epoch [2/200], Loss: 1.5737\n",
      "Validation metrics are:  {'sequence_accuracy': 0.04065040650406504, 'mrr': 0.282040369578462, 'top_k_accuracy': {20: 0.6964769647696477, 15: 0.6287262872628726, 10: 0.5040650406504065, 5: 0.37398373983739835, 1: 0.16260162601626016}}\n",
      "Epoch [3/200], Loss: 0.7199\n",
      "Validation metrics are:  {'sequence_accuracy': 0.13550135501355012, 'mrr': 0.34395015144253915, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6504065040650406, 10: 0.5799457994579946, 5: 0.4742547425474255, 1: 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"BRNNTextInfillerSE_batchsize64_{EMBEDDINGS_DIM}\" + (\"withKLLoss\" if hyperparams[\"criterion\"] == 'NLLWithKL' else \"\")\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "#experiment.k_fold_cross_validation(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "#                                   test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x,\n",
    "#                                   #num_epochs=2, num_folds=2) # debug\n",
    "#                                   num_epochs=hyperparams[\"num_epochs\"], num_folds=hyperparams[\"num_folds\"])\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                   test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=200, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "CnvKLPhGhy51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.2501\n",
      "Validation metrics are:  {'sequence_accuracy': 0.08672086720867209, 'mrr': 0.31904275837959123, 'top_k_accuracy': {20: 0.7046070460704607, 15: 0.6422764227642277, 10: 0.5311653116531165, 5: 0.41192411924119243, 1: 0.2140921409214092}}\n",
      "Epoch [2/200], Loss: 0.7124\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2222222222222222, 'mrr': 0.4020212776449771, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6233062330623306, 5: 0.5040650406504065, 1: 0.28184281842818426}}\n",
      "Epoch [3/200], Loss: 0.4714\n",
      "Validation metrics are:  {'sequence_accuracy': 0.23577235772357724, 'mrr': 0.38481559260073317, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.6964769647696477, 10: 0.6043360433604336, 5: 0.4905149051490515, 1: 0.27371273712737126}}\n",
      "Epoch [4/200], Loss: 0.3641\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2899728997289973, 'mrr': 0.4231922749361374, 'top_k_accuracy': {20: 0.7804878048780488, 15: 0.7371273712737128, 10: 0.6531165311653117, 5: 0.5121951219512195, 1: 0.3224932249322493}}\n",
      "Epoch [5/200], Loss: 0.3264\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.41904985801835193, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7181571815718157, 10: 0.6504065040650406, 5: 0.5230352303523035, 1: 0.3116531165311653}}\n",
      "Epoch [6/200], Loss: 0.3087\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3062330623306233, 'mrr': 0.4384755217432945, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7289972899728997, 10: 0.6639566395663956, 5: 0.5528455284552846, 1: 0.3252032520325203}}\n",
      "Epoch [7/200], Loss: 0.2909\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.42854488133868895, 'top_k_accuracy': {20: 0.7831978319783198, 15: 0.7289972899728997, 10: 0.6802168021680217, 5: 0.5392953929539296, 1: 0.3170731707317073}}\n",
      "Epoch [8/200], Loss: 0.2779\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.43381710493166575, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7181571815718157, 10: 0.6531165311653117, 5: 0.5420054200542005, 1: 0.32791327913279134}}\n",
      "Epoch [9/200], Loss: 0.2676\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.43141428319480996, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7181571815718157, 10: 0.6476964769647696, 5: 0.5447154471544715, 1: 0.33062330623306235}}\n",
      "Epoch [10/200], Loss: 0.2605\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4299303589378406, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5311653116531165, 1: 0.33604336043360433}}\n",
      "Epoch [11/200], Loss: 0.2547\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4502514238098019, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.3523035230352303}}\n",
      "Epoch [17/200], Loss: 0.2272\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4408817856531943, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5447154471544715, 1: 0.34146341463414637}}\n",
      "Epoch [18/200], Loss: 0.2299\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.42493274351745364, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7073170731707317, 10: 0.6504065040650406, 5: 0.5474254742547425, 1: 0.3224932249322493}}\n",
      "Epoch [19/200], Loss: 0.2302\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44239373635989315, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5474254742547425, 1: 0.34959349593495936}}\n",
      "Epoch [20/200], Loss: 0.2272\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4330922676789172, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7262872628726287, 10: 0.6639566395663956, 5: 0.5257452574525745, 1: 0.33604336043360433}}\n",
      "Epoch [21/200], Loss: 0.2268\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4389570475581161, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7018970189701897, 10: 0.6449864498644986, 5: 0.5392953929539296, 1: 0.33875338753387535}}\n",
      "Epoch [22/200], Loss: 0.2297\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.43754467143950937, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6178861788617886, 5: 0.5392953929539296, 1: 0.34688346883468835}}\n",
      "Epoch [23/200], Loss: 0.2346\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34959349593495936, 'mrr': 0.4490792392327166, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7046070460704607, 10: 0.6341463414634146, 5: 0.5284552845528455, 1: 0.3604336043360434}}\n",
      "Epoch [24/200], Loss: 0.2348\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.44161653245479887, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7262872628726287, 10: 0.6476964769647696, 5: 0.5257452574525745, 1: 0.3523035230352303}}\n",
      "Epoch [25/200], Loss: 0.2334\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.4409051377123803, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7018970189701897, 10: 0.6368563685636857, 5: 0.5284552845528455, 1: 0.3523035230352303}}\n",
      "Epoch [26/200], Loss: 0.2338\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4446920078986743, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7208672086720868, 10: 0.6504065040650406, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [27/200], Loss: 0.2334\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45814145179599447, 'top_k_accuracy': {20: 0.7886178861788617, 15: 0.7371273712737128, 10: 0.6666666666666666, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [28/200], Loss: 0.2319\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4396215168828754, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7127371273712737, 10: 0.6395663956639567, 5: 0.5501355013550135, 1: 0.34688346883468835}}\n",
      "Epoch [29/200], Loss: 0.2345\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33604336043360433, 'mrr': 0.4442130823744582, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7398373983739838, 10: 0.6693766937669376, 5: 0.5420054200542005, 1: 0.34417344173441733}}\n",
      "Epoch [30/200], Loss: 0.2357\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4452027973028896, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5284552845528455, 1: 0.34959349593495936}}\n",
      "Epoch [31/200], Loss: 0.2401\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44124415615695767, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [32/200], Loss: 0.2431\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4477954054104646, 'top_k_accuracy': {20: 0.7886178861788617, 15: 0.7262872628726287, 10: 0.6639566395663956, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [33/200], Loss: 0.2441\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.45011128594701433, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7208672086720868, 10: 0.6531165311653117, 5: 0.5365853658536586, 1: 0.3604336043360434}}\n",
      "Epoch [34/200], Loss: 0.2451\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45286317287110134, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [35/200], Loss: 0.2453\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4423560957555479, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7100271002710027, 10: 0.6504065040650406, 5: 0.5555555555555556, 1: 0.34959349593495936}}\n",
      "Epoch [36/200], Loss: 0.2470\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45334476248108935, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [37/200], Loss: 0.2499\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4460265414168531, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7262872628726287, 10: 0.6666666666666666, 5: 0.5528455284552846, 1: 0.34688346883468835}}\n",
      "Epoch [38/200], Loss: 0.2504\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.43901860228162615, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6964769647696477, 10: 0.6260162601626016, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [39/200], Loss: 0.2504\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.452644474950744, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7018970189701897, 10: 0.6341463414634146, 5: 0.5338753387533876, 1: 0.36314363143631434}}\n",
      "Epoch [40/200], Loss: 0.2542\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.43640420432453064, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.6937669376693767, 10: 0.5989159891598916, 5: 0.5230352303523035, 1: 0.33875338753387535}}\n",
      "Epoch [41/200], Loss: 0.2603\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.44110185096654086, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5338753387533876, 1: 0.34688346883468835}}\n",
      "Epoch [42/200], Loss: 0.2622\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44047549643510403, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6775067750677507, 10: 0.6287262872628726, 5: 0.5121951219512195, 1: 0.35501355013550134}}\n",
      "Epoch [43/200], Loss: 0.2620\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.449508976574878, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5338753387533876, 1: 0.35772357723577236}}\n",
      "Epoch [44/200], Loss: 0.2621\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4484498859537215, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.3523035230352303}}\n",
      "Epoch [45/200], Loss: 0.2651\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.43941892780092834, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7018970189701897, 10: 0.6422764227642277, 5: 0.5392953929539296, 1: 0.33875338753387535}}\n",
      "Epoch [46/200], Loss: 0.2668\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4452757534773057, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5555555555555556, 1: 0.34417344173441733}}\n",
      "Epoch [47/200], Loss: 0.2643\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4486637152077359, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5609756097560976, 1: 0.34959349593495936}}\n",
      "Epoch [48/200], Loss: 0.2663\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.449016139927795, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6666666666666666, 10: 0.6151761517615176, 5: 0.5420054200542005, 1: 0.3604336043360434}}\n",
      "Epoch [49/200], Loss: 0.2670\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4481805459179039, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6991869918699187, 10: 0.6531165311653117, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [50/200], Loss: 0.2689\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4366992891495713, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7181571815718157, 10: 0.6504065040650406, 5: 0.5582655826558266, 1: 0.3333333333333333}}\n",
      "Epoch [51/200], Loss: 0.2723\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4474115782360186, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5474254742547425, 1: 0.34688346883468835}}\n",
      "Epoch [52/200], Loss: 0.2727\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3333333333333333, 'mrr': 0.4438913995680929, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.6964769647696477, 10: 0.6531165311653117, 5: 0.5636856368563685, 1: 0.34688346883468835}}\n",
      "Epoch [53/200], Loss: 0.2747\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4381303767875076, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7181571815718157, 10: 0.6368563685636857, 5: 0.5203252032520326, 1: 0.34959349593495936}}\n",
      "Epoch [54/200], Loss: 0.2719\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4410541193914098, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6964769647696477, 10: 0.6314363143631436, 5: 0.5230352303523035, 1: 0.34688346883468835}}\n",
      "Epoch [55/200], Loss: 0.2728\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3333333333333333, 'mrr': 0.43711066633080103, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5528455284552846, 1: 0.33875338753387535}}\n",
      "Epoch [56/200], Loss: 0.2737\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4606568166090221, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5636856368563685, 1: 0.36314363143631434}}\n",
      "Epoch [57/200], Loss: 0.2747\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.46428464795586216, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7127371273712737, 10: 0.6476964769647696, 5: 0.5501355013550135, 1: 0.3794037940379404}}\n",
      "Epoch [58/200], Loss: 0.2746\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45392196809838803, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6910569105691057, 10: 0.6531165311653117, 5: 0.5447154471544715, 1: 0.36314363143631434}}\n",
      "Epoch [59/200], Loss: 0.2763\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.44358372349089303, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6585365853658537, 5: 0.5528455284552846, 1: 0.34417344173441733}}\n",
      "Epoch [60/200], Loss: 0.2790\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44491182382556294, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5311653116531165, 1: 0.35501355013550134}}\n",
      "Epoch [61/200], Loss: 0.2791\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4572562344599672, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6422764227642277, 5: 0.5555555555555556, 1: 0.3604336043360434}}\n",
      "Epoch [62/200], Loss: 0.2791\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46526350596481103, 'top_k_accuracy': {20: 0.7289972899728997, 15: 0.7046070460704607, 10: 0.6612466124661247, 5: 0.5772357723577236, 1: 0.3712737127371274}}\n",
      "Epoch [63/200], Loss: 0.2797\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4401641481085608, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5365853658536586, 1: 0.34146341463414637}}\n",
      "Epoch [64/200], Loss: 0.2800\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44850164770702444, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7127371273712737, 10: 0.6558265582655827, 5: 0.5609756097560976, 1: 0.34417344173441733}}\n",
      "Epoch [65/200], Loss: 0.2825\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4528797399439216, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5420054200542005, 1: 0.36314363143631434}}\n",
      "Epoch [66/200], Loss: 0.2836\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45373171562891795, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.35772357723577236}}\n",
      "Epoch [67/200], Loss: 0.2863\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4451531519174158, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6205962059620597, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [68/200], Loss: 0.2869\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4426973680273395, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7018970189701897, 10: 0.6395663956639567, 5: 0.5284552845528455, 1: 0.34959349593495936}}\n",
      "Epoch [69/200], Loss: 0.2864\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.4396047498419022, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6856368563685636, 10: 0.6205962059620597, 5: 0.5149051490514905, 1: 0.3604336043360434}}\n",
      "Epoch [70/200], Loss: 0.2877\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4373748197212212, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6856368563685636, 10: 0.6233062330623306, 5: 0.5365853658536586, 1: 0.34146341463414637}}\n",
      "Epoch [71/200], Loss: 0.2878\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.4584843485084116, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7127371273712737, 10: 0.6476964769647696, 5: 0.5447154471544715, 1: 0.3712737127371274}}\n",
      "Epoch [72/200], Loss: 0.2885\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45561422564017223, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5528455284552846, 1: 0.3604336043360434}}\n",
      "Epoch [73/200], Loss: 0.2873\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45438714194353935, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5501355013550135, 1: 0.35772357723577236}}\n",
      "Epoch [74/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.45863579775099655, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6314363143631436, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [75/200], Loss: 0.2877\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45045470850641695, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7018970189701897, 10: 0.6395663956639567, 5: 0.5284552845528455, 1: 0.3604336043360434}}\n",
      "Epoch [76/200], Loss: 0.2872\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.463616740990832, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.7100271002710027, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.37398373983739835}}\n",
      "Epoch [77/200], Loss: 0.2872\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4552566541036695, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6829268292682927, 10: 0.6341463414634146, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [78/200], Loss: 0.2873\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46503824893407786, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7154471544715447, 10: 0.6368563685636857, 5: 0.5365853658536586, 1: 0.3712737127371274}}\n",
      "Epoch [79/200], Loss: 0.2880\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4595507122530175, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5420054200542005, 1: 0.3712737127371274}}\n",
      "Epoch [80/200], Loss: 0.2887\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4536546027872326, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [81/200], Loss: 0.2880\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46497671317504596, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7018970189701897, 10: 0.6314363143631436, 5: 0.5474254742547425, 1: 0.37398373983739835}}\n",
      "Epoch [82/200], Loss: 0.2883\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45506001980256044, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.6964769647696477, 10: 0.6368563685636857, 5: 0.5338753387533876, 1: 0.36585365853658536}}\n",
      "Epoch [83/200], Loss: 0.2885\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46425725042611443, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5582655826558266, 1: 0.3712737127371274}}\n",
      "Epoch [84/200], Loss: 0.2893\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.464261832241127, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.6964769647696477, 10: 0.6368563685636857, 5: 0.5501355013550135, 1: 0.37669376693766937}}\n",
      "Epoch [85/200], Loss: 0.2893\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46487404582557545, 'top_k_accuracy': {20: 0.7831978319783198, 15: 0.7235772357723578, 10: 0.6368563685636857, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [86/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45550676301921034, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5474254742547425, 1: 0.36585365853658536}}\n",
      "Epoch [87/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.46156870541996725, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7073170731707317, 10: 0.6422764227642277, 5: 0.5230352303523035, 1: 0.38482384823848237}}\n",
      "Epoch [88/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44514453457340963, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6856368563685636, 10: 0.6368563685636857, 5: 0.5420054200542005, 1: 0.3523035230352303}}\n",
      "Epoch [89/200], Loss: 0.2937\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4243036601968377, 'top_k_accuracy': {20: 0.7127371273712737, 15: 0.6802168021680217, 10: 0.5989159891598916, 5: 0.5230352303523035, 1: 0.3170731707317073}}\n",
      "Epoch [90/200], Loss: 0.3204\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4361627359819182, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6747967479674797, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.33604336043360433}}\n",
      "Epoch [91/200], Loss: 0.3121\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.44779816331151745, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7208672086720868, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.3523035230352303}}\n",
      "Epoch [92/200], Loss: 0.2966\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45627573825756723, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [93/200], Loss: 0.2930\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.45935212946851817, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7235772357723578, 10: 0.6504065040650406, 5: 0.5392953929539296, 1: 0.37398373983739835}}\n",
      "Epoch [94/200], Loss: 0.2932\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46067941686686964, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7073170731707317, 10: 0.6504065040650406, 5: 0.5501355013550135, 1: 0.3685636856368564}}\n",
      "Epoch [95/200], Loss: 0.2932\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.46344854907719035, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7073170731707317, 10: 0.6395663956639567, 5: 0.5392953929539296, 1: 0.37669376693766937}}\n",
      "Epoch [96/200], Loss: 0.2924\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4629573532746183, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7100271002710027, 10: 0.6585365853658537, 5: 0.5528455284552846, 1: 0.3685636856368564}}\n",
      "Epoch [97/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45741754570535686, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [98/200], Loss: 0.2919\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.4626203173051575, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7344173441734417, 10: 0.6639566395663956, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [99/200], Loss: 0.2912\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46282311770693574, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7127371273712737, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [100/200], Loss: 0.2914\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4545235178827429, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7262872628726287, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.3604336043360434}}\n",
      "Epoch [101/200], Loss: 0.2922\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46440914766041475, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [102/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.46293717260450395, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7154471544715447, 10: 0.6531165311653117, 5: 0.5528455284552846, 1: 0.3794037940379404}}\n",
      "Epoch [103/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4655027486309156, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7073170731707317, 10: 0.6558265582655827, 5: 0.5555555555555556, 1: 0.37669376693766937}}\n",
      "Epoch [104/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4679704090489491, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7181571815718157, 10: 0.6531165311653117, 5: 0.5555555555555556, 1: 0.37669376693766937}}\n",
      "Epoch [105/200], Loss: 0.2913\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4709616137340876, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6937669376693767, 10: 0.6449864498644986, 5: 0.5555555555555556, 1: 0.3821138211382114}}\n",
      "Epoch [106/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4692364382590738, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7100271002710027, 10: 0.6449864498644986, 5: 0.5636856368563685, 1: 0.3821138211382114}}\n",
      "Epoch [107/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.45868482212765954, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.7208672086720868, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [108/200], Loss: 0.2916\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4669940725107325, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7100271002710027, 10: 0.6287262872628726, 5: 0.5528455284552846, 1: 0.3821138211382114}}\n",
      "Epoch [109/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46389051981235874, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7208672086720868, 10: 0.6395663956639567, 5: 0.5447154471544715, 1: 0.37398373983739835}}\n",
      "Epoch [110/200], Loss: 0.2907\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46302960206462485, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7235772357723578, 10: 0.6395663956639567, 5: 0.5636856368563685, 1: 0.3712737127371274}}\n",
      "Epoch [111/200], Loss: 0.2921\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4438622952702845, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7154471544715447, 10: 0.6368563685636857, 5: 0.5528455284552846, 1: 0.34688346883468835}}\n",
      "Epoch [112/200], Loss: 0.2974\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.42428110964705973, 'top_k_accuracy': {20: 0.7127371273712737, 15: 0.6639566395663956, 10: 0.6205962059620597, 5: 0.5474254742547425, 1: 0.3224932249322493}}\n",
      "Epoch [113/200], Loss: 0.3090\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4333492918492197, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5176151761517616, 1: 0.34417344173441733}}\n",
      "Epoch [114/200], Loss: 0.2971\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4304097228474006, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6775067750677507, 10: 0.6260162601626016, 5: 0.5420054200542005, 1: 0.3333333333333333}}\n",
      "Epoch [115/200], Loss: 0.2915\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45418556922915765, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6314363143631436, 5: 0.5447154471544715, 1: 0.36314363143631434}}\n",
      "Epoch [116/200], Loss: 0.2887\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.4642191947086885, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7100271002710027, 10: 0.6476964769647696, 5: 0.5691056910569106, 1: 0.37398373983739835}}\n",
      "Epoch [117/200], Loss: 0.2875\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4621405100683741, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [118/200], Loss: 0.2864\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45220769156532115, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6937669376693767, 10: 0.6476964769647696, 5: 0.5609756097560976, 1: 0.3604336043360434}}\n",
      "Epoch [119/200], Loss: 0.2866\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.45951090020996443, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6287262872628726, 5: 0.5420054200542005, 1: 0.37669376693766937}}\n",
      "Epoch [120/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3875338753387534, 'mrr': 0.4697874963119685, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5474254742547425, 1: 0.3902439024390244}}\n",
      "Epoch [121/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.46262998219135415, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6775067750677507, 10: 0.6205962059620597, 5: 0.5311653116531165, 1: 0.37669376693766937}}\n",
      "Epoch [122/200], Loss: 0.2853\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46456358220770927, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6205962059620597, 5: 0.5555555555555556, 1: 0.37398373983739835}}\n",
      "Epoch [123/200], Loss: 0.2849\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4686444223665247, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6341463414634146, 5: 0.5528455284552846, 1: 0.38482384823848237}}\n",
      "Epoch [124/200], Loss: 0.2838\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.4603247611457416, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5420054200542005, 1: 0.37669376693766937}}\n",
      "Epoch [125/200], Loss: 0.2828\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.4638946410339267, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.3794037940379404}}\n",
      "Epoch [126/200], Loss: 0.2797\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4672819080623839, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6910569105691057, 10: 0.6422764227642277, 5: 0.5691056910569106, 1: 0.37398373983739835}}\n",
      "Epoch [127/200], Loss: 0.2767\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.46616985211755035, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7046070460704607, 10: 0.6341463414634146, 5: 0.5447154471544715, 1: 0.38482384823848237}}\n",
      "Epoch [128/200], Loss: 0.2764\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.4664670351309928, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6829268292682927, 10: 0.6233062330623306, 5: 0.5582655826558266, 1: 0.38482384823848237}}\n",
      "Epoch [129/200], Loss: 0.2763\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46488401065221024, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.6991869918699187, 10: 0.6422764227642277, 5: 0.5582655826558266, 1: 0.3794037940379404}}\n",
      "Epoch [130/200], Loss: 0.2800\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.44296364220539863, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6016260162601627, 5: 0.5257452574525745, 1: 0.35772357723577236}}\n",
      "Epoch [131/200], Loss: 0.2884\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4395108553273903, 'top_k_accuracy': {20: 0.7100271002710027, 15: 0.6693766937669376, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.34417344173441733}}\n",
      "Epoch [132/200], Loss: 0.2892\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.45401009035844564, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7127371273712737, 10: 0.6720867208672087, 5: 0.5691056910569106, 1: 0.34959349593495936}}\n",
      "Epoch [133/200], Loss: 0.2815\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46860616878282196, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7127371273712737, 10: 0.6504065040650406, 5: 0.5555555555555556, 1: 0.37398373983739835}}\n",
      "Epoch [134/200], Loss: 0.2776\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.47180780040387127, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7154471544715447, 10: 0.6476964769647696, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [135/200], Loss: 0.2768\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4513649949671808, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7018970189701897, 10: 0.6531165311653117, 5: 0.5501355013550135, 1: 0.35501355013550134}}\n",
      "Epoch [136/200], Loss: 0.2767\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4633528139564754, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5474254742547425, 1: 0.3712737127371274}}\n",
      "Epoch [137/200], Loss: 0.2754\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4607757988006485, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [138/200], Loss: 0.2757\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.4742670112331803, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5528455284552846, 1: 0.38482384823848237}}\n",
      "Epoch [139/200], Loss: 0.2749\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4617676379444355, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6964769647696477, 10: 0.6504065040650406, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [140/200], Loss: 0.2739\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.45608613125988895, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [141/200], Loss: 0.2743\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4578526941737763, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5582655826558266, 1: 0.3604336043360434}}\n",
      "Epoch [142/200], Loss: 0.2726\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4650080511439994, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7127371273712737, 10: 0.6368563685636857, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [143/200], Loss: 0.2736\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4673115251377811, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7018970189701897, 10: 0.6422764227642277, 5: 0.5582655826558266, 1: 0.37398373983739835}}\n",
      "Epoch [144/200], Loss: 0.2720\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4508073467454164, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5420054200542005, 1: 0.34688346883468835}}\n",
      "Epoch [145/200], Loss: 0.2707\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.45666022075335433, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6991869918699187, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.3604336043360434}}\n",
      "Epoch [146/200], Loss: 0.2693\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4618287668991878, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5311653116531165, 1: 0.37398373983739835}}\n",
      "Epoch [147/200], Loss: 0.2710\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45872231292325333, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6964769647696477, 10: 0.6504065040650406, 5: 0.5447154471544715, 1: 0.36585365853658536}}\n",
      "Epoch [148/200], Loss: 0.2697\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.465488202109093, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5718157181571816, 1: 0.3685636856368564}}\n",
      "Epoch [149/200], Loss: 0.2682\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2899728997289973, 'mrr': 0.46119451090406105, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [150/200], Loss: 0.2691\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4570452947297486, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6910569105691057, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [151/200], Loss: 0.2701\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.4350934257337224, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6747967479674797, 10: 0.6178861788617886, 5: 0.5338753387533876, 1: 0.33875338753387535}}\n",
      "Epoch [152/200], Loss: 0.2713\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2764227642276423, 'mrr': 0.4315611328371936, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6802168021680217, 10: 0.6178861788617886, 5: 0.5338753387533876, 1: 0.3252032520325203}}\n",
      "Epoch [153/200], Loss: 0.2758\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2764227642276423, 'mrr': 0.4283285937888515, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5284552845528455, 1: 0.3252032520325203}}\n",
      "Epoch [154/200], Loss: 0.2753\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2845528455284553, 'mrr': 0.43556799386748896, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6910569105691057, 10: 0.6124661246612466, 5: 0.5311653116531165, 1: 0.33604336043360433}}\n",
      "Epoch [155/200], Loss: 0.2686\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.44686943872661816, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6856368563685636, 10: 0.6395663956639567, 5: 0.5392953929539296, 1: 0.3523035230352303}}\n",
      "Epoch [156/200], Loss: 0.2676\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.45431181341914245, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6720867208672087, 10: 0.6205962059620597, 5: 0.5284552845528455, 1: 0.36585365853658536}}\n",
      "Epoch [157/200], Loss: 0.2668\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45118108871436546, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7018970189701897, 10: 0.6476964769647696, 5: 0.5555555555555556, 1: 0.3523035230352303}}\n",
      "Epoch [158/200], Loss: 0.2658\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4514607856612823, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6937669376693767, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.35772357723577236}}\n",
      "Epoch [159/200], Loss: 0.2653\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4581246851127785, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6883468834688347, 10: 0.6341463414634146, 5: 0.5609756097560976, 1: 0.36314363143631434}}\n",
      "Epoch [160/200], Loss: 0.2637\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4629825841148014, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6964769647696477, 10: 0.6341463414634146, 5: 0.5555555555555556, 1: 0.36585365853658536}}\n",
      "Epoch [161/200], Loss: 0.2629\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.4569612357172983, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5718157181571816, 1: 0.34959349593495936}}\n",
      "Epoch [162/200], Loss: 0.2625\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4555257288805811, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6937669376693767, 10: 0.6287262872628726, 5: 0.5718157181571816, 1: 0.3604336043360434}}\n",
      "Epoch [163/200], Loss: 0.2642\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2953929539295393, 'mrr': 0.45448114352109614, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5663956639566395, 1: 0.3523035230352303}}\n",
      "Epoch [164/200], Loss: 0.2628\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45326810381729443, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6829268292682927, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3523035230352303}}\n",
      "Epoch [165/200], Loss: 0.2610\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4455058333573052, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6775067750677507, 10: 0.6314363143631436, 5: 0.5365853658536586, 1: 0.34688346883468835}}\n",
      "Epoch [166/200], Loss: 0.2592\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.45204368551373797, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6883468834688347, 10: 0.6558265582655827, 5: 0.5745257452574526, 1: 0.3523035230352303}}\n",
      "Epoch [167/200], Loss: 0.2601\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3062330623306233, 'mrr': 0.45728987024667106, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6585365853658537, 5: 0.5663956639566395, 1: 0.35772357723577236}}\n",
      "Epoch [168/200], Loss: 0.2580\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4588113846907427, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7046070460704607, 10: 0.6368563685636857, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [169/200], Loss: 0.2589\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4575990584940088, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6205962059620597, 5: 0.5663956639566395, 1: 0.36314363143631434}}\n",
      "Epoch [170/200], Loss: 0.2570\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.46010217098814354, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.3685636856368564}}\n",
      "Epoch [171/200], Loss: 0.2578\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4523370801301299, 'top_k_accuracy': {20: 0.7289972899728997, 15: 0.6937669376693767, 10: 0.6422764227642277, 5: 0.5718157181571816, 1: 0.34959349593495936}}\n",
      "Epoch [172/200], Loss: 0.2568\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4573129476884344, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6910569105691057, 10: 0.6504065040650406, 5: 0.5691056910569106, 1: 0.35772357723577236}}\n",
      "Epoch [173/200], Loss: 0.2559\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.45414588991376037, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7018970189701897, 10: 0.6504065040650406, 5: 0.5582655826558266, 1: 0.35501355013550134}}\n",
      "Epoch [174/200], Loss: 0.2573\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4584533363719266, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6693766937669376, 10: 0.6043360433604336, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [175/200], Loss: 0.2595\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.44906217080383043, 'top_k_accuracy': {20: 0.7046070460704607, 15: 0.6504065040650406, 10: 0.6124661246612466, 5: 0.5365853658536586, 1: 0.3685636856368564}}\n",
      "Epoch [176/200], Loss: 0.2670\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2953929539295393, 'mrr': 0.44753093117474324, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7018970189701897, 10: 0.6476964769647696, 5: 0.5582655826558266, 1: 0.34688346883468835}}\n",
      "Epoch [177/200], Loss: 0.2597\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4478107320474357, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6883468834688347, 10: 0.6287262872628726, 5: 0.5501355013550135, 1: 0.35501355013550134}}\n",
      "Epoch [178/200], Loss: 0.2565\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.45319018899575875, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6937669376693767, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [179/200], Loss: 0.2558\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.44773484580582446, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6775067750677507, 10: 0.6097560975609756, 5: 0.5203252032520326, 1: 0.35772357723577236}}\n",
      "Epoch [180/200], Loss: 0.2558\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4509427198381609, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.3523035230352303}}\n",
      "Epoch [181/200], Loss: 0.2544\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4611879229417281, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6422764227642277, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [182/200], Loss: 0.2538\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45721623401693046, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6964769647696477, 10: 0.6233062330623306, 5: 0.5474254742547425, 1: 0.3604336043360434}}\n",
      "Epoch [183/200], Loss: 0.2536\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.462072019520301, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3685636856368564}}\n",
      "Epoch [184/200], Loss: 0.2550\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4608661583339003, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5365853658536586, 1: 0.3712737127371274}}\n",
      "Epoch [185/200], Loss: 0.2534\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4595884521932542, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6829268292682927, 10: 0.6233062330623306, 5: 0.5528455284552846, 1: 0.3712737127371274}}\n",
      "Epoch [186/200], Loss: 0.2532\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4710635127838755, 'top_k_accuracy': {20: 0.7154471544715447, 15: 0.6937669376693767, 10: 0.6449864498644986, 5: 0.5582655826558266, 1: 0.38482384823848237}}\n",
      "Epoch [187/200], Loss: 0.2518\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45529030683135435, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5555555555555556, 1: 0.3604336043360434}}\n",
      "Epoch [188/200], Loss: 0.2501\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4655191689219137, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7073170731707317, 10: 0.6368563685636857, 5: 0.5528455284552846, 1: 0.37398373983739835}}\n",
      "Epoch [189/200], Loss: 0.2513\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4596683667629578, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6856368563685636, 10: 0.6368563685636857, 5: 0.5392953929539296, 1: 0.3685636856368564}}\n",
      "Epoch [190/200], Loss: 0.2505\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.47090591368530355, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6856368563685636, 10: 0.6314363143631436, 5: 0.5609756097560976, 1: 0.38482384823848237}}\n",
      "Epoch [191/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.46470029908336796, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5636856368563685, 1: 0.37398373983739835}}\n",
      "Epoch [192/200], Loss: 0.2502\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4663084351444116, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6856368563685636, 10: 0.6395663956639567, 5: 0.5609756097560976, 1: 0.37669376693766937}}\n",
      "Epoch [193/200], Loss: 0.2506\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4628840132018393, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7208672086720868, 10: 0.6558265582655827, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [194/200], Loss: 0.2509\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46567543903605446, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6883468834688347, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3794037940379404}}\n",
      "Epoch [195/200], Loss: 0.2528\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.46772718678932385, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6802168021680217, 10: 0.6341463414634146, 5: 0.5745257452574526, 1: 0.37398373983739835}}\n",
      "Epoch [196/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46481936632351417, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6422764227642277, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [197/200], Loss: 0.2495\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3252032520325203, 'mrr': 0.4657506171634889, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5528455284552846, 1: 0.37669376693766937}}\n",
      "Epoch [198/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4726922084028645, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6964769647696477, 10: 0.6260162601626016, 5: 0.5636856368563685, 1: 0.38482384823848237}}\n",
      "Epoch [199/200], Loss: 0.2486\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46677484676798425, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6829268292682927, 10: 0.6368563685636857, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [200/200], Loss: 0.2487\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.46909402036662573, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5636856368563685, 1: 0.3821138211382114}}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"CompositionTextInfiller_{EMBEDDINGS_DIM}\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "#experiment.k_fold_cross_validation(train_x, train_y, train_lengths, distorted_train_x, spm_train_x, spm_lengths_train,\n",
    "#                                   test_x, test_y, test_lengths, distorted_test_x, spm_test_x, spm_lengths_test,\n",
    "                                   #num_epochs=2, num_folds=2) # debug\n",
    "#                                   num_epochs=hyperparams[\"num_epochs\"], num_folds=hyperparams[\"num_folds\"])\n",
    "experiment.train(train_x, train_y, train_lengths, distorted_train_x, spm_train_x, spm_lengths_train,\n",
    "                   test_x, test_y, test_lengths, distorted_test_x, spm_test_x, spm_lengths_test, num_epochs=200, early_stop=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLTJjMsidp9J"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"test_learnable_composition\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "experiment.test(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOdVE6BAXxjt"
   },
   "outputs": [],
   "source": [
    "train_x.shape, test_x.shape, len(seq_train_x), len(seq_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv1BBzcHZhiY"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"brnn_se_with_kl\"\n",
    "\n",
    "experiment = Experiment(None, experiment_name=experiment_name)\n",
    "experiment.load_experiment()\n",
    "\n",
    "# train additional epochs\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                 test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=1, verbose=True)\n",
    "\n",
    "# test the model\n",
    "experiment.test(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3vW2Hg2Wqsx"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"test_gemini\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                 test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=50, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OUp0TYTMdHC"
   },
   "outputs": [],
   "source": [
    "def predict_linear_b_token(input_sequence, token_predictions):\n",
    "    \"\"\"\n",
    "    Creates a prompt for an LLM to predict the next token in a Linear B sequence.\n",
    "\n",
    "    Args:\n",
    "        input_sequence (str): The partial Linear B sequence\n",
    "        token_predictions (list): List of tuples (token, probability) of possible next tokens\n",
    "\n",
    "    Returns:\n",
    "        str: XML-formatted prompt for an LLM\n",
    "    \"\"\"\n",
    "    # Extract the context (everything before the missing token)\n",
    "    context = input_sequence.split(\"?\")[0].strip()\n",
    "\n",
    "    # Extract the top 20 token predictions\n",
    "    top_predictions = token_predictions[:20]\n",
    "\n",
    "    # Create the XML-formatted prompt\n",
    "    prompt = f\"\"\"<linear_b_prediction>\n",
    "  <context>{context}</context>\n",
    "  <possible_tokens>\n",
    "\"\"\"\n",
    "\n",
    "    # Add each candidate token with its probability\n",
    "    for token, probability in top_predictions:\n",
    "        prompt += f\"    <token probability=\\\"{probability:.4f}\\\">{token}</token>\\n\"\n",
    "\n",
    "    prompt += \"\"\"  </possible_tokens>\n",
    "  <instructions>\n",
    "    You are helping to predict the next token in a Linear B sequence. Linear B was used to write Mycenaean Greek around 1400-1200 BCE.\n",
    "\n",
    "    Based ONLY on:\n",
    "    1. The context provided above\n",
    "    2. Your understanding of Linear B syllabic patterns\n",
    "    3. Transliterate input into ancient greek to get more context information from the incomplete tokens sequence\n",
    "    4. Only select a syllabogram, do not choose logograms or numbers\n",
    "    5. Space is both word separator and token separator\n",
    "\n",
    "    Do not choose simply the most probable next token from the list of possible tokens: consider all possible options as possible replacements.\n",
    "    Compare all possible options, also using Chain of Thoughts.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - Do NOT use knowledge of actual Linear B texts or fragments\n",
    "    - Do NOT access or reference any external sources\n",
    "    - Do NOT explain your reasoning\n",
    "    - Respond ONLY with the single token you predict, nothing else\n",
    "  </instructions>\n",
    "</linear_b_prediction>\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Sample data from the example\n",
    "input_sequence = \"SOS a ja me no   e re pa te ja pi   ka ru ?   *220   1   ta ra nu   a ja me no   e re pa te ja pi   ka ru pi   *220   1\"\n",
    "token_predictions = [('ro', 0.14198297262191772), ('no', 0.12454625219106674), ('si', 0.09385930001735687), ('te', 0.08285455405712128), ('tya', 0.04424416646361351), ('mo', 0.03839782625436783), ('wo', 0.029186910018324852), ('de', 0.027668841183185577), ('qo', 0.027635548263788223), ('jo', 0.02637413516640663), ('i', 0.025987736880779266), ('ko', 0.01489566545933485), ('NUM', 0.013128552585840225), ('se', 0.010486927814781666), ('ta', 0.010461430996656418), ('we', 0.010421977378427982), ('o', 0.01035719271749258), ('qe', 0.010198087431490421), ('sa', 0.010087325237691402), ('na', 0.009956919588148594)]\n",
    "prompt = predict_linear_b_token(input_sequence, token_predictions)\n",
    "print(prompt)\n",
    "GOOGLE_API_KEY=\"[redacted]\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "response = gemini_model.generate_content(prompt)\n",
    "pred = response.text.strip()\n",
    "print(output_mapping[pred], inv_map[output_mapping[pred]], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzFv607n-yhh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def evaluate_model(model, test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, batch_size, hyperparams, verbose=False):\n",
    "    model.eval()\n",
    "    test_loader = create_loader(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, batch_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        total_corrects = 0\n",
    "        gemini_correct = 0\n",
    "        metrics = None\n",
    "        num_sequences = 0\n",
    "\n",
    "        for batch_x, batch_y, batch_lengths, word_emb, phon_emb in test_loader:\n",
    "            batch_x, batch_y, batch_lengths, word_emb, phon_emb = (\n",
    "                batch_x.to(device), batch_y.to(device), batch_lengths.to(device),\n",
    "                word_emb.to(device), phon_emb.to(device)\n",
    "            )\n",
    "\n",
    "            if hyperparams[\"model\"] in {\"BRNNTextInfiller\", \"FCNTextInfiller\", \"ConvTextInfiller\", \"ConvTextInfillerRNN\", \"BiConvTextInfiller\"}:\n",
    "                model_output = model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "            elif hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                model_output = model.generate(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "            if verbose:\n",
    "                inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "                for i, (input_seq, gt_seq, out_seq) in enumerate(zip(inp, gt, out)):\n",
    "                    print(f\"TEST n. {num_sequences + 1}\")\n",
    "                    print(f\"Input: {input_seq}\")\n",
    "                    print(f\"Ground Truth: {gt_seq}\")\n",
    "                    print(f\"Output: {out_seq}\")\n",
    "\n",
    "                    # Find the index j where batch_x has the max value\n",
    "                    j = torch.argmax(batch_x[i]).item()\n",
    "                    if j > 0:\n",
    "                        top_preds = torch.topk(model_output[i, j-1], 20)\n",
    "                        indices = top_preds.indices.tolist()\n",
    "                        tokens = [inv_map[i] for i in indices]\n",
    "                        probabilities = torch.exp(top_preds.values).tolist()  # Convert log probabilities to actual probabilities\n",
    "                        print(f\"Top 20 predictions for token index {j-1}: {list(zip(tokens, probabilities))}\")\n",
    "                        prompt = predict_linear_b_token(input_seq, list(zip(tokens, probabilities)))\n",
    "                        gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "                        response = gemini_model.generate_content(prompt)\n",
    "                        pred = response.text.strip()\n",
    "                        print(pred, inv_map[batch_y[i, j-1].item()])\n",
    "                        if output_mapping[pred] ==  batch_y[i, j-1]:\n",
    "                            gemini_correct += 1\n",
    "                        time.sleep(5) # Avoid Gemini blocking\n",
    "                    num_sequences += 1\n",
    "\n",
    "            batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "            metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "        metrics = compute_final_metrics(metrics)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"TEST FINISHED\")\n",
    "        print(f\"GEMINI CORRECT: {gemini_correct} / {num_sequences}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "evaluate_model(experiment.model, test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, experiment.batch_size, experiment.hyperparams, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ASfy4nGDQ3ZA",
    "Bzm3_1MAa915",
    "-Ca5j5iS0qat",
    "-CM84L7pqoj0",
    "PArR6E3SElyC",
    "NMzlfXi5ZBXi",
    "VByz0YR6e0wl",
    "LIa0iX34efK-",
    "-qubc_YVfkZ2",
    "THVkwEMUesYG",
    "Qf9HEKgIa0aQ",
    "EAqaPwXaAHNE",
    "QKVUfb_Ks5wT",
    "sLgdHwYEJqXm",
    "orwm78mS7Q9c",
    "7elfqidiFx9z",
    "TYrpaOIu2_lQ",
    "M5yEV0dkIah7",
    "6labFmFVLx0k",
    "mtjQWbLwLviQ",
    "pwkouvbR5sUg",
    "gTN0seM0ZeMS",
    "nKGKQINpiOjf",
    "tO0zgTaV3uiY",
    "xQtEmYC3pEcV",
    "obuLB4rasOdi",
    "nI3Ul8KCs1hv",
    "vYmgonqwtAOv",
    "vu9ifIcOvJ34",
    "6NNyKMajwQ0z",
    "7SDXTbmly8wm",
    "Sb1XkEqWk5uG",
    "3VzJkbCQlBDu",
    "r45qGiVOlEQJ",
    "mvAvEDpclHQR",
    "DS9G66palJnO",
    "8dU1iu5QQH-O",
    "JaTDgRdkQBOl",
    "Ez_-5lhQkHBD",
    "Gisi9B669oNQ",
    "dk0tqyCsxdsb",
    "ZJyuylWgxX58",
    "a4g4oCsQPwJy",
    "xW33X_JoQi-y",
    "_3o3VulFSA6a",
    "7O_exCdBTx85",
    "J6her1ZpgLaS",
    "fYpB3YBqVla4",
    "O5-PzMfDGWWA",
    "PiF0WQ1UYbxS",
    "q2mD8AD1X89w",
    "TYEuAhe5ahUG",
    "B0nPynQ3avWF",
    "ZUW1774iLcV3",
    "9eZVPovYqqVT",
    "0_JGjDSQQcRJ",
    "jqSJBCKr3g3Q",
    "IWLFe1YAClPM",
    "MeshvmB2jRqU",
    "9svVyxHZ8MRi",
    "hWT6GrV--8Rh",
    "yNMMKBIcIZwf",
    "9p3GM8JLqaQy",
    "Er8DJ-hIxCZg",
    "12nNn5MGQkXY",
    "RFEKd7wnGIGp",
    "uiDssS3AtLjc",
    "rdENsy1li13g",
    "0guTkDZ7nI2C",
    "O_lchCPdO3lK",
    "ItxZ4t0HgO0n",
    "D8tKyaEpgZlB",
    "-i_N_Z-Mghu7",
    "D32xz_Oz_SU0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
