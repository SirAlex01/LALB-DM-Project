{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX2Kz1jxRkkt",
    "outputId": "c3e515bc-98a8-485a-a09f-9de23aae41a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.local/lib/python3.10/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.local/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.local/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install sentencepiece\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from datetime import datetime\n",
    "#from google.colab import drive\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from enum import Enum\n",
    "import logging\n",
    "import google.generativeai as genai\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "prefix_path = './DMPROJECT/'\n",
    "USE_BRNN = True #set to true if using BRNN, false for transformer\n",
    "USE_TRANSFORMER = False\n",
    "USE_FCN = False\n",
    "USE_CONV = False\n",
    "USE_COMPOSITION = False\n",
    "USE_BICONV = False\n",
    "#assert sum([USE_BRNN, USE_TRANSFORMER, USE_FCN, USE_CONV]) == 1, \"Exactly one model must be selected (set one flag to True).\"\n",
    "\n",
    "EMBEDDINGS_DIM = 256 #Embeddings size for phonetic embeddings, word embeddings and transformer token embeddings\n",
    "\n",
    "#Default brnn is use EOS true and use SOS false\n",
    "USE_SOS_IN_X = False if USE_TRANSFORMER else True\n",
    "USE_EOS_IN_X = False\n",
    "\n",
    "#Default brnn is use EOS true and use SOS false\n",
    "USE_SOS_IN_Y = False\n",
    "USE_EOS_IN_Y = True\n",
    "\n",
    "USE_ONE_HOT_ENCODING = False #One-Hot instead of tokenization+embedding\n",
    "EMBED_FOR_GRAPH = False\n",
    "\n",
    "#Embeddings usage\n",
    "USE_WORD_EMB = False\n",
    "\n",
    "#Type\n",
    "USE_TFIDF = False #use tfidf features\n",
    "USE_NLLWithKL = False #use NLLLwithKL loss\n",
    "USE_RANKED_LOSS_CONTRIB = False #use ranking in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYZPD2IHUgaK",
    "outputId": "793df5fa-92a1-4ce0-908d-0ce28c2ddcd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 17 for reproducibility!\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 17  # You can change this to any fixed number\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure deterministic behavior for CUDA (if using GPU)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n",
    "\n",
    "# Ensure reproducibility in cuDNN operations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Disabling this may slow down training but ensures reproducibility\n",
    "np.random.seed(SEED)                      # NumPy RNG\n",
    "torch.manual_seed(SEED) \n",
    "#PLEASE BE DETERMINISTICK\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "# Set environment variables for deterministic behavior\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # Ensures CUDA operations remain deterministic in newer PyTorch versions\n",
    "\n",
    "print(f\"Random seed set to {SEED} for reproducibility!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "Current GPU device: 0\n",
      "GPU name: NVIDIA A100-SXM4-80GB\n",
      "Total CPU RAM: 2015.66 GB\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Total GPU RAM: 79.15 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current GPU device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Total RAM in GB\n",
    "total_memory = psutil.virtual_memory().total / (1024 ** 3)\n",
    "print(f\"Total CPU RAM: {total_memory:.2f} GB\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(gpu_index).total_memory / (1024 ** 3)\n",
    "\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Total GPU RAM: {total_gpu_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA GPU not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "87I6xVc5gWP6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMfIbj4oyexW"
   },
   "outputs": [],
   "source": [
    "if EMBED_FOR_GRAPH:\n",
    "    USE_SOS_IN_X = False\n",
    "    USE_EOS_IN_Y = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASfy4nGDQ3ZA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDA LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "euP9xvH7Qwbc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DMPROJECT/signs_LB.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#LB\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_lb \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/DMPROJECT/signs_LB.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DMPROJECT/signs_LB.csv'"
     ]
    }
   ],
   "source": [
    "#LB\n",
    "df_lb = pd.read_csv('/content/drive/MyDrive/DMPROJECT/signs_LB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er725Pt6SZbO"
   },
   "outputs": [],
   "source": [
    "signs = df_lb['sign'].unique()\n",
    "signs = sorted(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wyo8XZDoRoM"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each sign in df_lb\n",
    "sign_counts_lb = df_lb[\"sign\"].value_counts()\n",
    "sign_counts_lb = sign_counts_lb.head(70)\n",
    "\n",
    "# Create a bar plot for the signs in df_lb\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "sns.barplot(x=sign_counts_lb.index, y=sign_counts_lb.values, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Signs in df_lb\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzm3_1MAa915"
   },
   "source": [
    "##EDA LA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79aiz3ZEdaSg"
   },
   "outputs": [],
   "source": [
    "df_la = pd.read_csv('/content/drive/MyDrive/DMPROJECT/signs.csv')\n",
    "df_la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXcXXpnVdt9t"
   },
   "outputs": [],
   "source": [
    "signs = df_la['sign'].unique()\n",
    "signs = sorted(signs)\n",
    "len(signs)\n",
    "signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tywOPnT7ersE"
   },
   "outputs": [],
   "source": [
    "df_sites = pd.read_csv('/content/drive/MyDrive/DMPROJECT/sites_data.csv')\n",
    "df_la_sites = df_sites.merge(df_la, on=\"document_name\")\n",
    "\n",
    "sites_to_remove = [\"Syme\", \"Pyrgos\", \"Psykhro\", \"Papoura\", \"Mycenae\", \"Melos\", \"Kythera\", \"Kea\", \"Haghios Stephanos\", \"Gournia\"]\n",
    "df_la_sites.drop(df_la_sites[df_la_sites[\"site\"].isin(sites_to_remove)].index, inplace=True)\n",
    "df_la_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHE8AfGMfVEM"
   },
   "outputs": [],
   "source": [
    "#Distribution signs per site\n",
    "freq_df = df_la_sites.groupby(\"site\")[\"sign\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Display frequency table\n",
    "unique_sites = df_la_sites[\"site\"].unique()\n",
    "\n",
    "for site in unique_sites:\n",
    "    site_df = df_la_sites[df_la_sites[\"site\"] == site]  # Filter data for the site\n",
    "    sign_counts = site_df[\"sign\"].value_counts()  # Count occurrences\n",
    "\n",
    "    # Get top 20 most frequent signs\n",
    "    top_20_sign_counts = sign_counts.head(50)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "    sns.barplot(x=top_20_sign_counts.index, y=top_20_sign_counts.values, palette=\"viridis\")\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel(\"Sign\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Top 20 Most Frequent Signs for Site {site}\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG6hZRhvk6nC"
   },
   "outputs": [],
   "source": [
    "site_signs = {site: set(df_la_sites[df_la_sites[\"site\"] == site][\"sign\"].unique()) for site in unique_sites}\n",
    "\n",
    "# Find the intersection of signs across all sites\n",
    "common_signs = set.intersection(*site_signs.values())\n",
    "\n",
    "# Filter the dataframe to only include rows with common signs\n",
    "df_common_signs = df_la_sites[df_la_sites[\"sign\"].isin(common_signs)]\n",
    "\n",
    "# Count the occurrences of these common signs\n",
    "common_sign_counts = df_common_signs[\"sign\"].value_counts()\n",
    "\n",
    "# Create a bar plot for the common signs\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size\n",
    "sns.barplot(x=common_sign_counts.index, y=common_sign_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Common Signs Across All Sites\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfyjS2kjpmd9"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each sign in df_lb\n",
    "sign_counts_la = df_la[\"sign\"].value_counts()\n",
    "sign_counts_la = sign_counts_la.head(70)\n",
    "\n",
    "# Create a bar plot for the signs in df_lb\n",
    "plt.figure(figsize=(16, 8))  # Adjust the figure size\n",
    "sns.barplot(x=sign_counts_la.index, y=sign_counts_la.values, palette=\"viridis\")\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Sign\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Signs in df_lb\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ca5j5iS0qat",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDA LB LA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxsWQtvr0tRn"
   },
   "outputs": [],
   "source": [
    "SEQUENCES_LA = os.path.join(prefix_path, \"sequences.csv\")\n",
    "SEQUENCES_LB = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_LA = pd.read_csv(SEQUENCES_LA)\n",
    "df_LB = pd.read_csv(SEQUENCES_LB)\n",
    "\n",
    "\n",
    "def compare(sequence_LA, sequence_LB):\n",
    "    sequence_LA = sequence_LA.split(\"-\")\n",
    "    sequence_LB = sequence_LB.split(\"-\")\n",
    "    i, j = 0, 0\n",
    "\n",
    "    score = 1\n",
    "    correspondences = 0\n",
    "\n",
    "    while i < len(sequence_LA) and j < len(sequence_LB):\n",
    "        if sequence_LA[i] == sequence_LB[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "            correspondences += 1\n",
    "        elif sequence_LA[i][0] == sequence_LB[j][0]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "            score -= 0.1\n",
    "            #correspondences += 1\n",
    "        else:\n",
    "            score -= 0.5\n",
    "            if len(sequence_LA) < len(sequence_LB):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "    score -= 0.1 * (len(sequence_LA)-i + len(sequence_LB)-j)\n",
    "    return score, correspondences\n",
    "\n",
    "dic = defaultdict(lambda: set())\n",
    "\n",
    "# Iterate over each sequence in LA\n",
    "for index_LA, row_LA in df_LA.iterrows():\n",
    "    if row_LA[\"complete\"] and row_LA[\"sequence\"] not in dic:\n",
    "        for index_LB, row_LB in df_LB.iterrows():\n",
    "            if abs(row_LA[\"length\"] - row_LB[\"length\"]) <= 2:\n",
    "                score, correspondences = compare(row_LA['sequence'], row_LB['sequence'])\n",
    "                if score > 0.5 and correspondences >= 2:\n",
    "                    dic[row_LA['sequence']].add((row_LB['sequence'], score, row_LB[\"complete\"]))\n",
    "                    print(row_LA['sequence'], row_LB['sequence'], score, correspondences)\n",
    "\n",
    "for key, value in dic.items():\n",
    "    dic[key] = sorted(list(value), key=lambda x: x[1], reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO-UEBvqCLfd"
   },
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CM84L7pqoj0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ALEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7abbP_0CqtI2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "SEQUENCES_LA = os.path.join(prefix_path, \"sequences.csv\")\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "seq_la = pd.read_csv(SEQUENCES_LA)\n",
    "\n",
    "# Print total number of sequences\n",
    "total_sequences = len(seq_la)\n",
    "print(f\"Total sequences: {total_sequences}\")\n",
    "\n",
    "complete_sequences = seq_la['complete'].sum()\n",
    "print(f\"Sequences which are marked as complete: {complete_sequences}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSxP_IEKMxhN"
   },
   "outputs": [],
   "source": [
    "seq_la.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6abhniMLNsL0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def reconstruct_LA_documents():\n",
    "    # Load the signs.csv file\n",
    "    signs_path = os.path.join(prefix_path, \"signs.csv\")\n",
    "    sequences_path = os.path.join(prefix_path, \"sequences.csv\")\n",
    "    signs_df = pd.read_csv(signs_path)\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    # Dictionary to hold reconstructed documents\n",
    "    documents = {}\n",
    "\n",
    "    # Group by 'document_name' and sort by 'sign_number'\n",
    "    grouped_signs = signs_df.sort_values(by=['document_name', 'sign_number']).groupby('document_name')\n",
    "    grouped_sequences = sequences_df.sort_values(by=['document_name', 'sequence_number']).groupby('document_name')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_signs:\n",
    "        documents[document_name] = []\n",
    "\n",
    "        # gather all sequences from the appropriate document\n",
    "        seq = grouped_sequences.get_group(document_name)['sequence'].tolist() if document_name in grouped_sequences.groups else []\n",
    "        for i in range(len(seq)):\n",
    "            seq[i] = seq[i].split(\"-\")\n",
    "\n",
    "        # Initialize variables\n",
    "        seq_idx = 0\n",
    "        seq_len = 0\n",
    "        stack = []\n",
    "        for sign in group['sign']:\n",
    "            # if the sequence is actually complete, we can put it in the document collection\n",
    "            if len(seq) > 0 and sign == seq[seq_idx][seq_len] and seq_len == len(seq[seq_idx])-1:\n",
    "                seq_len = 0\n",
    "                stack = []\n",
    "                documents[document_name].append(\"-\".join(seq[seq_idx]))\n",
    "                if seq_idx < len(seq)-1:\n",
    "                    seq_idx += 1\n",
    "            # we are completing the current sequence\n",
    "            elif len(seq) > 0 and sign == seq[seq_idx][seq_len]:\n",
    "                seq_len += 1\n",
    "                stack.append(sign)\n",
    "            # the symbol does not correspond to the current sequence, so we push the new symbol (and all those recognized from the end of the previous sequence)\n",
    "            else:\n",
    "                stack.append(sign)\n",
    "                documents[document_name] += stack\n",
    "                stack = []\n",
    "                seq_len = 0\n",
    "\n",
    "        documents[document_name] = \" \".join(documents[document_name])\n",
    "    return documents\n",
    "reconstruct_LA_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmQXz9kaLwe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = reconstruct_LA_documents()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with improved settings\n",
    "\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # Convert the document dictionary into a list of strings (concatenated words per document)\n",
    "    document_names = list(documents.keys())\n",
    "    corpus = list(documents.values())\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        token_pattern=r\"[a-zA-Z\\-\\*\\d]+\",  # Keep special characters including ?\n",
    "        min_df=1,  # Include all words\n",
    "        sublinear_tf=True,  # Prevent extreme weighting\n",
    "        lowercase=False  # Keep original case\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert TF-IDF matrix to a DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=document_names, columns=feature_names)\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "# Example dictionary (your processed documents)\n",
    "documents = reconstruct_LA_documents()\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidf_result = compute_tfidf(documents)\n",
    "\n",
    "# Display the results\n",
    "print(tfidf_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PArR6E3SElyC"
   },
   "source": [
    "## UGARITIC/LB TASK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7HM6nHTzj0w",
    "outputId": "a30635f1-5499-4263-b83c-7740085987ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: colorlog in ./.local/lib/python3.10/site-packages (6.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorlog\n",
    "!pip install ortools\n",
    "!pip install enlighten\n",
    "!pip install treelib\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v60bw2n1bA3a"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from functools import wraps\n",
    "from inspect import signature\n",
    "from collections import defaultdict, namedtuple\n",
    "from collections.abc import Callable, Hashable\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from prettytable import PrettyTable as pt\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from colorlog import TTYColoredFormatter\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "import torch.sparse\n",
    "import itertools\n",
    "from ortools.graph.python import min_cost_flow as mcf\n",
    "import types\n",
    "import enlighten\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from treelib import Tree\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from rapidfuzz.process import cdist\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMzlfXi5ZBXi"
   },
   "source": [
    "## To RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VByz0YR6e0wl"
   },
   "source": [
    "### ArgLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0EpEUWAe3Il"
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_properties(*names):\n",
    "    \"\"\"Add properties as methods to classes.\"\"\"\n",
    "\n",
    "    def decorator(cls):\n",
    "        for name in names:\n",
    "            # NOTE The keyword is necessary.\n",
    "            setattr(cls, name, property(lambda self, name=name: getattr(self, f'_{name}')))\n",
    "        return cls\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def set_properties(*names, **values):\n",
    "    \"\"\"Set a private variable and use it as a property for an instance.\"\"\"\n",
    "\n",
    "    def decorator(self):\n",
    "        for name in names:\n",
    "            setattr(self, f'_{name}', values[name])\n",
    "\n",
    "        return self\n",
    "\n",
    "    return decorator\n",
    "\n",
    "#Decorator wraps old init witht he properties of *name\n",
    "\n",
    "def has_properties(*names):\n",
    "\n",
    "    def decorator(cls):\n",
    "        cls = add_properties(*names)(cls)\n",
    "        old_init = cls.__init__\n",
    "\n",
    "        @wraps(old_init)\n",
    "        def new_init(self, *args, **kwargs):\n",
    "\n",
    "            func_sig = signature(old_init)\n",
    "            bound = func_sig.bind(self, *args, **kwargs)\n",
    "            bound.apply_defaults()\n",
    "            all_args = bound.arguments\n",
    "            self = set_properties(*names, **all_args)(self)\n",
    "            old_init(self, *args, **kwargs)\n",
    "\n",
    "        cls.__init__ = new_init\n",
    "        return cls\n",
    "\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIa0iX34efK-"
   },
   "source": [
    "### DevMisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPIdQz0IbF-v"
   },
   "outputs": [],
   "source": [
    "class Map(dict):\n",
    "\t\"\"\"\n",
    "\tAbandoned the original way of doing it:\n",
    "\t\tself.__dict__ = self\n",
    "\tThis introduces a self-reference and makes gc fail to collect garbage\n",
    "\tin time. As a result, pytorch can't properly empty cache if a tensor is\n",
    "\tstored in it.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __getattr__(self, key):\n",
    "\t\ttry:\n",
    "\t\t\treturn self[key]\n",
    "\t\texcept KeyError:\n",
    "\t\t\traise AttributeError\n",
    "\n",
    "\tdef __setattr__(self, key, value):\n",
    "\t\tself[key] = value\n",
    "\n",
    "\tdef update(self, *args, **kwargs):\n",
    "\t\t'''\n",
    "\t\tReturn self.\n",
    "\t\t'''\n",
    "\t\tsuper(Map, self).update(*args, **kwargs)\n",
    "\t\treturn self\n",
    "\n",
    "\tdef apply(self, func, ignored=set()):\n",
    "\t\tfor key in self:\n",
    "\t\t\tif key in ignored:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif isinstance(self[key], Map):\n",
    "\t\t\t\tself[key].apply(func, ignored=ignored)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself[key] = func(self[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVEjpkRKeTSS"
   },
   "outputs": [],
   "source": [
    "_CachedItem = namedtuple('_CachedItem', ['persist', 'value'])\n",
    "_CACHE = dict()\n",
    "_USE_CACHE = True\n",
    "\n",
    "def cache(full=True, persist=False):\n",
    "    global _USE_CACHE\n",
    "\n",
    "    def descriptor(func):\n",
    "        func_sig = signature(func)\n",
    "        def decorator(self, *args, **kwargs):\n",
    "            if not _USE_CACHE:\n",
    "                return func(self, *args, **kwargs)\n",
    "\n",
    "            bound = func_sig.bind(self, *args, **kwargs)\n",
    "            bound.apply_defaults()\n",
    "            items = [(k, v) for k, v in bound.arguments.items() if not isinstance(v, dict)]\n",
    "            arg_key = frozenset(items)\n",
    "            if full:\n",
    "                key = (id(self), func.__name__, arg_key)\n",
    "            else:\n",
    "                key = (id(self), func.__name__)\n",
    "            if key in _CACHE:\n",
    "                return _CACHE[key].value\n",
    "            else:\n",
    "                ret = func(self, *args, **kwargs)\n",
    "                _CACHE[key] = _CachedItem(persist, ret)\n",
    "                return ret\n",
    "\n",
    "        return decorator\n",
    "\n",
    "    return descriptor\n",
    "\n",
    "def clear_cache():\n",
    "\tglobal _CACHE\n",
    "\t# First pass to get keys to be removed.\n",
    "\tto_remove = list()\n",
    "\tfor k, item in _CACHE.items():\n",
    "\t\tif not item.persist:\n",
    "\t\t\tto_remove.append(k)\n",
    "\t# Now remove them.\n",
    "\tfor k in to_remove:\n",
    "\t\tdel _CACHE[k]\n",
    "\n",
    "def set_cache(flag):\n",
    "\tglobal _USE_CACHE\n",
    "\tassert flag in [True, False]\n",
    "\t_USE_CACHE = flag\n",
    "\n",
    "####################################### structured cache #################################\n",
    "\n",
    "class _StructuredCache:\n",
    "\t'''\n",
    "\tAssuming that the return is a Map object, this cache will selectively keep some\n",
    "\tattributes while removing the rest.\n",
    "\tYou can also dynamically select what to cache, useful when you want to perform analysis.\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# What should be kept.\n",
    "\t\tself._to_keep = dict()\n",
    "\t\t# What should be cached.\n",
    "\t\tself._to_cache = defaultdict(set)\n",
    "\t\t# What is cached. Note that each instance method has its own cache, keyed by the object's unique id.\n",
    "\t\tself.clear_cache()\n",
    "\t\t# The mapping from the object id to the object.\n",
    "\t\tself._id2obj = dict()\n",
    "\n",
    "\tdef keep(self, name, ret):\n",
    "\t\t'''Only keep what should be kept.'''\n",
    "\t\tif self._to_keep[name] is None:\n",
    "\t\t\treturn ret\n",
    "\t\telse:\n",
    "\t\t\treturn Map(**{k: ret[k] for k in self._to_keep[name] if k in ret})\n",
    "\n",
    "\tdef __contains__(self, key):\n",
    "\t\treturn key in self._to_keep\n",
    "\n",
    "\tdef register_keep(self, name, *to_keep):\n",
    "\t\t'''Record what to keep for each function.'''\n",
    "\t\tassert name not in self\n",
    "\t\tif len(to_keep) == 0:\n",
    "\t\t\tself._to_keep[name] = None # NOTE None means keeping everthing.\n",
    "\t\telse:\n",
    "\t\t\tself._to_keep[name] = set(to_keep)\n",
    "\n",
    "\tdef register_cache(self, name, *keys):\n",
    "\t\t'''Register cache for all instances of the same registered function with ``name``.'''\n",
    "\t\tself._to_cache[name].update(keys)\n",
    "\n",
    "\tdef cache(self, name, obj, ret):\n",
    "\t\tid_ = id(obj)\n",
    "\t\tif id_ not in self._id2obj:\n",
    "\t\t\tself._id2obj[id_] = obj\n",
    "\t\telse:\n",
    "\t\t\tassert self._id2obj[id_] is obj # Make sure it's the same object.\n",
    "\t\tfor k in self._to_cache[name]:\n",
    "\t\t\tassert k not in self._cache[name][id_]\n",
    "\t\t\tself._cache[name][id_][k] = ret[k]\n",
    "\n",
    "\tdef get_cache(self, name, *keys):\n",
    "\t\t'''Get all caches generated from the same function.'''\n",
    "\t\tret = list()\n",
    "\t\tfor id_ in self._cache[name]:\n",
    "\t\t\tobj = self._id2obj[id_]\n",
    "\t\t\tcache = self._cache[name][id_]\n",
    "\t\t\tret.append((obj, {k: cache[k] for k in keys}))\n",
    "\t\treturn ret\n",
    "\n",
    "\tdef clear_cache(self):\n",
    "\t\tself._cache = defaultdict(lambda: defaultdict(defaultdict))\n",
    "\n",
    "_SC = _StructuredCache()\n",
    "def sc(name, *to_keep):\n",
    "\tglobal _SC\n",
    "\n",
    "\tdef descriptor(func):\n",
    "\n",
    "\t\t@wraps(func)\n",
    "\t\tdef decorator(self, *args, **kwargs):\n",
    "\t\t\tret = func(self, *args, **kwargs)\n",
    "\t\t\tassert isinstance(ret, Map)\n",
    "\t\t\t_SC.cache(name, self, ret)\n",
    "\t\t\treturn _SC.keep(name, ret)\n",
    "\n",
    "\t\treturn decorator\n",
    "\n",
    "\t_SC.register_keep(name, *to_keep)\n",
    "\treturn descriptor\n",
    "\n",
    "def sc_clear_cache():\n",
    "\tglobal _SC\n",
    "\t_SC.clear_cache()\n",
    "\n",
    "def sc_register_cache(name, *keys):\n",
    "\tglobal _SC\n",
    "\t_SC.register_cache(name, *keys)\n",
    "\n",
    "def sc_get_cache(name, *keys):\n",
    "\tglobal _SC\n",
    "\treturn _SC.get_cache(name, *keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcT12Yotfu-Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tensor(data, dtype=None, requires_grad=False, use_cuda=True):\n",
    "    use_cuda = os.environ.get('CUDA_VISIBLE_DEVICES', False) and use_cuda # NOTE only use cuda when it's not overriden and there is a device available\n",
    "\n",
    "    # If data is a tensor already, move to gpu if use_cuda\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        if use_cuda:\n",
    "            return data.cuda()\n",
    "        return data\n",
    "\n",
    "    if dtype is None: # NOTE infer dtype\n",
    "        dtype = 'f'\n",
    "        if isinstance(data, np.ndarray) and issubclass(data.dtype.type, np.integer):\n",
    "            dtype = 'l'\n",
    "\n",
    "    # NOTE directly declare data. I believe it's faster on cuda, although I'm not entirely sure\n",
    "    requires_grad = requires_grad and dtype == 'f'\n",
    "    assert dtype in ['f', 'l']\n",
    "    #if use_cuda:\n",
    "    #    module = getattr(torch, 'cuda')\n",
    "    #else:\n",
    "    #    module = torch\n",
    "    #if dtype == 'f':\n",
    "    #    cls = getattr(module, 'FloatTensor')\n",
    "    #    dtype = 'float32'\n",
    "    #elif dtype == 'l':\n",
    "    #    cls = getattr(module, 'LongTensor')\n",
    "    #    dtype = 'int64'\n",
    "    #ret = cls(np.asarray(data, dtype=dtype))\n",
    "    # Set PyTorch data type and device\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "    if dtype == 'f':\n",
    "        torch_dtype = torch.float32\n",
    "    elif dtype == 'l':\n",
    "        torch_dtype = torch.int64\n",
    "\n",
    "    # Create tensor properly\n",
    "    ret = torch.tensor(np.asarray(data, dtype=dtype), dtype=torch_dtype, device=device)\n",
    "\n",
    "    ret.requires_grad = requires_grad\n",
    "    return ret\n",
    "\n",
    "def get_zeros(*shape, **kwargs):\n",
    "    if len(shape) == 1 and isinstance(shape[0], torch.Size): # NOTE deal with 1D tensor whose shape cannot be unpacked\n",
    "        shape = list(shape[0])\n",
    "    return get_tensor(np.zeros(shape), **kwargs)\n",
    "\n",
    "def get_eye(n):\n",
    "    return get_tensor(np.eye(n))\n",
    "\n",
    "def counter(iterable, *args, max_size=0, interval=1000, **kwargs):\n",
    "    total = 0\n",
    "    for i, item in enumerate(iterable, *args, **kwargs):\n",
    "        yield item\n",
    "        total += 1\n",
    "        if total % interval == 0:\n",
    "            logging.debug(f'{total}')\n",
    "            sys.stdout.flush()\n",
    "        if max_size and total == max_size:\n",
    "            logging.info(f'Reached max size')\n",
    "            break\n",
    "    logging.debug(f'Finished enumeration of size {total}')\n",
    "\n",
    "def freeze(mod):\n",
    "    for p in mod.parameters():\n",
    "        p.requires_grad = False\n",
    "    for m in mod.children():\n",
    "        freeze(m)\n",
    "\n",
    "def sort_all(anchor, *others):\n",
    "    '''\n",
    "    Sort everything (``anchor`` and ``others``) in this based on the lengths of ``anchor``.\n",
    "    '''\n",
    "    # Check everything is an numpy array.\n",
    "    for a in (anchor, ) + others:\n",
    "        assert isinstance(a, np.ndarray)\n",
    "    #  Check everything has the same length in the first dimension.\n",
    "    l = len(anchor)\n",
    "    for o in others:\n",
    "        assert len(o) == l\n",
    "    # Sort by length.\n",
    "    lens = np.asarray([len(x) for x in anchor], dtype='int64')\n",
    "    inds = np.argsort(lens)[::-1]\n",
    "    # Return everything after sorting.\n",
    "    return [lens[inds]] + [anchor[inds]] + [o[inds] for o in others]\n",
    "\n",
    "def pprint_cols(data, num_cols=4):\n",
    "    t = pt()\n",
    "    num_rows = len(data) // num_cols + (len(data) % num_cols > 0)\n",
    "    for col in range(num_cols - 1):\n",
    "        t.add_column(f'Column:{col+1}', data[col * num_rows: (col + 1) * num_rows])\n",
    "    t.add_column(f'Column:{num_cols}', data[(num_cols - 1) * num_rows:] + [''] * ((num_rows - len(data) % num_rows) % num_rows))\n",
    "    t.align = 'l'\n",
    "    print(t)\n",
    "\n",
    "def check(t):\n",
    "    if (torch.isnan(t).any() | torch.isinf(t).any()).item():\n",
    "        breakpoint()\n",
    "\n",
    "def canonicalize(shape, dim):\n",
    "    if dim < 0:\n",
    "        return len(shape) + dim\n",
    "    else:\n",
    "        return dim\n",
    "\n",
    "def divide(tensor, dim, div_shape):\n",
    "    prev_shape = tensor.shape\n",
    "    dim = canonicalize(prev_shape, dim)\n",
    "    if -1 not in div_shape:\n",
    "        total = reduce(mul, div_shape, 1)\n",
    "        assert total == prev_shape[dim]\n",
    "    new_shape = prev_shape[:dim] + tuple(div_shape) + prev_shape[dim + 1:]\n",
    "    return tensor.view(*new_shape)\n",
    "\n",
    "def merge(tensor, dims):\n",
    "    prev_shape = tensor.shape\n",
    "    dims = [canonicalize(prev_shape, dim) for dim in dims]\n",
    "    for a, b in zip(dims[:-1], dims[1:]):\n",
    "        assert b == a + 1\n",
    "    total = reduce(mul, [prev_shape[d] for d in dims], 1)\n",
    "    new_shape = prev_shape[:dims[0]] + (total, ) + prev_shape[dims[-1] + 1:]\n",
    "    return tensor.view(*new_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VBhRVn6zdKG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from MUSE\n",
    "'''\n",
    "\n",
    "\n",
    "# From https://stackoverflow.com/questions/2183233/how-to-add-a-custom-loglevel-to-pythons-logging-facility.\n",
    "def addLoggingLevel(levelName, levelNum, methodName=None):\n",
    "    \"\"\"\n",
    "    Comprehensively adds a new logging level to the `logging` module and the\n",
    "    currently configured logging class.\n",
    "\n",
    "    `levelName` becomes an attribute of the `logging` module with the value\n",
    "    `levelNum`. `methodName` becomes a convenience method for both `logging`\n",
    "    itself and the class returned by `logging.getLoggerClass()` (usually just\n",
    "    `logging.Logger`). If `methodName` is not specified, `levelName.lower()` is\n",
    "    used.\n",
    "\n",
    "    To avoid accidental clobberings of existing attributes, this method will\n",
    "    raise an `AttributeError` if the level name is already an attribute of the\n",
    "    `logging` module or if the method name is already present\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> addLoggingLevel('TRACE', logging.DEBUG - 5)\n",
    "    >>> logging.getLogger(__name__).setLevel(\"TRACE\")\n",
    "    >>> logging.getLogger(__name__).trace('that worked')\n",
    "    >>> logging.trace('so did this')\n",
    "    >>> logging.TRACE\n",
    "    5\n",
    "\n",
    "    \"\"\"\n",
    "    if not methodName:\n",
    "        methodName = levelName.lower()\n",
    "\n",
    "    #if hasattr(logging, levelName):\n",
    "    #    raise AttributeError('{} already defined in logging module'.format(levelName))\n",
    "    #if hasattr(logging, methodName):\n",
    "    #    raise AttributeError('{} already defined in logging module'.format(methodName))\n",
    "    #if hasattr(logging.getLoggerClass(), methodName):\n",
    "    #    raise AttributeError('{} already defined in logger class'.format(methodName))\n",
    "\n",
    "    # This method was inspired by the answers to Stack Overflow post\n",
    "    # http://stackoverflow.com/q/2183233/2988730, especially\n",
    "    # http://stackoverflow.com/a/13638084/2988730\n",
    "    def logForLevel(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(levelNum):\n",
    "            self._log(levelNum, message, args, **kwargs)\n",
    "\n",
    "    def logToRoot(message, *args, **kwargs):\n",
    "        logging.log(levelNum, message, *args, **kwargs)\n",
    "\n",
    "    logging.addLevelName(levelNum, levelName)\n",
    "    setattr(logging, levelName, levelNum)\n",
    "    setattr(logging.getLoggerClass(), methodName, logForLevel)\n",
    "    setattr(logging, methodName, logToRoot)\n",
    "\n",
    "\n",
    "addLoggingLevel('IMP', 25)\n",
    "\n",
    "\n",
    "class LogFormatter(TTYColoredFormatter):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):  # , color=False):\n",
    "        fmt = '%(log_color)s%(levelname)s - %(time)s - %(elapsed)s at %(filename)s:%(lineno)d - %(message)s'\n",
    "        super(LogFormatter, self).__init__(\n",
    "            fmt,\n",
    "            log_colors={\n",
    "                'DEBUG': 'white',\n",
    "                'INFO': 'green',\n",
    "                'IMP': 'cyan',\n",
    "                'WARNING': 'yellow',\n",
    "                'ERROR': 'red',\n",
    "                'CRITICAL': 'red,bg_white'},\n",
    "            *args,\n",
    "            **kwargs)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def format(self, record):\n",
    "        # only need to set timestamps once -- all changes are stored in the record object\n",
    "        if not hasattr(record, 'elapsed'):\n",
    "            record.elapsed = timedelta(seconds=round(record.created - self.start_time))\n",
    "            record.time = time.strftime('%x %X')\n",
    "            # if self.colored:\n",
    "            prefix = \"%s - %s - %s at %s:%d\" % (\n",
    "                record.levelname,\n",
    "                record.time,\n",
    "                record.elapsed,\n",
    "                record.filename,\n",
    "                record.lineno\n",
    "            )\n",
    "            message = record.getMessage()\n",
    "            # If a message starts with a line break, we will keep the original line break without autoindentation.\n",
    "            if not message.startswith('\\n'):\n",
    "                message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n",
    "            record.msg = message\n",
    "            record.args = ()  # NOTE avoid evaluating the message again duing getMessage call.\n",
    "        x = super(LogFormatter, self).format(record)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_logger(filepath=None, log_level='INFO'):\n",
    "    \"\"\"\n",
    "    Create a logger.\n",
    "    \"\"\"\n",
    "    # create console handler and set level to info\n",
    "    console_handler = logging.StreamHandler()\n",
    "    # create log formatter\n",
    "    colorlog_formatter = LogFormatter(stream=console_handler.stream)\n",
    "    console_handler.setLevel(getattr(logging, log_level))\n",
    "    console_handler.setFormatter(colorlog_formatter)\n",
    "\n",
    "    # create logger and set level to debug\n",
    "    logger = logging.getLogger()\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(log_level)\n",
    "    logger.propagate = False\n",
    "    logger.addHandler(console_handler)\n",
    "    if filepath:\n",
    "        # create file handler and set level to debug\n",
    "        file_handler = logging.FileHandler(filepath, \"a\")\n",
    "        file_handler.setLevel(log_level)\n",
    "        log_formatter = LogFormatter(stream=file_handler.stream)\n",
    "        file_handler.setFormatter(log_formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    # reset logger elapsed time\n",
    "    def reset_time():\n",
    "        log_formatter.start_time = time.time()\n",
    "    logger.reset_time = reset_time\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def log_this(log_level='DEBUG', msg='', arg_list=None):\n",
    "    \"\"\"\n",
    "    A decorator that logs the functionality, the beginning and the end of the function.\n",
    "    It can optionally print out arg values in arg_list.\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        new_msg = msg or func.__name__\n",
    "        new_arg_list = arg_list or list()\n",
    "        def log_func(msg): return logging.log(getattr(logging, log_level), msg)\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            log_func(f'*STARTING* {new_msg}')\n",
    "\n",
    "            if new_arg_list:\n",
    "\n",
    "                func_sig = signature(func)\n",
    "                bound = func_sig.bind(*args, **kwargs)\n",
    "                bound.apply_defaults()\n",
    "                all_args = bound.arguments\n",
    "\n",
    "                arg_msg = {name: all_args[name] for name in new_arg_list}\n",
    "                log_func(f'*ARG_LIST* {arg_msg}')\n",
    "\n",
    "            ret = func(*args, **kwargs)\n",
    "            log_func(f'*FINISHED* {new_msg}')\n",
    "            return ret\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def log_pp(obj):\n",
    "    '''\n",
    "    Log ``obj`` with better indentations.\n",
    "    '''\n",
    "    logging.info(('\\n' + str(obj)).replace('\\n', '\\n' + ' ' * 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kMrdDBXMgTJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plain(value):\n",
    "    '''Convert tensors or numpy arrays to one scalar.'''\n",
    "    # Get to str, int or float first.\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        assert value.numel() == 1\n",
    "        value = value.item()\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        assert value.size == 1\n",
    "        value = value[0]\n",
    "    # Format it nicely.\n",
    "    if isinstance(value, (str, int)):\n",
    "        value = value\n",
    "    elif isinstance(value, float):\n",
    "        value = float(f'{value:.3f}')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return value\n",
    "\n",
    "\n",
    "class Metric:\n",
    "\n",
    "    def __init__(self, name, value, weight, report_mean=True):\n",
    "        self.name = name\n",
    "        self._v = value\n",
    "        self._w = weight\n",
    "        self._report_mean = report_mean\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.report_mean:\n",
    "            return f'{plain(self._v)}/{plain(self._w)}={plain(self.mean)}'\n",
    "        else:\n",
    "            return f'{plain(self.total)}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Metric(name={self.name}, report_mean={self.report_mean})'\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Metric):\n",
    "            assert self == other, 'Cannot add two different metrics.'\n",
    "            assert self.report_mean == other.report_mean\n",
    "            return Metric(self.name, self._v + other._v, self._w + other._w, report_mean=self.report_mean)\n",
    "        else:\n",
    "            # NOTE This is useful for sum() call.\n",
    "            assert isinstance(other, (int, float)) and other == 0\n",
    "            return self\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def rename(self, name):\n",
    "        '''This is in-place.'''\n",
    "        self.name = name\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._v\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self._w if self.report_mean else 'N/A'\n",
    "\n",
    "    @property\n",
    "    def report_mean(self):\n",
    "        return self._report_mean\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        if self.report_mean:\n",
    "            return self._v / self._w\n",
    "        else:\n",
    "            return 'N/A'\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self._v\n",
    "\n",
    "    def clear(self):\n",
    "        self._v = 0\n",
    "        self._w = 0\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    def __init__(self, *metrics):\n",
    "        # Check all of metrics are of the same type. Either all str or all Metric.\n",
    "        types = set([type(m) for m in metrics])\n",
    "        assert len(types) <= 1\n",
    "\n",
    "        if len(types) == 1:\n",
    "            if types.pop() is str:\n",
    "                self._metrics = {k: Metric(k, 0, 0) for k in keys}\n",
    "            else:\n",
    "                self._metrics = {metric.name: metric for metric in metrics}\n",
    "        else:\n",
    "            self._metrics = dict()\n",
    "\n",
    "    def __str__(self):\n",
    "        out = '\\n'.join([f'{k}: {m}' for k, m in self._metrics.items()])\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Metrics({\", \".join(self._metrics.keys())})'\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Metric):\n",
    "            other = Metrics(other)\n",
    "        union_keys = set(self._metrics.keys()) | set(other._metrics.keys())\n",
    "        metrics = list()\n",
    "        for k in union_keys:\n",
    "            m1 = self._metrics.get(k, 0)\n",
    "            m2 = other._metrics.get(k, 0)\n",
    "            metrics.append(m1 + m2)\n",
    "        return Metrics(*metrics)\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return super().__getattribute__('_metrics')[key]\n",
    "        except KeyError:\n",
    "            raise AttributeError(f'Cannot find this attribute {key}')\n",
    "\n",
    "    def get_table(self, title=''):\n",
    "        t = pt()\n",
    "        if title:\n",
    "            t.title = title\n",
    "        t.field_names = 'name', 'value', 'weight', 'mean'\n",
    "        for k in sorted(self._metrics.keys()):\n",
    "            metric = self._metrics[k]\n",
    "            t.add_row([k, plain(metric.value), plain(metric.weight), plain(metric.mean)])\n",
    "        t.align = 'l'\n",
    "        return t\n",
    "\n",
    "    def clear(self):\n",
    "        for m in self._metrics.values():\n",
    "            m.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMQcRjGPM743"
   },
   "outputs": [],
   "source": [
    "_manager = enlighten.get_manager()\n",
    "_stage_names = set()\n",
    "\n",
    "def clear_stages():\n",
    "    global _manager\n",
    "    global _stage_names\n",
    "    _manager = enlighten.get_manager()\n",
    "    _stage_names = set()\n",
    "\n",
    "def _check_name(name):\n",
    "    assert name not in _stage_names\n",
    "    _stage_names.add(name)\n",
    "\n",
    "\n",
    "def _reset_pbar(pbar):\n",
    "    pbar.count = 0\n",
    "    pbar.start = time.time()\n",
    "\n",
    "\n",
    "@has_properties('name', 'num_steps', 'parent')\n",
    "class _Stage:\n",
    "\n",
    "    def __init__(self, name, num_steps=1, parent=None):\n",
    "        _check_name(name)\n",
    "\n",
    "        self._pbars = dict()\n",
    "        self.substages = list()\n",
    "        if self.num_steps > 1:\n",
    "            self.add_pbar(name, total=self.num_steps)\n",
    "\n",
    "    def update_pbars(self):\n",
    "        for pbar in self._pbars.values():\n",
    "            if pbar.total == pbar.count:\n",
    "                _reset_pbar(pbar)\n",
    "            pbar.update()\n",
    "\n",
    "    def reset_pbars(self, recursive=False):\n",
    "        for pbar in self._pbars.values():\n",
    "            _reset_pbar(pbar)\n",
    "        if recursive:\n",
    "            for substage in self.substages:\n",
    "                substage.reset_pbars(recursive=True)\n",
    "\n",
    "    def add_pbar(self, name, total=None, unit='samples'):\n",
    "        if name in self._pbars:\n",
    "            raise NameError(f'Name {name} already exists.')\n",
    "        pbar = _manager.counter(\n",
    "            desc=name,\n",
    "            total=total,\n",
    "            unit=unit,\n",
    "            leave=False)\n",
    "        pbar.refresh()\n",
    "        self._pbars[name] = pbar\n",
    "\n",
    "    def add_stage(self, name, num_steps=1):\n",
    "        stage = _Stage(name, num_steps=num_steps, parent=self)\n",
    "        self.substages.append(stage)\n",
    "        return stage\n",
    "\n",
    "    # def adjoin_stage(self, stage):\n",
    "    #     assert isinstance(stage, _Stage)\n",
    "    #     self.substages.append(stage)\n",
    "    #     return self\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'\"{self.name}\"'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Stage(name={self.name}, num_steps={self.num_steps})'\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        missing = list()\n",
    "        for name, pbar_meta in state_dict['_pbars'].items():\n",
    "            try:\n",
    "                pbar = self._pbars[name]\n",
    "            except KeyError:\n",
    "                missing.append(f'pbar:{name}')\n",
    "                continue\n",
    "            pbar.count = pbar_meta['count']\n",
    "            pbar.refresh()\n",
    "        for s1, s2 in zip(self.substages, state_dict['_stages']):\n",
    "            s1.load_state_dict(s2)\n",
    "        if missing:\n",
    "            raise RuntimeError(f'Missing {missing}')\n",
    "\n",
    "    def state_dict(self):\n",
    "        ret = dict()\n",
    "        # NOTE pbar itself cannot be serialized for some reason.\n",
    "        ret['_pbars'] = {name: {'count': pbar.count} for name, pbar in self._pbars.items()}\n",
    "        stage_ret = list()\n",
    "        for s in self.substages:\n",
    "            stage_ret.append(s.state_dict())\n",
    "        ret['_stages'] = stage_ret\n",
    "        return ret\n",
    "\n",
    "\n",
    "@has_properties('step', 'substage_idx')\n",
    "class _Node:\n",
    "    \"\"\"A wrapper of stage that contains step and substage_idx information.\"\"\"\n",
    "\n",
    "    def __init__(self, stage, step, substage_idx):\n",
    "        self.stage = stage\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.stage.name\n",
    "\n",
    "    def is_last(self):\n",
    "        last_step = (self.step == self.stage.num_steps - 1)\n",
    "        if self.stage.substages:\n",
    "            last_substage = (self.substage_idx == len(self.stage.substages) - 1)\n",
    "            last = last_substage and last_step\n",
    "        else:\n",
    "            last = last_step\n",
    "        return last\n",
    "\n",
    "    def next_node(self):\n",
    "        \"\"\"Return whether next node will increment the step.\"\"\"\n",
    "        if self.stage.substages:\n",
    "            new_substage_idx = self.substage_idx + 1\n",
    "            incremented = False\n",
    "            if new_substage_idx == len(self.stage.substages):\n",
    "                new_substage_idx = 0\n",
    "                incremented = True\n",
    "            new_step = self.step + incremented\n",
    "            return _Node(self.stage, new_step, new_substage_idx), incremented\n",
    "        else:\n",
    "            return _Node(self.stage, self.step + 1, None), True\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.stage}: {self.step}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Node(stage={str(self.stage)}, step={self.step}, substage_idx={self.substage_idx})'\n",
    "\n",
    "\n",
    "class _Path:\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        self._nodes = list()\n",
    "        self._nodes_dict = dict()\n",
    "        self._schedule = schedule\n",
    "        self._get_first_path(self._schedule)\n",
    "        self._finished = False\n",
    "\n",
    "    def _add(self, node):\n",
    "        # Check that this is a valid extension of the original path.\n",
    "        if len(self._nodes) == 0:\n",
    "            safe = True\n",
    "        else:\n",
    "            last_node = self._nodes[-1]\n",
    "            safe = last_node.stage.substages[last_node.substage_idx] is node.stage\n",
    "        assert safe\n",
    "        # Add it.\n",
    "        self._nodes.append(node)\n",
    "        self._nodes_dict[node.stage.name] = node.step\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = ' -> '.join([str(node) for node in self._nodes])\n",
    "        return ret\n",
    "\n",
    "    def _get_first_path(self, stage_or_node):\n",
    "\n",
    "        def helper(stage_or_node):\n",
    "            if isinstance(stage_or_node, _Stage):\n",
    "                stage = stage_or_node\n",
    "                if stage.substages:\n",
    "                    self._add(_Node(stage, 0, 0))\n",
    "                    helper(stage.substages[0])\n",
    "                else:\n",
    "                    self._add(_Node(stage, 0, None))  # None means there is no substage.\n",
    "            else:\n",
    "                assert isinstance(stage_or_node, _Node)\n",
    "                node = stage_or_node\n",
    "                if node.stage.substages:\n",
    "                    new_node = _Node(node.stage, node.step, node.substage_idx)\n",
    "                    self._add(new_node)\n",
    "                    child_node = _Node(new_node.stage.substages[new_node.substage_idx], 0, 0)\n",
    "                    helper(child_node)\n",
    "                else:\n",
    "                    self._add(node)\n",
    "\n",
    "        helper(stage_or_node)\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._finished\n",
    "\n",
    "    def next_path(self):\n",
    "        \"\"\"Note that this is in-place. It returns the nodes incremented.\"\"\"\n",
    "        # First backtrack to the first ancestor that hasn't been completed yet.\n",
    "        assert not self._finished\n",
    "        i = len(self._nodes)\n",
    "        while i > 0:\n",
    "            i -= 1\n",
    "            last_node = self._nodes[i]\n",
    "            if not last_node.is_last():\n",
    "                break\n",
    "        # Now complete it.\n",
    "        if last_node.is_last():\n",
    "            self._finished = True\n",
    "            affected_nodes = self._nodes[1:]\n",
    "        else:\n",
    "            affected_nodes = self._nodes[i + 1:]  # NOTE Everything that is last will be incremented.\n",
    "            self._nodes = self._nodes[:i]\n",
    "            next_node, incremented = last_node.next_node()\n",
    "            if incremented:\n",
    "                affected_nodes.append(next_node)\n",
    "            self._get_first_path(next_node)\n",
    "        return affected_nodes\n",
    "\n",
    "    @property\n",
    "    def leaf_node(self):\n",
    "        return self._nodes[-1]\n",
    "\n",
    "    def get_step(self, key):\n",
    "        return self._nodes_dict[key]\n",
    "\n",
    "\n",
    "class _Schedule(_Stage):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name, num_steps=1)\n",
    "        self._path = None\n",
    "\n",
    "    def _build_path(self):\n",
    "        self._path = _Path(self)\n",
    "\n",
    "    def update(self):\n",
    "        affected_nodes = self._path.next_path()\n",
    "\n",
    "        for node in affected_nodes:\n",
    "            node.stage.update_pbars()\n",
    "\n",
    "    def as_tree(self):\n",
    "        tree = Tree()  # NOTE Store the tree structure for treelib.\n",
    "        tree.create_node(repr(self), id(self))\n",
    "\n",
    "        def helper(stage):\n",
    "            for substage in stage.substages:\n",
    "                tree.create_node(repr(substage), id(substage), parent=id(stage))\n",
    "                helper(substage)\n",
    "\n",
    "        helper(self)\n",
    "\n",
    "        sys.stdout = io.StringIO()\n",
    "        tree.show()\n",
    "        output = sys.stdout.getvalue()\n",
    "        sys.stdout = sys.__stdout__\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def current_stage(self):\n",
    "        return self._path.leaf_node\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._path.finished\n",
    "\n",
    "    def get_step(self, key):\n",
    "        return self._path.get_step(key)\n",
    "\n",
    "    def fix_schedule(self):\n",
    "        self._build_path()\n",
    "\n",
    "    def reset(self):\n",
    "        self._build_path()\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.clear_best()\n",
    "        self._schedule = _Schedule(name)\n",
    "        self._metrics = Metrics()\n",
    "\n",
    "    def schedule_as_tree(self):\n",
    "        return self._schedule.as_tree()\n",
    "\n",
    "    @property\n",
    "    def schedule(self):\n",
    "        return self._schedule\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self._metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._schedule.reset()\n",
    "\n",
    "    def add_stage(self, name, num_steps=1):\n",
    "        return self._schedule.add_stage(name, num_steps=num_steps)\n",
    "\n",
    "    def clear_best(self):\n",
    "        self.best_score = None\n",
    "        self.best_stage = None\n",
    "\n",
    "    def check_metrics(self, epoch):\n",
    "        log_pp(self._metrics.get_table(title=f'Epoch: {epoch}'))\n",
    "\n",
    "    def clear_metrics(self):\n",
    "        self._metrics.clear()\n",
    "\n",
    "    def update_metrics(self, metrics):\n",
    "        self._metrics += metrics\n",
    "\n",
    "    def state_dict(self):\n",
    "        ret = {'_schedule': self._schedule.state_dict(), 'best_score': self.best_score,\n",
    "               'best_stage': self.best_stage, '_metrics': self._metrics}\n",
    "        return ret\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        #self._schedule.load_state_dict(state_dict['_schedule'])\n",
    "        self.best_score = state_dict['best_score']\n",
    "        self.best_stage = state_dict['best_stage']\n",
    "        self._metrics = state_dict['_metrics']\n",
    "        round_num, last_step = extract_round_and_last_step(state_dict['_schedule'])\n",
    "        #logging.critical(state_dict['_schedule'])\n",
    "        #logging.critical(f\"{round_num}, {last_step}\")\n",
    "        #self.skip_to_checkpoint_progress(round_num, last_step)\n",
    "        while not (self.get('round') == round_num):\n",
    "            self._schedule.update()\n",
    "            #logging.critical(f\"{self.current_stage.name}, {self.current_stage.step}\")\n",
    "        # Skip to last M-step inside that round if needed\n",
    "        while not (self.current_stage.name == \"M step\" and self.current_stage.step == last_step):\n",
    "            self._schedule.update()\n",
    "            #logging.critical(f\"{self.current_stage.name}, {self.current_stage.step}\")\n",
    "        self._schedule.update()\n",
    "        #logging.critical(f\"{self.current_stage}, {self.get('round') }\")\n",
    "\n",
    "\n",
    "    def update_best(self, score, mode='min', quiet=False):\n",
    "        \"\"\"Update the best score and best stage.\n",
    "\n",
    "        Args:\n",
    "            score: score for the current stage\n",
    "            mode (str, optional): take the maximum or the minimum as the best score. Defaults to 'min'.\n",
    "            quiet (bool, optional): flag to suppress outputting the best score. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            updated (bool): whether the best score has been updated or not\n",
    "        \"\"\"\n",
    "        score = plain(score)\n",
    "        updated = False\n",
    "\n",
    "        def should_update():\n",
    "            if score is None:\n",
    "                return False\n",
    "            if self.best_score is None:\n",
    "                return True\n",
    "            if mode == 'max' and self.best_score < score:\n",
    "                return True\n",
    "            if mode == 'min' and self.best_score > score:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        updated = should_update()\n",
    "        if updated:\n",
    "            self.best_score = score\n",
    "            self.best_stage = str(self.current_stage)\n",
    "        if self.best_score is not None and not quiet:\n",
    "            logging.info(f'Best score is {self.best_score:.3f} at stage {self.best_stage}')\n",
    "        return updated\n",
    "\n",
    "    def update(self):\n",
    "        self._schedule.update()\n",
    "\n",
    "    @property\n",
    "    def current_stage(self):\n",
    "        return self._schedule.current_stage\n",
    "\n",
    "    @property\n",
    "    def finished(self):\n",
    "        return self._schedule.finished\n",
    "\n",
    "    def get(self, key):\n",
    "        return self._schedule.get_step(key)\n",
    "\n",
    "    def fix_schedule(self):\n",
    "        self._schedule.fix_schedule()\n",
    "\n",
    "    def reset_pbars(self):\n",
    "        self._schedule.reset_pbars(recursive=True)\n",
    "\n",
    "\n",
    "def extract_round_and_last_step(schedule_state_dict):\n",
    "    try:\n",
    "        outer_stage = schedule_state_dict['_stages'][0]\n",
    "        round_count = outer_stage['_pbars']['round']['count']\n",
    "\n",
    "        # Assuming the last executed step is inside substages\n",
    "        substages = outer_stage['_stages']\n",
    "        last_step = None\n",
    "        for sub in substages:\n",
    "            if 'M step' in sub['_pbars']:\n",
    "                last_step = sub['_pbars']['M step']['count']\n",
    "                break\n",
    "\n",
    "        return round_count, last_step\n",
    "    except (KeyError, IndexError) as e:\n",
    "        raise ValueError(\"Invalid schedule state_dict format\") from e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qubc_YVfkZ2"
   },
   "source": [
    "### CognateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4l3bbdigfnnB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CognateSet:\n",
    "    '''\n",
    "    Similar to the concept of synset in wordnet. Each cognate set contains words that are cognates with each other.\n",
    "    If A is cognate to B1 and B2, then all of them will be stored here.\n",
    "    Note that this doesn't gurantee that B1 and B2 are semantically similar. The word \"bank\" might be cognate to\n",
    "    two totally different words in another language. In other words, while the cognate relation across languages are\n",
    "    perserved here, nothing can be definitely said wrt the relation within the same language.\n",
    "    '''\n",
    "    IDX = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self._data = defaultdict(set)\n",
    "        self.idx = CognateSet.IDX\n",
    "        CognateSet.IDX += 1\n",
    "\n",
    "    def add(self, lang, *words):\n",
    "        words = [w for w in words if w != '_']\n",
    "        if words:\n",
    "            self._data[lang].update(words)  # '_' is a placeholder.\n",
    "\n",
    "    def is_in(self, word, lang):\n",
    "        if lang not in self._data:  # Do this to avoid spurious keys for defaultdict.\n",
    "            return False\n",
    "        return word in self._data[lang]\n",
    "\n",
    "    def __contains__(self, lang):\n",
    "        return lang in self._data\n",
    "\n",
    "    def items(self):\n",
    "        return self._data.items()\n",
    "\n",
    "    def __getitem__(self, lang):\n",
    "        if not lang in self:\n",
    "            raise KeyError\n",
    "        else:\n",
    "            return self._data[lang]\n",
    "\n",
    "    def to_df(self):\n",
    "        data = list()\n",
    "        for l, s in self._data.items():\n",
    "            for w in s:\n",
    "                data.append((w, l, self.idx))\n",
    "        return pd.DataFrame(data, columns=['word', 'lang', 'idx'])\n",
    "\n",
    "\n",
    "class CognateDict:\n",
    "    '''\n",
    "    A big dictionary that stores every cognate set. This works by storing many CognateSet's, and each\n",
    "    word is then mapped to one set.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, langs):\n",
    "        self._cs = dict()\n",
    "        self._langs = langs\n",
    "        # For each language, map a word to the CognateSet's it is in.\n",
    "        self._keys = {l: defaultdict(list) for l in langs}\n",
    "\n",
    "    @property\n",
    "    def langs(self):\n",
    "        return self._langs\n",
    "\n",
    "    def add(self, *cognate_sets):\n",
    "        for cognate_set in cognate_sets:\n",
    "            for lang, words in cognate_set.items():\n",
    "                for w in words:\n",
    "                    self._keys[lang][w].append(cognate_set.idx)\n",
    "            self._cs[cognate_set.idx] = cognate_set\n",
    "\n",
    "    def find(self, word, lang):\n",
    "        if word not in self._keys[lang]:\n",
    "            raise KeyError(f'Word {word} in language {lang} not in the dictionary.')\n",
    "\n",
    "        ret = defaultdict(set)\n",
    "        for idx in self._keys[lang][word]:\n",
    "            cs = self._cs[idx]\n",
    "            for l, words in cs.items():\n",
    "                ret[l].update(words)\n",
    "        return ret\n",
    "\n",
    "    @cache(persist=True, full=False)\n",
    "    def to_df(self):\n",
    "        dfs = [cs.to_df() for cs in self._cs.values()]\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def get_wordlist(self, lang):\n",
    "        df = self.to_df()\n",
    "        return sorted(set(df[df['lang'] == lang]['word']))\n",
    "\n",
    "\n",
    "class CognateList:\n",
    "    '''\n",
    "    List of cognates, possibly with noncognates as well. This is the main class used for the stream object.\n",
    "    '''\n",
    "\n",
    "#    def __init__(self, cognate_path, lost_lang, known_lang, max_size=0, only_valid=True):\n",
    "#        self.all_langs = set([lost_lang, known_lang])\n",
    "#\n",
    "#        cognates = list()\n",
    "#        with Path(cognate_path).open(encoding='utf8') as fcog:\n",
    "#            header = fcog.readline().strip().split(\"\\t\")\n",
    "#            # transliterated_linear_b, greek\n",
    "#            header_langs = [header[0], header[1]]\n",
    "#\n",
    "#            for line in counter(fcog, max_size=max_size):\n",
    "#                tokens = line.strip().split('\\t')\n",
    "#                is_valid = tokens[3] == '1'\n",
    "#                tokens = [tokens[0], tokens[2]]\n",
    "#                if is_valid or not only_valid:\n",
    "#                    cog = CognateSet()\n",
    "#                    for l, t in zip(header_langs, tokens):\n",
    "#                        if l in self.all_langs:\n",
    "#                            cog.add(l, *t.split('|'))\n",
    "#                    cognates.append(cog)\n",
    "#\n",
    "#\n",
    "#        self._cognates = cognates\n",
    "#        self._cognate_dict = CognateDict(self.all_langs)\n",
    "#        self._cognate_dict.add(*cognates)\n",
    "\n",
    "    def __init__(self, cognate_path, lost_lang, known_lang, max_size=0):\n",
    "        self.all_langs = set([lost_lang, known_lang])\n",
    "\n",
    "        cognates = list()\n",
    "        with Path(cognate_path).open(encoding='utf8') as fcog:\n",
    "            header_langs = fcog.readline().strip().split(\"\\t\")\n",
    "            for line in counter(fcog, max_size=max_size):\n",
    "                tokens = line.strip().split('\\t')\n",
    "                cog = CognateSet()\n",
    "                for l, t in zip(header_langs, tokens):\n",
    "                    if l in self.all_langs:\n",
    "                        cog.add(l, *t.split('|'))\n",
    "                cognates.append(cog)\n",
    "        self._cognates = cognates\n",
    "        self._cognate_dict = CognateDict(self.all_langs)\n",
    "        self._cognate_dict.add(*cognates)\n",
    "\n",
    "    def get_wordlist(self, lang):\n",
    "        return self._cognate_dict.get_wordlist(lang)\n",
    "\n",
    "    def has_cognate(self, w, lang):\n",
    "        cs = self._cognate_dict.find(w.form, w.lang)\n",
    "        return lang in cs\n",
    "\n",
    "    def is_cognate(self, w1, w2):\n",
    "        cs = self._cognate_dict.find(w1.form, w1.lang)\n",
    "        if w2.lang in cs:\n",
    "            return w2.form in cs[w2.lang]\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THVkwEMUesYG"
   },
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMDkd0R4d_ku"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_VOCABS = dict()\n",
    "_COG_LIST = None\n",
    "\n",
    "\n",
    "def get_vocab(lang):\n",
    "    return _VOCABS[lang]\n",
    "\n",
    "\n",
    "def get_vocab_size(lang):\n",
    "    return len(get_vocab(lang))\n",
    "\n",
    "\n",
    "def get_words(lang):\n",
    "    return get_vocab(lang).words\n",
    "\n",
    "\n",
    "def get_forms(lang):\n",
    "    return get_vocab(lang).forms\n",
    "\n",
    "\n",
    "def is_cognate(w1, w2):\n",
    "    global _COG_LIST\n",
    "    return _COG_LIST.is_cognate(w1, w2)\n",
    "\n",
    "\n",
    "def has_cognate(w, lang):\n",
    "    global _COG_LIST\n",
    "    return _COG_LIST.has_cognate(w, lang)\n",
    "\n",
    "\n",
    "def clear_vocabs():\n",
    "    global _COG_LIST\n",
    "    global _VOCABS\n",
    "    _COG_LIST = None\n",
    "    _VOCABS = dict()\n",
    "\n",
    "\n",
    "def build_vocabs(path, lost_lang, known_lang, max_size=0):#, only_valid=True):\n",
    "    global _COG_LIST\n",
    "\n",
    "    assert _COG_LIST is None\n",
    "    cog_list = CognateList(path, lost_lang, known_lang, max_size=max_size)#, only_valid=only_valid)\n",
    "\n",
    "    for lang in [lost_lang, known_lang]:\n",
    "        if lang in _VOCABS:\n",
    "            raise ValueError(f'There already is a vocab for {lang}')\n",
    "        _VOCABS[lang] = _Vocab(cog_list.get_wordlist(lang), lang)\n",
    "    _COG_LIST = cog_list\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, order=True)\n",
    "class Word:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n",
    "\n",
    "\n",
    "@has_properties('lang')\n",
    "class _Vocab:\n",
    "\n",
    "    def __init__(self, wordlist, lang):\n",
    "        assert len(wordlist) == len(set(wordlist))  # Make sure they are all unique.\n",
    "        self._build(wordlist)\n",
    "\n",
    "    def _build(self, wordlist):\n",
    "        self._id2word = list()\n",
    "        self._form2id = dict()\n",
    "        for w in wordlist:\n",
    "            w = Word(self.lang, w, len(self._id2word))\n",
    "            self._id2word.append(w)\n",
    "            self._form2id[w.form] = len(self._form2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def words(self):\n",
    "        return np.asarray(self._id2word)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def forms(self):\n",
    "        return np.asarray([word.form for word in self.words])\n",
    "\n",
    "    def cognate_to(self, lang):\n",
    "        global _COG_LIST\n",
    "        return np.asarray([w for w in self.words if _COG_LIST.has_cognate(w, lang)])\n",
    "\n",
    "    def get_word_from_form(self, form):\n",
    "        return self._id2word[self._form2id[form]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEsmopy9rRv0"
   },
   "source": [
    "#### Test Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtlv--YRgPHj"
   },
   "outputs": [],
   "source": [
    "#clear_vocabs()\n",
    "#USE_RAW = False\n",
    "#file_name = \"cognates.cog\" if not USE_RAW else \"linear_b-greek.cog\"\n",
    "#lost_lang = \"transliterated_linear_b\" if not USE_RAW else \"linear_b\"\n",
    "#cog_path = os.path.join(prefix_path, file_name)\n",
    "#known_lang = \"greek\"\n",
    "#build_vocabs(cog_path, lost_lang, known_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNPIID47N7EM"
   },
   "outputs": [],
   "source": [
    "#get_words('transliterated_linear_b')[1].id_seq, get_words('transliterated_linear_b')[1].char_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBPS9HBnhxlZ"
   },
   "outputs": [],
   "source": [
    "#lb_words = get_words(lost_lang)\n",
    "#greek_words = get_words(known_lang)\n",
    "## \t\n",
    "#lin_b_test = \"si-mi-te-u\" if not USE_RAW else \"\"\n",
    "#for w in lb_words:\n",
    "#    if w.form == lin_b_test:\n",
    "#        print(w)\n",
    "#        print(has_cognate(w, known_lang))\n",
    "#        break\n",
    "#for g in greek_words:\n",
    "#    if g.form == \"\":\n",
    "#        print(g)\n",
    "#        print(has_cognate(g, lost_lang))\n",
    "#        print(is_cognate(g, w))\n",
    "#        print(is_cognate(w, g))\n",
    "#        break\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2l5zpz_oYym"
   },
   "outputs": [],
   "source": [
    "#w = Word(\"transliterated_linear_b\", \"ko-wo\", 0)\n",
    "#w, w.char_seq, w.id_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf9HEKgIa0aQ"
   },
   "source": [
    "### Charsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRqXIVjOazRX"
   },
   "outputs": [],
   "source": [
    "\n",
    "PAD_ID = 0\n",
    "SOW_ID = 1\n",
    "EOW_ID = 2\n",
    "UNK_ID = 3\n",
    "EOS_ID = 4\n",
    "\n",
    "PAD = '<PAD>'\n",
    "SOW = '<SOW>'\n",
    "EOW = '<EOW>'\n",
    "UNK = '<UNK>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "START_CHAR = [PAD, SOW, EOW, UNK, EOS]\n",
    "\n",
    "_CHARSETS = dict()\n",
    "\n",
    "\n",
    "def register_charset(lang):\n",
    "    global _CHARSETS\n",
    "\n",
    "    def decorated(cls):\n",
    "        assert lang not in _CHARSETS\n",
    "        _CHARSETS[lang] = cls\n",
    "        return cls\n",
    "\n",
    "    return decorated\n",
    "\n",
    "\n",
    "def get_charset(lang):\n",
    "    '''\n",
    "    Make sure only one charset is ever created.\n",
    "    '''\n",
    "    global _CHARSETS\n",
    "    cls_or_obj = _CHARSETS[lang]\n",
    "    if isinstance(cls_or_obj, type):\n",
    "        _CHARSETS[lang] = cls_or_obj()\n",
    "    return _CHARSETS[lang]\n",
    "\n",
    "\n",
    "def _recursive_map(func, lst):\n",
    "    ret = list()\n",
    "    for item in lst:\n",
    "        if isinstance(item, (list, np.ndarray)):\n",
    "            ret.append(_recursive_map(func, item))\n",
    "        else:\n",
    "            ret.append(func(item))\n",
    "    return ret\n",
    "\n",
    "\n",
    "class BaseCharset(object):\n",
    "\n",
    "    _CHARS = u''\n",
    "    _FEATURES = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self._id2char = START_CHAR + self.__class__._CHARS\n",
    "        self._char2id = dict(zip(self._id2char, range(len(self._id2char))))\n",
    "        self._feat_dict = {}\n",
    "        for f in self.features:\n",
    "            self._feat_dict['char'] = None\n",
    "            self._feat_dict[f] = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2char)\n",
    "\n",
    "    def char2id(self, char):\n",
    "        def map_func(c): return self._char2id.get(c, UNK_ID)\n",
    "        if isinstance(char, str):\n",
    "            return map_func(char)\n",
    "        elif isinstance(char, (np.ndarray, list)):\n",
    "            return np.asarray(_recursive_map(map_func, char))\n",
    "            # return np.asarray([np.asarray(list(map(map_func, ch))) for ch in char])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def id2char(self, id_):\n",
    "        #def map_func(i): return self._id2char[i] *** remove next line and restore this\n",
    "        def map_func(i): return self._id2char[i] if i < len(self._id2char) else \"<LOGOGRAM>\"\n",
    "        if isinstance(id_, int):\n",
    "            return map_func(id_)\n",
    "        elif isinstance(id_, (np.ndarray, list)):\n",
    "            return np.asarray(_recursive_map(map_func, id_))\n",
    "            # id_.tolist()\n",
    "            # if id_.ndim == 2:\n",
    "            #     return np.asarray([np.asarray(list(map(map_func, i))) for i in id_])\n",
    "            # elif id_.ndim == 3:\n",
    "            #     return np.asarray([self.id2char(i) for i in id_])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def get_tokens(self, ids):\n",
    "        if torch.is_tensor(ids):\n",
    "            ids = ids.cpu().numpy()\n",
    "        chars = self.id2char(ids)\n",
    "\n",
    "        def get_2d_tokens(chars):\n",
    "            tokens = list()\n",
    "            for char_seq in chars:\n",
    "                token = ''\n",
    "                for c in char_seq:\n",
    "                    if c == EOW:\n",
    "                        break\n",
    "                    elif c in START_CHAR:\n",
    "                        c = '|'\n",
    "                    token += c\n",
    "                tokens.append(token)\n",
    "            return np.asarray(tokens)\n",
    "\n",
    "        if chars.ndim == 3:\n",
    "            a, b, _ = chars.shape\n",
    "            chars = chars.reshape(a * b, -1)\n",
    "            tokens = get_2d_tokens(chars).reshape(a, b)\n",
    "        else:\n",
    "            tokens = get_2d_tokens(chars)\n",
    "        return tokens\n",
    "\n",
    "    def process(self, word):\n",
    "        # How to process chars in word. This function is language-dependent.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return self._FEATURES\n",
    "\n",
    "\n",
    "@register_charset('greek')\n",
    "class ElCharSet(BaseCharset):\n",
    "\n",
    "    _CHARS = list('fhy')\n",
    "    _FEATURES = ['']\n",
    "\n",
    "\n",
    "@register_charset('transliterated_linear_b')\n",
    "class LinbLatinCharSet(BaseCharset):\n",
    "    # _CHARS is a list (check .split(\"-\"))\n",
    "    _CHARS = 'a-e-i-o-u-da-de-di-do-du-dwe-dwo-ja-je-jo-ka-ke-ki-ko-ku-ma-me-mi-mo-mu-na-ne-ni-no-nu-nwa-pa-pe-pi-po-pu-pte-phu-qa-qe-qi-qo-ra-re-ri-ro-ru-rya-rai-ryo-sa-se-si-so-su-ta-te-ti-to-tu-tya-twe-two-wa-we-wi-wo-za-ze-zo-ha-ai-au-*18-*19-*22-*34-*35-*47-*49-*56-*63-*64-*65-*79-*82-*83-*86'.split(\"-\")\n",
    "    _FEATURES = ['']\n",
    "\n",
    "\n",
    "@register_charset('linear_b')\n",
    "class MinoanCharSet(BaseCharset):\n",
    "\n",
    "    _CHARS = list('')\n",
    "    _FEATURES = ['']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_charset('greek')._CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAqaPwXaAHNE"
   },
   "source": [
    "#### Test Charset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA_uDAew5rN9"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(known_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "radj284b-djZ"
   },
   "outputs": [],
   "source": [
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['<EOW>', '<UNK>', 'h', '']))\n",
    "#print(len(ch))\n",
    "#print(ch.get_tokens([i for i in range(len(ch))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4C6yMttAKjm"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(lost_lang)\n",
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['<EOW>', '<UNK>', 'te', 'po']))\n",
    "#print(len(ch))\n",
    "#print(ch.get_tokens([i for i in range(len(ch))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCKN374-oGaq"
   },
   "outputs": [],
   "source": [
    "#ch = get_charset(lost_lang)\n",
    "#print(ch.id2char([2, 3, 5, 7]))\n",
    "#print(ch.char2id(['ko', 'wo', '<EOW>']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKVUfb_Ks5wT"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgizMl8hs49N"
   },
   "outputs": [],
   "source": [
    "def pad_to_dense(a, dtype='f'):\n",
    "    '''\n",
    "    Modified from https://stackoverflow.com/questions/37676539/numpy-padding-matrix-of-different-row-size.\n",
    "    '''\n",
    "    assert dtype in ['f', 'l']\n",
    "    dtype = 'float32' if dtype == 'f' else 'int64'\n",
    "    maxlen = max(map(len, a))\n",
    "    ret = np.zeros((len(a), maxlen), dtype=dtype)\n",
    "    for i, row in enumerate(a):\n",
    "        row = np.asarray(row, dtype=dtype)  # force dtype conversion\n",
    "        ret[i, :len(row)] += row\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "@has_properties('lang')\n",
    "class WordlistDataset(Dataset):\n",
    "    \"\"\"This is for one language.\"\"\"\n",
    "\n",
    "    def __init__(self, words, lang, indices=None):\n",
    "        assert isinstance(words[0], Word), \"words list must contain instances of class Word\"\n",
    "        if indices is not None:\n",
    "            palle = []\n",
    "            indices = set(indices)\n",
    "            for w in words:\n",
    "                if w.idx in indices:\n",
    "                    palle.append(w)\n",
    "            #logging.critical(f\"WORDS: {len(palle)}, {len(words)}\")\n",
    "            self._words = palle\n",
    "        else:\n",
    "            self._words = words\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._words)\n",
    "\n",
    "    @cache(persist=True, full=True)\n",
    "    @cache(persist=True, full=True)\n",
    "    def __getitem__(self, idx):\n",
    "        word = self._words[idx]\n",
    "        return Map(word=word, form=word.form, lang=self.lang, char_seq=word.char_seq, id_seq=word.id_seq)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        return collate_fn([self[i] for i in range(len(self))])\n",
    "\n",
    "\n",
    "class VocabDataset(WordlistDataset):\n",
    "\n",
    "    def __init__(self, lang, indices=None):\n",
    "        super().__init__(get_words(lang), lang, indices=indices)\n",
    "\n",
    "\n",
    "def _get_item(key, batch):\n",
    "    # raised an exception as arrays have different lengths\n",
    "    return np.array([record[key] for record in batch], dtype=object)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words = _get_item('word', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "    char_seqs = _get_item('char_seq', batch)\n",
    "    id_seqs = _get_item('id_seq', batch)\n",
    "    lengths, words, forms, char_seqs, id_seqs = sort_all(words, forms, char_seqs, id_seqs)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    id_seqs = pad_to_dense(id_seqs, dtype='l')\n",
    "    id_seqs = get_tensor(id_seqs[:, :max_len])\n",
    "\n",
    "    lang = batch[0].lang\n",
    "\n",
    "\n",
    "    return Map(\n",
    "        words=words, forms=forms, char_seqs=char_seqs, id_seqs=id_seqs, lengths=lengths, lang=lang)\n",
    "\n",
    "\n",
    "def _prepare_stats(name, *rows):\n",
    "    table = pt()\n",
    "    table.field_names = 'lang', 'size'\n",
    "    for row in rows:\n",
    "        table.add_row(row)\n",
    "    table.align = 'l'\n",
    "    table.title = name\n",
    "    return table\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'cognate_only')\n",
    "class LostKnownDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.known_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for known_batch in super().__iter__():\n",
    "            lost_batch = self.datasets[self.lost_lang].entire_batch\n",
    "            num_samples = len(known_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@has_properties('lost_lang', 'known_lang', 'cognate_only')\n",
    "class LostKnownDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False, evaluation_mode=False, indices=None):\n",
    "        self.datasets = dict()\n",
    "        #logging.critical(f\"WORDS: {len(indices)}\")\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang, indices=indices)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang, indices=indices)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "        #logging.critical(f\"{self.datasets[self.lost_lang]}, {self.datasets[self.known_lang]}\")\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "        #logging.critical(f\"{batch_size}, {len(self.datasets[self.lost_lang])}, {len(indices)}\")\n",
    "        super().__init__(self.datasets[self.known_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for known_batch in super().__iter__():\n",
    "            lost_batch = self.datasets[self.lost_lang].entire_batch\n",
    "            num_samples = len(known_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LawEQT2Ft5cp"
   },
   "source": [
    "#### Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FocIHeotmsz"
   },
   "outputs": [],
   "source": [
    "#train_data_loader = LostKnownDataLoader(lost_lang, known_lang, 2, cognate_only=False)\n",
    "#eval_data_loader = LostKnownDataLoader(lost_lang, known_lang, 2, cognate_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByeSYq7ht-CH"
   },
   "outputs": [],
   "source": [
    "#create_logger(filepath='./log', log_level=\"DEBUG\")\n",
    "#log_pp(train_data_loader.stats('sdrogo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLgdHwYEJqXm"
   },
   "source": [
    "### Model Submodules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSNAT4IE60Ix"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTMState(object):\n",
    "\n",
    "    def __init__(self, states):\n",
    "        self.states = states\n",
    "\n",
    "    @classmethod\n",
    "    def from_pytorch(cls, states):\n",
    "        hs, cs = states\n",
    "        _, bs, d = hs.shape\n",
    "        hs = hs.view(-1, 2, bs, d) #changes shape to num_layers/2, 2, bs, d\n",
    "        cs = cs.view(-1, 2, bs, d)\n",
    "        nl = hs.shape[0]\n",
    "        #sums on the direction 2 after unbinding num_layers/2 so final is num_layers/2, bs, d\n",
    "        states = [(h.sum(dim=0), c.sum(dim=0)) for h, c in zip(hs.unbind(dim=0), cs.unbind(dim=0))]\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @classmethod\n",
    "    def stack(cls, iterator_states, dim):\n",
    "        nl = len(iterator_states[0])\n",
    "        hs = [list() for _ in range(nl)]\n",
    "        cs = [list() for _ in range(nl)]\n",
    "        for states in iterator_states:\n",
    "            for i, state in enumerate(states.states):\n",
    "                h, c = state\n",
    "                hs[i].append(h)\n",
    "                cs[i].append(c)\n",
    "        states = list()\n",
    "        for i in range(nl):\n",
    "            h = torch.stack(hs[i], dim)\n",
    "            c = torch.stack(cs[i], dim)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @classmethod\n",
    "    def zero_state(cls, num_layers, shape):\n",
    "        states = list()\n",
    "        for _ in range(num_layers):\n",
    "            h = get_zeros(*shape)\n",
    "            c = get_zeros(*shape)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.states[0][0].shape\n",
    "\n",
    "    def clone(self):\n",
    "        new_states = [(s[0].clone(), s[1].clone()) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def dim(self):\n",
    "        return self.states[0][0].dim()\n",
    "\n",
    "    def unsqueeze(self, dim):\n",
    "        new_states = [(s[0].unsqueeze(dim), s[1].unsqueeze(dim)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def view(self, *sizes):\n",
    "        new_states = [(s[0].view(*sizes), s[1].view(*sizes)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def unbind(self, dim):\n",
    "        n = self.states[0][0].shape[dim]\n",
    "        ret = [list() for _ in range(n)]\n",
    "        for s in self.states:\n",
    "            h, c = s\n",
    "            hs = h.unbind(dim)\n",
    "            cs = c.unbind(dim)\n",
    "            for i, (h, c) in enumerate(zip(hs, cs)):\n",
    "                ret[i].append((h, c))\n",
    "        ret = tuple(LSTMState(s) for s in ret)\n",
    "        return ret\n",
    "\n",
    "    def expand(self, *sizes):\n",
    "        new_states = [(s[0].expand(*sizes), s[1].expand(*sizes)) for s in self.states]\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def contiguous(self):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0].contiguous()\n",
    "            c = s[1].contiguous()\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def detach_(self):\n",
    "        for s in self.states:\n",
    "            s[0].detach_()\n",
    "            s[1].detach_()\n",
    "\n",
    "    def size(self):\n",
    "        return self.states[0][0].size()\n",
    "\n",
    "    def cat(self, other, dim):\n",
    "        states = list()\n",
    "        for s1, s2 in zip(self.states, other.states):\n",
    "            h = torch.cat([s1[0], s2[0]], dim)\n",
    "            c = torch.cat([s1[1], s2[1]], dim)\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0] * other\n",
    "            c = s[1] * other\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        states = list()\n",
    "        for s1, s2 in zip(self.states, other.states):\n",
    "            h = s1[0] + s2[0]\n",
    "            c = s1[1] + s2[1]\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.states[-1][0]\n",
    "\n",
    "    def get(self, ind):\n",
    "        return self.states[ind]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        states = list()\n",
    "        for s in self.states:\n",
    "            h = s[0][key]\n",
    "            c = s[1][key]\n",
    "            states.append((h, c))\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        for s1, s2 in zip(self.states, item.states):\n",
    "            s1[0][key] = s2[0]\n",
    "            s1[1][key] = s2[1]\n",
    "\n",
    "\n",
    "class UniversalCharEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, langs, char_emb_dim, universal_charset_size, mapping_temperature=0.0):\n",
    "        super(UniversalCharEmbedding, self).__init__()\n",
    "        self.langs = langs\n",
    "        self.charsets = {l: get_charset(l) for l in langs}\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.universal_charset_size = universal_charset_size\n",
    "        self.mapping_temperature = mapping_temperature\n",
    "        # U in the paper\n",
    "        self.char_emb = nn.Embedding(self.universal_charset_size, self.char_emb_dim)\n",
    "        # Wx and Wy in the paper\n",
    "        self.char_weights = nn.ModuleDict({\n",
    "            l: nn.Embedding(len(self.charsets[l]), self.universal_charset_size)\n",
    "            for l in self.langs})\n",
    "        self.device = next(self.parameters()).device\n",
    "\n",
    "    # takes embeddings of the input char sequence char_seq\n",
    "    def forward(self, char_seq, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        char_seq = char_seq.to(self.device)\n",
    "        return char_emb[char_seq]\n",
    "\n",
    "    # projects embedded sequence into the space of characters of language lang\n",
    "    # calculates input_ * U^T * Wy^T for language y\n",
    "    def project(self, input_, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return input_.matmul(char_emb.t())\n",
    "\n",
    "    # calculates Wx * U\n",
    "    @cache(full=True)\n",
    "    def get_char_weight(self, lang):\n",
    "        mapping = self.mapping(lang)\n",
    "        char_emb = mapping.matmul(self.char_emb.weight)\n",
    "        return char_emb\n",
    "\n",
    "    # calculates Wx for language x\n",
    "    @cache(full=True)\n",
    "    def mapping(self, lang):\n",
    "        weight = self.char_weights[lang].weight\n",
    "        if self.mapping_temperature > 0.0:\n",
    "            # NOTE use log_softmax first for more numerical stability\n",
    "            weight = torch.log_softmax(weight / self.mapping_temperature, dim=-1).exp()\n",
    "        return weight\n",
    "\n",
    "    # computes similarity between characters of the two languages\n",
    "    def char_sim_mat(self, lang1, lang2):\n",
    "        # Wx\n",
    "        x = normalize(self.mapping(lang1), dim=-1)\n",
    "        # Wy\n",
    "        y = normalize(self.mapping(lang2), dim=-1)\n",
    "        # mat = Wx * Wy^T\n",
    "        mat = x.matmul(y.t())\n",
    "        return mat\n",
    "\n",
    "    # this function computes similarity scores between the universal embeddings of the characters of the languages\n",
    "    def char_softmax(self, lang1, lang2):\n",
    "        # w1 = Wx * U\n",
    "        w1 = self.get_char_weight(lang1)\n",
    "        # w2 = Wy * U\n",
    "        w2 = self.get_char_weight(lang2)\n",
    "        mat = w1.matmul(w2.t())\n",
    "        # mat = Wx * U * U^T * Wy^T = w1 * w2^T. Shape: nx, ny\n",
    "        l1_l2 = mat.log_softmax(dim=-1).exp() # shape becomes: nx, ny (notice softmax on last dim)\n",
    "        l2_l1 = mat.log_softmax(dim=0).exp().t() # shape becomes nx, ny (notice softmax on first dim and .t())\n",
    "\n",
    "        return l1_l2, l2_l1\n",
    "\n",
    "    # this computes a mapping between the chars of the two languages in both directions:\n",
    "    # for each character of the two words, the top k (k=3) similar characters of the other language\n",
    "    # are retrieved and put into a dictionary. Also the values of similarity are returned\n",
    "    def char_mapping(self, l1, l2):\n",
    "        l1_l2, l2_l1 = self.char_softmax(l1, l2)\n",
    "        # get topk most similar items\n",
    "        def get_topk(a2b, a_cs, b_cs):\n",
    "            # s = values of similarity, idx = indexes of most similar items\n",
    "            s, idx = a2b[4:].topk(3, dim=-1) # notice: first 4 token (PAD, SOW, EOW, UNK) are excluded\n",
    "            a = a_cs.id2char(np.arange(4, len(a2b)).reshape(1, -1)).reshape(-1)\n",
    "            b = b_cs.id2char(idx.cpu().numpy())\n",
    "            d = {aa: ' '.join(bb) for aa, bb in zip(a, b)}\n",
    "            return d, s\n",
    "        l1_l2 = get_topk(l1_l2, self.charsets[l1], self.charsets[l2])\n",
    "        l2_l1 = get_topk(l2_l1, self.charsets[l2], self.charsets[l1])\n",
    "        return l1_l2, l2_l1\n",
    "\n",
    "    # calculates weight * Wy * U for language y\n",
    "    def soft_emb(self, weight, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return weight.matmul(char_emb)\n",
    "\n",
    "    def get_start_emb(self, lang):\n",
    "        char_emb = self.get_char_weight(lang)\n",
    "        return char_emb[SOW_ID]\n",
    "\n",
    "\n",
    "class MultiLayerLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0):\n",
    "        super(MultiLayerLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        cells = [nn.LSTMCell(input_size, hidden_size)] + \\\n",
    "                [nn.LSTMCell(hidden_size, hidden_size) for _ in range(self.num_layers - 1)]\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "\n",
    "    # initializes all LSTM cells with the mean of the hidden states of the batch and of the cell states of the batch\n",
    "    # NOTE: unused in the code, as it works by default with a single layer and therefore passes the single state directly\n",
    "    #       to the forward. I believed it is needed in case we wanna use a multi-layer decoder\n",
    "    def init_state(self, bs, encoding):\n",
    "        states = list()\n",
    "        for _ in range(self.num_layers):\n",
    "            state = (encoding[0].mean(dim=0), encoding[1].mean(dim=0))\n",
    "            states.append(state)\n",
    "        return LSTMState(states)\n",
    "\n",
    "    def forward(self, input_, states):\n",
    "        assert len(states) == self.num_layers\n",
    "\n",
    "        new_states = list()\n",
    "        # computes new states for the decoder\n",
    "        # input_ becomes each time the output of the previous layer\n",
    "        for i in range(self.num_layers):\n",
    "            new_state = self.cells[i](input_, states.get(i))\n",
    "            new_states.append(new_state)\n",
    "            input_ = new_state[0]\n",
    "            input_ = self.drop(input_)\n",
    "        return LSTMState(new_states)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '%d, %d, num_layers=%d' % (self.input_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "\n",
    "class GlobalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_src_size, input_tgt_size, dropout=0.0):\n",
    "        super(GlobalAttention, self).__init__()\n",
    "\n",
    "        self.input_src_size = input_src_size\n",
    "        self.input_tgt_size = input_tgt_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.Wa = nn.Parameter(torch.Tensor(input_src_size, input_tgt_size))\n",
    "        #self.Wf = nn.Parameter(torch.Tensor(256, input_tgt_size))  # Project FastText to dt\n",
    "\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "\n",
    "    @cache(full=False)\n",
    "    # simply multiplies Wa and h_s\n",
    "    def _get_Wh_s(self, h_s):\n",
    "        bs, l, _ = h_s.shape # bs x sl x ds\n",
    "        # shape of Wa is ds x dt\n",
    "        # There is some weird bug with dropout layer if dropout rate is zero\n",
    "        Wh_s = self.drop(h_s).reshape(bs * l, -1).mm(self.Wa).view(bs, l, -1)\n",
    "        return Wh_s\n",
    "\n",
    "    def forward(self, ctx_t, h_s, mask_src):#, fasttext_embs):\n",
    "        bs, sl, ds = h_s.size()\n",
    "        dt = ctx_t.shape[-1] # bs x dt\n",
    "\n",
    "        Wh_s = self._get_Wh_s(h_s)  # bs x sl x dt\n",
    "\n",
    "        # combine decoder out with fasttext embeddings\n",
    "        scores = Wh_s.matmul(self.drop(ctx_t).unsqueeze(dim=-1)).squeeze(dim=-1)  # bs x sl\n",
    "        scores = scores * mask_src + (-9999.) * (1.0 - mask_src)\n",
    "        almt_distr = nn.functional.log_softmax(scores, dim=-1).exp()  # bs x sl\n",
    "\n",
    "        #fasttext_ctx = fasttext_embs.matmul(self.Wf)  # [bs, dt]\n",
    "\n",
    "        # Shared Wh_s from earlier (projected h_s): [bs, sl, dt]\n",
    "        #scores_ft = Wh_s.matmul(fasttext_ctx.unsqueeze(-1)).squeeze(-1)  # [bs, sl]\n",
    "        #scores_ft = scores_ft * mask_src + (-9999.) * (1.0 - mask_src)\n",
    "        #ft_contribute = nn.functional.log_softmax(scores_ft, dim=-1).exp()\n",
    "        return almt_distr#, ft_contribute\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'src=%d, tgt=%d' % (self.input_src_size, self.input_tgt_size)\n",
    "\n",
    "# none: only sum\n",
    "# absolute: give a list of norms, each tensor will be scaled by the corresponding inputed norm\n",
    "# relative: give a list of fractions and a multiplier, each tensor will be scaled following paper's formula\n",
    "#           ex. ratios = [1.0, 0.7] and multiplier = 1\n",
    "class NormControlledResidual(nn.Module):\n",
    "\n",
    "    def __init__(self, norms_or_ratios=None, multiplier=1.0, control_mode=None):\n",
    "        super(NormControlledResidual, self).__init__()\n",
    "\n",
    "        assert control_mode in ['none', 'relative', 'absolute']\n",
    "\n",
    "        self.control_mode = control_mode\n",
    "        self.norms_or_ratios = None\n",
    "        if self.control_mode in ['relative', 'absolute']:\n",
    "            self.norms_or_ratios = norms_or_ratios\n",
    "            if self.control_mode == 'relative':\n",
    "                assert self.norms_or_ratios[0] == 1.0\n",
    "\n",
    "        self.multiplier = multiplier\n",
    "\n",
    "    def anneal_ratio(self):\n",
    "        if self.control_mode == 'relative':\n",
    "            new_ratios = [self.norms_or_ratios[0]]\n",
    "            for r in self.norms_or_ratios[1:]:\n",
    "                r = min(r * self.multiplier, 1.0)\n",
    "                new_ratios.append(r)\n",
    "            self.norms_or_ratios = new_ratios\n",
    "            logging.debug('Ratios are now [%s]' % (', '.join(map(lambda f: '%.2f' % f, self.norms_or_ratios))))\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        if self.control_mode == 'none':\n",
    "            output = sum(inputs)\n",
    "        else:\n",
    "            assert len(inputs) == len(self.norms_or_ratios)\n",
    "            outs = list()\n",
    "            if self.control_mode == 'absolute':\n",
    "                for inp, norm in zip(inputs, self.norms_or_ratios):\n",
    "                    if norm >= 0.0:  # NOTE a negative value means no control applied\n",
    "                        outs.append(normalize(inp, dim=-1) * norm)\n",
    "                    else:\n",
    "                        outs.append(inp)\n",
    "            else:\n",
    "                outs.append(inputs[0])\n",
    "                norm_base = inputs[0].norm(dim=-1, keepdim=True)\n",
    "                for inp, ratio in zip(inputs[1:], self.norms_or_ratios[1:]):\n",
    "                    if ratio >= 0.0:  # NOTE same here\n",
    "                        norm_actual = inp.norm(dim=-1, keepdim=True)\n",
    "                        max_norm = norm_base * ratio\n",
    "                        too_big = norm_actual > max_norm\n",
    "                        adjusted_norm = torch.where(too_big, max_norm, norm_actual)\n",
    "                        outs.append(normalize(inp, dim=-1) * adjusted_norm)\n",
    "                    else:\n",
    "                        outs.append(inp)\n",
    "            output = sum(outs)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOSgQ9c0KPxN"
   },
   "outputs": [],
   "source": [
    "@has_properties('lang')\n",
    "class Trie:\n",
    "    '''\n",
    "        A trie that efficiently computes the log probs for every word.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lang):\n",
    "\n",
    "        words = get_words(lang)\n",
    "        self._max_length = max(map(len, words))  # NOTE EOW has been taken care of by __len__\n",
    "        self._prepare_weight()\n",
    "        self.clear_cache()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._eff_weight = self._weight\n",
    "        self._eff_max_length = self._max_length\n",
    "\n",
    "    def _prepare_weight(self):\n",
    "        rows = list()\n",
    "        cols = list()\n",
    "\n",
    "        words = get_words(self.lang)\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        self._word2rows = defaultdict(list)\n",
    "        # here the char_seq is transformed into a one-hot encoded vector\n",
    "        for row, word in enumerate(words):\n",
    "            # OUR MODIFICATION\n",
    "            # padding the sequence with EOW to preserve original length in the mapping wrt the starting sequence\n",
    "            char_seq = word.char_seq  # already a NumPy array\n",
    "            padded_length = self._max_length - len(word)\n",
    "            if padded_length > 0:\n",
    "                # NumPy padding using concatenation\n",
    "                pad = np.full(padded_length, EOW, dtype=char_seq.dtype)\n",
    "                char_seq = np.concatenate([char_seq, pad])\n",
    "\n",
    "            for i, c in enumerate(char_seq):\n",
    "                cid = charset.char2id(c)\n",
    "                self._word2rows[word].append(len(rows))\n",
    "                rows.append(row)\n",
    "                # for each row a sparse vector in defined with a 1 only in the position defined by the cid\n",
    "                # the vector associated to each row has length len(charset)\n",
    "                cols.append(len(charset) * i + cid)\n",
    "        # rows and cols now define the indexes of the sparse tensor with the values one\n",
    "        data = np.ones(len(rows))\n",
    "        # NOTE This is ugly, but it avoids this issue in 0.4.1: https://github.com/pytorch/pytorch/issues/8856.\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len(words), self._max_length * len(charset)))\n",
    "        self._weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def _sample(self, words):\n",
    "        all_words = get_words(self.lang)\n",
    "        word_indices = list()\n",
    "        old_to_new = np.zeros([len(all_words)], dtype='int64')\n",
    "        self._eff_id2word = list()\n",
    "        self._eff_word2id = dict()\n",
    "        self._eff_max_length = max(map(len, words))\n",
    "        # creates new indices for the sample of words\n",
    "        # also takes the indexes of the characters corresponding to the sampled words\n",
    "        # from weight matrix\n",
    "        for w in words:\n",
    "            word_indices.extend(self._word2rows[w])\n",
    "            old_to_new[w.idx] = len(self._eff_id2word)\n",
    "            self._eff_id2word.append(w)\n",
    "            self._eff_word2id[w] = len(self._eff_word2id)\n",
    "        old_to_new = get_tensor(old_to_new)\n",
    "        #logging.debug(f\"Old to new: {old_to_new}, {len(old_to_new)}\")\n",
    "        #logging.debug(f\"words: {[w.idx for w in self._eff_word2id.keys()]}\")\n",
    "\n",
    "        indices = get_tensor(word_indices, dtype='l')\n",
    "        # here the indexes of the sampled words' characters are retrieved\n",
    "        old_rows, cols = self._weight._indices()[:, word_indices].unbind(dim=0)\n",
    "        # the new sparse vector will follow the new indexing of the words\n",
    "        rows = old_to_new[old_rows]\n",
    "        # sample the data corresponding to the characters of the sampled words\n",
    "        data = self._weight._values()[word_indices]\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        # OUR MODIFICATION\n",
    "        # Filter out padding beyond eff_max_length\n",
    "        max_col = self._eff_max_length * len(charset)\n",
    "        keep = cols < max_col\n",
    "\n",
    "        cols = cols[keep]\n",
    "        rows = rows[keep]\n",
    "        data = data[keep]\n",
    "\n",
    "        \n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            torch.stack([rows, cols], dim=0),\n",
    "            data,\n",
    "            (len(words), self._eff_max_length * len(charset)))\n",
    "        self._eff_weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def analyze(self, log_probs, almt_distr, words, lost_lengths):\n",
    "        self.clear_cache()\n",
    "        self._sample(words)\n",
    "\n",
    "        assert self._eff_max_length == len(log_probs)\n",
    "\n",
    "        tl, nc, bs = log_probs.shape\n",
    "        charset = get_charset(self.lang)\n",
    "        assert nc == len(charset)\n",
    "\n",
    "        # V x bs, or c_s x c_t -> bs x V\n",
    "        valid_log_probs = self._eff_weight.matmul(log_probs.view(-1, bs)).t()\n",
    "\n",
    "        sl = almt_distr.shape[-1]\n",
    "        pos = get_tensor(torch.arange(sl).float(), requires_grad=False)\n",
    "        mean_pos = (pos * almt_distr).sum(dim=-1)  # bs x tl\n",
    "        mean_pos = torch.cat([get_zeros(bs, 2, requires_grad=False).fill_(-1.0), mean_pos],\n",
    "                             dim=-1)\n",
    "\n",
    "        reg_weight = lost_lengths.float().view(-1, 1) - 1.0 - mean_pos[:, :-2]\n",
    "        reg_weight.clamp_(0.0, 1.0)\n",
    "        # here we should go through alignment between characters at distance 2 instead of 1 for the tgt length\n",
    "        # assuming that 1 LinB char -> 2 AncGreek chars\n",
    "        rel_pos = mean_pos[:, 2:] - mean_pos[:, :-2]  # bs x tl\n",
    "        rel_pos_diff = rel_pos - 1\n",
    "        margin = rel_pos_diff != 0\n",
    "        reg_loss = margin.float() * (rel_pos_diff ** 2)  # bs x tl\n",
    "        reg_loss = (reg_loss * reg_weight).sum()\n",
    "        #logging.debug(reg_loss)\n",
    "        out = Map(reg_loss=reg_loss, valid_log_probs=valid_log_probs)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL TRIE\n",
    "@has_properties('lang')\n",
    "class Trie:\n",
    "    '''\n",
    "        A trie that efficiently computes the log probs for every word.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lang):\n",
    "\n",
    "        words = get_words(lang)\n",
    "        self._max_length = max(map(len, words))  # NOTE EOW has been taken care of by __len__\n",
    "        self._prepare_weight()\n",
    "        self.clear_cache()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._eff_weight = self._weight\n",
    "        self._eff_max_length = self._max_length\n",
    "\n",
    "    def _prepare_weight(self):\n",
    "        rows = list()\n",
    "        cols = list()\n",
    "\n",
    "        words = get_words(self.lang)\n",
    "        charset = get_charset(self.lang)\n",
    "\n",
    "        self._word2rows = defaultdict(list)\n",
    "        # here the char_seq is transformed into a one-hot encoded vector\n",
    "        for row, word in enumerate(words):\n",
    "            for i, c in enumerate(word.char_seq):\n",
    "                cid = charset.char2id(c)\n",
    "                self._word2rows[word].append(len(rows))\n",
    "                rows.append(row)\n",
    "                # for each row a sparse vector in defined with a 1 only in the position defined by the cid\n",
    "                # the vector associated to each row has length len(charset)\n",
    "                cols.append(len(charset) * i + cid)\n",
    "        # rows and cols now define the indexes of the sparse tensor with the values one\n",
    "        data = np.ones(len(rows))\n",
    "        # NOTE This is ugly, but it avoids this issue in 0.4.1: https://github.com/pytorch/pytorch/issues/8856.\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len(words), self._max_length * len(charset)))\n",
    "        self._weight = get_tensor(weight)\n",
    "\n",
    "\n",
    "    def _sample(self, words):\n",
    "        #logging.debug(f\"LEN WORDS: {len(words)}\")\n",
    "        all_words = get_words(self.lang)\n",
    "        word_indices = list()\n",
    "        old_to_new = np.zeros([len(all_words)], dtype='int64')\n",
    "        self._eff_id2word = list()\n",
    "        self._eff_word2id = dict()\n",
    "        self._eff_max_length = max(map(len, words))\n",
    "        # creates new indices for the sample of words\n",
    "        # also takes the indexes of the characters corresponding to the sampled words\n",
    "        # from weight matrix\n",
    "        for w in words:\n",
    "            word_indices.extend(self._word2rows[w])\n",
    "            old_to_new[w.idx] = len(self._eff_id2word)\n",
    "            self._eff_id2word.append(w)\n",
    "            self._eff_word2id[w] = len(self._eff_word2id)\n",
    "        old_to_new = get_tensor(old_to_new)\n",
    "        indices = get_tensor(word_indices, dtype='l')\n",
    "        # here the indexes of the sampled words' characters are retrieved\n",
    "        old_rows, cols = self._weight._indices()[:, word_indices].unbind(dim=0)\n",
    "        # the new sparse vector will follow the new indexing of the words\n",
    "        rows = old_to_new[old_rows]\n",
    "        # sample the data corresponding to the characters of the sampled words\n",
    "        data = self._weight._values()[word_indices]\n",
    "        charset = get_charset(self.lang)\n",
    "        weight = torch.sparse.FloatTensor(\n",
    "            torch.stack([rows, cols], dim=0),\n",
    "            data,\n",
    "            (len(words), self._eff_max_length * len(charset)))\n",
    "        self._eff_weight = get_tensor(weight)\n",
    "        #logging.debug(f\"WEIGHT shape {weight.shape}\")\n",
    "\n",
    "\n",
    "    def analyze(self, log_probs, almt_distr, words, lost_lengths):\n",
    "        self.clear_cache()\n",
    "        self._sample(words)\n",
    "\n",
    "        assert self._eff_max_length == len(log_probs)\n",
    "\n",
    "        tl, nc, bs = log_probs.shape\n",
    "        charset = get_charset(self.lang)\n",
    "        assert nc == len(charset)\n",
    "\n",
    "        # V x bs, or c_s x c_t -> bs x V\n",
    "        valid_log_probs = self._eff_weight.matmul(log_probs.view(-1, bs)).t()\n",
    "\n",
    "        sl = almt_distr.shape[-1]\n",
    "        pos = get_tensor(torch.arange(sl).float(), requires_grad=False)\n",
    "\n",
    "        mean_pos = (pos * almt_distr).sum(dim=-1)  # bs x tl\n",
    "        mean_pos = torch.cat([get_zeros(bs, 2, requires_grad=False).fill_(-1.0), mean_pos],\n",
    "                             dim=-1)\n",
    "\n",
    "        reg_weight = lost_lengths.float().view(-1, 1) - 1.0 - mean_pos[:, :-2]\n",
    "        reg_weight.clamp_(0.0, 1.0)\n",
    "        # here we should go through alignment between characters at distance 2 instead of 1 for the tgt length\n",
    "        # assuming that 1 LinB char -> 2 AncGreek chars\n",
    "        rel_pos = mean_pos[:, 2:] - mean_pos[:, :-2]  # bs x tl\n",
    "        rel_pos_diff = rel_pos - 1\n",
    "        margin = rel_pos_diff != 0\n",
    "        reg_loss = margin.float() * (rel_pos_diff ** 2)  # bs x tl\n",
    "        reg_loss = (reg_loss * reg_weight).sum()\n",
    "\n",
    "        out = Map(reg_loss=reg_loss, valid_log_probs=valid_log_probs)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orwm78mS7Q9c"
   },
   "source": [
    "#### Test Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FJRSWmmgtZ9",
    "outputId": "660162a7-0cd0-4c51-bc9b-472e41069cde"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate the shapes for Wh_s and ctx_t\n",
    "batch_size = 4  # Example batch size\n",
    "source_sequence_length = 5  # Source sequence length\n",
    "target_hidden_size = 6  # Hidden size of the target\n",
    "target_sequence_length = 3  # Target sequence length\n",
    "\n",
    "# Random initialization of Wh_s and ctx_t\n",
    "Wh_s = torch.randn(batch_size, source_sequence_length, target_hidden_size)  # [batch_size, source_sequence_length, target_hidden_size]\n",
    "print(Wh_s.shape)\n",
    "ctx_t = torch.randn(batch_size, target_hidden_size)  # [batch_size, target_hidden_size]\n",
    "print(ctx_t.shape)\n",
    "# Perform the unsqueeze operation\n",
    "ctx_t_unsqueezed = ctx_t.unsqueeze(dim=-1)  # [batch_size, target_hidden_size, 1]\n",
    "\n",
    "# Perform the matrix multiplication\n",
    "Wh_s_drop = Wh_s  # No dropout for simplicity in this case\n",
    "scores = Wh_s_drop.matmul(ctx_t_unsqueezed).squeeze(dim=-1)  # [batch_size, source_sequence_length]\n",
    "\n",
    "# Show the result\n",
    "scores.shape, scores  # Shape and values of the attention scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sunw-P2bJpW3",
    "outputId": "3f8af9b3-a1c4-4148-d182-def4c7998d3b"
   },
   "outputs": [],
   "source": [
    "rows = rows = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "cols = [32*0 + 8, 32*1+13, 32*2+30, 32 * 3 + 8, 32 * 0 + 9, 32* 1 + 10, 32*2 + 15, 32 * 3 + 8]\n",
    "data = np.ones([len(rows)])\n",
    "max_len = 4\n",
    "len_char_set = 32\n",
    "len_words = 2\n",
    "weight = torch.sparse.FloatTensor(\n",
    "            get_tensor([rows, cols], dtype='l', use_cuda=False),\n",
    "            get_tensor(data, dtype='f', use_cuda=False),\n",
    "            (len_words, max_len * len_char_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMoiD_0kKlP9",
    "outputId": "bb599ed4-2929-4ab3-b149-d2560badcf91"
   },
   "outputs": [],
   "source": [
    "ors, c = weight._indices()[:, :4]\n",
    "print(ors, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7elfqidiFx9z"
   },
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikI8xzL1F2Wq"
   },
   "outputs": [],
   "source": [
    "def compute_expected_edits(known_charset, log_probs, wordlist, valid_log_probs, num_samples=10, alpha=1e1, edit=False):\n",
    "    logging.debug('Computing expected edits')\n",
    "    device = log_probs.device\n",
    "    log_probs = log_probs.transpose(0, 2).transpose(1, 2)  # size: bs x tl x C\n",
    "    log_probs = torch.log_softmax(log_probs * alpha, dim=-1) # alpha is just a scaling factor\n",
    "    probs = log_probs.exp()\n",
    "    bs, tl, nc = probs.shape\n",
    "    # get samples\n",
    "    if num_samples > 0:\n",
    "        samples = torch.multinomial(probs.cpu().reshape(bs * tl, nc), num_samples, replacement=True).to(device) #Added .cpu() and to() device to avoid cuda determinism error\n",
    "        samples = samples.view(bs, tl, num_samples)\n",
    "        # get tokens\n",
    "        tokens = known_charset.get_tokens(samples.transpose(1, 2))  # size: bs x num_samples\n",
    "        # get probs\n",
    "        sample_log_probs = log_probs[torch.arange(bs, device=device).long().view(-1, 1, 1),\n",
    "                                     torch.arange(tl, device=device).long().view(1, -1, 1), samples]  # bs x tl x ns\n",
    "        lengths = get_tensor(np.vectorize(len)(tokens) + 1, dtype='f').to(device)  # bs x num_samples\n",
    "        mask = get_tensor(torch.arange(tl, device=device)).float().view(\n",
    "            1, -1, 1).expand(bs, tl, num_samples).to(device) < lengths.unsqueeze(dim=1)\n",
    "        sample_log_probs = (mask.float() * sample_log_probs).sum(dim=1)  # bs x num_samples\n",
    "    else:  # This means we are taking the argmax according to token-level probs, not character-level probs.\n",
    "        # Take argmax\n",
    "        _, idx = valid_log_probs.max(dim=-1)\n",
    "        tokens = wordlist[idx.cpu().numpy()].reshape(bs, 1)\n",
    "        num_samples = 1\n",
    "        sample_log_probs = get_tensor(np.ones([bs, 1])).to(device)\n",
    "    # use chunks to get all edits\n",
    "    chunk_size = 1000\n",
    "    num_chunks = len(wordlist) // chunk_size + (len(wordlist) % chunk_size > 0)\n",
    "    expected_edits = list()\n",
    "    for i in range(num_chunks):\n",
    "        logging.debug('Computing chunk %d/%d' % (i + 1, num_chunks))\n",
    "        start = i * chunk_size\n",
    "        end = min(start + chunk_size, len(wordlist))\n",
    "\n",
    "        valid_log_prob_chunk = valid_log_probs[:, start: end]\n",
    "        if edit:\n",
    "            # get dists\n",
    "            dists = compute_dists(tokens, wordlist[start: end])  # bs x c_s x (1 + ns)\n",
    "            dists = get_tensor(dists, 'f').to(device)\n",
    "            # remove accidental hits\n",
    "            duplicates = compute_duplicates(tokens, wordlist[start: end])  # bs x c_s x (1 + ns)\n",
    "            duplicates = get_tensor(duplicates, 'f').to(device)\n",
    "            edit_chunk = dists * duplicates\n",
    "            # compute expected edits\n",
    "            ex_sample_log_probs = sample_log_probs.view(\n",
    "                bs, 1, num_samples).expand(-1, valid_log_prob_chunk.shape[-1], -1)\n",
    "            all_sample_log_probs = torch.cat([valid_log_prob_chunk.unsqueeze(dim=-1), ex_sample_log_probs], dim=-1)\n",
    "            # make it less sharp\n",
    "            all_sample_log_probs = all_sample_log_probs + (1.0 - duplicates) * (-999.)\n",
    "            logits = all_sample_log_probs  # * alpha\n",
    "            sm_log_probs = torch.log_softmax(logits, dim=-1)  # NOTE sm stands for softmax\n",
    "            sm_probs = sm_log_probs.exp()\n",
    "            expected_edits.append((edit_chunk * sm_probs).sum(dim=-1))\n",
    "        else:\n",
    "            expected_edits.append(-valid_log_prob_chunk.tensor.to(device))\n",
    "    return torch.cat(expected_edits, dim=1)\n",
    "\n",
    "\n",
    "def compute_dists(sample_forms, wordlist):\n",
    "    # global _DISTS_CACHE\n",
    "    bs, ns = sample_forms.shape\n",
    "    sample_forms = sample_forms.flatten()\n",
    "    edits = cdist(wordlist, sample_forms, scorer=Levenshtein.distance, workers=-1)\n",
    "    edits = edits.reshape(len(wordlist), bs, ns)\n",
    "    dists = np.transpose(edits, [1, 0, 2])\n",
    "    dists = np.concatenate([np.zeros([bs, len(wordlist), 1], dtype='int64'), dists], axis=-1)\n",
    "    dists = dists.astype('float32')\n",
    "    lengths = np.asarray(list(map(len, wordlist)))\n",
    "    sample_lengths = np.asarray(list(map(len, sample_forms))).reshape(bs, ns)\n",
    "    min_lengths = np.minimum(lengths.reshape(1, -1, 1), sample_lengths.reshape(bs, 1, ns))\n",
    "    min_lengths = np.concatenate([np.repeat(lengths.reshape(1, -1), bs, axis=0).reshape(bs, -1, 1),\n",
    "                                  min_lengths], axis=-1) + 1\n",
    "    dists = dists / min_lengths\n",
    "    return dists\n",
    "\n",
    "\n",
    "def compute_duplicates(sample_forms, wordlist):\n",
    "    bs, ns = sample_forms.shape\n",
    "    dups = np.ones([bs, len(wordlist), 1 + ns])\n",
    "    for i, b_samples in enumerate(sample_forms):\n",
    "        sampled = {}\n",
    "        for k, b_sample in enumerate(b_samples, 1):\n",
    "            if b_sample in sampled:\n",
    "                dups[i, :, k] = 0.0\n",
    "                continue\n",
    "            sampled[b_sample] = k\n",
    "        for j, orig in enumerate(wordlist):\n",
    "            if orig in sampled:\n",
    "                k = sampled[orig]\n",
    "                dups[i, j, k] = 0.0\n",
    "    return dups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T15faXi7wmDK"
   },
   "outputs": [],
   "source": [
    "#This class manages a matrix of similarity scores between words in the lost language and words in the known language. It updates iteratively the scores by calling a model and uses a (static?) momentum\n",
    "#to slow down the updates as the iterations pile up\n",
    "@has_properties('lost_lang', 'known_lang', 'num_cognates')\n",
    "class Flow:\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, momentum, num_cognates, evaluation_mode=False, split=None):\n",
    "        super().__init__()\n",
    "        lost_words = get_words(lost_lang)\n",
    "        known_words = get_words(known_lang)\n",
    "\n",
    "        if evaluation_mode:\n",
    "            words = []\n",
    "            train_idxs = set(split['train'])\n",
    "            for w in lost_words:\n",
    "                if w.idx in train_idxs:\n",
    "                    words.append(w)\n",
    "            lost_words = words\n",
    "        flow = get_tensor(np.zeros([len(lost_words), len(known_words)]))\n",
    "        self.flow = MagicTensor(flow, lost_words, known_words)\n",
    "        self._warmed_up = False\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def state_dict(self):\n",
    "        \"\"\"Use words as the indices.\"\"\"\n",
    "\n",
    "        return {'lost_forms': get_forms(self.lost_lang),\n",
    "                'known_forms': get_forms(self.known_lang),\n",
    "                'flow': self.flow}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        lost_forms = get_forms(self.lost_lang)\n",
    "        known_forms = get_forms(self.known_lang)\n",
    "        saved_lost_forms = state_dict['lost_forms']\n",
    "        saved_known_forms = state_dict['known_forms']\n",
    "        saved_flow = state_dict['flow']\n",
    "        assert (lost_forms == saved_lost_forms).all()\n",
    "        assert (known_forms == saved_known_forms).all()\n",
    "        self.flow.data.copy_(state_dict['flow'].tensor)\n",
    "        logging.critical(state_dict['flow'])\n",
    "    @log_this('IMP')\n",
    "    def warm_up(self):\n",
    "        value = self.num_cognates / self.flow.numel()\n",
    "        self.flow.tensor[:] = value\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def update(self, model, data_loader, num_cognates, edit, capacity):\n",
    "        model.eval()\n",
    "        entire_batch = data_loader.entire_batch\n",
    "        \n",
    "        model_ret = model(entire_batch, mode='flow', capacity=capacity, num_cognates=num_cognates, edit=edit)\n",
    "        new_flow = model_ret.flow\n",
    "        logging.critical(f\"FLOW TENSOR: {self.flow.tensor.shape}, FLOW MODEL:{new_flow.tensor.shape}\")\n",
    "        logging.critical(f\"new_flow: {new_flow}, {new_flow.tensor.sum()}, {data_loader}, {num_cognates}, {edit}, {capacity}\")\n",
    "        self._check_acc(new_flow)\n",
    "        self.flow = self.momentum * self.flow + (1.0 - self.momentum) * new_flow\n",
    "\n",
    "    def _check_acc(self, flow):\n",
    "        preds = flow.get_best(nonzero=True)\n",
    "        # Checking lost.\n",
    "        #logging.critical(f\"preds: {preds}\")\n",
    "        acc = sum([has_cognate(w, self.known_lang) for w in preds.keys()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the lost side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking known.\n",
    "        acc = sum([has_cognate(w, self.lost_lang) for w in preds.values()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the known side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking lost and known.\n",
    "        acc = sum([is_cognate(w1, w2) for w1, w2 in preds.items()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy for lost-known {acc} / {len(preds)} = {rate:.3f} ')\n",
    "\n",
    "    def select(self, lost_words, known_words):\n",
    "        \"\"\"Take the subtensor, specified by the words.\"\"\"\n",
    "        flow = self.flow.select_rows(lost_words).select_cols(known_words)\n",
    "        flow_k = flow.tensor.sum(dim=0)\n",
    "        flow_l = flow.tensor.sum(dim=1)\n",
    "\n",
    "        return {'flow': flow,\n",
    "                'flow_k': flow_k,\n",
    "                'flow_l': flow_l,\n",
    "                'total_flow_k': flow_k.sum(),\n",
    "                'total_flow_l': flow_l.sum()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79SKd4HyF9xS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def min_cost_flow(dists, demand, n_similar=None, capacity=1):\n",
    "    '''\n",
    "    Modified from https://developers.google.com/optimization/flow/mincostflow.\n",
    "\n",
    "    ``capacity`` controls how many lost tokens can be mapped to the same known token.\n",
    "    If it is set to -1, then there is no constraint at all, otherwise use its value.\n",
    "    '''\n",
    "    #logging.critical('Solving flow')\n",
    "    dists = (dists * 100.0).astype('int64')\n",
    "    max_demand = min(dists.shape[0], dists.shape[1])\n",
    "    if demand > max_demand:\n",
    "        logging.warning('demand too big, set to %d instead' % (max_demand))\n",
    "        demand = max_demand\n",
    "    # between each pair. For instance, the arc from node 0 to node 1 has a\n",
    "    # capacity of 15 and a unit cost of 4.\n",
    "    nt, ns = dists.shape\n",
    "    start_nodes = list()\n",
    "    end_nodes = list()\n",
    "    unit_costs = list()\n",
    "    capacities = list()\n",
    "    # source to c_t\n",
    "    for t in range(nt):\n",
    "        start_nodes.append(0)\n",
    "        end_nodes.append(t + 2)  # NOTE 0 is reserved for source, and 1 for sink\n",
    "        unit_costs.append(0)\n",
    "        capacities.append(1)\n",
    "    # c_s to sink\n",
    "    for s in range(ns):\n",
    "        start_nodes.append(s + 2 + nt)\n",
    "        end_nodes.append(1)\n",
    "        unit_costs.append(0)\n",
    "        if capacity == -1:\n",
    "            capacities.append(nt + ns)  # NOTE Ignore capacity constraint.\n",
    "        else:\n",
    "            capacities.append(capacity)\n",
    "    #logging.critical(f\"{nt}, {ns}\")\n",
    "    # c_t to c_s\n",
    "    if n_similar:  # and False:\n",
    "        idx = dists.argpartition(n_similar - 1, axis=1)[:, :n_similar]\n",
    "        all_words = set()\n",
    "        for t in range(nt):\n",
    "            all_s = idx[t]\n",
    "            all_words.update(all_s)\n",
    "        #    for s in all_s:\n",
    "        #        start_nodes.append(t + 2)\n",
    "        #        end_nodes.append(s + 2 + nt)\n",
    "        #        unit_costs.append(dists[t, s])\n",
    "        if len(all_words) < demand:\n",
    "            logging.warning('pruned too many words, adding some more')\n",
    "            # raised an exception, as random.sample needs a collection (e.g. list) as first input param\n",
    "            # added = random.sample(set(range(ns)) - all_words, demand - len(all_words))\n",
    "            added = random.sample(list(set(range(ns)) - all_words), demand - len(all_words))\n",
    "            all_words.update(added)\n",
    "        #    for s in added:\n",
    "        #        for t in range(nt):\n",
    "        #            start_nodes.append(t + 2)\n",
    "        #            end_nodes.append(s + 2 + nt)\n",
    "        #            unit_costs.append(dists[t, s])\n",
    "        for t, s in itertools.product(range(nt), all_words):\n",
    "            start_nodes.append(t + 2)\n",
    "            end_nodes.append(s + 2 + nt)\n",
    "            unit_costs.append(dists[t, s])\n",
    "            capacities.append(1)\n",
    "    else:\n",
    "        for t, s in itertools.product(range(nt), range(ns)):\n",
    "            start_nodes.append(t + 2)\n",
    "            end_nodes.append(s + 2 + nt)\n",
    "            unit_costs.append(dists[t, s])\n",
    "            capacities.append(1)\n",
    "    #logging.critical(f\"{sys.getsizeof(start_nodes)}, {len(start_nodes)}\")\n",
    "    # Define an array of supplies at each node.\n",
    "    supplies = [demand, -demand]  # + [0] * (nt + ns)\n",
    "\n",
    "    # Instantiate a SimpleMinCostFlow solver.\n",
    "    min_cost_flow = mcf.SimpleMinCostFlow()\n",
    "\n",
    "    # Add each arc.\n",
    "    #for i in range(0, len(start_nodes)):\n",
    "    #    min_cost_flow.AddArcWithCapacityAndUnitCost(\n",
    "    #        int(start_nodes[i]),\n",
    "    #        int(end_nodes[i]),\n",
    "    #        int(capacities[i]),\n",
    "    #        int(unit_costs[i])\n",
    "    #    )\n",
    "\n",
    "    # now library supports list initialization\n",
    "    min_cost_flow.add_arcs_with_capacity_and_unit_cost(\n",
    "            start_nodes,\n",
    "            end_nodes,\n",
    "            capacities,\n",
    "            unit_costs\n",
    "        )\n",
    "\n",
    "    # Add node supplies.\n",
    "    #for i in range(0, len(supplies)):\n",
    "    #    min_cost_flow.SetNodeSupply(i, supplies[i])\n",
    "    # also here list initialization\n",
    "    min_cost_flow.set_nodes_supplies(np.arange(0, len(supplies)), supplies)\n",
    "    #logging.critical(\"Starting solver\")\n",
    "    # Find the minimum cost flow between node 0 and node 4.\n",
    "    if min_cost_flow.solve() == min_cost_flow.OPTIMAL:\n",
    "        cost = min_cost_flow.optimal_cost()\n",
    "        flow = np.zeros([nt, ns])\n",
    "        for i in range(min_cost_flow.num_arcs()):\n",
    "            t = min_cost_flow.tail(i)\n",
    "            s = min_cost_flow.head(i)\n",
    "            if t > 1 and s > 1 + nt:\n",
    "                flow[t - 2, s - 2 - nt] = min_cost_flow.flow(i)\n",
    "        return flow, cost\n",
    "    else:\n",
    "        logging.error('There was an issue with the min cost flow input.')\n",
    "        raise RuntimeError('Min cost flow solver error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNR6ByCaHhOt"
   },
   "outputs": [],
   "source": [
    "\n",
    "_SAFE_METHODS = {'numel', '__str__', 'data', 'shape', 'unsqueeze'}\n",
    "_SAFE_METHODS_WITH_WRAPPER = {'log'}\n",
    "\n",
    "#Tensor to handle complex operations\n",
    "@has_properties('tensor', 'row_words', 'col_words')\n",
    "class MagicTensor:\n",
    "\n",
    "    def __init__(self, tensor, row_words, col_words):\n",
    "        assert tensor.ndimension() == 2\n",
    "        #logging.debug(f\"{row_words}\\n {isinstance(row_words[0], Word)}\\n {type(row_words[0])}\")\n",
    "        assert isinstance(row_words[0], Word)\n",
    "        assert isinstance(col_words[0], Word)\n",
    "        assert len(row_words) == tensor.shape[0]\n",
    "        assert len(col_words) == tensor.shape[1]\n",
    "\n",
    "        def freeze(words):\n",
    "            if isinstance(words, tuple):\n",
    "                return words\n",
    "            else:\n",
    "                return tuple(words)\n",
    "\n",
    "        # NOTE Freeze these to speed up checking procedure.\n",
    "        self._row_words = freeze(row_words)\n",
    "        self._col_words = freeze(col_words)\n",
    "        # Fast indexing.\n",
    "        self._word2row = {w: i for i, w in enumerate(self._row_words)}\n",
    "        self._word2col = {w: i for i, w in enumerate(self._col_words)}\n",
    "\n",
    "    def _check_value(self, other):\n",
    "        if isinstance(other, MagicTensor):\n",
    "            try:\n",
    "                assert self.row_words == other.row_words\n",
    "                assert self.col_words == other.col_words\n",
    "                return other.tensor\n",
    "            except AssertionError:\n",
    "                self._permute(other)\n",
    "                return other.tensor\n",
    "        elif isinstance(other, (float, int)):\n",
    "            return other\n",
    "        else:\n",
    "            raise NotImplementedError(f'Type {type(other)} not supported.')\n",
    "\n",
    "    @log_this()\n",
    "    def _permute(self, other):\n",
    "        # Have to re-index the my own tensor. But make sure that the set of words are identical first.\n",
    "        assert set(self.row_words) == set(other.row_words)\n",
    "        assert set(self.col_words) == set(other.col_words)\n",
    "        my_rows = [self._word2row[w] for w in other.row_words]\n",
    "        my_cols = [self._word2col[w] for w in other.col_words]\n",
    "        self._row_words = other.row_words\n",
    "        self._col_words = other.col_words\n",
    "        self._word2row = other._word2row\n",
    "        self._word2col = other._word2col\n",
    "        self._tensor = self._tensor[my_rows][:, my_cols]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MagicTensor({self.tensor!r})'\n",
    "\n",
    "    def __getattribute__(self, attr):\n",
    "        try:\n",
    "            return super().__getattribute__(attr)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                tensor = super().__getattribute__('_tensor')\n",
    "            except AttributeError:\n",
    "                raise\n",
    "\n",
    "            orig = getattr(tensor, attr)\n",
    "            if attr in _SAFE_METHODS:\n",
    "                setattr(self, attr, orig)\n",
    "            elif attr in _SAFE_METHODS_WITH_WRAPPER:\n",
    "\n",
    "                @wraps(orig)\n",
    "                def wrapper(self, *args, **kwargs):\n",
    "                    ret = orig(*args, **kwargs)\n",
    "                    return MagicTensor(ret, self.row_words, self.col_words)\n",
    "\n",
    "                setattr(self, attr, types.MethodType(wrapper, self))\n",
    "            else:\n",
    "                raise AttributeError\n",
    "            return getattr(self, attr)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        value = self._check_value(other)\n",
    "        return MagicTensor(self.tensor + value, self.row_words, self.col_words)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        value = self._check_value(other)\n",
    "        return MagicTensor(self.tensor * value, self.row_words, self.col_words)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        new_tensor = self.tensor[key]\n",
    "        if isinstance(key, tuple):\n",
    "            assert len(key) == 2\n",
    "            s0, s1 = key\n",
    "            return MagicTensor(new_tensor, self.row_words[s0], self.col_words[s1])\n",
    "        else:\n",
    "            return MagicTensor(new_tensor, self.row_words[key], self.col_words)\n",
    "\n",
    "    def select_rows(self, words):\n",
    "        ids = [self._word2row[w] for w in words]\n",
    "        return MagicTensor(self.tensor[ids], words, self.col_words)\n",
    "\n",
    "    def select_cols(self, words):\n",
    "        ids = [self._word2col[w] for w in words]\n",
    "        return MagicTensor(self.tensor[:, ids], self.row_words, words)\n",
    "    '''\n",
    "    def get_best(self, topk=1, nonzero=False, return_scores=False, exp_probs=True):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          - If return_scores=False:\n",
    "              dict {lost_word: known_word or [known_word1, ...]}\n",
    "          - If return_scores=True:\n",
    "              tuple of two dicts:\n",
    "                * ret_words:  {lost_word: known_word or [known_word1, ...]}\n",
    "                * ret_scores: {lost_word: prob or [prob1, ...]} (exp(log_prob) if exp_probs=True)\n",
    "        \n",
    "        Args:\n",
    "          - topk: number of predictions per lost word\n",
    "          - nonzero: if True, filters out results with score <= 0\n",
    "          - return_scores: if True, returns a separate dictionary of scores\n",
    "          - exp_probs: if True, applies exp(log_prob) to return real probabilities\n",
    "        \"\"\"\n",
    "        ret_words = dict()\n",
    "        ret_scores = dict() if return_scores else None\n",
    "    \n",
    "        topk_vals, topk_idxs = self.tensor.topk(topk, dim=-1)\n",
    "    \n",
    "        for lost_idx in range(len(self.row_words)):\n",
    "            lost = self.row_words[lost_idx]\n",
    "            known_list = []\n",
    "            score_list = []\n",
    "    \n",
    "            for rank in range(topk):\n",
    "                log_score = topk_vals[lost_idx, rank].item()\n",
    "                if nonzero and log_score <= 0:\n",
    "                    continue\n",
    "                known_idx = topk_idxs[lost_idx, rank].item()\n",
    "                known = self.col_words[known_idx]\n",
    "    \n",
    "                known_list.append(known)\n",
    "                if return_scores:\n",
    "                    prob = math.exp(log_score) if exp_probs else log_score\n",
    "                    score_list.append(prob)\n",
    "    \n",
    "            if known_list:\n",
    "                ret_words[lost] = known_list[0] if topk == 1 else known_list\n",
    "                if return_scores:\n",
    "                    ret_scores[lost] = score_list[0] if topk == 1 else score_list\n",
    "    \n",
    "        return (ret_words, ret_scores) if return_scores else ret_words\n",
    "    '''\n",
    "    #takes best scores for each word of the lost language\n",
    "    def get_best(self, nonzero=False):\n",
    "        ret = dict()\n",
    "        # this is an argmax\n",
    "        best_idx = self.tensor.max(dim=-1)[1].cpu().numpy()\n",
    "        for lost_idx, known_idx in enumerate(best_idx):\n",
    "            if not nonzero or self.tensor[lost_idx, known_idx].item() > 0:\n",
    "                lost = self.row_words[lost_idx]\n",
    "                known = self.col_words[known_idx]\n",
    "                assert lost not in ret\n",
    "                ret[lost] = known\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "RZR6rGyOQfS7",
    "outputId": "38325fb9-0111-4737-d352-f14c883e0529"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#This class manages a matrix of similarity scores between words in the lost language and words in the known language. It updates iteratively the scores by calling a model and uses a (static?) momentum\n",
    "#to slow down the updates as the iterations pile up\n",
    "@has_properties('lost_lang', 'known_lang', 'momentum', 'num_cognates')\n",
    "class Flow:\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, momentum, num_cognates):\n",
    "        super().__init__()\n",
    "        lost_words = get_words(lost_lang)\n",
    "        known_words = get_words(known_lang)\n",
    "        flow = get_tensor(np.zeros([len(lost_words), len(known_words)]))\n",
    "        flow = MagicTensor(flow, lost_words, known_words)\n",
    "\n",
    "        self._warmed_up = False\n",
    "        self.flow_file = \"flow_backup.pt\"\n",
    "\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "    def store_flow_tensor(self, flow):\n",
    "        logging.critical(f\"Saving Flow: {flow.shape}\")\n",
    "        torch.save(flow.tensor, self.flow_file)\n",
    "\n",
    "    def load_flow_tensor(self):\n",
    "        return torch.load(self.flow_file, weights_only=True)\n",
    "\n",
    "    def get_magic_tensor(self, flow):\n",
    "        lost_words = get_words(self.lost_lang)\n",
    "        known_words = get_words(self.known_lang)\n",
    "        return MagicTensor(flow, lost_words, known_words)\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Use words as the indices.\"\"\"\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        return {'lost_forms': get_forms(self.lost_lang),\n",
    "                'known_forms': get_forms(self.known_lang),\n",
    "                'flow': flow}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        lost_forms = get_forms(self.lost_lang)\n",
    "        known_forms = get_forms(self.known_lang)\n",
    "        saved_lost_forms = state_dict['lost_forms']\n",
    "        saved_known_forms = state_dict['known_forms']\n",
    "        assert (lost_forms == saved_lost_forms).all()\n",
    "        assert (known_forms == saved_known_forms).all()\n",
    "        self.store_flow_tensor(state_dict['flow'].tensor)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def warm_up(self):\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        value = self.num_cognates / flow.numel()\n",
    "        flow.tensor[:] = value\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def update(self, model, data_loader, num_cognates, edit, capacity):\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        model.eval()\n",
    "        entire_batch = data_loader.entire_batch\n",
    "        model_ret = model(entire_batch, mode='flow', capacity=capacity, num_cognates=num_cognates, edit=edit)\n",
    "        new_flow = model_ret.flow\n",
    "        self._check_acc(new_flow)\n",
    "        flow = self.momentum * flow + (1.0 - self.momentum) * new_flow\n",
    "        self.store_flow_tensor(flow)\n",
    "\n",
    "    def _check_acc(self, flow):\n",
    "        preds = flow.get_best(nonzero=True)\n",
    "        # Checking lost.\n",
    "        acc = sum([has_cognate(w, self.known_lang) for w in preds.keys()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the lost side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking known.\n",
    "        acc = sum([has_cognate(w, self.lost_lang) for w in preds.values()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy on the known side {acc} / {len(preds)} = {rate:.3f} ')\n",
    "        # Checking lost and known.\n",
    "        acc = sum([is_cognate(w1, w2) for w1, w2 in preds.items()])\n",
    "        rate = acc / len(preds)\n",
    "        logging.imp(f'Accuracy for lost-known {acc} / {len(preds)} = {rate:.3f} ')\n",
    "\n",
    "    def select(self, lost_words, known_words):\n",
    "        \"\"\"Take the subtensor, specified by the words.\"\"\"\n",
    "        flow = self.get_magic_tensor(self.load_flow_tensor())\n",
    "        flow = flow.select_rows(lost_words).select_cols(known_words)\n",
    "        flow_k = flow.tensor.sum(dim=0)\n",
    "        flow_l = flow.tensor.sum(dim=1)\n",
    "\n",
    "        return {'flow': flow,\n",
    "                'flow_k': flow_k,\n",
    "                'flow_l': flow_l,\n",
    "                'total_flow_k': flow_k.sum(),\n",
    "                'total_flow_l': flow_l.sum()}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYrpaOIu2_lQ"
   },
   "source": [
    "### FastText Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZYF5ECl3Fq-",
    "outputId": "bf86cb40-4f27-4dba-b473-8cd11080f78a"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "class LinearBWordEmbedding:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext\"), \"linearb_fasttext_model.bin\"),\n",
    "                 train_data_path=\"linearb_train.txt\",\n",
    "                 model='skipgram',\n",
    "                 dim=400,\n",
    "                 ws=5,\n",
    "                 min_count=1,\n",
    "                 epoch=50,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=3,\n",
    "                 neg=5,\n",
    "                 loss='softmax',\n",
    "                 bucket=2000000,\n",
    "                 thread=4,\n",
    "                 t=0.0001,\n",
    "                 word_ngrams=1,\n",
    "                 verbose=2\n",
    "                ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'model': model,\n",
    "            'dim': dim,\n",
    "            'ws': ws,\n",
    "            'minCount': min_count,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'neg': neg,\n",
    "            'loss': loss,\n",
    "            'bucket': bucket,\n",
    "            'thread': thread,\n",
    "            't': t,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, sentences):\n",
    "        processed = []\n",
    "        uppercase_pattern = re.compile(r'[A-Z]')\n",
    "        digits_only_pattern = re.compile(r'^\\d+$')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            filtered_tokens = []\n",
    "            for token in tokens:\n",
    "                token_no_hyphen = token.replace('-', '')\n",
    "                if uppercase_pattern.search(token_no_hyphen):\n",
    "                    continue\n",
    "                if digits_only_pattern.match(token_no_hyphen):\n",
    "                    continue\n",
    "                filtered_tokens.append(token_no_hyphen)\n",
    "            processed.append(filtered_tokens)\n",
    "        return processed\n",
    "\n",
    "    def _write_train_data(self, sentences):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for tokens in sentences:\n",
    "                f.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "    def train(self, sentences):\n",
    "        tokenized = self.preprocess_text(sentences)\n",
    "        self._write_train_data(tokenized)\n",
    "        self.model = fasttext.train_unsupervised(\n",
    "            input=self.train_data_path,\n",
    "            model=self.params['model'],\n",
    "            dim=self.params['dim'],\n",
    "            ws=self.params['ws'],\n",
    "            minCount=self.params['minCount'],\n",
    "            epoch=self.params['epoch'],\n",
    "            lr=self.params['lr'],\n",
    "            minn=self.params['minn'],\n",
    "            maxn=self.params['maxn'],\n",
    "            neg=self.params['neg'],\n",
    "            loss=self.params['loss'],\n",
    "            bucket=self.params['bucket'],\n",
    "            thread=self.params['thread'],\n",
    "            t=self.params['t'],\n",
    "            wordNgrams=self.params['wordNgrams'],\n",
    "            verbose=self.params['verbose']\n",
    "        )\n",
    "        # Optional: remove training data file after training\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if self.model:\n",
    "            return self.model.get_word_vector(word)\n",
    "        return None\n",
    "\n",
    "    def find_nearest_neighbors(self, word, k=5):\n",
    "        if self.model:\n",
    "            return self.model.get_nearest_neighbors(word, k)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "\n",
    "# train the model\n",
    "#embedding_model.train(sequences)\n",
    "#embedding_model.save_model()  # saves to default path 'linearb_fasttext_model.bin'\n",
    "\n",
    "embedding_model.load_model()  # loads from default path 'linearb_fasttext_model.bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [\"aminiso\", \"konoso\", \"dorikajo\", \"a-mi-ni-so\", \"ko-no-so\", \"do-ri-ka-jo\", ]\n",
    "dic = {word: embedding_model.find_nearest_neighbors(word) for word in words}\n",
    "logging.critical(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5yEV0dkIah7"
   },
   "source": [
    "### DecipherModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKmoyHvegoWr"
   },
   "outputs": [],
   "source": [
    "@has_properties('char_emb_dim', 'hidden_size', 'num_layers', 'dropout', 'universal_charset_size', 'lost_lang', 'known_lang', 'norms_or_ratios', 'control_mode', 'residual')\n",
    "class DecipherModel(nn.Module):\n",
    "\n",
    "    def __init__(self, trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(self.char_emb_dim, self.hidden_size, num_layers=self.num_layers,\n",
    "                               dropout=dropout, bidirectional=True, batch_first=True) # dropout set to 0 cause this shit manages to not be deterministic even with seeds and all kinds shit\n",
    "        self.decoder = MultiLayerLSTMCell(2*self.char_emb_dim,\n",
    "                                          self.hidden_size, self.num_layers, self.dropout)\n",
    "        self.attention = GlobalAttention(2 * self.hidden_size, self.hidden_size, dropout=self.dropout)\n",
    "        langs = (self.lost_lang, self.known_lang)\n",
    "        self.char_emb = UniversalCharEmbedding(langs, self.char_emb_dim, self.universal_charset_size)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 3, self.char_emb_dim), # MODIFIED 3 into 5\n",
    "            nn.LeakyReLU())\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        if self.residual:\n",
    "            self.controlled_residual = NormControlledResidual(\n",
    "                norms_or_ratios=self.norms_or_ratios, control_mode=self.control_mode)\n",
    "        self.trie = trie\n",
    "        #self.otherGPU = torch.device(\"cuda:1\")\n",
    "\n",
    "        #ft_dim is currently 256\n",
    "        self.ft_proj = nn.Linear(256, 2 * self.num_layers * self.hidden_size)\n",
    "        #self.ft_residual = nn.Linear(256, self.hidden_size)\n",
    "    def encode(self, batch, fasttext_embs=None):\n",
    "        inp_enc = self.char_emb(batch.lost.id_seqs, self.lost_lang)  # bs x L x d\n",
    "        # inp_enc = self.drop(inp_enc)\n",
    "        bs = inp_enc.shape[0]\n",
    "        inp_packed = nn.utils.rnn.pack_padded_sequence(self.drop(inp_enc), batch.lost.lengths.cpu(), batch_first=True)\n",
    "        # initial hidden and cell states are zeros\n",
    "        c = get_zeros(2 * self.num_layers, bs, self.hidden_size)  # NOTE bidirectional, therefore 2\n",
    "        h = get_zeros(2 * self.num_layers, bs, self.hidden_size)\n",
    "        \n",
    "        if fasttext_embs is None:\n",
    "            forms = batch.lost.forms\n",
    "            fasttext_embs = [get_tensor(embedding_model.get_vector(form.replace(\"-\", \"\")), dtype=\"f\") for form in forms]\n",
    "            #random.shuffle(fasttext_embs)\n",
    "            fasttext_embs = torch.stack(fasttext_embs)\n",
    "\n",
    "        #fasttext_embs = fasttext_embs.to(self.otherGPU) if EVAL[0] else fasttext_embs\n",
    "        #ft_proj = self.ft_proj(fasttext_embs) # [bs, 2 * num_layers * hidden_size]\n",
    "        #ft_proj = ft_proj.view(2 * self.num_layers, bs, self.hidden_size)\n",
    "        #c = ft_proj\n",
    "        \n",
    "        # encoding is a tuple (h, c)\n",
    "        #inp_packed = inp_packed.to(self.otherGPU) if EVAL[0] else inp_packed\n",
    "        #h = h.to(self.otherGPU) if EVAL[0] else h\n",
    "        h_s_packed, encoding = self.encoder(inp_packed, (h, c))\n",
    "        #encoding is a tuple of long and short term memory of 2*num_layers, bs, hs\n",
    "        h_s = nn.utils.rnn.pad_packed_sequence(h_s_packed, batch_first=True)[0]\n",
    "        #encoding = LSTMState.from_pytorch(encoding)\n",
    "        encoding = self.decoder.init_state(h_s.shape[0], encoding)\n",
    "        #h_s: bs, l, 2*hidden_size\n",
    "        #encoding: bs, hidden_size\n",
    "\n",
    "        return inp_enc, h_s, encoding#, fasttext_embs\n",
    "\n",
    "    def forward(self, batch, fasttext_embs=None):\n",
    "        # Remember to clear cache.\n",
    "        #logging.critical(f\"batch idseq {batch.lost.id_seqs[0]}\")\n",
    "        clear_cache()\n",
    "\n",
    "        lost = batch.lost.lang\n",
    "        known = batch.known.lang\n",
    "        # Encode.\n",
    "        #emb_s, h_s, encoding, fasttext_embs = self.encode(batch)\n",
    "        emb_s, h_s, encoding = self.encode(batch, fasttext_embs)\n",
    "\n",
    "        #logging.critical(f\"emb_s: {emb_s}\")\n",
    "        #logging.critical(f\"h_s: {h_s}\")\n",
    "        #logging.critical(f\"encoding: {encoding}\")\n",
    "\n",
    "        # mask out padding\n",
    "        mask_lost = (batch.lost.id_seqs != PAD_ID).float() # bs x sl\n",
    "        # Start decoding.\n",
    "        bs, sl, _ = h_s.shape\n",
    "        # recover the embedding of SOW\n",
    "        input_emb = self.char_emb.get_start_emb(known).expand(bs, -1)  # bs x d\n",
    "        #logging.critical(f\"input_emb: {input_emb}\")\n",
    "\n",
    "        # initialize decoder state using encoder states (combined for bidirectionality)\n",
    "        state = encoding\n",
    "        h_tilde = get_zeros(bs, self.char_emb_dim)\n",
    "        empty_ctx_s = get_zeros(bs, self.hidden_size * 2)\n",
    "        max_len = max(batch.known.lengths)\n",
    "        all_log_probs = list()\n",
    "        all_almt_distrs = list()\n",
    "\n",
    "            \n",
    "        for dec_step in range(max_len):\n",
    "            input_ = torch.cat([h_tilde, input_emb], dim=-1) #input emb is the embedding of start of word\n",
    "            input_ = self.drop(input_)\n",
    "            # updates LSTM states\n",
    "            state = self.decoder(input_, state)\n",
    "            # get hidden state of last LSTM state\n",
    "            ctx_t = state.get_output()  # bs x d\n",
    "            # get ctx_s\n",
    "            # it combines h_s (encoder's output) with ctx_t (decoder's output) and some learnable params\n",
    "            # we argue that this is some kind of encoder-decoder attention\n",
    "            almt_distr = self.attention(ctx_t, h_s, mask_lost) # bs x sl\n",
    "\n",
    "            #almt_distr, ft_contribute = self.attention(ctx_t, h_s, mask_lost, fasttext_embs) # bs x sl # MODIFIED\n",
    "            # uses attention on encoder's output and aggregates on sequence length\n",
    "            ctx_s = (almt_distr.view(bs, sl, 1) * h_s).sum(dim=1)  # bs x 2d\n",
    "            #ft_s = (ft_contribute.view(bs, sl, 1) * h_s).sum(dim=1) # MODIFIED\n",
    "            # get h_tilde\n",
    "            # concatenates attentioned encoder output with decoder output\n",
    "            cat = torch.cat([ctx_s, ctx_t], dim=-1)\n",
    "\n",
    "            #cat = torch.cat([ctx_s, ft_s, ctx_t], dim=-1) # MODIFIED\n",
    "            h_tilde_rnn = self.hidden(self.drop(cat))\n",
    "            if self.residual:\n",
    "                # uses attention weights also on encoder input!!!\n",
    "                ctx_s_emb = (almt_distr.view(bs, sl, 1) * emb_s).sum(dim=1)  # bs x d\n",
    "                h_tilde = self.controlled_residual(ctx_s_emb, h_tilde_rnn)#, self.ft_residual(fasttext_embs))\n",
    "            else:\n",
    "                h_tilde = h_tilde_rnn\n",
    "            # get probs\n",
    "            # projection lost->universal->known\n",
    "            logits = self.char_emb.project(self.drop(h_tilde), known)\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # bs x num_char\n",
    "            probs = log_probs.exp()\n",
    "\n",
    "            # back projection known->universal (to continue decoding)\n",
    "            input_emb = self.char_emb.soft_emb(probs, known)\n",
    "            # Collect stuff.\n",
    "            all_log_probs.append(log_probs.t())\n",
    "            all_almt_distrs.append(almt_distr)\n",
    "\n",
    "        log_probs = torch.stack(all_log_probs, dim=0)  # tl x nc x bs\n",
    "        almt_distr = torch.stack(all_almt_distrs, dim=1)  # bs x tl x sl\n",
    "\n",
    "        ret = self.trie.analyze(log_probs, almt_distr,\n",
    "                                batch.known.words, batch.lost.lengths)\n",
    "        ret.log_probs = log_probs\n",
    "        ret.valid_log_probs = MagicTensor(ret.valid_log_probs, batch.lost.words, batch.known.words)\n",
    "        ret.update(almt_distr=almt_distr)\n",
    "        return ret\n",
    "\n",
    "@has_properties('n_similar')\n",
    "class DecipherModelWithFlow(DecipherModel):\n",
    "    def __init__(self, trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual, n_similar):\n",
    "        super().__init__(trie, char_emb_dim, hidden_size, num_layers, dropout, universal_charset_size, lost_lang, known_lang, norms_or_ratios, control_mode, residual)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            batch,\n",
    "            num_cognates=None,\n",
    "            mode='mle',\n",
    "            edit=True,\n",
    "            capacity=1,\n",
    "            fasttext_embs=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        if mode == 'mle':\n",
    "            ret = super().forward(batch, fasttext_embs)\n",
    "            #logging.critical(f\"forza valid_log_probs {ret.valid_log_probs.tensor[0]}\")\n",
    "            #logging.critical(f\"forza log_probs {ret.log_probs[0]}\")\n",
    "            #logging.critical(f\"known: {batch.known.words[0]}\")\n",
    "            #logging.critical(f\"lost: {batch.lost.words[0]}\")\n",
    "\n",
    "        else:\n",
    "            assert not self.training\n",
    "            with torch.no_grad():\n",
    "                ret = super().forward(batch)\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "                ret.update(flow=flow, cost=cost, expected_edits=expected_edits)\n",
    "\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6labFmFVLx0k"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xOAdHJ7OqSs"
   },
   "outputs": [],
   "source": [
    "#import gc\n",
    "\n",
    "@has_properties('num_rounds', 'num_epochs_per_M_step', 'saved_path', 'learning_rate', 'log_dir', 'num_cognates', 'inc', 'warm_up_steps', 'capacity', 'save_all', 'eval_interval', 'reg_hyper', 'lost_lang', 'known_lang', 'momentum', 'check_interval')\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_data_loader, flow_data_loader, num_rounds, num_epochs_per_M_step, saved_path, learning_rate, log_dir, num_cognates, inc, warm_up_steps, capacity, save_all, eval_interval, reg_hyper, lost_lang, known_lang, momentum, check_interval, finetune=False, eval_mode=False, split=None):\n",
    "\n",
    "        self.tracker = Tracker('decipher')\n",
    "        stage = self.tracker.add_stage('round', self.num_rounds)\n",
    "        stage.add_stage('E step')\n",
    "        stage.add_stage('M step', self.num_epochs_per_M_step)\n",
    "        self.tracker.fix_schedule()\n",
    "        \n",
    "        self.eval_mode = eval_mode\n",
    "        self.split = split\n",
    "\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.flow_data_loader = flow_data_loader\n",
    "        self._init_optimizer()\n",
    "        self.flow = Flow(self.lost_lang, self.known_lang, self.momentum, self.num_cognates, self.eval_mode, self.split)\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "        self.finetune = finetune\n",
    "\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_params(self):\n",
    "        for name, param in self._get_trainable_params(names=True):\n",
    "            if param.ndimension() == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "                logging.debug('initialized %s' % name)\n",
    "            else:\n",
    "                if 'bias_ih' in name or 'bias_hh' in name:\n",
    "                    size = param.size(0)\n",
    "                    ind = torch.arange(size // 4, size // 2).long()\n",
    "                    param.data[ind] = 1.0\n",
    "                    logging.debug(f'Forget gate bias initialized to 1.0 for {name}')\n",
    "\n",
    "    def _get_trainable_params(self, names=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if names:\n",
    "                    yield name, param\n",
    "                else:\n",
    "                    yield param\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\" or name == \"flow\": logging.critical(src)\n",
    "\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        if not self.finetune:\n",
    "            try_load('flow')\n",
    "        else:\n",
    "            self.flow.warm_up()\n",
    "\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "        #logging.critical(self.round_num)\n",
    "        #logging.critical(self.model)\n",
    "        logging.critical(self.flow.flow)\n",
    "        #stage = self.tracker.add_stage('round', self.num_rounds)\n",
    "        #stage.add_stage('E step')\n",
    "        #stage.add_stage('M step', self.num_epochs_per_M_step)\n",
    "        logging.critical(f\"{self.tracker}, {self.tracker.current_stage}, {self.tracker.get('round')}\")\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict(),\n",
    "                'flow': self.flow.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "            logging.critical(self.round_num)\n",
    "            logging.critical(self.stage)\n",
    "            logging.critical(self.flow.flow)\n",
    "            logging.critical(self.flow.flow.shape)\n",
    "            logging.critical(self.tracker.schedule.substages)\n",
    "            logging.critical(f\"{self.tracker}, {self.tracker.get('round')}, {self.tracker.current_stage}, {self.tracker.metrics.num_samples}\")\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "            #return\n",
    "        else:\n",
    "            self._init_params()\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'E step':\n",
    "            self._do_E_step()\n",
    "        elif self.stage.name == 'M step':\n",
    "            self._do_M_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    def _do_E_step(self):\n",
    "        logging.critical(f\"round {self.round_num}\")\n",
    "        num_cognates = min((self.round_num - 1) * self.inc, self.num_cognates)\n",
    "        logging.critical(f\"{self.round_num}, {self.warm_up_steps}, {self.stage.step}\")\n",
    "        edit = self.round_num > self.warm_up_steps\n",
    "        warm_up = self.stage.step == 0 and self.round_num == 1\n",
    "        self._E_step_kernel(num_cognates, warm_up, edit)\n",
    "\n",
    "        #gc.collect()                  # Python garbage collection\n",
    "        #torch.cuda.empty_cache()      # Releases unused memory held by caching allocator\n",
    "        #torch.cuda.ipc_collect()      # Reclaims memory from inter-process communication\n",
    "\n",
    "\n",
    "    @log_this('IMP', arg_list=['num_cognates', 'warm_up', 'edit'])\n",
    "    def _E_step_kernel(self, num_cognates, warm_up, edit):\n",
    "        if warm_up:\n",
    "            logging.debug(\"WARMUP FLOW\")\n",
    "            self.flow.warm_up()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.flow.update(self.model, self.flow_data_loader, num_cognates, edit, self.capacity[0])\n",
    "                self._init_params()\n",
    "                self._init_optimizer()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return (self.round_num - 1) * self.num_epochs_per_M_step + self.stage.step + 1\n",
    "\n",
    "    def _prepare_flow(self, batch):\n",
    "        \"\"\"Add flow-related info to the batch.\"\"\"\n",
    "        flow_info = self.flow.select(batch.lost.words, batch.known.words)\n",
    "        batch.update(flow_info)\n",
    "\n",
    "    def _do_M_step(self, evaluator):\n",
    "        self._M_step_kernel()\n",
    "        self._do_post_M_step(evaluator)\n",
    "\n",
    "    def _M_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._M_step_kernel_loop(batch)\n",
    "\n",
    "    def _M_step_kernel_loop(self, batch, update=True):\n",
    "        self._prepare_flow(batch)\n",
    "        return self._do_M_step_batch(batch, update=update)\n",
    "\n",
    "    def _do_post_M_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        num_cognates = min(self.round_num * self.inc, self.num_cognates)\n",
    "        eval_scores = evaluator.evaluate(self.epoch, num_cognates)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            logging.critical(f\"{setting}, {score}\")\n",
    "            for split, value in score.items():\n",
    "                self.tb_writer.add_scalar(setting + '_' + split, value, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_M_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        #logging.critical(f\"{batch.lost.id_seqs.shape, batch.known.id_seqs.shape}\")\n",
    "        model_ret = self.model(batch)\n",
    "        #logging.critical(f\"{model_ret.log_probs.shape}, {model_ret.valid_log_probs.shape}\")\n",
    "        if update:\n",
    "            self._do_M_step_batch_update(model_ret, batch)\n",
    "\n",
    "        return model_ret\n",
    "\n",
    "    def _do_M_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        num_samples = Metric('num_samples', batch.num_samples, 0, report_mean=False)\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        metrics.loss.mean.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(self._get_trainable_params(), 5.0)\n",
    "        grad_norm = Metric('grad_norm', grad_norm * num_samples.total, num_samples.total)\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        metrics += Metrics(num_samples, grad_norm)\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "\n",
    "        # free my ram plz\n",
    "        #del model_ret\n",
    "        #del batch\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        reg_loss = Metric('reg_loss', model_ret.reg_loss, batch.total_flow_k)\n",
    "        nll_losses = torch.logsumexp((model_ret.valid_log_probs + (batch.flow + 1e-8).log()).tensor, dim=0)\n",
    "        nll_losses = nll_losses * batch.flow_k\n",
    "        nll_loss = Metric('nll_loss', -nll_losses.sum(), batch.total_flow_k)\n",
    "        loss = Metric('loss', self.reg_hyper * reg_loss.mean + nll_loss.mean, 1)\n",
    "\n",
    "        #logging.critical(f\"{model_ret.reg_loss.shape}, {model_ret.valid_log_probs.shape}, {batch.flow.shape}, {self.flow.flow.tensor.shape}\")        \n",
    "        return Metrics(loss, nll_loss, reg_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtjQWbLwLviQ"
   },
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjjnNUAaL1v0"
   },
   "outputs": [],
   "source": [
    "def move_batch_to_device(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, (list, tuple, set)):\n",
    "        return type(obj)(move_batch_to_device(x, device) for x in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: move_batch_to_device(v, device) for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        for attr, value in obj.__dict__.items():\n",
    "            moved = move_batch_to_device(value, device)\n",
    "            setattr(obj, attr, moved)\n",
    "        return obj\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'capacity', 'num_cognates')\n",
    "class Evaluator:\n",
    "    def __init__(self, model, data_loader, lost_lang, known_lang, capacity, num_cognates, split=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self._settings = list()\n",
    "        self.split = split\n",
    "\n",
    "    def add_setting(self, mode=None, edit=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        assert edit in [True, False]\n",
    "\n",
    "        lost_size = self.data_loader.size(self.lost_lang)\n",
    "        known_size = self.data_loader.size(self.known_lang)\n",
    "        if mode == 'mle':\n",
    "            self._settings.append(\n",
    "                EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, None, None))\n",
    "        else:\n",
    "            for c in self.capacity:\n",
    "                self._settings.append(\n",
    "                    EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, edit, c))\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'lost_size', 'known_size', 'mode', 'edit', 'capacity'\n",
    "        for s in self._settings:\n",
    "            table.add_row([getattr(s, field) for field in table.field_names])\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    #def evaluate(self, epoch, num_cognates):\n",
    "\n",
    "        #olddevice = torch.device(\"cuda:0\")\n",
    "        #newdevice = torch.device(\"cuda:1\")\n",
    "        #self.model.to(newdevice)\n",
    "                \n",
    "        #self.model.eval()\n",
    "        #table = pt()\n",
    "        #table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'score'\n",
    "        #EVAL[0] = True\n",
    "\n",
    "        #eval_scores = dict()\n",
    "        #for s in self._settings:\n",
    "          #  batch = self.data_loader.entire_batch\n",
    "            #move_batch_to_device(batch, newdevice)\n",
    "            #logging.critical(batch.lost.forms[0])\n",
    "         #   model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "            # Magic tensor to the rescue!\n",
    "         #   almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "         #   preds = almt.get_best()\n",
    "         #   acc = self._evaluate_one_setting(preds)\n",
    "         #   score = acc / len(preds)\n",
    "         #   fmt_score = f'{acc}/{len(preds)}={score:.3f}'\n",
    "         #   table.add_row([getattr(s, field) for field in table.field_names[:-1]] + [fmt_score])\n",
    "         #   eval_scores[str(s)] = score\n",
    "\n",
    "        #EVAL[0] = False\n",
    "\n",
    "        #table.align = 'l'\n",
    "        #table.title = f'Epoch: {epoch}'\n",
    "        #log_pp(table)\n",
    "        #self.model.to(olddevice)\n",
    "        #return eval_scores\n",
    "\n",
    "\n",
    "    def evaluate(self, epoch, num_cognates):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'score'\n",
    "    \n",
    "        eval_scores = dict()\n",
    "        \n",
    "        for s in self._settings:\n",
    "            total_acc = 0\n",
    "            total_preds = 0\n",
    "    \n",
    "            for batch in self.data_loader:\n",
    "                # Optionally move batch to appropriate device here\n",
    "                # move_batch_to_device(batch, device)\n",
    "    \n",
    "                model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "                almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "                preds = almt.get_best()\n",
    "    \n",
    "                acc = self._evaluate_one_setting(preds)\n",
    "                total_acc += acc\n",
    "                total_preds += len(preds)\n",
    "    \n",
    "            score = total_acc / total_preds\n",
    "            fmt_score = f'{total_acc}/{total_preds}={score:.3f}'\n",
    "            table.add_row([getattr(s, field) for field in table.field_names[:-1]] + [fmt_score])\n",
    "            eval_scores[str(s)] = score\n",
    "    \n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "    \n",
    "        return eval_scores\n",
    "\n",
    "\n",
    "    def _evaluate_one_setting(self, preds):\n",
    "        acc = 0\n",
    "        for lost, known in preds.items():\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_batch_to_device(obj, device):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, (list, tuple, set)):\n",
    "        return type(obj)(move_batch_to_device(x, device) for x in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: move_batch_to_device(v, device) for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        for attr, value in obj.__dict__.items():\n",
    "            moved = move_batch_to_device(value, device)\n",
    "            setattr(obj, attr, moved)\n",
    "        return obj\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang', 'capacity', 'num_cognates')\n",
    "class Evaluator:\n",
    "    def __init__(self, model, data_loader, lost_lang, known_lang, capacity, num_cognates, eval_mode=False, split=None):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self._settings = list()\n",
    "        self.eval_mode = eval_mode\n",
    "        self.split = {key: set(v) for key, v in split.items()} if eval_mode and split else None\n",
    "\n",
    "    def add_setting(self, mode=None, edit=None):\n",
    "        assert mode in ['mle', 'flow']\n",
    "        assert edit in [True, False]\n",
    "\n",
    "        lost_size = self.data_loader.size(self.lost_lang)\n",
    "        known_size = self.data_loader.size(self.known_lang)\n",
    "        if mode == 'mle':\n",
    "            self._settings.append(\n",
    "                EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, None, None))\n",
    "        else:\n",
    "            for c in self.capacity:\n",
    "                self._settings.append(\n",
    "                    EvalSetting(self.lost_lang, self.known_lang, lost_size, known_size, mode, edit, c))\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'lost_size', 'known_size', 'mode', 'edit', 'capacity'\n",
    "        for s in self._settings:\n",
    "            table.add_row([getattr(s, field) for field in table.field_names])\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch, num_cognates):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'lost', 'known', 'mode', 'edit', 'capacity', 'split', 'score'\n",
    "\n",
    "        eval_scores = dict()\n",
    "        for s in self._settings:\n",
    "            batch = self.data_loader.entire_batch\n",
    "            model_ret = self.model(batch, mode=s.mode, num_cognates=num_cognates, edit=s.edit, capacity=s.capacity)\n",
    "            almt = model_ret.valid_log_probs if s.mode == 'mle' else model_ret.flow\n",
    "            preds = almt.get_best()\n",
    "\n",
    "            if self.eval_mode and self.split:\n",
    "                split_scores = {}\n",
    "                for split_name, indices in self.split.items():\n",
    "                    acc = self._evaluate_one_setting(preds, indices)\n",
    "                    score = acc / len(indices) if len(indices) > 0 else 0\n",
    "                    fmt_score = f'{acc}/{len(indices)}={score:.3f}'\n",
    "                    table.add_row([getattr(s, field) for field in table.field_names[:-2]] + [split_name, fmt_score])\n",
    "                    split_scores[split_name] = score\n",
    "                eval_scores[str(s)] = split_scores\n",
    "            else:\n",
    "                acc = self._evaluate_one_setting(preds)\n",
    "                score = acc / len(preds)\n",
    "                fmt_score = f'{acc}/{len(preds)}={score:.3f}'\n",
    "                table.add_row([getattr(s, field) for field in table.field_names[:-2]] + ['all', fmt_score])\n",
    "                eval_scores[str(s)] = {'all': score}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def _evaluate_one_setting(self, preds, indices=None):\n",
    "        acc = 0\n",
    "        for idx, (lost, known) in enumerate(preds.items()):\n",
    "            if indices is not None and idx not in indices:\n",
    "                continue\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwkouvbR5sUg"
   },
   "source": [
    "### Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_splits_cache = {}\n",
    "\n",
    "def get_indices(subset, split=(0.6, 0.2, 0.2)):\n",
    "    if f'splits_{split}' not in _splits_cache:\n",
    "        assert(len(split)==3 and sum(split)==1.0), f\"what the hell is that split? {split}\"\n",
    "        indices = list(range(len(get_vocab('transliterated_linear_b'))))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        train_split = int(split[0] * len(indices))\n",
    "        val_split = int((split[0] + split[1]) * len(indices))\n",
    "\n",
    "        _splits_cache[f'splits_{split}'] = {\n",
    "            'train': indices[:train_split],\n",
    "            'validation': indices[train_split:val_split],\n",
    "            'test': indices[val_split:]\n",
    "        }\n",
    "\n",
    "    if subset not in _splits_cache[f'splits_{split}']:\n",
    "        raise ValueError(f\"Invalid split: {subset}\")\n",
    "    return _splits_cache[f'splits_{split}'][subset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ExNZGTk8GW1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@has_properties('cog_path', 'lost_lang', 'known_lang', 'batch_size')\n",
    "class Manager:\n",
    "\n",
    "    model_cls = DecipherModelWithFlow\n",
    "    trainer_cls = Trainer\n",
    "    \n",
    "    def __init__(self, cog_path, lost_lang, known_lang, batch_size, args):\n",
    "        self.loader_cls = LostKnownDataLoader\n",
    "        if args['finetune']: \n",
    "            self.loader_cls = BatchedLinearBDataLoader\n",
    "        self.split = None\n",
    "        build_vocabs(self.cog_path, self.lost_lang, self.known_lang)\n",
    "        \n",
    "        self.eval_mode = args.get('evaluation', False)\n",
    "        if self.eval_mode:\n",
    "            split = {\n",
    "                'train': get_indices('train', args['split']),\n",
    "                'validation': get_indices('validation', args['split']),\n",
    "                'test': get_indices('test', args['split'])\n",
    "            }\n",
    "            self.split = split\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.flow_data_loader, args[\"num_rounds\"], args[\"num_epochs_per_M_step\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"num_cognates\"], args[\"inc\"], args[\"warm_up_steps\"], args[\"capacity\"], args[\"save_all\"], args[\"eval_interval\"], args[\"reg_hyper\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"momentum\"], args[\"check_interval\"], args[\"finetune\"], self.eval_mode, self.split)\n",
    "        self.evaluator = Evaluator(self.model, self.eval_data_loader, args[\"lost_lang\"], args[\"known_lang\"], args[\"capacity\"], args[\"num_cognates\"], self.eval_mode, self.split)\n",
    "\n",
    "        self.evaluator.add_setting(mode='mle', edit=False)\n",
    "        self.evaluator.add_setting(mode='flow', edit=False)\n",
    "        self.evaluator.add_setting(mode='flow', edit=True)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "        self._show_data()\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        if self.eval_mode:\n",
    "            self.train_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=False, indices=self.split['train'])\n",
    "            logging.critical(\"TRAIN LOADER DONE\")\n",
    "            self.eval_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=True, indices=sum(self.split.values(), []))\n",
    "            logging.critical(\"EVAL LOADER DONE\")\n",
    "\n",
    "        else:\n",
    "            self.train_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=False)\n",
    "            self.eval_data_loader = self.loader_cls(self.lost_lang, self.known_lang, self.batch_size, cognate_only=True)\n",
    "\n",
    "        self.flow_data_loader = self.train_data_loader # NOTE The flow instance shares its entire_batch property with train_data_loader.\n",
    "\n",
    "    def _show_data(self):\n",
    "        log_pp(self.train_data_loader.stats('train'))\n",
    "        log_pp(self.eval_data_loader.stats('eval'))\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        trie = Trie(self.known_lang)\n",
    "        self.model = type(self).model_cls(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "        trie_weight_device = self.model.trie._weight.device\n",
    "        logging.critical(f\"model.trie.weight is on device: {trie_weight_device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNKkqNBKZERD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4euG7B2UCzu"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"final_results_translation_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final_translation.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 4964,\n",
    "    \"inc\" : 100, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.8, 0.1, 0.1) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_evaluation\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\":False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.6, 0.2, 0.2) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"finetune_40_rounds\"\n",
    "#EVAL = [False]\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 120, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "psLVyzjuSNA2",
    "outputId": "109692b3-8987-4774-e22a-b9cbf1f4892e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_80_rounds\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\":False\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RELOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:37:41 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1911,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_with_invalid.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'finetune': True,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 200,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/finetune',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 13308,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 90,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/finetune/saved.latest',\n",
      "           'seed': 17,\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"finetune\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 90, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TnUUMUF6DEu",
    "outputId": "a5396595-a69d-4057-e8b5-7fc882c75f32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:37:41 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1024,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_final.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'finetune': False,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 75,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/final_results_zero_long_term',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 1911,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 80,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/final_results_zero_long_term/saved.latest',\n",
      "           'seed': 17,\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"final_results_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : COG_PATH,\n",
    "    \"char_emb_dim\" : 400, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/18/25 00:38:10 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'batch_size': 1024,\n",
      "           'capacity': (3,),\n",
      "           'char_emb_dim': 400,\n",
      "           'check_interval': 10,\n",
      "           'cog_path': './DMPROJECT/cognates_final_translation.cog',\n",
      "           'control_mode': 'relative',\n",
      "           'dropout': 0.2,\n",
      "           'eval_interval': 10,\n",
      "           'evaluation': False,\n",
      "           'finetune': False,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 200,\n",
      "           'inc': 100,\n",
      "           'known_lang': 'greek',\n",
      "           'learning_rate': 0.005,\n",
      "           'log_dir': './DMPROJECT/repo_cinese/final_results_translation_zero_long_term',\n",
      "           'log_level': 'DEBUG',\n",
      "           'lost_lang': 'transliterated_linear_b',\n",
      "           'momentum': 0.9,\n",
      "           'n_similar': 10,\n",
      "           'norms_or_ratios': (1.0, 0.2),\n",
      "           'num_cognates': 4964,\n",
      "           'num_epochs_per_M_step': 100,\n",
      "           'num_layers': 16,\n",
      "           'num_rounds': 80,\n",
      "           'random': False,\n",
      "           'reg_hyper': 0.5,\n",
      "           'residual': True,\n",
      "           'save_all': False,\n",
      "           'saved_path': './DMPROJECT/repo_cinese/final_results_translation_zero_long_term/saved.latest',\n",
      "           'seed': 17,\n",
      "           'split': (0.8, 0.1, 0.1),\n",
      "           'universal_charset_size': 400,\n",
      "           'warm_up_steps': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = \"final_results_translation_zero_long_term\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final_translation.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 4964,\n",
    "    \"inc\" : 100, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10,#10, # 10\n",
    "    \"check_interval\" : 10,#0,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": False,\n",
    "    \"evaluation\": False, \n",
    "    \"split\": (0.8, 0.1, 0.1) \n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CYWpKUn050IH",
    "outputId": "b9e9b91a-6578-47f0-98c9-c8b4ea38aacc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 1000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 2000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 3000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:57 - 4000\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:17 - 0:00:08 at 3837214380.py:62 - Finished enumeration of size 4964\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +--------------------------------+\n",
      "          |             train              |\n",
      "          +-------------------------+------+\n",
      "          | lang                    | size |\n",
      "          +-------------------------+------+\n",
      "          | transliterated_linear_b | 4964 |\n",
      "          | greek                   | 7818 |\n",
      "          +-------------------------+------+\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +--------------------------------+\n",
      "          |              eval              |\n",
      "          +-------------------------+------+\n",
      "          | lang                    | size |\n",
      "          +-------------------------+------+\n",
      "          | transliterated_linear_b | 4964 |\n",
      "          | greek                   | 7818 |\n",
      "          +-------------------------+------+\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          DecipherModelWithFlow(\n",
      "            (encoder): LSTM(400, 200, num_layers=16, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "            (decoder): MultiLayerLSTMCell(\n",
      "              800, 200, num_layers=16\n",
      "              (drop): Dropout(p=0.2, inplace=False)\n",
      "              (cells): ModuleList(\n",
      "                (0): LSTMCell(800, 200)\n",
      "                (1-15): 15 x LSTMCell(200, 200)\n",
      "              )\n",
      "            )\n",
      "            (attention): GlobalAttention(\n",
      "              src=400, tgt=200\n",
      "              (drop): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "            (char_emb): UniversalCharEmbedding(\n",
      "              (char_emb): Embedding(400, 400)\n",
      "              (char_weights): ModuleDict(\n",
      "                (transliterated_linear_b): Embedding(93, 400)\n",
      "                (greek): Embedding(33, 400)\n",
      "              )\n",
      "            )\n",
      "            (hidden): Sequential(\n",
      "              (0): Linear(in_features=600, out_features=400, bias=True)\n",
      "              (1): LeakyReLU(negative_slope=0.01)\n",
      "            )\n",
      "            (drop): Dropout(p=0.2, inplace=False)\n",
      "            (controlled_residual): NormControlledResidual()\n",
      "            (ft_proj): Linear(in_features=256, out_features=6400, bias=True)\n",
      "          )\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>round 100%|| 80/80 [00:01&lt;00:00, 69.22 samples/s]</pre>\n",
       "  </div>\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>M step 100%|| 100/100 [00:00&lt;00:00, 19763.01 samples/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mIMP - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:143 - *STARTING* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:143 - *FINISHED* _init_optimizer\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          Stage(name=decipher, num_steps=1)\n",
      "           Stage(name=round, num_steps=80)\n",
      "               Stage(name=E step, num_steps=1)\n",
      "               Stage(name=M step, num_steps=100)\n",
      "          \n",
      "          \u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:38:19 - 0:00:09 at 1009358421.py:172 - \n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\n",
      "          | lost                    | known | lost_size | known_size | mode | edit  | capacity |\n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | mle  | None  | None     |\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | flow | False | 3        |\n",
      "          | transliterated_linear_b | greek | 4964      | 7818       | flow | True  | 3        |\n",
      "          +-------------------------+-------+-----------+------------+------+-------+----------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:56 - {'_schedule': {'_pbars': {}, '_stages': [{'_pbars': {'round': {'count': 79}}, '_stages': [{'_pbars': {}, '_stages': []}, {'_pbars': {'M step': {'count': 99}}, '_stages': []}]}]}, 'best_score': None, 'best_stage': None, '_metrics': Metrics(loss, grad_norm, nll_loss, reg_loss, num_samples)}\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:56 - {'lost_forms': array(['*18-jo', '*18-to-no', '*22-ja-ro', ..., 'zo-wa', 'zo-wi-jo',\n",
      "                                                                    'zo-wo'], dtype='<U29'), 'known_forms': array(['ff', 'ff', 'ff', ..., '', '', ''],\n",
      "                                                                   dtype='<U18'), 'flow': MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))}\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3930245403.py:39 - MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))\u001b[0m\n",
      "\u001b[36mIMP - 09/18/25 00:38:20 - 0:00:10 at 1009358421.py:50 - Loaded saved states from ./DMPROJECT/repo_cinese/final_results_translation_zero_long_term/saved.latest\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:75 - MagicTensor(tensor([[9.5761e-01, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     ...,\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08],\n",
      "                                                                     [3.1050e-08, 3.1050e-08, 3.1050e-08,  ..., 3.1050e-08, 3.1050e-08,\n",
      "                                                                      3.1050e-08]], device='cuda:0'))\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/18/25 00:38:20 - 0:00:10 at 3486201529.py:79 - <__main__.Tracker object at 0x7ffdd6777070>, \"M step\": 99, 79\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:24 - 0:00:15 at 2413757980.py:2 - Computing expected edits\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 1/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 2/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 3/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 4/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 5/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 6/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 7/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:38:25 - 0:00:15 at 2413757980.py:32 - Computing chunk 8/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:40 - 0:01:31 at 2413757980.py:2 - Computing expected edits\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:41 - 0:01:32 at 2413757980.py:32 - Computing chunk 1/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:43 - 0:01:34 at 2413757980.py:32 - Computing chunk 2/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:45 - 0:01:35 at 2413757980.py:32 - Computing chunk 3/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:47 - 0:01:37 at 2413757980.py:32 - Computing chunk 4/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:48 - 0:01:39 at 2413757980.py:32 - Computing chunk 5/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:50 - 0:01:40 at 2413757980.py:32 - Computing chunk 6/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:52 - 0:01:42 at 2413757980.py:32 - Computing chunk 7/8\u001b[0m\n",
      "\u001b[37mDEBUG - 09/18/25 00:39:54 - 0:01:44 at 2413757980.py:32 - Computing chunk 8/8\u001b[0m\n",
      "\u001b[32mINFO - 09/18/25 00:41:14 - 0:03:04 at 1009358421.py:172 - \n",
      "          +-------------------------------------------------------------------------------------+\n",
      "          |                                      Epoch: -1                                      |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\n",
      "          | lost                    | known | mode | edit  | capacity | split | score           |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\n",
      "          | transliterated_linear_b | greek | mle  | None  | None     | all   | 2652/4964=0.534 |\n",
      "          | transliterated_linear_b | greek | flow | False | 3        | all   | 2705/4964=0.545 |\n",
      "          | transliterated_linear_b | greek | flow | True  | 3        | all   | 2752/4964=0.554 |\n",
      "          +-------------------------+-------+------+-------+----------+-------+-----------------+\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lost_transliterated_linear_b__known_greek__mode_mle__edit_None__capacity_None': {'all': 0.5342465753424658},\n",
       " 'lost_transliterated_linear_b__known_greek__mode_flow__edit_False__capacity_3': {'all': 0.5449234488315874},\n",
       " 'lost_transliterated_linear_b__known_greek__mode_flow__edit_True__capacity_3': {'all': 0.5543916196615633}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. (Re)build the Manager exactly as before.\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "man = Manager(\n",
    "    args[\"cog_path\"],\n",
    "    args[\"lost_lang\"],\n",
    "    args[\"known_lang\"],\n",
    "    args[\"batch_size\"],\n",
    "    args,\n",
    ")\n",
    "# 2. Load *everything* from disk: model + flow + tracker.\n",
    "man.trainer.load()\n",
    "#man._get_trained_model(args[\"saved_path\"])\n",
    "# 3. Rebuild a fresh Evaluator on the *loaded* man.model.\n",
    "\n",
    "# 4. Finally, evaluate with exactly the same num_cognates the model saw when it was saved.\n",
    "#    (Optionally, you can recompute it from `man.trainer.tracker.get('round') + 1` and `args[\"inc\"]`.)\n",
    "man.evaluator.evaluate(-1, args[\"num_cognates\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k25s_lh08Awx"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"old_data\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "old_args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if old_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(old_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = old_args[\"gpu\"]\n",
    "if not old_args[\"random\"]:\n",
    "    random.seed(old_args[\"seed\"])\n",
    "    np.random.seed(old_args[\"seed\"])\n",
    "    torch.manual_seed(old_args[\"seed\"])\n",
    "\n",
    "\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "man = Manager(old_args[\"cog_path\"], old_args[\"lost_lang\"], old_args[\"known_lang\"], old_args[\"batch_size\"], old_args)\n",
    "\n",
    "model = man._get_trained_model(old_args['saved_path'])\n",
    "#man.trainer.load()\n",
    "del man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wR0OhepUCJ6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"for_pipeline_try\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 5, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 3882,#919,\n",
    "    \"inc\" : 500,#50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args, transfer=True, model=model)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbNgpKHjKhQI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"for_pipeline_try\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 15, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 3882,#919,\n",
    "    \"inc\" : 500,#50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSBs7tLqADUI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EXP_NAME = \"old_data_2\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"transliterated_linear_b-greek.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 225, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1024, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "create_logger(filepath=args[\"log_dir\"] + '/log', log_level=args[\"log_level\"])\n",
    "log_pp(pformat(args))\n",
    "\n",
    "def main(args):\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    clear_vocabs()\n",
    "    clear_stages()\n",
    "\n",
    "    manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "    manager.train()\n",
    "\n",
    "    l1_l2, l2_l1 = manager.model.char_emb.char_mapping(args[\"lost_lang\"], args[\"known_lang\"])\n",
    "    # per la gag\n",
    "    dic, _ = l1_l2\n",
    "    for k, v in dic.items():\n",
    "      logging.info(f\"Symbol {k}: {v}\")\n",
    "    #print(l2_l1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n",
    "#MAYBE After loading we should nonetheless reinitialize the model (as it was not reinitialized before saving at the last previous step!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTN0seM0ZeMS"
   },
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKGKQINpiOjf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Old Model New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO0zgTaV3uiY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Import Linear B sequences from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JB-WfWkDdGKr"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\" and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    indexes = [defaultdict(list) for seq in sequences]\n",
    "    for j, seq in enumerate(sequences):\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "                indexes[j][int(pos)].append(int(pos2))\n",
    "\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res, indexes\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis, idxs):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "            idxs.pop(j)\n",
    "    return seq, mis, idxs\n",
    "\n",
    "\n",
    "def split_dataset(base, *others, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a base list and any number of other aligned lists into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        base (list): The primary list to shuffle and split.\n",
    "        *others (list): Any number of other lists aligned with the base list.\n",
    "        train_ratio (float): Proportion of data to use for training (default 0.9).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_base, test_base, train_others..., test_others...)\n",
    "    \"\"\"\n",
    "    length = len(base)\n",
    "    assert all(len(lst) == length for lst in others), \"All input lists must have the same length\"\n",
    "\n",
    "    indices = np.random.permutation(length)\n",
    "\n",
    "    # Shuffle the base and others\n",
    "    base_shuffled = [base[i] for i in indices]\n",
    "    others_shuffled = [[lst[i] for i in indices] for lst in others]\n",
    "\n",
    "    train_size = int(train_ratio * length)\n",
    "\n",
    "    # Split the base list\n",
    "    train_base = base_shuffled[:train_size]\n",
    "    test_base = base_shuffled[train_size:]\n",
    "\n",
    "    # Split all other lists\n",
    "    train_others = [lst[:train_size] for lst in others_shuffled]\n",
    "    test_others = [lst[train_size:] for lst in others_shuffled]\n",
    "\n",
    "    return (train_base, test_base, *train_others, *test_others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5Jii4wGF3blM"
   },
   "outputs": [],
   "source": [
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing, idxs = create_missing_dataset(sequences)\n",
    "sequences, missing, unknown= clean_datasets(sequences, missing, idxs)\n",
    "idxs = [idx for idx in range(len(sequences))]\n",
    "logging.info(f\"{sequences[1]}, {missing[1]}, {unknown[1]}, {idxs[1]}\")\n",
    "\n",
    "train_seqs, test_seqs, train_missings, train_unknown, train_idxs, test_missings, test_unknown, test_idxs = split_dataset(sequences, missing, unknown, idxs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQtEmYC3pEcV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Phonetic Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obuLB4rasOdi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "E5reuit9sNmy"
   },
   "outputs": [],
   "source": [
    "class SyllableAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size=None, embedding_dim=None, hidden_dim=None, latent_dim=None, charsIndexes=None, verbose=False):\n",
    "        super(SyllableAutoencoder, self).__init__()\n",
    "        if not vocab_size or not embedding_dim or not hidden_dim or not latent_dim:\n",
    "            if verbose:\n",
    "                print(\"Missing parameters for autoencoder\")\n",
    "            return\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Map hidden state to latent vector\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "        # Output projection for each time step to vocabulary size (for reconstruction)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.charsIndexes = charsIndexes #vocabulary of the SyllableAutoencoder\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.encoder_rnn(packed)\n",
    "        if isinstance(hidden, tuple): #we don't need wider context\n",
    "            hidden = hidden[0]\n",
    "        last_hidden = hidden[-1]\n",
    "        latent = self.fc_mu(last_hidden)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent, target_seq, lengths):\n",
    "        # Prepare the initial hidden state for decoder from latent vector\n",
    "        hidden_init = self.fc_latent(latent).unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        # Embed the target sequence (teacher forcing during training)\n",
    "        embedded = self.embedding(target_seq)\n",
    "\n",
    "        # Initialize the cell state as zeros (or you could use hidden_init too)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        # Combine into a tuple for LSTM senn non funzia fareshi\n",
    "        hidden_init = (hidden_init, cell_init)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.decoder_rnn(packed, hidden_init)\n",
    "        decoder_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        logits = self.output_projection(decoder_out)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        latent = self.encode(x, lengths)\n",
    "        #For training, use the input sequence as target (teacher forcing)\n",
    "        logits = self.decode(latent, x, lengths)\n",
    "        return logits, latent\n",
    "\n",
    "    def get_probabilities(self, logits):\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    #Given syllable, returns its tensor one-hot-encoded representation\n",
    "    def syl2tensor(self, syl):\n",
    "        return torch.tensor([self.charsIndexes[c] for c in syl], dtype=torch.long).unsqueeze(0), torch.tensor([len(syl)])\n",
    "\n",
    "    #Given list of syllables, produces dictionary <syllable> : <embedding>\n",
    "    def get_embeddings(self, syllables, vocab=None): #original vocabulary of transformer\n",
    "\n",
    "      #Get mapping <syllable> : <one-hot-encoded-syllable> from original vocabulary, positions in the list are the encodings\n",
    "      if vocab != None:\n",
    "        mapping = {}\n",
    "        count = 0\n",
    "        for tok in vocab:\n",
    "          mapping[tok] = count\n",
    "          count += 1\n",
    "\n",
    "      embeddings_per_syllable = {}\n",
    "      for syl in syllables:\n",
    "        syl_, length = self.syl2tensor(syl)\n",
    "        out = self.encode(syl_, length).squeeze(0).detach().numpy()\n",
    "        syl = syl.replace(\"0\", \"\") #remove padding when saving\n",
    "\n",
    "        #Returns representation <one-hot-encoded-syllable> : <embedding>\n",
    "        if vocab != None:\n",
    "          syl = mapping[syl]\n",
    "\n",
    "        embeddings_per_syllable[syl] = out\n",
    "\n",
    "      return embeddings_per_syllable\n",
    "\n",
    "    def save_embeddings(self, filename, syllables, prefix_path=prefix_path, tokenList=None):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      phonetic_embeddings = self.get_embeddings(syllables, tokenList)\n",
    "      np.save(embeddings_path, phonetic_embeddings)\n",
    "\n",
    "    def load_embeddings(self, filename, prefix_path=prefix_path):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      return np.load(embeddings_path, allow_pickle=True).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI3Ul8KCs1hv"
   },
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AIV7pBLDs4IY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SyllableDataset(Dataset):\n",
    "    def __init__(self, syllables, char2idx):\n",
    "        self.syllables = syllables\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.syllables)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        syll = self.syllables[idx]\n",
    "        #Convert each character to its corresponding index\n",
    "        token_indices = [self.char2idx[c] for c in syll]\n",
    "        length = len(token_indices)\n",
    "        return torch.tensor(token_indices, dtype=torch.long), length\n",
    "\n",
    "def collate_fn_syl(batch):\n",
    "    sequences, lengths = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return sequences_padded, torch.tensor(lengths)\n",
    "\n",
    "charsIndexes = {'0': 0,'a': 1,'b': 2,'c': 3,'d': 4,'e': 5,'f': 6,'g': 7,'h': 8,'i': 9,'j': 10,'k': 11,'l': 12,'m': 13,'n': 14,'o': 15,'p': 16,'q': 17,'r': 18,'s': 19,'t': 20,'u': 21,'v': 22,'w': 23,'x': 24,'y': 25,'z': 26}\n",
    "indexesChars = {v: k for k, v in charsIndexes.items()} #reverse mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "k9sCD1bRtVx-"
   },
   "outputs": [],
   "source": [
    "syllables = [] #dataset for unsupervised learning, '0' is padding\n",
    "charset = get_charset(\"transliterated_linear_b\")._CHARS[:-15]\n",
    "\n",
    "#Get syllable tokens\n",
    "for tok in charset:\n",
    "    syllables.append(tok)\n",
    "\n",
    "max_len_syl = max(len(syl) for syl in syllables)\n",
    "#Pad tokens to max length (3)\n",
    "for i in range(len(syllables)):\n",
    "  if len(syllables[i]) < max_len_syl:\n",
    "    syllables[i] += '0'*(max_len_syl-len(syllables[i]))\n",
    "\n",
    "#get dataset\n",
    "dataset = SyllableDataset(syllables, charsIndexes)\n",
    "data_loader_syl = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_syl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYmgonqwtAOv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yQxieV1RtC_y"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "##define autoencoder\n",
    "#vocab_size = len(charsIndexes)\n",
    "#embedding_dim =  16\n",
    "#hidden_dim = 32\n",
    "#latent_dim = EMBEDDINGS_DIM #THIS IS THE SIZE THAT MUST BE EQUAL TO TRANSFORMER VECTOR, Dimension of encoder result\n",
    "## COME HERE IF YOU NET TO DEBUG DEBUG DEBUG (look prev line !!!!)\n",
    "#\n",
    "#syllable_autoencoder = SyllableAutoencoder(vocab_size, embedding_dim, hidden_dim, latent_dim, charsIndexes) ##IMPORTANT, this is the autoencoder model used later to create and save the embeddings\n",
    "#optimizer = optim.Adam(syllable_autoencoder.parameters(), lr=0.001)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=charsIndexes['0'])\n",
    "#\n",
    "##Train autoencoder\n",
    "#num_epochs = 300\n",
    "#for epoch in range(num_epochs):\n",
    "#    total_loss = 0.0\n",
    "#    for batch, lengths in data_loader_syl:\n",
    "#        optimizer.zero_grad()\n",
    "#\n",
    "#        logits, latent = syllable_autoencoder(batch, lengths)\n",
    "#        logits_flat = logits.view(-1, vocab_size)  # shape: (batch_size*seq_length, vocab_size)\n",
    "#        targets_flat = batch.view(-1)              # shape: (batch_size*seq_length,)\n",
    "#\n",
    "#        loss = criterion(logits_flat, targets_flat)\n",
    "#        loss.backward()\n",
    "#        optimizer.step()\n",
    "#        total_loss += loss.item()\n",
    "#\n",
    "#    avg_loss = total_loss / len(data_loader_syl)\n",
    "#    if epoch % 50 == 0:\n",
    "#      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "#\n",
    "#print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu9ifIcOvJ34",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "UjO8QC3IvLrk"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#Test just to see if it works as autoencoder, ignores padding, input == output for good autoencoding\n",
    "#with torch.no_grad():\n",
    "#  for batch, lengths in data_loader_syl:\n",
    "#    logits, latent = syllable_autoencoder(batch, lengths)\n",
    "#    g_truth = list(batch.detach().numpy())\n",
    "#    for el in range(len(g_truth)):\n",
    "#      g_truth[el] = [indexesChars[i] for i in g_truth[el]]\n",
    "#    probs = syllable_autoencoder.get_probabilities(logits) #(batch, syllable, char)\n",
    "#    predicted_indices = list(torch.argmax(probs, dim=-1).detach().numpy())\n",
    "#    for el in range(len(predicted_indices)):\n",
    "#      predicted_indices[el] = [indexesChars[i] for i in predicted_indices[el]]\n",
    "#    print(\"GroundTruth & Predicted: \")\n",
    "#    print(g_truth)\n",
    "#    print(predicted_indices)\n",
    "#    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "zuC4mBr5vhir"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "##Assess quality of embeddings through clustering\n",
    "#from sklearn.cluster import KMeans\n",
    "#lengths = [3] #each syllable is always of len 3\n",
    "#embeddings_per_syllable = syllable_autoencoder.get_embeddings(syllables)\n",
    "#\n",
    "#syllable_keys = list(embeddings_per_syllable.keys())\n",
    "#embedding_list = [embeddings_per_syllable[s] for s in syllable_keys]\n",
    "#\n",
    "#\n",
    "#num_clusters = 14\n",
    "#kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "#clusters = kmeans.fit_predict(embedding_list)\n",
    "#\n",
    "#for cluster_id in range(num_clusters): #print clusters\n",
    "#    cluster_syllables = [syllable_keys[i] for i in range(len(syllable_keys)) if clusters[i] == cluster_id]\n",
    "#    print(f\"Cluster {cluster_id}: {cluster_syllables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "nrqDGDpTvp85"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "#\n",
    "##PLOT CLUSTERS PRINTED IN PREVIOUS CELL\n",
    "#embedding_array = np.array(embedding_list)  #shape: (num_samples, embedding_dim)\n",
    "#\n",
    "#tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "#embedding_2d = tsne.fit_transform(embedding_array)\n",
    "#\n",
    "#colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "#\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#for cluster_id in range(num_clusters):\n",
    "#    indices = [i for i, c in enumerate(clusters) if c == cluster_id]\n",
    "#    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],\n",
    "#                color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "#\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "#plt.title(\"Clustering of Syllable Embeddings (t-SNE Projection)\")\n",
    "#plt.xlabel(\"t-SNE Dimension 1\")\n",
    "#plt.ylabel(\"t-SNE Dimension 2\")\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NNyKMajwQ0z",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Save and Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "rQxTCmw5wTEJ"
   },
   "outputs": [],
   "source": [
    "# REMOVE COMMENTS IF YOU NEED TO RETRAIN AND SAVE THE MODEL\n",
    "\n",
    "#embeddings = syllable_autoencoder.get_embeddings(syllables)\n",
    "#embeddings\n",
    "##Save embeddings on file with one-hot-encoded mapping\n",
    "#syllable_autoencoder.save_embeddings(\"syllable_embeddings_one-hot-encoded.npy\", syllables, tokenList=charset)\n",
    "#syllable_autoencoder.save_embeddings(\"syllable_embeddings.npy\", syllables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "upXWuTS8xwoM"
   },
   "outputs": [],
   "source": [
    "#Load embeddings for further use\n",
    "#loaded_embeddings = syllable_autoencoder.load_embeddings(\"syllable_embeddings.npy\")\n",
    "#loaded_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SDXTbmly8wm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Reimport embeddings for usage in Text Infiller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8969Qyu0zAw-"
   },
   "outputs": [],
   "source": [
    "syllable_autoencoder = SyllableAutoencoder()\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(\"syllable_embeddings.npy\")\n",
    "\n",
    "def recover_phonetic_embeddings(embedding_dict, dataset):\n",
    "    all_embeddings = []  # To store all sequence embeddings\n",
    "    embed_size = len(list(embedding_dict.values())[0])  # Assuming all embeddings have the same size\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_embeddings = []  # To store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            for syllable in word.split(\"-\"):\n",
    "                if syllable in embedding_dict:\n",
    "                    emb = embedding_dict[syllable] # Get the word embedding\n",
    "                else:\n",
    "                    emb = np.zeros(embed_size)\n",
    "                seq_embeddings.append(emb)  # Add to the sequence's embeddings\n",
    "            if i != len(seq) - 1: # this adds spaces' embeddings !\n",
    "                space_embed = np.zeros(embed_size)\n",
    "                seq_embeddings.append(space_embed)  # Add to the sequence's embeddings\n",
    "\n",
    "        # adding zeros for SOS at the beginning of the sequence\n",
    "        sos = np.zeros(embed_size)\n",
    "        seq_embeddings = [sos] + seq_embeddings\n",
    "\n",
    "        # Concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = np.stack(seq_embeddings)\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return all_embeddings  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "phonetic_embeddings_train_x = recover_phonetic_embeddings(loaded_embeddings, train_missings)\n",
    "phonetic_embeddings_test_x = recover_phonetic_embeddings(loaded_embeddings, test_missings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SWlgPua1QQY",
    "outputId": "16d1b206-1d16-45b9-b2c2-9c1710902fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko-wa NUM ?-wo NUM\n",
      "ko-wa NUM ko-wo NUM\n",
      "(10, 512)\n",
      "[0. 0. 0.]\n",
      "[-0.96574688 -0.06027425  0.32187825]\n",
      "[-0.56332594  1.07461643 -0.75313926]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[-0.57493556  1.25258875 -0.68535912]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "print(train_missings[n])\n",
    "print(train_seqs[n])\n",
    "print(phonetic_embeddings_train_x[n].shape)\n",
    "for i in range(len(phonetic_embeddings_train_x[n])):\n",
    "  print(phonetic_embeddings_train_x[n][i][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb1XkEqWk5uG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sequence Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "jqWURIG4VLFc"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, order=True)\n",
    "class Word:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "t9v_w__6X1Gy"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sequence:\n",
    "    def __init__(self, form, missing_form, unknown, phonetic_embedding, idx, vocab):\n",
    "        self.form = form\n",
    "        self.missing_form = missing_form\n",
    "        self.word_list = []\n",
    "        self.unknown = unknown\n",
    "        self.vocab = vocab\n",
    "        self.phonetic_embedding = phonetic_embedding\n",
    "        self.idx = idx\n",
    "        for form_w in form.split(\" \"):\n",
    "            word_idx = vocab.get_form_idx(form_w)\n",
    "            self.word_list.append(vocab.get_word(word_idx))\n",
    "\n",
    "    def num_words(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(s) for s in self.word_list])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Sequence idx={self.idx} form='{self.form}' missing_form='{self.missing_form}'\"\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        seq = []\n",
    "        missing = []\n",
    "        base_logo_idx_vocab = self.vocab.log_start\n",
    "        base_logo_idx_tokens = len(get_charset(self.vocab.lang))\n",
    "\n",
    "        for i, word in enumerate(self.word_list):\n",
    "            # tokenize each word and extend the list\n",
    "            tokenized_word = word.id_seq.copy()\n",
    "\n",
    "            if tokenized_word[0] == UNK_ID:\n",
    "                tokenized_word[0] = base_logo_idx_tokens + (word.idx - base_logo_idx_vocab)\n",
    "            seq.extend(tokenized_word)\n",
    "\n",
    "            # replace unknown tokens with unknown index in tokenized sequence\n",
    "            for u in self.unknown[i]:\n",
    "                tokenized_word[u] = UNK_ID\n",
    "\n",
    "            missing.extend(tokenized_word)\n",
    "\n",
    "        #adjustments to tokenized sequence:\n",
    "        # y always ends with EOS\n",
    "        seq[-1] = EOS_ID\n",
    "        # x always starts with SOW\n",
    "        missing = [SOW_ID] + missing[:-1]\n",
    "\n",
    "        return missing, seq\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, sequence_list, lang):\n",
    "        self.word_list = set()\n",
    "        self.log_start = 0\n",
    "        logo_list = set()\n",
    "        self.lang = lang\n",
    "\n",
    "        for seq in sequence_list:\n",
    "            for word in seq.split(\" \"):\n",
    "                if (word.startswith(\"*\") and not \"-\" in word) or word[0].isupper() or word == \"1\" or word == \"2\":\n",
    "                    logo_list.add(word)\n",
    "                else:\n",
    "                    self.word_list.add(word)\n",
    "\n",
    "        self.word_list = list(sorted(self.word_list))\n",
    "        self.log_start = len(self.word_list)\n",
    "        logo_list = list(sorted(logo_list))\n",
    "        self.word_list.extend(logo_list)\n",
    "\n",
    "        self.build_logo_vocab(sequence_list)\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.form2idx = {w: i for i, w in enumerate(self.word_list)}\n",
    "        word_list = [Word(self.lang, w, i) for i, w in enumerate(self.word_list)]\n",
    "        self.word_list = word_list\n",
    "\n",
    "    @property\n",
    "    def get_words(self):\n",
    "        return self.word_list[:self.log_start]\n",
    "\n",
    "    @property\n",
    "    def get_logograms(self):\n",
    "        return self.word_list[self.log_start:]\n",
    "\n",
    "    @property\n",
    "    def get_vocabulary(self):\n",
    "        return self.word_list\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.word_list[idx]\n",
    "\n",
    "    def get_form_idx(self, form, status=False):\n",
    "        #status: True means that it is a word, False means it is a Logogram/Numeral\n",
    "        if status:\n",
    "            return self.form2idx[form], self.form2idx[form] < self.log_start\n",
    "        return self.form2idx[form]\n",
    "\n",
    "    def build_logo_vocab(self, sequences):\n",
    "        self.logogram_vocab = {}\n",
    "        mappings = {\n",
    "            \"AES\": \"\",\n",
    "            \"AUR\": \"\",\n",
    "            \"BOS\": \"\",\n",
    "            \"BOSf\": \"\",\n",
    "            \"BOSm\": \"\",\n",
    "            \"CAP\": \"\",\n",
    "            \"CAPf\": \"\",\n",
    "            \"CAPm\": \"\",\n",
    "            \"CORN\": \"\",\n",
    "            \"CROC\": \"\",\n",
    "            \"CYP\": \"\",\n",
    "            \"EQU\": \"\",\n",
    "            \"EQUf\": \"\",\n",
    "            \"EQUm\": \"\",\n",
    "            \"FAR\": \"\",\n",
    "            \"GAL\": \"\",\n",
    "            \"GRA\": \"\",\n",
    "            \"HAS\": \"\",\n",
    "            \"HORD\": \"\",\n",
    "            \"LANA\": \"\",\n",
    "            \"LUNA\": \"\",\n",
    "            \"OLIV\": \"\",\n",
    "            \"OVIS\": \"\",\n",
    "            \"OVISf\": \"\",\n",
    "            \"OVISm\": \"\",\n",
    "            \"SUS\": \"\",\n",
    "            \"SUSf\": \"\",\n",
    "            \"SUSm\": \"\",\n",
    "            \"TELA\": \"\",\n",
    "            \"VIN\": \"\",\n",
    "            \"JAC\": \"\",\n",
    "            \"BIG\": \"\",\n",
    "            \"AROM\": \"\",\n",
    "            \"ARB\": \"\",\n",
    "            \"ALV\": \"\",\n",
    "            \"ARM\": \"\",\n",
    "            \"CUR\": \"\",\n",
    "            \"MUL\": \"\",\n",
    "            \"OLE\": \"\",\n",
    "            \"PUG\": \"\",\n",
    "            \"ROTA\": \"\",\n",
    "            \"TUN\": \"\",\n",
    "            \"VIR\": \"\",\n",
    "            \"TELAI\": \"\",\n",
    "            \"TELHA\": \"\",\n",
    "            \"CAPS\": \"\",\n",
    "            \"VAS\": \"\"\n",
    "        }\n",
    "        for seq in sequences:\n",
    "          for word in seq.split(\" \"):\n",
    "                if word in mappings: #correctly translate logograms from latin\n",
    "                    self.logogram_vocab[word] = mappings[word]\n",
    "                #handle variants\n",
    "                elif \"VAS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VAS\"]\n",
    "                elif \"AROM+CYP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"CYP\"]\n",
    "                elif \"BOS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"BOS\"]\n",
    "                elif \"CAP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"CAP\"]\n",
    "                elif \"CYP\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"CYP\"]\n",
    "                elif \"EQU\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"EQU\"]\n",
    "                elif  \"GRA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"GRA\"]\n",
    "                elif \"OLE\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OLE\"]\n",
    "                elif \"OLIV\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OLIV\"]\n",
    "                elif \"OVIS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"OVIS\"]\n",
    "                elif \"SUS\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"SUS\"]\n",
    "                elif \"ROTA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"ROTA\"]\n",
    "                elif \"TUN\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TUN\"]\n",
    "                elif \"VIR\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VIR\"]\n",
    "                elif \"TELAI\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELAI\"]\n",
    "                elif \"TELHA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELHA\"]\n",
    "                elif \"TELA\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"TELA\"]\n",
    "                elif \"VIN\" in word:\n",
    "                    self.logogram_vocab[word] = mappings[\"VIN\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "f2cL-CBZWoEC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize Vocabulary with ALL known LinB words\n",
    "lang = \"transliterated_linear_b\"\n",
    "our_vocabulary = Vocabulary(sequences, lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAkFxhoQ4x8M",
    "outputId": "c5152d8d-8049-4120-eb53-aac2991d8d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4215"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_vocabulary.get_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "vVvD5H1NL-1g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_to_dense(a, dtype='f'):\n",
    "    '''\n",
    "    Pads a list of arrays to create a dense 2D or 3D array, depending on input shape.\n",
    "    - 2D case: list of 1D arrays  shape (batch_size, max_seq_len)\n",
    "    - 3D case: list of 2D arrays  shape (batch_size, max_seq_len, feature_dim)\n",
    "\n",
    "    Args:\n",
    "        a: list of 1D or 2D numpy arrays\n",
    "        dtype: 'f' (float32) or 'l' (int64)\n",
    "\n",
    "    Returns:\n",
    "        A padded dense numpy array.\n",
    "    '''\n",
    "    assert dtype in ['f', 'l']\n",
    "    dtype = np.float32 if dtype == 'f' else np.int64\n",
    "\n",
    "    if len(a) == 0:\n",
    "        raise ValueError(\"Input list is empty\")\n",
    "\n",
    "    first_elem = a[0]\n",
    "    if isinstance(first_elem, list):\n",
    "        first_elem = np.array(first_elem)\n",
    "\n",
    "    if first_elem.ndim == 1:\n",
    "        # Case: list of 1D arrays  pad to 2D\n",
    "        maxlen = max(len(row) for row in a)\n",
    "        result = np.zeros((len(a), maxlen), dtype=dtype)\n",
    "        for i, row in enumerate(a):\n",
    "            row = np.array(row, dtype=dtype)\n",
    "            result[i, :len(row)] = row\n",
    "        return result\n",
    "\n",
    "    elif first_elem.ndim == 2:\n",
    "        # Case: list of 2D arrays  pad to 3D\n",
    "        maxlen = max(arr.shape[0] for arr in a)\n",
    "        feature_dim = first_elem.shape[1]\n",
    "        result = np.zeros((len(a), maxlen, feature_dim), dtype=dtype)\n",
    "        for i, arr in enumerate(a):\n",
    "            arr = np.array(arr, dtype=dtype)\n",
    "            result[i, :arr.shape[0], :] = arr\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Only lists of 1D or 2D arrays are supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "DSLQJ0PzJGJt"
   },
   "outputs": [],
   "source": [
    "def collate_seq(batch):\n",
    "    sequences = _get_item('sequence', batch)\n",
    "    xs = _get_item('x', batch)\n",
    "    ys = _get_item('y', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "    missings = _get_item('missing', batch)\n",
    "    phonetics = _get_item('phonetic', batch)\n",
    "\n",
    "    lengths, sequences, xs, ys, forms, missings, phonetics = sort_all(sequences, xs, ys, forms, missings, phonetics)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    xs = pad_to_dense(xs, dtype='l')\n",
    "    ys = pad_to_dense(ys, dtype='l')\n",
    "    phonetics = pad_to_dense(phonetics, dtype='f')\n",
    "\n",
    "    xs = get_tensor(xs[:, :max_len])\n",
    "    ys = get_tensor(ys[:, :max_len])\n",
    "    phonetics = get_tensor(phonetics[:, :max_len, :])\n",
    "\n",
    "    lang = batch[0].lang\n",
    "    return Map(sequences=sequences, x=xs, y=ys, forms=forms, missings=missings, phonetic=phonetics, lengths=lengths, lang=lang, num_samples=len(sequences))\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, missing, unknown, phonetic_embeddings, idxs, vocab):\n",
    "        self.sequences = [Sequence(s, m, u, phon, idx, vocab) for (s, m, u, phon, idx) in zip(sequences, missing, unknown, phonetic_embeddings, idxs)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        x, y = seq.tokenized_sequence\n",
    "        return Map(sequence=seq, x=x, y=y, lang=self.sequences[idx].word_list[0].lang, form=seq.form, missing=seq.missing_form, phonetic=seq.phonetic_embedding)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        return collate_seq([self[i] for i in range(len(self))])\n",
    "\n",
    "class SequenceDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.dataset)\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_seq)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return self.dataset.entire_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    #def stats(self, name):\n",
    "    #    row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "    #    row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "    #    table = _prepare_stats(name, row1, row2)\n",
    "    #    return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "pW-gixlt9lkk"
   },
   "outputs": [],
   "source": [
    "#DEBUG\n",
    "\n",
    "#train_dataset = SequenceDataset(train_seqs, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs, our_vocabulary)\n",
    "#train_data_loader = SequenceDataLoader(train_dataset, 8)\n",
    "#n = 0\n",
    "#train_data_loader.entire_batch.phonetic[n], train_data_loader.entire_batch.lengths[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VzJkbCQlBDu",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Text Infiller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ttl8pohjA7AM"
   },
   "outputs": [],
   "source": [
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "\n",
    "        # they are sorted\n",
    "        packed_embedded = pack_padded_sequence(embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        pred = self.softmax(out)\n",
    "        return Map(predictions=pred, embeddings=embedded, encoding=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "MOGJEH0KELab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SqueezeExcitationFusion(nn.Module):\n",
    "    def __init__(self, d_model, norms_or_ratios, control_mode='relative', reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)  # Global avg pooling (batch, d_model, 1)\n",
    "\n",
    "        # Fully connected layers to model interactions\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // reduction_ratio),  # Reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // reduction_ratio, d_model),  # Restore dimensionality\n",
    "            nn.Sigmoid()  # Normalize importance scores between 0 and 1\n",
    "        )\n",
    "        self.fuse_embeddings = NormControlledResidual(norms_or_ratios=norms_or_ratios, control_mode=control_mode)\n",
    "\n",
    "    def forward(self, token_emb, phon_emb):\n",
    "        # Concatenate embeddings along the feature dimension\n",
    "        fusion_emb = self.fuse_embeddings(token_emb, phon_emb) # (batch, seq, d_model)\n",
    "\n",
    "        # Squeeze: Compute global context (reduce across sequence dimension)\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1))  # (batch, d_model, 1)\n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  # Flatten to (batch, d_model)\n",
    "\n",
    "        # Excitation: Generate importance weights\n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "\n",
    "        # Scale embeddings by learned importance scores\n",
    "        fused_emb = fusion_emb * attention_weights  # Adaptive weighting\n",
    "\n",
    "        return fused_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "o9TtTC7gF8cn"
   },
   "outputs": [],
   "source": [
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, norms_or_ratios):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.se_fusion = SqueezeExcitationFusion(d_model=embed_size, norms_or_ratios=norms_or_ratios)\n",
    "    def forward(self, batch):\n",
    "\n",
    "        embedded = self.embedding(batch.x)\n",
    "        embedded = self.se_fusion(embedded, batch.phonetic)\n",
    "\n",
    "        # they are sorted\n",
    "        packed_embedded = pack_padded_sequence(embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        pred = self.softmax(out)\n",
    "        return Map(predictions=pred, embeddings=embedded, encoding=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r45qGiVOlEQJ"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5h3JTztH5z_b"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 97) (3728850356.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[52], line 97\u001b[0;36m\u001b[0m\n\u001b[0;31m    logging.critical(f\"{setting}, )\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 97)\n"
     ]
    }
   ],
   "source": [
    "@has_properties('num_epochs', 'saved_path', 'learning_rate', 'log_dir', 'save_all', 'eval_interval', 'check_interval')\n",
    "class TextInfillerTrainer:\n",
    "    def __init__(self, model, train_data_loader, test_data_loader, num_epochs, saved_path, learning_rate, log_dir, save_all, eval_interval, check_interval):\n",
    "\n",
    "        self.tracker = Tracker('text-infilling')\n",
    "        stage = self.tracker.add_stage('round', self.num_epochs)\n",
    "        stage.add_stage('train step')\n",
    "        self.tracker.fix_schedule()\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self._init_optimizer()\n",
    "        self._init_loss()\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_loss(self):\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"none\")\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'train step':\n",
    "            self._do_train_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return self.round_num\n",
    "\n",
    "    def _do_train_step(self, evaluator):\n",
    "        self._train_step_kernel()\n",
    "        self._do_post_train_step(evaluator)\n",
    "\n",
    "    def _train_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._do_train_step_batch(batch)\n",
    "\n",
    "    def _do_post_train_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        eval_scores = evaluator.evaluate(self.epoch)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            logging.critical(f\"{setting}, )\n",
    "            self.tb_writer.add_scalar(setting, score, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_train_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        model_ret = self.model(batch)\n",
    "        if update:\n",
    "            self._do_train_step_batch_update(model_ret, batch)\n",
    "        return model_ret\n",
    "\n",
    "    def _do_train_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        metrics.loss.mean.backward()\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        # NOTE This means we are conditioning on one specific flow.\n",
    "        flattened_pred = model_ret.predictions.view(-1, model_ret.predictions.shape[-1])\n",
    "\n",
    "        flattened_gt = batch.y.view(-1)\n",
    "        nll_loss = self.nll_loss(flattened_pred, flattened_gt)\n",
    "\n",
    "        nll_loss = Metric('nll_loss', nll_loss.sum(dim=-1), batch.num_samples)\n",
    "        loss = Metric('loss', nll_loss.mean, 1)\n",
    "\n",
    "        return Metrics(loss, nll_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvAvEDpclHQR",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Wkcp6d41jgvD"
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvalSetting:\n",
    "    lost: str\n",
    "    known: str\n",
    "    lost_size: int\n",
    "    known_size: int\n",
    "    mode: str\n",
    "    edit: bool\n",
    "    capacity: int\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'lost_{self.lost}__known_{self.known}__mode_{self.mode}__edit_{self.edit}__capacity_{self.capacity}'\n",
    "\n",
    "class TextInfillerEvaluator:\n",
    "    def __init__(self, model, data_loader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'sequence_accuracy', 'mrr', 'top_1', 'top_5', 'top_10', 'top_15', 'top_20'\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'sequence_accuracy', 'mrr', 'top_1', 'top_5', 'top_10', 'top_15', 'top_20'\n",
    "\n",
    "        metrics = Metrics()\n",
    "\n",
    "        for batch in self.data_loader:\n",
    "            model_ret = self.model(batch)\n",
    "            # Magic tensor to the rescue!\n",
    "            gt = batch.y\n",
    "            preds = model_ret.predictions\n",
    "\n",
    "            # Mask to ignore PAD_IDs in the ground truth\n",
    "            self.mask = (gt != PAD_ID)\n",
    "\n",
    "            seq_acc = self.compute_sequence_accuracy(preds, gt)\n",
    "            mrr, top_1, top_5, top_10, top_15, top_20 = self.compute_mrr(preds, batch)\n",
    "\n",
    "            new_metrics = Metrics(seq_acc, mrr, top_1, top_5, top_10, top_15, top_20)\n",
    "            metrics += new_metrics\n",
    "\n",
    "        values = [epoch] + [getattr(metrics, field).mean for field in table.field_names[1:]]\n",
    "        table.add_row(values)\n",
    "\n",
    "        eval_scores = {fn: val for (fn, val) in zip(table.field_names, values)}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def compute_sequence_accuracy(self, preds, gt):\n",
    "        pred_indices = preds.argmax(dim=-1)\n",
    "\n",
    "        # Compare with ground truth and check where predictions are correct\n",
    "        correct = (pred_indices == gt) & self.mask\n",
    "        # Mean over the batch\n",
    "        accuracy = correct.sum(dim=-1)\n",
    "        return Metric('sequence_accuracy', (accuracy.sum(dim=0)).item(), accuracy.shape[0])\n",
    "\n",
    "    def compute_mrr(self, preds, batch):\n",
    "        mrrs = []\n",
    "        top_1 = []\n",
    "        top_5 = []\n",
    "        top_10 = []\n",
    "        top_15 = []\n",
    "        top_20 = []\n",
    "\n",
    "        for b in range(batch.x.size(0)):\n",
    "            unk_positions = (batch.x[b] == UNK_ID).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            for pos in unk_positions:\n",
    "                if pos.item() == 0:\n",
    "                    continue  # skip if index - 1 would be invalid\n",
    "\n",
    "                gt_token = batch.y[b, pos - 1].item()\n",
    "                pred_probs = preds[b, pos - 1]\n",
    "\n",
    "                # Get descending ranks (most likely first)\n",
    "                sorted_indices = torch.argsort(pred_probs, descending=True)\n",
    "\n",
    "                # Find the rank (1-based) of the ground truth token\n",
    "                rank = (sorted_indices == gt_token).nonzero(as_tuple=True)[0].item() + 1\n",
    "\n",
    "                mrrs.append(1.0 / rank)\n",
    "                top_1.append(1 if rank <= 1 else 0)\n",
    "                top_5.append(1 if rank <= 5 else 0)\n",
    "                top_10.append(1 if rank <= 10 else 0)\n",
    "                top_15.append(1 if rank <= 15 else 0)\n",
    "                top_20.append(1 if rank <= 20 else 0)\n",
    "\n",
    "        return Metric('mrr', sum(mrrs), len(mrrs)), Metric('top_1', sum(top_1), len(top_1)), Metric('top_5', sum(top_5), len(top_5)), Metric('top_10', sum(top_10), len(top_10)), Metric('top_15', sum(top_15), len(top_15)), Metric('top_20', sum(top_20), len(top_20))\n",
    "\n",
    "\n",
    "    def _evaluate_one_setting(self, preds):\n",
    "        acc = 0\n",
    "        for lost, known in preds.items():\n",
    "            if is_cognate(lost, known):\n",
    "                acc += 1\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS9G66palJnO"
   },
   "source": [
    "#### Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "GLz-jm9Ez9CE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BRNNTextInfiller' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@has_properties\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextInfillerManager\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     model_cls \u001b[38;5;241m=\u001b[39m BRNNTextInfiller\n\u001b[1;32m      5\u001b[0m     trainer_cls \u001b[38;5;241m=\u001b[39m TextInfillerTrainer\n",
      "Cell \u001b[0;32mIn[252], line 4\u001b[0m, in \u001b[0;36mTextInfillerManager\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@has_properties\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextInfillerManager\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     model_cls \u001b[38;5;241m=\u001b[39m \u001b[43mBRNNTextInfiller\u001b[49m\n\u001b[1;32m      5\u001b[0m     trainer_cls \u001b[38;5;241m=\u001b[39m TextInfillerTrainer\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, lang, batch_size, args, vocabulary):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BRNNTextInfiller' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "@has_properties('data', 'lang', 'batch_size', 'vocabulary')\n",
    "class TextInfillerManager:\n",
    "\n",
    "    model_cls = BRNNTextInfiller\n",
    "    trainer_cls = TextInfillerTrainer\n",
    "\n",
    "    def __init__(self, data, lang, batch_size, args, vocabulary):\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.test_data_loader, args[\"num_epochs\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"save_all\"], args[\"eval_interval\"], args[\"check_interval\"])\n",
    "        self.evaluator = TextInfillerEvaluator(self.model, self.test_data_loader)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        train_seqs, train_missings, train_unknown, train_phon, train_idxs, test_seqs, test_missings, test_unknown, test_phon, test_idxs = self.data\n",
    "        self.vocab_size = len(self.vocabulary.get_vocabulary) - self.vocabulary.log_start + len(get_charset(self.lang))\n",
    "        train_dataset = SequenceDataset(train_seqs, train_missings, train_unknown, train_phon, train_idxs, self.vocabulary)\n",
    "        self.train_data_loader = SequenceDataLoader(train_dataset, self.batch_size)\n",
    "        test_dataset = SequenceDataset(test_seqs, test_missings, test_unknown, test_phon, test_idxs, self.vocabulary)\n",
    "        self.test_data_loader = SequenceDataLoader(test_dataset, self.batch_size)\n",
    "\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        self.model = type(self).model_cls(self.vocab_size, args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"norms_or_ratios\"])\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xaS0wzUlMKN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "g_l8xbsTqY3S",
    "outputId": "1dcb2eed-5da4-4a57-cc9f-4dedd0972783",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m [\u001b[43mtrain_seqs\u001b[49m, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs,\n\u001b[1;32m      2\u001b[0m         test_seqs, test_missings, test_unknown, phonetic_embeddings_test_x, test_idxs]\n\u001b[1;32m      3\u001b[0m lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransliterated_linear_b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_seqs' is not defined"
     ]
    }
   ],
   "source": [
    "data = [train_seqs, train_missings, train_unknown, phonetic_embeddings_train_x, train_idxs,\n",
    "        test_seqs, test_missings, test_unknown, phonetic_embeddings_test_x, test_idxs]\n",
    "lang = \"transliterated_linear_b\"\n",
    "batch_size = 32\n",
    "log_dir = os.path.join(prefix_path, \"BRNN\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "brnn_args = {\n",
    "    \"embed_size\": EMBEDDINGS_DIM,\n",
    "    \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "    \"num_layers\": 8,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 200,\n",
    "    \"saved_path\": os.path.join(log_dir, \"saved.latest\"),#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": log_dir,\n",
    "    \"save_all\": True,\n",
    "    \"eval_interval\": 1,\n",
    "    \"check_interval\": 1,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"norms_or_ratios\": [1.0, 0.8],\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "if brnn_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(brnn_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = brnn_args[\"gpu\"]\n",
    "\n",
    "#create_logger(filepath=brnn_args[\"log_dir\"] + '/log', log_level=brnn_args[\"log_level\"])\n",
    "#log_pp(pformat(brnn_args))\n",
    "\n",
    "clear_stages()\n",
    "\n",
    "\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args, our_vocabulary)\n",
    "#tim.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "7numMNdUEAjp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextInfillerManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n\u001b[1;32m      2\u001b[0m clear_stages()\n\u001b[0;32m----> 3\u001b[0m tim \u001b[38;5;241m=\u001b[39m \u001b[43mTextInfillerManager\u001b[49m(data, lang, batch_size, brnn_args, our_vocabulary)\n\u001b[1;32m      4\u001b[0m tim\u001b[38;5;241m.\u001b[39m_get_trained_model(brnn_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m tim\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextInfillerManager' is not defined"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "clear_stages()\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args, our_vocabulary)\n",
    "tim._get_trained_model(brnn_args[\"saved_path\"])\n",
    "tim.evaluator.evaluate(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Supporting Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Linear B documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ALEXNEWCODE\n",
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "    reconstruct_nums = defaultdict(list)\n",
    "    \n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "                reconstruct_nums[document_name].append(w)\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents, reconstruct_nums\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\" and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    indexes = [defaultdict(list) for seq in sequences]\n",
    "    for j, seq in enumerate(sequences):\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "                indexes[j][int(pos)].append(int(pos2))\n",
    "\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res, indexes\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis, idxs):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "            idxs.pop(j)\n",
    "    return seq, mis, idxs\n",
    "\n",
    "\n",
    "def split_dataset(base, *others, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a base list and any number of other aligned lists into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        base (list): The primary list to shuffle and split.\n",
    "        *others (list): Any number of other lists aligned with the base list.\n",
    "        train_ratio (float): Proportion of data to use for training (default 0.9).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (train_base, test_base, train_others..., test_others...)\n",
    "    \"\"\"\n",
    "    length = len(base)\n",
    "    assert all(len(lst) == length for lst in others), \"All input lists must have the same length\"\n",
    "\n",
    "    indices = np.random.permutation(length)\n",
    "\n",
    "    # Shuffle the base and others\n",
    "    base_shuffled = [base[i] for i in indices]\n",
    "    others_shuffled = [[lst[i] for i in indices] for lst in others]\n",
    "\n",
    "    train_size = int(train_ratio * length)\n",
    "\n",
    "    # Split the base list\n",
    "    train_base = base_shuffled[:train_size]\n",
    "    test_base = base_shuffled[train_size:]\n",
    "\n",
    "    # Split all other lists\n",
    "    train_others = [lst[:train_size] for lst in others_shuffled]\n",
    "    test_others = [lst[train_size:] for lst in others_shuffled]\n",
    "\n",
    "    return (train_base, test_base, *train_others, *test_others)\n",
    "\n",
    "\n",
    "'''\n",
    "def create_documents_dataset(documents):\n",
    "    processed_docs = defaultdict(list)\n",
    "    for doc_id, doc in documents.items():\n",
    "        fully_complete = True\n",
    "        curr_seq = []\n",
    "        for seq, complete in doc:\n",
    "            fully_complete = fully_complete and complete and (not \"?\" in seq)\n",
    "            if seq != \"separatum\" and not \"?\" in seq and seq != \"qs\":\n",
    "                curr_seq.append(seq)\n",
    "            if (not complete or \"?\" in seq or seq == \"separatum\" or seq == \"qs\") and len(curr_seq) > 0:\n",
    "                processed_docs[doc_id].append([\" \".join(curr_seq), fully_complete])\n",
    "                curr_seq = []\n",
    "                fully_complete = True\n",
    "        if len(curr_seq) > 0:\n",
    "            processed_docs[doc_id].append([\" \".join(curr_seq), fully_complete])\n",
    "    return processed_docs\n",
    "'''\n",
    "\n",
    "def create_documents_dataset(documents):\n",
    "    processed_docs = defaultdict(list)\n",
    "    seq_to_doc = defaultdict(list)\n",
    "    for doc_id, doc in documents.items():\n",
    "        fully_complete = True\n",
    "        curr_seq = []\n",
    "        for seq, complete in doc:\n",
    "            is_break = (not complete) or (\"?\" in seq) or (seq in {\"separatum\", \"qs\", \"mut\"})\n",
    "\n",
    "            # Contribute to the current chunk only if it's a normal token   \n",
    "            if seq not in {\"separatum\", \"qs\", \"mut\"} and (\"?\" not in seq):\n",
    "                curr_seq.append([seq, complete])\n",
    "                seq_to_doc[seq].append([doc_id, complete])\n",
    "            \n",
    "            # Update completeness (note: separatum/qs don't affect completeness)\n",
    "            if not complete or (\"?\" in seq):\n",
    "                fully_complete = False\n",
    "                \n",
    "            # Break and flush when needed\n",
    "            if is_break and len(curr_seq) > 0:\n",
    "                processed_docs[doc_id].append([curr_seq, fully_complete])\n",
    "                curr_seq = []\n",
    "                fully_complete = True\n",
    "                \n",
    "        if len(curr_seq) > 0:\n",
    "            processed_docs[doc_id].append([curr_seq, fully_complete])\n",
    "    return processed_docs, seq_to_doc\n",
    "\n",
    "corpus_LB, nums = reconstruct_LB_documents(replace_numerals=True)\n",
    "docs_data, sequence_data = create_documents_dataset(corpus_LB)\n",
    "docs_data # Dictionary{doc_id: List[Tuple[List[Tuple[seq, complete]], fully_complete]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_seq_data = {}\n",
    "for seq, docs_list in sequence_data.items():\n",
    "    complete_count = 0\n",
    "    app_count = 0\n",
    "    docs_dict = {}\n",
    "    for seq_info in docs_list:\n",
    "        complete = seq_info[1]\n",
    "        docs_dict[seq_info[0]] = complete\n",
    "        if complete:\n",
    "            complete_count += 1\n",
    "        app_count += 1\n",
    "    new_seq_data[seq] = [docs_dict, complete_count / app_count]\n",
    "    \n",
    "sequence_data = new_seq_data\n",
    "sequence_data # Dictionary{seq: Tuple[Dictionary{doc_id: complete}, complete_count]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data['a-de-te'], sequence_data['*18-jo'], sequence_data['*56-po-so'], sequence_data['a-ka-i-jo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstraction for Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Completeness(Enum):\n",
    "    INCOMPLETE = 0\n",
    "    MOSTLY_INCOMPLETE = 1\n",
    "    UNCERTAIN = 2\n",
    "    MOSTLY_COMPLETE = 3\n",
    "    COMPLETE = 4\n",
    "\n",
    "def classify_completeness(p):\n",
    "    if p <= 0.0: #0.0\n",
    "        return Completeness.INCOMPLETE\n",
    "    if p <= 1/3: #0.0 -> 0.33\n",
    "        return Completeness.MOSTLY_INCOMPLETE\n",
    "    if p < 2/3: #0.33 -> 0.67\n",
    "        return Completeness.UNCERTAIN\n",
    "    if p < 1.0: #0.67 -> 1.0\n",
    "        return Completeness.MOSTLY_COMPLETE\n",
    "    return Completeness.COMPLETE #1.0\n",
    " \n",
    "    \n",
    "@dataclass(frozen=True, order=True)\n",
    "class WordWithCompleteness:\n",
    "    lang: str\n",
    "    form: str\n",
    "    idx: int\n",
    "    completeness: Completeness\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def char_seq(self):\n",
    "        chars = self.form.split(\"-\") if self.lang.startswith(\"transliterated\") else list(self.form)\n",
    "        return np.asarray(chars + [EOW])\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def id_seq(self):\n",
    "        return get_charset(self.lang).char2id(self.char_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        # length + 1 due to EOW\n",
    "        return self.form.count(\"-\")+2 if self.lang.startswith(\"transliterated\") else len(self.form) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, sequence_data, numerals, lang=\"transliterated_linear_b\"):\n",
    "        self.word_list = set()\n",
    "        self.log_start = 0\n",
    "        logo_list = set()\n",
    "        self.lang = lang\n",
    "\n",
    "        for word in sequence_data.keys():\n",
    "            if (word.startswith(\"*\") and not \"-\" in word) or word[0].isupper() or word in {\"1\", \"2\"}:\n",
    "                logo_list.add(word)\n",
    "            else:\n",
    "                self.word_list.add(word)\n",
    "        nums = set()\n",
    "        for num_list in numerals.values():\n",
    "            for num in num_list:\n",
    "                nums.add(num)\n",
    "        nums=sorted(list(nums), key=lambda x:int(x))\n",
    "\n",
    "        self.word_list = list(sorted(self.word_list))\n",
    "        self.log_start = len(self.word_list)\n",
    "        logo_list = list(sorted(logo_list))\n",
    "        self.word_list.extend(logo_list)\n",
    "        self.word_list.extend(nums)\n",
    "        \n",
    "        self.build_logo_vocab(sequence_data)\n",
    "        self.build(sequence_data, nums)\n",
    "\n",
    "    def build(self, sequence_data, numerals):\n",
    "        self.form2idx = {w: i for i, w in enumerate(self.word_list)}\n",
    "        word_list = [WordWithCompleteness(self.lang, w, i, classify_completeness(sequence_data[w][1]) if i < len(self.word_list) - len(numerals) else Completeness.COMPLETE) for i, w in enumerate(self.word_list)]\n",
    "        self.word_list = word_list\n",
    "\n",
    "    @property\n",
    "    def get_words(self):\n",
    "        return self.word_list[:self.log_start]\n",
    "\n",
    "    @property\n",
    "    def get_logograms(self):\n",
    "        return self.word_list[self.log_start:]\n",
    "\n",
    "    @property\n",
    "    def get_vocabulary(self):\n",
    "        return self.word_list\n",
    "\n",
    "    def get_word(self, idx):\n",
    "        return self.word_list[idx]\n",
    "\n",
    "    def get_form_idx(self, form, status=False):\n",
    "        #status: True means that it is a word, False means it is a Logogram/Numeral\n",
    "        if status:\n",
    "            return self.form2idx[form], self.form2idx[form] < self.log_start\n",
    "        return self.form2idx[form]\n",
    "\n",
    "    def build_logo_vocab(self, sequence_data):\n",
    "        self.logogram_vocab = {}\n",
    "        mappings = {\n",
    "            \"AES\": \"\",\n",
    "            \"ARG\":\"\",\n",
    "            \"AUR\": \"\",\n",
    "            \"BOS\": \"\",\n",
    "            \"BOSf\": \"\",\n",
    "            \"BOSm\": \"\",\n",
    "            \"CAP\": \"\",\n",
    "            \"CAPf\": \"\",\n",
    "            \"CAPm\": \"\",\n",
    "            \"CERV\": \"\",\n",
    "            \"CORN\": \"\",\n",
    "            \"CROC\": \"\",\n",
    "            \"CYP\": \"\",\n",
    "            \"EQU\": \"\",\n",
    "            \"EQUf\": \"\",\n",
    "            \"EQUm\": \"\",\n",
    "            \"FAR\": \"\",\n",
    "            \"GAL\": \"\",\n",
    "            \"GRA\": \"\",\n",
    "            \"HAS\": \"\",\n",
    "            \"HORD\": \"\",\n",
    "            \"LANA\": \"\",\n",
    "            \"LUNA\": \"\",\n",
    "            \"OLIV\": \"\",\n",
    "            \"OVIS\": \"\",\n",
    "            \"OVISf\": \"\",\n",
    "            \"OVISm\": \"\",\n",
    "            \"SAG\": \"\",\n",
    "            \"SUS\": \"\",\n",
    "            \"SUSf\": \"\",\n",
    "            \"SUSm\": \"\",\n",
    "            \"TELA\": \"\",\n",
    "            \"VIN\": \"\",\n",
    "            \"JAC\": \"\",\n",
    "            \"BIG\": \"\",\n",
    "            \"AROM\": \"\",\n",
    "            \"ARB\": \"\",\n",
    "            \"ALV\": \"\",\n",
    "            \"ARM\": \"\",\n",
    "            \"CUR\": \"\",\n",
    "            \"MUL\": \"\",\n",
    "            \"OLE\": \"\",\n",
    "            \"PUG\": \"\",\n",
    "            \"ROTA\": \"\",\n",
    "            \"TUN\": \"\",\n",
    "            \"VIR\": \"\",\n",
    "            \"TELAI\": \"\",\n",
    "            \"TELHA\": \"\",\n",
    "            \"CAPS\": \"\",\n",
    "            \"VAS\": \"\",\n",
    "            \"A+RE+PA\": \"\",\n",
    "            \"KA+NA+KO\": \"\",\n",
    "            \"KA+PO\": \"\", \n",
    "            \"ME+RI\": \"\",\n",
    "            \"TU+RYO\": \"\",\n",
    "            \"NI\": \"\",\n",
    "            \"MO\": \"\",\n",
    "            \"ZE\": \"\",\n",
    "            \"KO\": \"\",\n",
    "            \"SA\": \"\",\n",
    "            \"KU\": \"\",\n",
    "            \"SE\": \"\",\n",
    "            \"MA\": \"\",\n",
    "            \"GUP\": \"\"\n",
    "        }\n",
    "        for word in sequence_data.keys():\n",
    "              if word in mappings: #correctly translate logograms from latin\n",
    "                  self.logogram_vocab[word] = mappings[word]\n",
    "              #handle variants\n",
    "              elif \"VAS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VAS\"]\n",
    "              elif \"AROM+CYP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"CYP\"]\n",
    "              elif \"AROM+KO\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"AROM\"] + mappings[\"KO\"]\n",
    "              elif \"BOS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"BOS\"]\n",
    "              elif \"CAP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"CAP\"]\n",
    "              elif \"CYP\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"CYP\"]\n",
    "              elif \"EQU\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"EQU\"]\n",
    "              elif \"GRA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"GRA\"]\n",
    "              elif \"OLE\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OLE\"]\n",
    "              elif \"OLIV\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OLIV\"]\n",
    "              elif \"OVIS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"OVIS\"]\n",
    "              elif \"SUS\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"SUS\"]\n",
    "              elif \"ROTA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"ROTA\"]\n",
    "              elif \"TUN\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TUN\"]\n",
    "              elif \"VIR\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VIR\"]\n",
    "              elif \"TELAI\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELAI\"]\n",
    "              elif \"TELHA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELHA\"]\n",
    "              elif \"TELA\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"TELA\"]\n",
    "              elif \"VIN\" in word:\n",
    "                  self.logogram_vocab[word] = mappings[\"VIN\"]\n",
    "\n",
    "\n",
    "my_voc = Vocabulary(sequence_data, nums)\n",
    "my_voc.get_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in my_voc.get_vocabulary:\n",
    "    if w.completeness == Completeness.INCOMPLETE:\n",
    "        print(w, sequence_data[w.form])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fragment, Document and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fragment:\n",
    "    def __init__(self, sequence_list, complete, idx, vocab):\n",
    "        self.word_list = []\n",
    "        self.complete_list = []\n",
    "        self.complete = complete\n",
    "        self.idx = idx\n",
    "        self.vocab = vocab\n",
    "\n",
    "        for seq in sequence_list:\n",
    "            form_w, complete = seq\n",
    "            word_idx = vocab.get_form_idx(form_w)\n",
    "            self.word_list.append(vocab.get_word(word_idx))\n",
    "            self.complete_list.append(complete)\n",
    "\n",
    "    @property\n",
    "    def form(self):\n",
    "        return \" \".join([w.form for w in self.word_list])\n",
    "\n",
    "    def num_words(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(s) for s in self.word_list])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Fragment idx={self.idx} form='{self.form}' complete='{self.complete}'\"\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        seq = []\n",
    "        base_logo_idx_vocab = self.vocab.log_start\n",
    "        base_logo_idx_tokens = len(get_charset(self.vocab.lang))\n",
    "\n",
    "        for i, word in enumerate(self.word_list):\n",
    "            # tokenize each word and extend the list\n",
    "            tokenized_word = word.id_seq.copy()\n",
    "\n",
    "            if tokenized_word[0] == UNK_ID:\n",
    "                tokenized_word[0] = base_logo_idx_tokens + (word.idx - base_logo_idx_vocab)\n",
    "            seq.extend(tokenized_word)\n",
    "\n",
    "\n",
    "        #adjustments to tokenized sequence:\n",
    "        # y always ends with EOS\n",
    "        seq[-1] = EOS_ID\n",
    "        return seq\n",
    "\n",
    "    def reconstruct_numerals(self, num_list):\n",
    "        num_idx = 0\n",
    "        for i, w in enumerate(self.word_list):\n",
    "            if w.form == \"NUM\":\n",
    "                self.word_list[i] = self.vocab.get_word(self.vocab.get_form_idx(num_list[num_idx]))\n",
    "                num_idx += 1\n",
    "        return num_idx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, fragments_list, idx):\n",
    "        self.fragments = fragments_list\n",
    "        self.idx = idx\n",
    "        self.idx_to_frag = {frag.idx: list_idx for list_idx, frag in enumerate(fragments_list)}\n",
    "\n",
    "    @property\n",
    "    def form(self):\n",
    "        return \" [...] \".join([f.form for f in self.fragments])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Document idx={self.idx} form='{self.form}'\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fragments)\n",
    "\n",
    "    def num_words(self):\n",
    "        return sum(f.num_words() for f in self.fragments)\n",
    "\n",
    "    @property\n",
    "    def word_list(self):\n",
    "        return sum([f.word_list for f in self.fragments], [])\n",
    "\n",
    "    @property\n",
    "    def tokenized_sequence(self):\n",
    "        return sum([f.tokenized_sequence for f in self.fragments], [])\n",
    "\n",
    "    def get_fragment(self, frag_idx):\n",
    "        frag_list_idx = self.idx_to_frag[frag_idx]\n",
    "        return self.fragments[frag_list_idx]\n",
    "\n",
    "    def reconstruct_numerals(self, numerals):\n",
    "        for frag in self.fragments:\n",
    "            next_idx = frag.reconstruct_numerals(numerals)\n",
    "            numerals = numerals[next_idx:]\n",
    "        assert len(numerals)==0, \"Did not consume all numerals!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, documents_data, vocab):\n",
    "        self.documents = {}\n",
    "        self.frag_to_doc = []\n",
    "\n",
    "        for doc_idx, fragments_list in documents_data.items():\n",
    "            curr_frag_list = []\n",
    "            for fragment_info in fragments_list:\n",
    "                sequence_list, complete = fragment_info\n",
    "                frag_idx = len(self.frag_to_doc)\n",
    "                curr_frag = Fragment(sequence_list, complete, frag_idx, vocab)\n",
    "                curr_frag_list.append(curr_frag)                                \n",
    "                self.frag_to_doc.append(doc_idx)\n",
    "            self.documents[doc_idx] = Document(curr_frag_list, doc_idx)\n",
    "    \n",
    "    @property\n",
    "    def num_documents(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    @property\n",
    "    def num_fragments(self):\n",
    "        return len(self.frag_to_doc)\n",
    "\n",
    "    def get_document(self, doc_idx):\n",
    "        return self.documents[doc_idx]\n",
    "\n",
    "    def get_document_no(self, list_idx):\n",
    "        doc_idx = list(self.documents.keys())[list_idx]\n",
    "        return self.get_document(doc_idx)\n",
    "\n",
    "    def get_fragment(self, frag_idx):\n",
    "        doc_idx = self.frag_to_doc[frag_idx]\n",
    "        return self.documents[doc_idx].get_fragment(frag_idx)\n",
    "\n",
    "    def restore_numerals(self, numerals):\n",
    "        for doc_idx, nums in numerals.items():\n",
    "            self.get_document(doc_idx).reconstruct_numerals(numerals[doc_idx])\n",
    "        \n",
    "corpus = Corpus(docs_data, my_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_data[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Fragment(docs_data[3][0][0], docs_data[3][0][1], 0, my_voc)\n",
    "f.tokenized_sequence, f.form, EOS_ID, EOW_ID, f.complete_list, f.word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [f.tokenized_sequence, [1]*3]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_document(13), corpus.get_document_no(8), corpus.get_fragment(11), corpus.get_fragment(12), corpus.num_documents, corpus.num_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = corpus.get_fragment(11)\n",
    "f.word_list, f.complete_list, f.tokenized_sequence, corpus.get_fragment(12).tokenized_sequence, corpus.get_document_no(8).tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.get_document_no(0), corpus.get_document_no(0).tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_voc.get_logograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cset = get_charset(\"transliterated_linear_b\")\n",
    "len(cset), cset.id2char(92), cset.id2char(4), cset.char2id('ta'), cset.id2char(60)\n",
    "for doc in corpus.documents.values():\n",
    "    if len(cset) in doc.tokenized_sequence:\n",
    "        print(doc, doc.tokenized_sequence, doc.tokenized_sequence.index(len(cset)), doc.word_list)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logos = set([v.form for v in my_voc.get_logograms])\n",
    "logos -= set(my_voc.logogram_vocab.keys())\n",
    "logos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_file = os.path.join(prefix_path, \"linb_words_translation.tsv\")\n",
    "with open(dump_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "    # header\n",
    "    writer.writerow([\"word\", \"completeness_level\", \"documents_info\"])\n",
    "\n",
    "    for w in my_voc.get_words:\n",
    "        tuple_to_save = [\n",
    "            w.form,\n",
    "            w.completeness.name,\n",
    "            [(doc_id, complete) for (doc_id, complete) in sequence_data[w.form][0].items()]\n",
    "        ]\n",
    "        writer.writerow(tuple_to_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dump_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 9:  # header + 9 rows = 10 total\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dU1iu5QQH-O"
   },
   "source": [
    "### AuxiliaryClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHb6DLR_QscX"
   },
   "outputs": [],
   "source": [
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "DATASET_PATH = os.path.join(prefix_path, \"classifiers_dataset.csv\")\n",
    "LOST_LANG = \"transliterated_linear_b\"\n",
    "KNOWN_LANG = \"greek\"\n",
    "TASKS = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "classes = [4, 4, 3]\n",
    "CLASSES_PER_TASK = dict(zip(TASKS, classes))\n",
    "TASK = TASKS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naO519QW_y0D"
   },
   "outputs": [],
   "source": [
    "def parse_file_simple(data_path, task):\n",
    "   with open(data_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "       reader = csv.reader(csvfile)\n",
    "       header = next(reader)\n",
    "       # Ensure the task is one of the fields in position 1, 2, or 3\n",
    "       valid_fields = header[1:4]  # Fields at index 1, 2, 3\n",
    "       assert task in valid_fields, f\"Task must be one of the fields: {valid_fields}. Got '{task}'.\"\n",
    "       task_index = header.index(task)\n",
    "       dataset = []\n",
    "       for row in reader:\n",
    "           if len(row) > task_index and row[task_index] != \"-1\":\n",
    "               form = row[0].strip()\n",
    "               dataset.append((form, int(row[task_index])))\n",
    "   return dataset\n",
    "\n",
    "\n",
    "class AuxiliaryClassifier:\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        if self.task == \"word_type\":\n",
    "            self.labels = [\"anthroponym/animal name/theonym\", \"toponym\", \"ethnonym\", \"common\"]\n",
    "        elif self.task == \"part_of_speech\":\n",
    "            self.labels = [\"noun\", \"verb\", \"adjective\", \"adverb\"]\n",
    "        elif self.task == \"inflection\":\n",
    "            self.labels = [\"thematic in -o\", \"thematic in -a\", \"athematic\"]\n",
    "        self.labels = {k: v for (k, v) in enumerate(self.labels)}\n",
    "        self.features = None\n",
    "\n",
    "    def retrieve_data(self, data_path):\n",
    "        data = parse_file_simple(data_path, self.task)\n",
    "        return data\n",
    "\n",
    "    def tokenize_data(self, data):\n",
    "        assert data is not None and len(data) > 0, \"Data is empty.\"\n",
    "        if isinstance(data[0], tuple) and len(data[0]) == 2:\n",
    "            assert self.features is None, \"Features already initialized.\"\n",
    "            self.features = FeatureUnion([\n",
    "                (\"syllables\", CountVectorizer(tokenizer=lambda x: x.split(\"-\"), token_pattern=None)),\n",
    "                (\"char_ngrams\", TfidfVectorizer(analyzer='char', ngram_range=(2,4), preprocessor=lambda x: x.replace(\"-\", \"\"))),\n",
    "            ])\n",
    "            X, y = zip(*data)\n",
    "            X_features = self.features.fit_transform(X)\n",
    "            return X_features, y\n",
    "        elif isinstance(data[0], str):\n",
    "            assert self.features is not None, \"Features not initialized, cannot run the model.\"\n",
    "            X_features = self.features.transform(data)\n",
    "            return X_features\n",
    "        else:\n",
    "            raise ValueError(\"Data must be a list of tuples (word, label) or a list of strings.\")\n",
    "\n",
    "    def split_data(self, X, y, test_size=0.2, random_state=SEED):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def initialize_model(self, model_cls, **kwargs):\n",
    "        self.model = model_cls(**kwargs)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not initialized. Call initialize_model first.\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not initialized. Call initialize_model first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"AuxiliaryClassifier({self.task})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhlRGUiDGk1Y"
   },
   "outputs": [],
   "source": [
    "def train_classifier(task, data_path=DATASET_PATH, model_cls=LinearSVC, seed=SEED, **kwargs):\n",
    "    aux = AuxiliaryClassifier(task)\n",
    "    data = aux.retrieve_data(data_path)\n",
    "    X, y = aux.tokenize_data(data)\n",
    "    X_train, X_test, y_train, y_test = aux.split_data(X, y)\n",
    "    aux.initialize_model(model_cls, random_state=seed, **kwargs)\n",
    "    aux.fit(X_train, y_train)\n",
    "    y_test_pred = aux.predict(X_test)\n",
    "    #print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "    #print(classification_report(y_test, y_test_pred, target_names=aux.labels.values()))\n",
    "    #print(confusion_matrix(y_test, y_test_pred))\n",
    "    return aux\n",
    "\n",
    "def use_classifier(aux, words):\n",
    "    assert aux is not None, \"Classifier not initialized. Call train_classifier first.\"\n",
    "    X = aux.tokenize_data(words)\n",
    "    y_pred = aux.predict(X)\n",
    "    labels = {}\n",
    "    for word, label in zip(words, y_pred):\n",
    "        labels[word]=aux.labels[label]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaTDgRdkQBOl"
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7Pv1CdiP_7Q"
   },
   "outputs": [],
   "source": [
    "aux_classifiers = [train_classifier(TASKS[i], max_iter=2000) for i in range(len(TASKS))]\n",
    "sentence = [\"wa-na-ka\", \"ko-no-so\", \"a-mi-ni-si-jo\"]\n",
    "class_results = [use_classifier(aux, sentence) for aux in aux_classifiers]\n",
    "class_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez_-5lhQkHBD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Auxiliary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "zyylabH6kKAE"
   },
   "outputs": [],
   "source": [
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "DATASET_PATH = os.path.join(prefix_path, \"classifiers_dataset.csv\")\n",
    "LOST_LANG = \"transliterated_linear_b\"\n",
    "KNOWN_LANG = \"greek\"\n",
    "TASKS = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "classes = [4, 4, 3]\n",
    "CLASSES_PER_TASK = dict(zip(TASKS, classes))\n",
    "TASK = TASKS[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gisi9B669oNQ"
   },
   "source": [
    "#### Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJgQxdw-ZuJw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_file(dataset_path, task, vocab, lang=LOST_LANG):\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "\n",
    "        # Ensure the task is one of the fields in position 1, 2, or 3\n",
    "        valid_fields = header[1:4]  # Fields at index 1, 2, 3\n",
    "        assert task in valid_fields, f\"Task must be one of the fields: {valid_fields}. Got '{task}'.\"\n",
    "\n",
    "        task_index = header.index(task)\n",
    "        dataset = []\n",
    "        next_idx = len(get_words(lang))\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) > task_index and row[task_index] != \"-1\":\n",
    "                form = row[0].strip()\n",
    "                try:\n",
    "                    word = vocab.get_word_from_form(form)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Word '{form}' not found in vocabulary, creating new Word object.\")\n",
    "                    #logging.error(\"Exception occurred\", exc_info=True)\n",
    "                    word = Word(lang, form, next_idx)\n",
    "                    next_idx += 1\n",
    "\n",
    "                dataset.append((word, int(row[task_index])))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "try:\n",
    "    build_vocabs(COG_PATH, LOST_LANG, KNOWN_LANG)\n",
    "except Exception as e:\n",
    "    logging.warning(\"Vocabulary already initialized\")\n",
    "luo_lb_vocab = get_vocab(LOST_LANG)\n",
    "data = parse_file(DATASET_PATH, TASK, luo_lb_vocab)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2/0.8, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk0tqyCsxdsb"
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "M6wh6yP3q6FD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def collate_aux(batch):\n",
    "    words = _get_item('word', batch)\n",
    "    xs = _get_item('x', batch)\n",
    "    ys = _get_item('y', batch)\n",
    "    forms = _get_item('form', batch)\n",
    "\n",
    "    lengths, words, xs, ys, forms = sort_all(words, xs, ys, forms)\n",
    "    lengths = get_tensor(lengths, dtype='l')\n",
    "    # Trim the id_seqs.\n",
    "    max_len = max(lengths).item()\n",
    "    xs = pad_to_dense(xs, dtype='l')\n",
    "\n",
    "    xs = get_tensor(xs[:, :max_len])\n",
    "    ys = get_tensor(ys, dtype='l')\n",
    "\n",
    "    lang = batch[0].lang\n",
    "    return Map(words=words, x=xs, y=ys, forms=forms, lengths=lengths, lang=lang, num_samples=len(words))\n",
    "\n",
    "\n",
    "class AuxiliaryClassifierDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word, y = self.data[idx]\n",
    "        x = word.id_seq\n",
    "        return Map(word=word, x=x, y=y, form=word.form, lang=word.lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        return collate_aux([self[i] for i in range(len(self))])  # real code!\n",
    "\n",
    "class AuxiliaryDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.dataset)\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_aux)\n",
    "\n",
    "    @property\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return self.dataset.entire_batch\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJyuylWgxX58"
   },
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "yz1-bWI5xW5A"
   },
   "outputs": [],
   "source": [
    "dataset = AuxiliaryClassifierDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "12DW2ZSp8I_j"
   },
   "outputs": [],
   "source": [
    "entire_batch = dataset.entire_batch\n",
    "xs = entire_batch.x\n",
    "forms = entire_batch.forms\n",
    "results = []\n",
    "\n",
    "for i in range(xs.shape[0]):\n",
    "    form_parts = forms[i].split('-')\n",
    "    for j in range(xs.shape[1]):\n",
    "        if xs[i, j].item() == UNK_ID:\n",
    "            # Check if j is within the parts range\n",
    "            char_at_j = form_parts[j] if j < len(form_parts) else None\n",
    "            if char_at_j == \"ge\": logging.critical(f\"PORCODIO {forms[i]}\")\n",
    "            results.append((i, j, char_at_j))\n",
    "\n",
    "# Print all occurrences\n",
    "for i, j, char in results:\n",
    "    print(f\"Row {i}, Index {j}: Token == 3, Corresponding char in form: {char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "2GPiRgTLITEo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:{'words': array([Word(lang='transliterated_linear_b', form='e-re-wi-jo-po-ti-ni-ja', idx=524),\n",
      "       Word(lang='transliterated_linear_b', form='de-we-ro-ai-ko-ra-i-ja', idx=322),\n",
      "       Word(lang='transliterated_linear_b', form='e-u-ru-po-to-re-mo-jo', idx=578),\n",
      "       ..., Word(lang='transliterated_linear_b', form='o', idx=1062),\n",
      "       Word(lang='transliterated_linear_b', form='dwo', idx=409),\n",
      "       Word(lang='transliterated_linear_b', form='pe', idx=1931)],\n",
      "      dtype=object), 'x': tensor([[ 6, 48, 70,  ..., 32, 17,  2],\n",
      "        [11, 69, 50,  ...,  7, 17,  2],\n",
      "        [ 6,  9, 51,  ..., 28, 19,  2],\n",
      "        ...,\n",
      "        [ 8,  2,  0,  ...,  0,  0,  0],\n",
      "        [16,  2,  0,  ...,  0,  0,  0],\n",
      "        [37,  2,  0,  ...,  0,  0,  0]], device='cuda:0'), 'y': tensor([0, 1, 0,  ..., 3, 3, 0], device='cuda:0'), 'forms': array(['e-re-wi-jo-po-ti-ni-ja', 'de-we-ro-ai-ko-ra-i-ja',\n",
      "       'e-u-ru-po-to-re-mo-jo', ..., 'o', 'dwo', 'pe'], dtype=object), 'lengths': tensor([9, 9, 9,  ..., 2, 2, 2], device='cuda:0'), 'lang': 'transliterated_linear_b', 'num_samples': 3162}\n"
     ]
    }
   ],
   "source": [
    "logging.critical(entire_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3162"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "xpEx8_fjI45-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:{'words': array([Word(lang='transliterated_linear_b', form='ha-ka-ha-ki-ri-jo', idx=3110),\n",
      "       Word(lang='transliterated_linear_b', form='ne-qa-sa-pi', idx=2701),\n",
      "       Word(lang='transliterated_linear_b', form='ta-si-ko-no', idx=2986),\n",
      "       Word(lang='transliterated_linear_b', form='ai-ka-na-jo', idx=3125),\n",
      "       Word(lang='transliterated_linear_b', form='a-te-mi-to', idx=226),\n",
      "       Word(lang='transliterated_linear_b', form='pe-ko-to', idx=1226),\n",
      "       Word(lang='transliterated_linear_b', form='o-ne-u', idx=1090),\n",
      "       Word(lang='transliterated_linear_b', form='a-ro-zo', idx=2113),\n",
      "       Word(lang='transliterated_linear_b', form='ke-ra-no', idx=760),\n",
      "       Word(lang='transliterated_linear_b', form='to-e', idx=1705)],\n",
      "      dtype=object), 'x': tensor([[75, 20, 75, 22, 49, 19,  2],\n",
      "        [31, 43, 55, 38,  2,  0,  0],\n",
      "        [60, 57, 23, 33,  2,  0,  0],\n",
      "        [76, 20, 30, 19,  2,  0,  0],\n",
      "        [ 5, 61, 27, 63,  2,  0,  0],\n",
      "        [37, 23, 63,  2,  0,  0,  0],\n",
      "        [ 8, 31,  9,  2,  0,  0,  0],\n",
      "        [ 5, 50, 74,  2,  0,  0,  0],\n",
      "        [21, 47, 33,  2,  0,  0,  0],\n",
      "        [63,  6,  2,  0,  0,  0,  0]], device='cuda:0'), 'y': tensor([1, 3, 0, 3, 0, 3, 0, 3, 3, 3], device='cuda:0'), 'forms': array(['ha-ka-ha-ki-ri-jo', 'ne-qa-sa-pi', 'ta-si-ko-no', 'ai-ka-na-jo',\n",
      "       'a-te-mi-to', 'pe-ko-to', 'o-ne-u', 'a-ro-zo', 'ke-ra-no', 'to-e'],\n",
      "      dtype=object), 'lengths': tensor([7, 5, 5, 5, 5, 4, 4, 4, 4, 3], device='cuda:0'), 'lang': 'transliterated_linear_b', 'num_samples': 10}\n"
     ]
    }
   ],
   "source": [
    "dataloader = AuxiliaryDataLoader(dataset, batch_size=10)\n",
    "for batch in dataloader:\n",
    "    logging.critical(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4g4oCsQPwJy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "B1hBxAYfPyEj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BRNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, num_classes, depth=1):\n",
    "        super(BRNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers,\n",
    "            bidirectional=True, batch_first=True, dropout=dropout #perche bidirectional\n",
    "        )\n",
    "\n",
    "        input_size = hidden_size * 2  # bidirectional\n",
    "        #layers = []\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        #for _ in range(depth):\n",
    "        #    layers.append(nn.Linear(input_size, input_size))\n",
    "        #    layers.append(nn.ReLU())\n",
    "        #    layers.append(nn.Dropout(dropout))\n",
    "        #    layers.append(nn.LayerNorm(input_size))\n",
    "\n",
    "        #self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(input_size, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=batch.x.shape[1])\n",
    "\n",
    "        # Pooling over the sequence dimension (dim=1)\n",
    "        out = out.transpose(1, 2)  # shape: (batch_size, hidden_size, seq_len)\n",
    "        pooled = self.avg_pool(out).squeeze(-1)  # shape: (batch_size, hidden_size*2)\n",
    "\n",
    "        ## Pass through hidden layers and classification head\n",
    "        #pooled = self.hidden_layers(pooled)  # (batch_size, hidden_size*2)\n",
    "        logits = self.output_layer(pooled)   # (batch_size, num_classes)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        return log_probs  # shape: (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "b-1oJohYxhxU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BRNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, num_classes, depth=1):\n",
    "        super(BRNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_ID)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers,\n",
    "            bidirectional=True, batch_first=True, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=16, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size * 2, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embedded = self.embedding(batch.x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, batch.lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "\n",
    "        rnn_out, _ = self.brnn(packed_embedded)\n",
    "        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True, total_length=batch.x.shape[1])  # [B, L, H*2]\n",
    "\n",
    "        # --- Attention: Self-attention over RNN outputs ---\n",
    "        # MultiheadAttention expects input of shape (batch, seq, embed) when batch_first=True\n",
    "        # Generate attention mask (True = ignore, False = keep)\n",
    "        max_len = batch.x.size(1)\n",
    "        attn_mask = torch.arange(max_len, device=batch.lengths.device)[None, :] >= batch.lengths[:, None]  # [B, L]\n",
    "\n",
    "        # Apply multi-head attention (query=key=value=rnn_out)\n",
    "        attended_out, _ = self.attention(rnn_out, rnn_out, rnn_out, key_padding_mask=attn_mask)  # [B, L, H*2]\n",
    "\n",
    "        # Reduce to a single vector (e.g. mean over time, ignoring pads)\n",
    "        attn_mask_float = (~attn_mask).float().unsqueeze(-1)  # [B, L, 1]\n",
    "        summed = (attended_out * attn_mask_float).sum(dim=1)  # [B, H*2]\n",
    "        lengths = batch.lengths.unsqueeze(1).clamp(min=1).to(summed.dtype)\n",
    "        pooled = summed / lengths  # [B, H*2]\n",
    "\n",
    "        logits = self.output_layer(pooled)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "qvQUM4fUakYC"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv1dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv1dLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x))\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A single fully connected block with activation, dropout, and optional normalization.\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, normalize):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = normalize\n",
    "        if self.normalize:\n",
    "            self.layer_norm = nn.LayerNorm(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ConvTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, num_layers, dropout, normalize, num_classes):\n",
    "        super(ConvTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers: each kernel size gets its own Conv1D layer\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # Fully connected layers\n",
    "        #self.fc_layers = [FCBlock(num_filters * len(kernel_sizes), num_filters * len(kernel_sizes), dropout, normalize) for _ in range(num_layers-1)]\n",
    "        #self.fc_layers.append(FCBlock(num_filters * len(kernel_sizes), num_filters, dropout, normalize))\n",
    "        #self.fc_layers = nn.ModuleList(self.fc_layers)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(batch.x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "\n",
    "        #conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        ## Fully connected layers with normalization\n",
    "        #for fc in self.fc_layers:\n",
    "        #    conv_out = fc(conv_out)\n",
    "\n",
    "        #out = conv_out.transpose(1, 2)  # shape: (B, C*k, L)\n",
    "        pooled = self.avg_pool(conv_out).squeeze(-1)  # shape: (B, C*k)\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(pooled) # shape: (B, out_dim)\n",
    "        logits = torch.nn.functional.layer_norm(logits, logits.size()[1:])\n",
    "\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW33X_JoQi-y"
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "mGgC3cIpQlA0"
   },
   "outputs": [],
   "source": [
    "@has_properties('num_epochs', 'saved_path', 'learning_rate', 'log_dir', 'save_all', 'eval_interval', 'check_interval')\n",
    "class TextClassifierTrainer:\n",
    "    def __init__(self, model, train_data_loader, test_data_loader, num_epochs, saved_path, learning_rate, log_dir, save_all, eval_interval, check_interval):\n",
    "        self.tracker = Tracker('text-classifier')\n",
    "        stage = self.tracker.add_stage('round', self.num_epochs)\n",
    "        stage.add_stage('train step')\n",
    "        self.tracker.fix_schedule()\n",
    "        self.model = model\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self._init_optimizer()\n",
    "        self._init_loss()\n",
    "        self.tb_writer = SummaryWriter(self.log_dir)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_optimizer(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @log_this('IMP')\n",
    "    def _init_loss(self):\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"none\")\n",
    "        self.nll_loss2 = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"mean\")\n",
    "\n",
    "    def load(self):\n",
    "        ckpt = torch.load(self.saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "        try_load('optimizer')\n",
    "        try_load('tracker')\n",
    "        logging.imp(f'Loaded saved states from {self.saved_path}')\n",
    "\n",
    "    def save(self, suffix='latest'):\n",
    "        if self.log_dir:\n",
    "            logging.info(f'Saving to {self.log_dir}')\n",
    "\n",
    "            ckpt = {\n",
    "                'model': self.model.state_dict(),\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "                'tracker': self.tracker.state_dict()\n",
    "            }\n",
    "\n",
    "            torch.save(ckpt, os.path.join(self.log_dir, f'saved.{suffix}'))\n",
    "            logging.info('Finished saving decipher trainer')\n",
    "\n",
    "    def train(self, evaluator):\n",
    "        if self.saved_path:\n",
    "            self.load()\n",
    "\n",
    "        while not self.tracker.finished:\n",
    "            self._train_loop(evaluator)\n",
    "\n",
    "    @property\n",
    "    def round_num(self):\n",
    "        return self.tracker.get('round') + 1\n",
    "\n",
    "    @property\n",
    "    def stage(self):\n",
    "        return self.tracker.current_stage\n",
    "\n",
    "    def _train_loop(self, evaluator):\n",
    "        if self.stage.name == 'train step':\n",
    "            self._do_train_step(evaluator)\n",
    "        else:\n",
    "            raise RuntimeError(f'Not recognized stage name {self.stage.name}')\n",
    "        self.tracker.update()\n",
    "\n",
    "    @property\n",
    "    def epoch(self):\n",
    "        return self.round_num\n",
    "\n",
    "    def _do_train_step(self, evaluator):\n",
    "        self._train_step_kernel()\n",
    "        self._do_post_train_step(evaluator)\n",
    "\n",
    "    def _train_step_kernel(self):\n",
    "        for batch in self.train_data_loader:\n",
    "            self._do_train_step_batch(batch)\n",
    "\n",
    "    def _do_post_train_step(self, evaluator):\n",
    "        if self.epoch % self.eval_interval == 0:\n",
    "            self._do_eval(evaluator)\n",
    "        if self.epoch % self.check_interval == 0:\n",
    "            self._do_check()\n",
    "\n",
    "    def _do_eval(self, evaluator):\n",
    "        eval_scores = evaluator.evaluate(self.epoch)\n",
    "        # Tensorboard\n",
    "        for setting, score in eval_scores.items():\n",
    "            self.tb_writer.add_scalar(setting, score, global_step=self.epoch)\n",
    "        self.tb_writer.flush()\n",
    "\n",
    "        #debug\n",
    "        train_preds = self.model(self.train_data_loader.entire_batch)\n",
    "        train_acc = evaluator.compute_accuracy(train_preds, self.train_data_loader.entire_batch.y)\n",
    "        train_loss = self.nll_loss2(train_preds, self.train_data_loader.entire_batch.y)\n",
    "        logging.critical(f\"'train_acc', {train_acc.mean}, loss: {train_loss}, global_step={self.epoch}\")\n",
    "\n",
    "        # Save\n",
    "        self.save()\n",
    "        if self.save_all:\n",
    "            self.save(suffix=self.epoch)\n",
    "\n",
    "    def _do_check(self):\n",
    "        self.tracker.check_metrics(self.epoch)\n",
    "        self.tb_writer.add_scalar('loss', self.tracker.metrics.loss.mean, global_step=self.epoch)\n",
    "        self.tracker.clear_metrics()\n",
    "\n",
    "    def _do_train_step_batch(self, batch, update=True):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        # Run it.\n",
    "        model_ret = self.model(batch)\n",
    "        if update:\n",
    "            self._do_train_step_batch_update(model_ret, batch)\n",
    "        return model_ret\n",
    "\n",
    "    def _do_train_step_batch_update(self, model_ret, batch):\n",
    "        # Get the metrics.\n",
    "        metrics = self._analyze_model_return(model_ret, batch)\n",
    "        # Compute gradients and backprop.\n",
    "        #metrics.loss.mean.backward()\n",
    "        lss = self.nll_loss2(model_ret, batch.y)\n",
    "        #logging.critical(f\"Loss Value: {lss}\")\n",
    "        lss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Update metrics.\n",
    "        self.tracker.update_metrics(metrics)\n",
    "\n",
    "    def _analyze_model_return(self, model_ret, batch):\n",
    "        # NOTE This means we are conditioning on one specific flow.\n",
    "        #logging.critical(f\"{model_ret.shape}, {batch.y.shape}, {batch.x.shape}\")\n",
    "        nll_loss = self.nll_loss(model_ret, batch.y)\n",
    "\n",
    "        nll_loss = Metric('nll_loss', nll_loss.sum(dim=-1), batch.num_samples)\n",
    "        loss = Metric('loss', nll_loss.mean, 1)\n",
    "\n",
    "        return Metrics(loss, nll_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3o3VulFSA6a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "qaHOhNmDSDQs"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TextClassifierEvaluator:\n",
    "    def __init__(self, model, data_loader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def __str__(self):\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'accuracy'\n",
    "        table.align = 'l'\n",
    "        return str(table)\n",
    "\n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        table = pt()\n",
    "        table.field_names = 'epoch', 'accuracy'\n",
    "\n",
    "        metrics = Metrics()\n",
    "\n",
    "        for batch in self.data_loader:\n",
    "            model_ret = self.model(batch)\n",
    "            # Magic tensor to the rescue!\n",
    "            gt = batch.y\n",
    "            preds = model_ret\n",
    "\n",
    "            seq_acc = self.compute_accuracy(preds, gt)\n",
    "\n",
    "            new_metrics = Metrics(seq_acc)\n",
    "            metrics += new_metrics\n",
    "\n",
    "        values = [epoch] + [getattr(metrics, field).mean for field in table.field_names[1:]]\n",
    "        table.add_row(values)\n",
    "\n",
    "        eval_scores = {fn: val for (fn, val) in zip(table.field_names, values)}\n",
    "\n",
    "        table.align = 'l'\n",
    "        table.title = f'Epoch: {epoch}'\n",
    "        log_pp(table)\n",
    "        return eval_scores\n",
    "\n",
    "    def compute_accuracy(self, preds, gt):\n",
    "        logging.critical(preds.shape)\n",
    "        pred_indices = preds.argmax(dim=-1)\n",
    "        logging.critical(pred_indices.shape)\n",
    "        # Compare with ground truth and check where predictions are correct\n",
    "        correct = (pred_indices == gt)\n",
    "        # Mean over the batch\n",
    "        accuracy = correct.sum(dim=-1)\n",
    "        return Metric('accuracy', accuracy, len(pred_indices))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O_exCdBTx85",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "7chanGmKTz87"
   },
   "outputs": [],
   "source": [
    "\n",
    "@has_properties('data', 'lang', 'batch_size')\n",
    "class TextClassifierManager:\n",
    "\n",
    "    model_cls = BRNNTextClassifier\n",
    "    #model_cls = ConvTextClassifier\n",
    "    trainer_cls = TextClassifierTrainer\n",
    "\n",
    "    def __init__(self, data, lang, batch_size, args):\n",
    "        self._get_data()\n",
    "        self._get_model(args)\n",
    "        self._get_trainer_and_evaluator(args)\n",
    "\n",
    "    def _get_trainer_and_evaluator(self, args):\n",
    "        self.trainer = type(self).trainer_cls(self.model, self.train_data_loader, self.test_data_loader, args[\"num_epochs\"], args[\"saved_path\"], args[\"learning_rate\"], args[\"log_dir\"], args[\"save_all\"], args[\"eval_interval\"], args[\"check_interval\"])\n",
    "        self.evaluator = TextClassifierEvaluator(self.model, self.test_data_loader)\n",
    "        log_pp(self.trainer.tracker.schedule_as_tree())\n",
    "        log_pp(self.evaluator)\n",
    "\n",
    "    def _get_data(self):\n",
    "        self._get_data_loaders()\n",
    "\n",
    "\n",
    "    def _get_data_loaders(self):\n",
    "        train_data, test_data = self.data\n",
    "        train_dataset = AuxiliaryClassifierDataset(train_data)\n",
    "        self.train_data_loader = AuxiliaryDataLoader(train_dataset, self.batch_size)\n",
    "        test_dataset = AuxiliaryClassifierDataset(test_data)\n",
    "        self.test_data_loader = AuxiliaryDataLoader(test_dataset, self.batch_size)\n",
    "\n",
    "\n",
    "    def _get_model(self, args):\n",
    "        self.model = type(self).model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"num_classes\"], args[\"depth\"])\n",
    "        #self.model = type(self).model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"num_filters\"], args[\"kernel_sizes\"], args[\"num_layers\"], args[\"dropout\"], args[\"normalize\"], args[\"num_classes\"])\n",
    "\n",
    "        log_pp(self.model)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train(self.evaluator)\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6her1ZpgLaS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0ORBW5ZeWb0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:172 - \n",
      "          {'check_interval': 10,\n",
      "           'depth': 0,\n",
      "           'dropout': 0.2,\n",
      "           'embed_size': 256,\n",
      "           'eval_interval': 10,\n",
      "           'gpu': '0',\n",
      "           'hidden_size': 512,\n",
      "           'learning_rate': 0.0001,\n",
      "           'log_dir': './DMPROJECT/auxiliary_classifiers/word_type',\n",
      "           'log_level': 'INFO',\n",
      "           'num_classes': 4,\n",
      "           'num_epochs': 200,\n",
      "           'num_layers': 16,\n",
      "           'save_all': None,\n",
      "           'saved_path': None,\n",
      "           'task': 'word_type',\n",
      "           'vocab_size': 93}\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:172 - \n",
      "          BRNNTextClassifier(\n",
      "            (embedding): Embedding(93, 256, padding_idx=0)\n",
      "            (brnn): RNN(256, 512, num_layers=16, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "            (attention): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output_layer): Linear(in_features=1024, out_features=4, bias=True)\n",
      "            (log_softmax): LogSoftmax(dim=-1)\n",
      "          )\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>round  12%|                                         |  24/200 [10:35&lt;1h 17:37, 0.04 samples/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mIMP - 09/13/25 17:40:40 - 0:00:00 at 1009358421.py:143 - *STARTING* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *FINISHED* _init_optimizer\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *STARTING* _init_loss\u001b[0m\n",
      "\u001b[36mIMP - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:143 - *FINISHED* _init_loss\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:172 - \n",
      "          Stage(name=text-classifier, num_steps=1)\n",
      "           Stage(name=round, num_steps=200)\n",
      "               Stage(name=train step, num_steps=1)\n",
      "          \n",
      "          \u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:40:43 - 0:00:03 at 1009358421.py:172 - \n",
      "          +-------+----------+\n",
      "          | epoch | accuracy |\n",
      "          +-------+----------+\n",
      "          +-------+----------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:03 - 0:04:23 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:04 - 0:04:24 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:41 - torch.Size([25, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:05 - 0:04:25 at 2442567354.py:43 - torch.Size([25])\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:05 - 0:04:25 at 1009358421.py:172 - \n",
      "          +-----------------------------------------+\n",
      "          |                Epoch: 10                |\n",
      "          +-------+---------------------------------+\n",
      "          | epoch | accuracy                        |\n",
      "          +-------+---------------------------------+\n",
      "          | 10    | tensor(0.3160, device='cuda:0') |\n",
      "          +-------+---------------------------------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2442567354.py:41 - torch.Size([2529, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2442567354.py:43 - torch.Size([2529])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:104 - 'train_acc', 0.3222617506980896, loss: 0.7462226152420044, global_step=10\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:42 - Saving to ./DMPROJECT/auxiliary_classifiers/word_type\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 2691311305.py:51 - Finished saving decipher trainer\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:45:06 - 0:04:26 at 1009358421.py:172 - \n",
      "          +--------------------------------------+\n",
      "          |              Epoch: 10               |\n",
      "          +----------+----------+--------+-------+\n",
      "          | name     | value    | weight | mean  |\n",
      "          +----------+----------+--------+-------+\n",
      "          | loss     | 288.403  | 800    | 0.361 |\n",
      "          | nll_loss | 9035.674 | 25290  | 0.357 |\n",
      "          +----------+----------+--------+-------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:27 - 0:08:47 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:28 - 0:08:48 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([32, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([32])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:41 - torch.Size([25, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:29 - 0:08:49 at 2442567354.py:43 - torch.Size([25])\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:29 - 0:08:49 at 1009358421.py:172 - \n",
      "          +-----------------------------------------+\n",
      "          |                Epoch: 20                |\n",
      "          +-------+---------------------------------+\n",
      "          | epoch | accuracy                        |\n",
      "          +-------+---------------------------------+\n",
      "          | 20    | tensor(0.2970, device='cuda:0') |\n",
      "          +-------+---------------------------------+\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2442567354.py:41 - torch.Size([2529, 4])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2442567354.py:43 - torch.Size([2529])\u001b[0m\n",
      "\u001b[31m\u001b[47mCRITICAL - 09/13/25 17:49:30 - 0:08:50 at 2691311305.py:104 - 'train_acc', 0.3396599292755127, loss: 0.71871018409729, global_step=20\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:30 - 0:08:50 at 2691311305.py:42 - Saving to ./DMPROJECT/auxiliary_classifiers/word_type\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:31 - 0:08:51 at 2691311305.py:51 - Finished saving decipher trainer\u001b[0m\n",
      "\u001b[32mINFO - 09/13/25 17:49:31 - 0:08:51 at 1009358421.py:172 - \n",
      "          +--------------------------------------+\n",
      "          |              Epoch: 20               |\n",
      "          +----------+----------+--------+-------+\n",
      "          | name     | value    | weight | mean  |\n",
      "          +----------+----------+--------+-------+\n",
      "          | loss     | 268.186  | 800    | 0.335 |\n",
      "          | nll_loss | 8412.519 | 25290  | 0.333 |\n",
      "          +----------+----------+--------+-------+\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = [train_data, test_data]\n",
    "lang = LOST_LANG\n",
    "batch_size = 32\n",
    "log_dir = os.path.join(os.path.join(prefix_path, \"auxiliary_classifiers\"), TASK)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "brnn_args = {\n",
    "    \"vocab_size\": len(get_charset(lang)),\n",
    "    \"embed_size\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 16,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 200,\n",
    "    \"saved_path\": None,#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"log_dir\": log_dir,\n",
    "    \"save_all\": None,\n",
    "    \"eval_interval\": 10,\n",
    "    \"check_interval\": 10,\n",
    "    \"depth\": 0,\n",
    "    \"num_classes\": CLASSES_PER_TASK[TASK],\n",
    "    \"task\": TASK,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "'''\n",
    "hyperparams = {\n",
    "    #\"model\": \"ConvTextInfillerRNN\",\n",
    "    \"embed_size\": 256,\n",
    "    \"num_filters\": 256,\n",
    "    \"kernel_sizes\": [9, 11, 35, 57, 75],  # Example kernel sizes\n",
    "    \"num_layers\": 1,\n",
    "    \"dropout\": 0.2,\n",
    "    \"normalize\": True\n",
    "}\n",
    "'''\n",
    "#for l,v in hyperparams.items(): brnn_args[l] = v\n",
    "\n",
    "\n",
    "if brnn_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(brnn_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = brnn_args[\"gpu\"]\n",
    "\n",
    "create_logger(filepath=brnn_args[\"log_dir\"] + '/log', log_level=brnn_args[\"log_level\"])\n",
    "log_pp(pformat(brnn_args))\n",
    "\n",
    "clear_stages()\n",
    "\n",
    "\n",
    "tcm = TextClassifierManager(data, lang, batch_size, brnn_args)\n",
    "tcm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qCvm8h1rTYh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Prova senza luo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "uFImTDF-sS8e"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"vocab_size\": len(get_charset(LOST_LANG)),\n",
    "    \"embed_size\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 16,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"saved_path\": None,#os.path.join(log_dir, \"saved.latest\"),\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"save_all\": None,\n",
    "    \"eval_interval\": 10,\n",
    "    \"check_interval\": 10,\n",
    "    \"depth\": 0,\n",
    "    \"num_classes\": CLASSES_PER_TASK[TASK],\n",
    "    \"task\": TASK,\n",
    "    \"log_level\": \"INFO\",\n",
    "    \"gpu\": \"0\"\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    #\"model\": \"ConvTextInfillerRNN\",\n",
    "    \"embed_size\": 256,\n",
    "    \"num_filters\": 256,\n",
    "    \"kernel_sizes\": [1, 3, 7, 9, 11],  # Example kernel sizes\n",
    "    \"num_layers\": 8,\n",
    "    \"dropout\": 0.2,\n",
    "    \"normalize\": True\n",
    "}\n",
    "#for l,v in hyperparams.items(): args[l] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "5soMin9hrad6"
   },
   "outputs": [],
   "source": [
    "train_dataset = AuxiliaryClassifierDataset(train_data)\n",
    "train_data_loader = AuxiliaryDataLoader(train_dataset, 32)\n",
    "test_dataset = AuxiliaryClassifierDataset(test_data)\n",
    "test_data_loader = AuxiliaryDataLoader(test_dataset, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "opdl-uj1sq1e"
   },
   "outputs": [],
   "source": [
    "model_cls = BRNNTextClassifier#ConvTextClassifier\n",
    "model_loss = nn.NLLLoss(ignore_index=PAD_ID, reduction=\"mean\")\n",
    "#model = model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"num_filters\"], args[\"kernel_sizes\"], args[\"num_layers\"], args[\"dropout\"], args[\"normalize\"], args[\"num_classes\"])\n",
    "model =  model_cls(args[\"vocab_size\"], args[\"embed_size\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"num_classes\"], args[\"depth\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=args[\"learning_rate\"])#, weight_decay=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdcDx_HdtzqO"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import math\n",
    "\n",
    "def train(model, train_data_loader, test_data_loader, loss_fn, optimizer, num_epochs=50, print_every=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            inputs = Map(x=batch.x.to(device), lengths=batch.lengths.to(device))\n",
    "            labels = batch.y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            if math.isnan(loss.item()):\n",
    "                print(f\"[Epoch {epoch:03d}]  NaN detected PORCODIO in loss! Skipping this batch.\")\n",
    "                continue  # Skip this batch to avoid breaking backprop\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total if total > 0 else 0\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"[Epoch {epoch:03d}] Train Loss: {total_loss:.4f} | Train Accuracy: {acc:.2f}%\")\n",
    "            print(total_loss)\n",
    "            # --- Evaluation on test data ---\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_data_loader:\n",
    "                    inputs = Map(x=batch.x.to(device), lengths=batch.lengths.to(device))\n",
    "                    labels = batch.y.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    test_correct += (preds == labels).sum().item()\n",
    "                    test_total += labels.size(0)\n",
    "\n",
    "            test_acc = 100.0 * test_correct / test_total if test_total > 0 else 0\n",
    "            print(f\"[Epoch {epoch:03d}] Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "            print(test_loss)\n",
    "\n",
    "train(model, train_data_loader, test_data_loader, model_loss, optimizer, num_epochs=100, print_every=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYpB3YBqVla4"
   },
   "source": [
    "#### SkLearn classifiers baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ahVoVJLcVv7O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Word(lang='transliterated_linear_b', form='*56-ra-ku-ja', idx=0), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ma-o', idx=1), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-me-we', idx=2), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ra-te-ja', idx=3), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-rya', idx=5), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-te', idx=6), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-de-te-re', idx=7), 3),\n",
       " (Word(lang='transliterated_linear_b', form='a-di-nwa-ta', idx=8), 0),\n",
       " (Word(lang='transliterated_linear_b', form='a-di-ri-ja-pi', idx=9), 3)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "o5Kszl-pV_fY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word objects: [Word(lang='transliterated_linear_b', form='*56-ra-ku-ja', idx=0), Word(lang='transliterated_linear_b', form='a-da-ma-o', idx=1), Word(lang='transliterated_linear_b', form='a-da-me-we', idx=2), Word(lang='transliterated_linear_b', form='a-da-ra-te-ja', idx=3), Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), Word(lang='transliterated_linear_b', form='a-de-rya', idx=5), Word(lang='transliterated_linear_b', form='a-de-te', idx=6), Word(lang='transliterated_linear_b', form='a-de-te-re', idx=7), Word(lang='transliterated_linear_b', form='a-di-nwa-ta', idx=8), Word(lang='transliterated_linear_b', form='a-di-ri-ja-pi', idx=9)]\n",
      "Labels: [1, 0, 2, 1, 0, 1, 2, 2, 1, 2]\n",
      "Original: [(Word(lang='transliterated_linear_b', form='ma-so-mo', idx=2578), 0), (Word(lang='transliterated_linear_b', form='ko-ru-ta-ta', idx=2509), 1), (Word(lang='transliterated_linear_b', form='se-ri-no-te', idx=1581), 2), (Word(lang='transliterated_linear_b', form='qo-u-ka-ra-o-i', idx=1448), 0), (Word(lang='transliterated_linear_b', form='a-da-ra-ti-jo', idx=4), 0), (Word(lang='transliterated_linear_b', form='si-ja-ma-to', idx=2883), 1), (Word(lang='transliterated_linear_b', form='wi-su-ro', idx=1861), 0), (Word(lang='transliterated_linear_b', form='a-qi-ja-i', idx=2056), 1), (Word(lang='transliterated_linear_b', form='a-*35-ka', idx=1933), 1), (Word(lang='transliterated_linear_b', form='da-i-ra', idx=2297), 1)]\n",
      "Forms to be vectorized: ['*56-ra-ku-ja', 'a-da-ma-o', 'a-da-me-we', 'a-da-ra-te-ja', 'a-da-ra-ti-jo', 'a-de-rya', 'a-de-te', 'a-de-te-re', 'a-di-nwa-ta', 'a-di-ri-ja-pi']\n",
      "Shape of feature matrix: (2979, 5381)\n",
      "First row (dense): 1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 3. Extract X (Word) and y (label)\n",
    "\n",
    "X = [word for word, label in data]\n",
    "\n",
    "y = [label for word, label in data]\n",
    "\n",
    "# Log what we're feeding in\n",
    "print(\"Raw Word objects:\", X[:10])\n",
    "print(\"Labels:\", y[:10])\n",
    "print(\"Original:\", train_data[:10])\n",
    "\n",
    "# Optional: show the forms that will actually be used for feature extraction\n",
    "forms = [word.form for word in X]\n",
    "print(\"Forms to be vectorized:\", forms[:10])\n",
    "\n",
    "# Custom transformer to extract simple numeric features per word form\n",
    "def word_length(forms):\n",
    "    # returns a numpy array of shape (n_samples, 1) with word lengths (counting syllables)\n",
    "    return np.array([len(f.split(\"-\")) for f in forms]).reshape(-1, 1)\n",
    "\n",
    "def vowel_count(forms):\n",
    "    vowels = set(\"aeiou\")\n",
    "    counts = []\n",
    "    for f in forms:\n",
    "        # count vowels ignoring dashes\n",
    "        counts.append(sum(ch in vowels for ch in f.replace(\"-\", \"\")))\n",
    "    return np.array(counts).reshape(-1, 1)\n",
    "\n",
    "# 4. Feature extractor with syllables and character n-grams\n",
    "features = FeatureUnion([\n",
    "    (\"syllables\", CountVectorizer(tokenizer=lambda x: x.split(\"-\"), token_pattern=None)),\n",
    "    (\"char_ngrams\", TfidfVectorizer(analyzer='char', ngram_range=(2,4), preprocessor=lambda x: x.replace(\"-\", \"\"))),\n",
    "    #(\"word_length\", FunctionTransformer(word_length, validate=False)),\n",
    "    #(\"vowel_count\", FunctionTransformer(vowel_count, validate=False)),\n",
    "])\n",
    "\n",
    "# 5. Fit and transform\n",
    "X_features = features.fit_transform(forms)\n",
    "\n",
    "# 6. Log output\n",
    "print(\"Shape of feature matrix:\", X_features.shape)\n",
    "print(\"First row (dense):\", X_features[0].toarray().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "qKMz5CYOZru7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression\n",
      "Train Accuracy: 0.906547285954113\n",
      "Validation Accuracy: 0.8187919463087249\n",
      "Test Accuracy: 0.7835570469798657\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87       318\n",
      "           1       0.80      0.71      0.75       159\n",
      "           2       0.61      0.56      0.59       119\n",
      "\n",
      "    accuracy                           0.78       596\n",
      "   macro avg       0.75      0.73      0.73       596\n",
      "weighted avg       0.78      0.78      0.78       596\n",
      "\n",
      "\n",
      " Random Forest\n",
      "Train Accuracy: 0.9155008393956351\n",
      "Validation Accuracy: 0.7483221476510067\n",
      "Test Accuracy: 0.7248322147651006\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.81       318\n",
      "           1       0.83      0.48      0.61       159\n",
      "           2       0.66      0.47      0.55       119\n",
      "\n",
      "    accuracy                           0.72       596\n",
      "   macro avg       0.73      0.63      0.66       596\n",
      "weighted avg       0.73      0.72      0.71       596\n",
      "\n",
      "\n",
      " Linear SVM\n",
      "Train Accuracy: 0.9826524902070509\n",
      "Validation Accuracy: 0.8338926174496645\n",
      "Test Accuracy: 0.825503355704698\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       318\n",
      "           1       0.83      0.82      0.83       159\n",
      "           2       0.64      0.65      0.64       119\n",
      "\n",
      "    accuracy                           0.83       596\n",
      "   macro avg       0.79      0.79      0.79       596\n",
      "weighted avg       0.83      0.83      0.83       596\n",
      "\n",
      "\n",
      " Multinomial Naive Bayes\n",
      "Train Accuracy: 0.7839955232232793\n",
      "Validation Accuracy: 0.7315436241610739\n",
      "Test Accuracy: 0.6963087248322147\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.95      0.79       318\n",
      "           1       0.78      0.40      0.53       159\n",
      "           2       0.72      0.43      0.54       119\n",
      "\n",
      "    accuracy                           0.70       596\n",
      "   macro avg       0.72      0.59      0.62       596\n",
      "weighted avg       0.71      0.70      0.67       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HistGradientBoosting\n",
      "Train Accuracy: 0.9188584219362059\n",
      "Validation Accuracy: 0.802013422818792\n",
      "Test Accuracy: 0.7432885906040269\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       318\n",
      "           1       0.77      0.67      0.72       159\n",
      "           2       0.52      0.47      0.49       119\n",
      "\n",
      "    accuracy                           0.74       596\n",
      "   macro avg       0.70      0.67      0.68       596\n",
      "weighted avg       0.74      0.74      0.74       596\n",
      "\n",
      "\n",
      " Neural Network (MLP)\n",
      "Train Accuracy: 0.8393956351426972\n",
      "Validation Accuracy: 0.7466442953020134\n",
      "Test Accuracy: 0.7248322147651006\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.82       318\n",
      "           1       0.76      0.49      0.60       159\n",
      "           2       0.70      0.46      0.56       119\n",
      "\n",
      "    accuracy                           0.72       596\n",
      "   macro avg       0.73      0.63      0.66       596\n",
      "weighted avg       0.73      0.72      0.71       596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "SEED = 17\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, \n",
    "                                              random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200,\n",
    "                                            max_depth=30,\n",
    "                                            random_state=SEED),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000, random_state=SEED),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_iter=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4\n",
    "    ),\n",
    "    \"Neural Network (MLP)\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100,),  # default is one hidden layer with 100 neurons\n",
    "        max_iter=300,\n",
    "        alpha=1e-4,\n",
    "        solver='adam',\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10\n",
    "    )\n",
    "}\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2/0.8, random_state=SEED)\n",
    "\n",
    "split_strings = [\"Train\", \"Validation\", \"Test\"]\n",
    "X_list = [X_train, X_val, X_test]\n",
    "y_list = [y_train, y_val, y_test]\n",
    "# Train & Evaluate\n",
    "for name, model in models.items():\n",
    "    for split_s, X_vec, y_vec in zip(split_strings, X_list, y_list):\n",
    "\n",
    "        if name in [\"HistGradientBoosting\", \"Neural Network (MLP)\"]:\n",
    "            # These models expect dense input\n",
    "            X_vec = X_vec.toarray()\n",
    "        if split_s.lower() == \"train\":\n",
    "            model.fit(X_vec, y_vec)\n",
    "            print(f\"\\n {name}\")\n",
    "\n",
    "        y_pred = model.predict(X_vec)\n",
    "        print(f\"{split_s} Accuracy:\", accuracy_score(y_vec, y_pred))\n",
    "        if split_s.lower() == \"test\":\n",
    "            print(\"Classification Report (Test):\")\n",
    "            print(classification_report(y_vec, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression\n",
      "Fold accuracies:      [0.7936 0.8087 0.8272 0.8003 0.8185]\n",
      "Fold macro-F1:        [0.7551 0.774  0.7988 0.7676 0.7818]\n",
      "Mean acc  std:      0.8097  0.0121\n",
      "Mean f1_macro  std: 0.7755  0.0146\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      1590\n",
      "           1       0.81      0.77      0.79       735\n",
      "           2       0.72      0.62      0.66       654\n",
      "\n",
      "    accuracy                           0.81      2979\n",
      "   macro avg       0.79      0.77      0.78      2979\n",
      "weighted avg       0.81      0.81      0.81      2979\n",
      "\n",
      "\n",
      " Random Forest\n",
      "Fold accuracies:      [0.7349 0.7483 0.7483 0.7265 0.7445]\n",
      "Fold macro-F1:        [0.6808 0.6914 0.6908 0.6664 0.6886]\n",
      "Mean acc  std:      0.7405  0.0086\n",
      "Mean f1_macro  std: 0.6836  0.0094\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.93      0.81      1590\n",
      "           1       0.81      0.59      0.68       735\n",
      "           2       0.75      0.44      0.56       654\n",
      "\n",
      "    accuracy                           0.74      2979\n",
      "   macro avg       0.76      0.65      0.68      2979\n",
      "weighted avg       0.75      0.74      0.72      2979\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Linear SVM\n",
      "Fold accuracies:      [0.8372 0.8305 0.8523 0.8255 0.8403]\n",
      "Fold macro-F1:        [0.8057 0.7948 0.8255 0.7989 0.8109]\n",
      "Mean acc  std:      0.8372  0.0092\n",
      "Mean f1_macro  std: 0.8072  0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      1590\n",
      "           1       0.83      0.86      0.84       735\n",
      "           2       0.73      0.65      0.69       654\n",
      "\n",
      "    accuracy                           0.84      2979\n",
      "   macro avg       0.81      0.80      0.81      2979\n",
      "weighted avg       0.83      0.84      0.83      2979\n",
      "\n",
      "\n",
      " Multinomial Naive Bayes\n",
      "Fold accuracies:      [0.7282 0.7198 0.7332 0.7248 0.7429]\n",
      "Fold macro-F1:        [0.6678 0.6542 0.676  0.6613 0.6856]\n",
      "Mean acc  std:      0.7298  0.0079\n",
      "Mean f1_macro  std: 0.6690  0.0110\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81      1590\n",
      "           1       0.79      0.53      0.63       735\n",
      "           2       0.74      0.46      0.57       654\n",
      "\n",
      "    accuracy                           0.73      2979\n",
      "   macro avg       0.75      0.64      0.67      2979\n",
      "weighted avg       0.74      0.73      0.71      2979\n",
      "\n",
      "\n",
      " HistGradientBoosting\n",
      "Fold accuracies:      [0.7836 0.7752 0.8138 0.7785 0.7765]\n",
      "Fold macro-F1:        [0.7404 0.7371 0.7763 0.7375 0.7302]\n",
      "Mean acc  std:      0.7855  0.0144\n",
      "Mean f1_macro  std: 0.7443  0.0163\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1590\n",
      "           1       0.76      0.74      0.75       735\n",
      "           2       0.68      0.58      0.62       654\n",
      "\n",
      "    accuracy                           0.79      2979\n",
      "   macro avg       0.76      0.74      0.74      2979\n",
      "weighted avg       0.78      0.79      0.78      2979\n",
      "\n",
      "\n",
      " Neural Network (MLP)\n",
      "Fold accuracies:      [0.7735 0.8037 0.8037 0.7869 0.8286]\n",
      "Fold macro-F1:        [0.7286 0.7662 0.7648 0.7496 0.7962]\n",
      "Mean acc  std:      0.7993  0.0185\n",
      "Mean f1_macro  std: 0.7611  0.0222\n",
      "Classification Report (OOF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87      1590\n",
      "           1       0.80      0.77      0.78       735\n",
      "           2       0.69      0.58      0.63       654\n",
      "\n",
      "    accuracy                           0.80      2979\n",
      "   macro avg       0.77      0.75      0.76      2979\n",
      "weighted avg       0.79      0.80      0.79      2979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Densifier for models that don't accept sparse input\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        return X.toarray()\n",
    "\n",
    "dense_models = {\"HistGradientBoosting\", \"Neural Network (MLP)\"}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scoring = {\"acc\": \"accuracy\", \"f1_macro\": \"f1_macro\"}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Wrap with densifier only if needed\n",
    "    est = make_pipeline(DenseTransformer(), model) if name in dense_models else model\n",
    "\n",
    "    scores = cross_validate(\n",
    "        est, X_features, y, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f\"\\n {name}\")\n",
    "    print(\"Fold accuracies:     \", np.round(scores[\"test_acc\"], 4))\n",
    "    print(\"Fold macro-F1:       \", np.round(scores[\"test_f1_macro\"], 4))\n",
    "    print(\"Mean acc  std:      {:.4f}  {:.4f}\".format(scores[\"test_acc\"].mean(),\n",
    "                                                       scores[\"test_acc\"].std()))\n",
    "    print(\"Mean f1_macro  std: {:.4f}  {:.4f}\".format(scores[\"test_f1_macro\"].mean(),\n",
    "                                                       scores[\"test_f1_macro\"].std()))\n",
    "\n",
    "    # Out-of-fold predictions for an overall classification report\n",
    "    y_pred_oof = cross_val_predict(est, X_features, y, cv=cv, n_jobs=-1)\n",
    "    print(\"Classification Report (OOF):\")\n",
    "    print(classification_report(y, y_pred_oof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "t3VfTuT9BcBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Loss: 0.9436\n",
      "Epoch 10/50 - Loss: 0.8314\n",
      "Epoch 15/50 - Loss: 0.6973\n",
      "Epoch 20/50 - Loss: 0.5510\n",
      "Epoch 25/50 - Loss: 0.4169\n",
      "Epoch 30/50 - Loss: 0.3098\n",
      "Epoch 35/50 - Loss: 0.2281\n",
      "Epoch 40/50 - Loss: 0.1676\n",
      "Epoch 45/50 - Loss: 0.1251\n",
      "Epoch 50/50 - Loss: 0.0955\n",
      "\n",
      " Train Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.9871\n",
      "\n",
      " Validation Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.7584\n",
      "\n",
      " Test Accuracy (PyTorch Dumb MLP + LogSoftmax): 0.7450\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 2. Define the model\n",
    "class DumbMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DumbMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.LogSoftmax(dim=1)  # final log-probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "n_classes = len(set(y))  # assumes classes are 0, 1, ..., n-1\n",
    "\n",
    "model = DumbMLP(input_dim=X_train.shape[1], output_dim=n_classes)\n",
    "\n",
    "# 3. Loss and Optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Training\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. Evaluation\n",
    "model.eval()\n",
    "split_strings = [\"Train\", \"Validation\", \"Test\"]\n",
    "in_vecs = [X_train_tensor, X_val_tensor, X_test_tensor]\n",
    "out_vecs = [y_train_tensor, y_val_tensor, y_test_tensor]\n",
    "for split_s, x_vec, y_vec in zip(split_strings, in_vecs, out_vecs):\n",
    "    with torch.no_grad():\n",
    "        test_output = model(x_vec)\n",
    "        predicted = torch.argmax(test_output, dim=1)\n",
    "        acc = accuracy_score(y_vec.numpy(), predicted.numpy())\n",
    "        print(f\"\\n {split_s} Accuracy (PyTorch Dumb MLP + LogSoftmax): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "YazxyTY2gsRm"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=30, random_state=SEED),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=2000, random_state=SEED),\n",
    "    \"Multinomial NB\": MultinomialNB(),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(\n",
    "        max_iter=100,       # fewer boosting rounds for speed\n",
    "        max_depth=10,       # shallower trees\n",
    "        learning_rate=0.1,  # standard learning rate\n",
    "        random_state=SEED,\n",
    "        early_stopping=True,  # stops training if no improvement\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10,\n",
    "        tol=1e-4\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n9sP8gIgNKw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Define weights for models (you decide these)\n",
    "weights = {\n",
    "    \"Logistic Regression\": 5,\n",
    "    \"Linear SVM\": 5,\n",
    "    \"Random Forest\": 1,\n",
    "    \"Multinomial NB\": 1,\n",
    "    \"HistGradientBoosting\": 3\n",
    "}\n",
    "\n",
    "# Fit all models (same as you have)\n",
    "for name, model in models.items():\n",
    "    if name == \"HistGradientBoosting\":\n",
    "        model.fit(X_train.toarray(), y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "# Get class labels (assuming all models use the same label order)\n",
    "classes = models[\"Logistic Regression\"].classes_\n",
    "\n",
    "# Collect weighted probabilities\n",
    "weighted_probs = np.zeros((X_test.shape[0], len(classes)))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"Linear SVM\":\n",
    "        # Use decision_function + softmax\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        if len(decision_scores.shape) == 1:\n",
    "            # Binary classification, convert to 2-class probs\n",
    "            decision_scores = np.vstack([-decision_scores, decision_scores]).T\n",
    "        probs = softmax(decision_scores, axis=1)\n",
    "    elif name == \"HistGradientBoosting\":\n",
    "        probs = model.predict_proba(X_test.toarray())\n",
    "    else:\n",
    "        probs = model.predict_proba(X_test)\n",
    "\n",
    "    weighted_probs += weights[name] * probs\n",
    "\n",
    "# Normalize weighted_probs (optional, but sum of weights used)\n",
    "weighted_probs /= sum(weights.values())\n",
    "\n",
    "# Final ensemble prediction\n",
    "y_pred_ensemble = classes[np.argmax(weighted_probs, axis=1)]\n",
    "\n",
    "# Evaluate ensemble\n",
    "print(\"\\n Ensemble weighted voting results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
    "\n",
    "print(classification_report(y_test, y_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "IXFUR9M3Jo28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in ./.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from fasttext) (1.23.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.local/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.local/lib/python3.10/site-packages (from fasttext) (80.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: numpy 1.23.5\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2.0 in ./.local/lib/python3.10/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "!pip uninstall -y thinc\n",
    "!pip uninstall -y spacy\n",
    "!pip uninstall -y fastai\n",
    "!pip uninstall -y numpy\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7jXgc6kRBwW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgxsiJlcJTv6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LinearBWordClassifier:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext\"), \"linearb_fasttext_classifier.bin\"),\n",
    "                 train_data_path=\"linearb_supervised_train.txt\",\n",
    "                 dim=1000,\n",
    "                 epoch=100,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=6,\n",
    "                 word_ngrams=1,\n",
    "                 bucket=2000000,\n",
    "                 loss='softmax',\n",
    "                 verbose=2\n",
    "                 ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'dim': dim,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'bucket': bucket,\n",
    "            'loss': loss,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "    def preprocess_pairs(self, word_label_pairs):\n",
    "        # Return new list with hyphens removed from word.form\n",
    "        return [((word.form.replace(\"-\", \"\")), label) for word, label in word_label_pairs]\n",
    "\n",
    "    def _write_supervised_data(self, word_label_pairs):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for word, label in word_label_pairs:\n",
    "                f.write(f\"__label__{label} {word}\\n\")\n",
    "\n",
    "    def train(self, word_label_pairs):\n",
    "        clean_data = self.preprocess_pairs(word_label_pairs)\n",
    "        self._write_supervised_data(clean_data)\n",
    "        self.model = fasttext.train_supervised(\n",
    "            input=self.train_data_path,\n",
    "            **self.params\n",
    "        )\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def predict(self, word, k=1):\n",
    "        if self.model:\n",
    "            label, prob = self.model.predict(str(word), k=k)  # ensure string input\n",
    "            return label[0].replace(\"__label__\", \"\"), prob[0]\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    def evaluate_accuracy(self, word_label_pairs):\n",
    "        clean_data = self.preprocess_pairs(word_label_pairs)\n",
    "        y_true = [str(label) for _, label in clean_data]\n",
    "        y_pred = [self.predict(word)[0] for word, _ in clean_data]\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace this with your real word-label list\n",
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Train\n",
    "classifier = LinearBWordClassifier()\n",
    "classifier.train(train_data)\n",
    "classifier.save_model()\n",
    "\n",
    "# Evaluate\n",
    "train_acc = classifier.evaluate_accuracy(train_data)\n",
    "test_acc = classifier.evaluate_accuracy(test_data)\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4xvLfAjoWNQ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text Infilling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5-PzMfDGWWA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### First part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "z85VlGAuobKU"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, brnn_model, loaders, brnn_args, vocabulary, topk_values):\n",
    "        self.brnn_model = brnn_model\n",
    "        self.loaders = loaders\n",
    "        self.brnn_args = brnn_args\n",
    "        self.vocabulary = vocabulary\n",
    "        self.topks = topk_values\n",
    "\n",
    "    def create_words_dataset(self):\n",
    "        for topk in self.topks:\n",
    "            word_dict, word2seq = self.brute_force_infillings(topk)\n",
    "            self.write_to_file(word_dict, word2seq, topk)\n",
    "\n",
    "    def write_to_file(self, word_dict, word2seq, topk):\n",
    "        file_path = os.path.join(prefix_path, f\"linb_words_top_{topk}.tsv\")\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word, valid in sorted(word_dict.items()):\n",
    "                #logging.critical(f\"{word}\\t{word2seq[word]}\")\n",
    "                f.write(f\"{word}\\t{int(valid)}\\t{sorted([(seq.idx, val) for (seq, val) in word2seq[word]])}\\n\")  # Each line: key TAB value\n",
    "\n",
    "\n",
    "    def brute_force_infillings(self, topk):\n",
    "        forms = [w.form for w in self.vocabulary.get_words]\n",
    "        word_dict = {form: True if \"*\" not in form else False for form in forms}\n",
    "        word2seq = defaultdict(set)\n",
    "        for loader in self.loaders:\n",
    "            for batch in loader:\n",
    "                for seq in batch.sequences:\n",
    "                    if seq.idx == 127:\n",
    "                        logging.critical(f\"{seq.form}, {seq.missing_form}\")\n",
    "                    for word in seq.word_list:\n",
    "                        word2seq[word.form].add((seq, 1))\n",
    "\n",
    "                word_bf = self.brute_force_infillings_batch(batch, topk)\n",
    "                for w in word_bf:\n",
    "                    if w not in word_dict:\n",
    "                        word_dict[w] = False\n",
    "                    word2seq[w].update(word_bf[w])\n",
    "\n",
    "        return word_dict, word2seq\n",
    "\n",
    "    def brute_force_infillings_batch(self, batch, topk):\n",
    "        word_bf = defaultdict(set)\n",
    "\n",
    "        brnn_ret = self.brnn_model(batch)\n",
    "        # Get indices of UNK tokens\n",
    "        batch_indices, token_indices = torch.where(batch.x == UNK_ID)\n",
    "\n",
    "        # Get the predictions just before each missing token\n",
    "        # Shape: [num_missings, vocab_size]\n",
    "        predictions_before_missing = brnn_ret.predictions[batch_indices, token_indices - 1]\n",
    "\n",
    "        # Get top 10 predicted token IDs for each missing position\n",
    "        # Shape: [num_missings, 10]\n",
    "        topk_values, topk_indices = torch.topk(predictions_before_missing, k=topk, dim=-1)\n",
    "        predicted_sequence = torch.argmax(brnn_ret.predictions, dim=-1)\n",
    "        incomplete = self.extract_incomplete_words(batch.x)\n",
    "\n",
    "        cset = get_charset(batch.lang)\n",
    "        for i in range(batch.x.shape[0]):\n",
    "            word = incomplete[i][0]\n",
    "            missing_idx = token_indices[i] - incomplete[i][1]\n",
    "            for j, infilling in enumerate(topk_indices[i]):\n",
    "                word[missing_idx] = infilling\n",
    "                word_str = word.cpu().numpy()\n",
    "                word_str = cset.id2char(word_str)\n",
    "                word_str = \"-\".join(word_str)\n",
    "\n",
    "                if \"<\" not in word_str:\n",
    "                    word_bf[word_str].add((batch.sequences[i], topk_values[i][j].item()))\n",
    "                #DEBUG\n",
    "                #else:\n",
    "                #    logging.critical(f\"{word_str}, {topk_values[i][j].item()}\")\n",
    "        return word_bf\n",
    "\n",
    "\n",
    "\n",
    "    def extract_incomplete_words(self, sequence_batch):\n",
    "        result = []\n",
    "        for i in range(sequence_batch.shape[0]):\n",
    "            seq = sequence_batch[i]\n",
    "            pos = (seq == UNK_ID).nonzero(as_tuple=True)[0][0].item()\n",
    "\n",
    "            init = (seq[:pos] == EOW_ID).nonzero(as_tuple=True)[0]\n",
    "            end = (seq[pos + 1:] == EOW_ID).nonzero(as_tuple=True)[0]\n",
    "            if len(init) > 0:\n",
    "                init = init[-1].item() + 1\n",
    "            else:\n",
    "                init = 1\n",
    "\n",
    "            if len(end) > 0:\n",
    "                end = end[0].item() + pos + 1\n",
    "            else:\n",
    "                end = (seq[pos + 1:] == PAD_ID).nonzero(as_tuple=True)[0]\n",
    "                if len(end) > 0:\n",
    "                    end = end[0].item() + pos + 1\n",
    "                else:\n",
    "                    end = seq.shape[0]\n",
    "            word = seq[init:end]\n",
    "            result.append((word, init))\n",
    "        return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "-B0c4ITBqlNr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p \u001b[38;5;241m=\u001b[39m Pipeline(\u001b[43mtim\u001b[49m\u001b[38;5;241m.\u001b[39m_get_trained_model(brnn_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]), [tim\u001b[38;5;241m.\u001b[39mtrain_data_loader, tim\u001b[38;5;241m.\u001b[39mtest_data_loader], brnn_args, our_vocabulary, topk_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mcreate_words_dataset()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tim' is not defined"
     ]
    }
   ],
   "source": [
    "p = Pipeline(tim._get_trained_model(brnn_args[\"saved_path\"]), [tim.train_data_loader, tim.test_data_loader], brnn_args, our_vocabulary, topk_values=[5,10,15,20])\n",
    "p.create_words_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfd_sJpKGbqD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Various Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSLjWESoJLM5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size):#, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        #if not cognate_only:\n",
    "        self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        #else:\n",
    "        #    lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "        #    self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "class LoadTrainedModel:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "        self.dataset = BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"])\n",
    "        trie = Trie(args[\"known_lang\"])\n",
    "        self.model = DecipherModelWithFlow(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "\n",
    "        self.model = self._get_trained_model(args['saved_path'])\n",
    "\n",
    "    def _get_trained_model(self, saved_path):\n",
    "\n",
    "        ckpt = torch.load(saved_path, weights_only=False)\n",
    "\n",
    "        def try_load(name):\n",
    "            src = ckpt[name]\n",
    "            if name == \"tracker\": logging.critical(src)\n",
    "            dest = getattr(self, name)\n",
    "            try:\n",
    "                dest.load_state_dict(src)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(e)\n",
    "\n",
    "        try_load('model')\n",
    "\n",
    "        log_pp(self.model)\n",
    "\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            self.model.cuda()\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        logging.critical(f\"Model is on device: {device}\")\n",
    "        trie_weight_device = self.model.trie._weight.device\n",
    "        logging.critical(f\"model.trie.weight is on device: {trie_weight_device}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    @property\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    @property\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "EXP_NAME = \"full_data_residual_old\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_final.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "luo_args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 128, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if luo_args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(luo_args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = luo_args[\"gpu\"]\n",
    "\n",
    "# we already used a seed\n",
    "#if not args[\"random\"]:\n",
    "#    random.seed(luo_args[\"seed\"])\n",
    "#    np.random.seed(luo_args[\"seed\"])\n",
    "#    torch.manual_seed(luo_args[\"seed\"])\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "#CAMBIAMENTI FATTI IN: MAGIC_TENSOR\n",
    "luo_args[\"saved_path\"] = SAVED_PATH\n",
    "ltm = LoadTrainedModel(luo_args)\n",
    "model = ltm.get_model\n",
    "dataset = ltm.get_dataset\n",
    "\n",
    "logging.critical(model)\n",
    "\n",
    "translations = {}\n",
    "for batch in dataset:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    # Magic tensor to the rescue!\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds = almt.get_best()\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        translations[k.form] = v.form\n",
    "        #logging.critical(f\"{k.form}\\t{v.form}\")\n",
    "'''\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXdqO-mMEyg6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear_vocabs()\n",
    "#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\n",
    "#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "dataset = man.train_data_loader.entire_batch#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\n",
    "translations = {}\n",
    "\n",
    "model_ret = man.model(dataset, mode=\"mle\", num_cognates=1911, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "#logging.critical(preds)\n",
    "for k, v in preds.items():\n",
    "    translations[k.form] = v.form\n",
    "    #logging.critical(f\"{k.form}\\t{v.form}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = man.train_data_loader.entire_batch\n",
    "forms = dataset.lost.forms\n",
    "fasttext_embs = [get_tensor(embedding_model.get_vector(form.replace(\"-\", \"\")), dtype=\"f\") for form in forms]\n",
    "fasttext_embs = torch.stack(fasttext_embs)\n",
    "logging.critical(fasttext_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk2jzQZWQ0Fl"
   },
   "outputs": [],
   "source": [
    "args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "\n",
    "# Step 3: Force garbage collection and free GPU memory\n",
    "gc.collect()                      # Collect CPU memory\n",
    "torch.cuda.empty_cache()         # Release unused GPU memory\n",
    "torch.cuda.ipc_collect()         # Collect interprocess memory (if needed)\n",
    "\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "translations = {}\n",
    "\n",
    "#logging.critical(batch)\n",
    "model_ret = model(data_loader.entire_batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "preds = model_ret.valid_log_probs.get_best()\n",
    "for k, v in preds.items():\n",
    "    translations[k.form] = v.form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F3WOy0-3qrQ",
    "outputId": "468f72b0-741e-45b5-acbf-4eb1688ea45a"
   },
   "outputs": [],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for word in check_words:\n",
    "    logging.critical(f\"{word}\\t{translations[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASPO_Cd--C5k"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_with_fasttext_original\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 40, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 200, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0'#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnTW5y4p_B3-"
   },
   "outputs": [],
   "source": [
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WTcWGPYF-Fmt",
    "outputId": "4b104308-3be3-4edf-d794-a71cc8d5b692"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "7YX6aGwVfG6x",
    "outputId": "07e34326-033a-4168-fc6e-bb1e3363391a"
   },
   "outputs": [],
   "source": [
    "translations = {}\n",
    "\n",
    "for batch in data_loader:\n",
    "    if len(batch.lost.forms) != args[\"batch_size\"]:\n",
    "        break\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None, fasttext_embs=fasttext_embs)\n",
    "    # Magic tensor to the rescue!\n",
    "    preds = model_ret.valid_log_probs.get_best()\n",
    "    for k, v in preds.items():\n",
    "        translations[k.form] = v.form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy0d5OtYBWCM",
    "outputId": "661d75e4-b637-4eac-b9d7-ca586aaad8f0"
   },
   "outputs": [],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for word in check_words:\n",
    "    logging.critical(f\"{word}\\t{translations[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab5jJORncSi5"
   },
   "outputs": [],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "\n",
    "for batch in data_loader:\n",
    "    logging.critical(len(batch.lost.forms))\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    logging.critical(log_probs.shape)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    logging.critical(predicted_ids.max())\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    reconstructed_dict = {}\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(reconstructed_dict.items()):\n",
    "    logging.critical(f\"{k}, {v}\")\n",
    "    if i > 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiF0WQ1UYbxS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Dataset translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Gk8OC8hrNV9",
    "outputId": "d45a2e90-f1c5-4d6a-93ac-021108104020"
   },
   "outputs": [],
   "source": [
    "#translations directly from the dataset\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def extract_dict_from_tsv(tsv_path):\n",
    "    result = {}\n",
    "    with open(tsv_path, newline='', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "        reader.fieldnames = [field.strip() for field in reader.fieldnames]  # Clean headers\n",
    "        for row in reader:\n",
    "            key = row.get(\"transliterated_linear_b\")\n",
    "            value = row.get(\"greek\")\n",
    "            if key and value:\n",
    "                result[key.strip()] = value.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tsv_file_path = os.path.join(prefix_path, \"cognates_final.cog\")\n",
    "translations = extract_dict_from_tsv(tsv_file_path)\n",
    "\n",
    "# Optional: print a preview\n",
    "for k, v in list(translations.items())[:5]:\n",
    "    logging.debug(f\"{k} -> {v}\")\n",
    "\n",
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\"]\n",
    "for key in check_words:\n",
    "    logging.debug(f\"{key} -> {translations[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Model translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"full_data_80_rounds\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog or cognates_with_invalid.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 80, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 1911,\n",
    "    \"inc\" : 75, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : COG_PATH,\n",
    "    \"char_emb_dim\" : 400, # changed\n",
    "    \"hidden_size\" : 200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0'#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"finetune\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"cognates_with_invalid.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 90, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : SAVED_PATH, #SAVED_PATH,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 13308,\n",
    "    \"inc\" : 200, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ), #(10, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 400,#200, # changed\n",
    "    \"hidden_size\" : 200,#200, # changed\n",
    "    \"num_layers\" : 16, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 400,#400,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.2),# 0.2),\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 1911, # changed\n",
    "    \"momentum\" : 0.9, #0.5,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "\n",
    "@has_properties('lost_lang', 'known_lang')\n",
    "class BatchedLinearBDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, lost_lang, known_lang, batch_size, cognate_only=False):\n",
    "        self.datasets = dict()\n",
    "        # this means we retrieve only words of the lost language\n",
    "        if not cognate_only:\n",
    "            self.datasets[self.lost_lang] = VocabDataset(lost_lang)\n",
    "        # otherwise, we retireve word in known language and also corresponding cognates in the lost language\n",
    "        else:\n",
    "            lost_words = get_vocab(lost_lang).cognate_to(known_lang)\n",
    "            self.datasets[self.lost_lang] = WordlistDataset(lost_words, lost_lang)\n",
    "        self.datasets[self.known_lang] = VocabDataset(known_lang)\n",
    "\n",
    "        if batch_size:\n",
    "            shuffle = True\n",
    "        else:\n",
    "            batch_size = len(self.datasets[self.known_lang])\n",
    "            shuffle = False\n",
    "\n",
    "        super().__init__(self.datasets[self.lost_lang], batch_size=batch_size,\n",
    "                         shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for lost_batch in super().__iter__():\n",
    "            known_batch = self.datasets[self.known_lang].entire_batch\n",
    "            num_samples = len(lost_batch.words)\n",
    "            yield Map(lost=lost_batch, known=known_batch, num_samples=num_samples)\n",
    "\n",
    "    @property\n",
    "    @cache(persist=True)\n",
    "    def entire_batch(self):\n",
    "        \"\"\"Return the entire dataset as a batch. This shold have a persistent order among the words.\"\"\"\n",
    "        return Map(known=self.datasets[self.known_lang].entire_batch, lost=self.datasets[self.lost_lang].entire_batch)\n",
    "\n",
    "    def size(self, lang):\n",
    "        return len(self.datasets[lang])\n",
    "\n",
    "    def stats(self, name):\n",
    "        row1 = [self.lost_lang, len(self.datasets[self.lost_lang])]\n",
    "        row2 = [self.known_lang, len(self.datasets[self.known_lang])]\n",
    "        table = _prepare_stats(name, row1, row2)\n",
    "\n",
    "        return table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#clear_vocabs()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_data_loader\u001b[38;5;241m.\u001b[39mentire_batch\u001b[38;5;66;03m#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m reconstructed_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "#clear_vocabs()\n",
    "#args[\"cog_path\"] = os.path.join(prefix_path, \"cognates_final_old.cog\")\n",
    "#build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "dataset = man.train_data_loader.entire_batch#BatchedLinearBDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]).entire_batch\n",
    "translations = {}\n",
    "reconstructed_dict = {}\n",
    "model_ret = man.model(dataset, mode=\"mle\", num_cognates=1911, edit=False, capacity=None)\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "#logging.critical(preds)\n",
    "for k, v in preds.items():\n",
    "    if k.form not in translations:\n",
    "        if type(v) == list:\n",
    "            translations[k.form] = \"|\".join([word.form for word in v])\n",
    "        else:\n",
    "            translations[k.form] = v.form    #logging.critical(f\"{k.form}\\t{v.form}\")\n",
    "\n",
    "log_probs = model_ret.log_probs\n",
    "log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "\n",
    "# Build dictionary mapping from original form to predicted sequence\n",
    "for i, seq in enumerate(predicted_ids):\n",
    "    decoded = \"\".join([charset[idx] for idx in seq])\n",
    "    key = dataset.lost.forms[i]\n",
    "    reconstructed_dict[key] = decoded\n",
    "\n",
    "logging.debug(f\"Having {len(translations)} translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import math\n",
    "\n",
    "reconstructed_dict = {}\n",
    "reconstruction_confidences = {}\n",
    "\n",
    "log_probs = model_ret.log_probs\n",
    "log_probs = log_probs.permute(2, 0, 1)  # shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "\n",
    "# Compute exp(max log_prob) per character in the prediction\n",
    "max_log_probs = log_probs.gather(-1, predicted_ids.unsqueeze(-1)).squeeze(-1)  # shape: (batch_size, seq_len)\n",
    "exp_max_log_probs = max_log_probs.exp()  # shape: (batch_size, seq_len)\n",
    "\n",
    "for i, (seq, prob_seq) in enumerate(zip(predicted_ids, exp_max_log_probs)):\n",
    "    decoded = \"\".join([charset[idx] for idx in seq])\n",
    "    key = dataset.lost.forms[i]\n",
    "\n",
    "    reconstructed_dict[key] = decoded\n",
    "    reconstruction_confidences[key] = prob_seq.tolist()  # list of float probabilities for each char\n",
    "    reconstruction_confidences[key] = [str(i)[:5] for i in reconstruction_confidences[key]]\n",
    "logging.debug(f\"Having {len(reconstructed_dict)} reconstructions with confidences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in list(translations.keys())[:100]:\n",
    "    logging.debug(f\"{k}, {translations[k]}, {reconstructed_dict[k]}, {reconstruction_confidences[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_ret\u001b[49m\u001b[38;5;241m.\u001b[39malmt_distr[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ret' is not defined"
     ]
    }
   ],
   "source": [
    "model_ret.almt_distr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage value of top-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prediction across B and L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topk_vals, topk_idxs  \u001b[38;5;66;03m# Optional: return for further use\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(analyze_topk(\u001b[43mmodel_ret\u001b[49m\u001b[38;5;241m.\u001b[39mlog_probs, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ret' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "\n",
    "def analyze_topk(log_probs: torch.Tensor, top_k: int = 2):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape (L, C, B), reshape to (B, L, C), then:\n",
    "    - compute top-k values for each (B, L) along C\n",
    "    - print the average score for the last k classes across B and L\n",
    "    \"\"\"\n",
    "    # Step 1: Reshape (L, C, B) -> (B, L, C)\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # new shape: (B, L, C)\n",
    "\n",
    "    # Step 2: Get top-k values and indices along C\n",
    "    topk_vals, topk_idxs = torch.topk(log_probs, top_k, dim=2)\n",
    "\n",
    "    # Step 3: Get average values for each of the top-k ranks\n",
    "    for i in range(top_k):\n",
    "        avg_val = math.exp(topk_vals[:, :, i].mean().item())\n",
    "        logging.error(f\"Average value of top-{i+1} prediction across B and L: {avg_val:.4f}\")\n",
    "\n",
    "    return topk_vals, topk_idxs  # Optional: return for further use\n",
    "\n",
    "logging.debug(analyze_topk(model_ret.log_probs, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save only what you want\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Save model before cleanup\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 2: Clear vocabs and rebuild\u001b[39;00m\n\u001b[1;32m      7\u001b[0m clear_vocabs()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "translations = {}\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "#model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "'''\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    #analyze_topk(log_probs, 5)\n",
    "    # PyTorch style\n",
    "    ret = trie.analyze(log_probs, model_ret.almt_distr, batch.known.words, batch.lost.lengths)\n",
    "    ret.log_probs = log_probs\n",
    "    ret.valid_log_probs = MagicTensor(ret.valid_log_probs, batch.lost.words, batch.known.words)\n",
    "\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    preds = ret.valid_log_probs.get_best()\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "        translations[key] = preds[batch.lost.words[i]].form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a-mi-ni-so'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreconstructed_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma-mi-ni-so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'a-mi-ni-so'"
     ]
    }
   ],
   "source": [
    "reconstructed_dict[\"a-mi-ni-so\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'man' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m translations \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save only what you want\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mman\u001b[49m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Save model before cleanup\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Clear vocabs and rebuild\u001b[39;00m\n\u001b[1;32m      8\u001b[0m clear_vocabs()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'man' is not defined"
     ]
    }
   ],
   "source": [
    "charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS  # Greek character list (length = 1911)\n",
    "reconstructed_dict = {}\n",
    "translations = {}\n",
    "\n",
    "# Save only what you want\n",
    "model = man.model  # Save model before cleanup\n",
    "# Step 2: Clear vocabs and rebuild\n",
    "clear_vocabs()\n",
    "build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "\n",
    "model.trie = trie\n",
    "# Step 4: Recreate DataLoader cleanly\n",
    "data_loader = BatchedLinearBDataLoader(\n",
    "    args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"]\n",
    ")\n",
    "'''\n",
    "                known = batch.known.lang\n",
    "                known_forms = batch.known.forms\n",
    "                known_charset = get_charset(known)\n",
    "                expected_edits = compute_expected_edits(\n",
    "                    known_charset, ret.log_probs, known_forms, ret.valid_log_probs, edit=edit)\n",
    "\n",
    "                flow, cost = min_cost_flow(expected_edits.cpu().numpy(), num_cognates,\n",
    "                                           capacity=capacity, n_similar=self.n_similar)\n",
    "                flow = MagicTensor(get_tensor(flow), batch.lost.words, batch.known.words)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "all_log_probs = []\n",
    "all_valid_log_probs = []\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "    \n",
    "    log_probs = model_ret.log_probs  # shape: (batch_size, seq_len, vocab_size)\n",
    "    all_log_probs.append(log_probs.cpu())\n",
    "    all_valid_log_probs.append(model_ret.valid_log_probs.tensor.cpu())\n",
    "    # PyTorch style\n",
    "    log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "    predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "    # Build dictionary mapping from original form to predicted sequence\n",
    "    for i, seq in enumerate(predicted_ids):\n",
    "        decoded = \"\".join([charset[idx] for idx in seq])\n",
    "        key = batch.lost.forms[i]\n",
    "        reconstructed_dict[key] = decoded\n",
    "\n",
    "known = batch.known.lang\n",
    "known_forms = batch.known.forms\n",
    "known_charset = get_charset(known)\n",
    "log_probs = torch.cat(all_log_probs, dim=2).cpu()  # now [tl  nc  sum(bs_i)]\n",
    "valid_log_probs = torch.cat(all_valid_log_probs, dim=0).cpu()\n",
    "\n",
    "expected_edits = compute_expected_edits(known_charset, log_probs, known_forms, valid_log_probs, edit=True) # also edit=False should be ok\n",
    "flow, cost = min_cost_flow(expected_edits.detach().cpu().numpy(), 13308, capacity=3, n_similar=10)\n",
    "flow = MagicTensor(get_tensor(flow), data_loader.entire_batch.lost.words, data_loader.entire_batch.known.words)\n",
    "preds = flow.get_best()\n",
    "for k, v in preds.items():\n",
    "    if k.form not in translations:\n",
    "        if type(v) == list:\n",
    "            translations[k.form] = \"|\".join([word.form for word in v])\n",
    "        else:\n",
    "            translations[k.form] = v.form\n",
    "        translations[k.form] += f\"|{reconstructed_dict[k.form].replace('<EOW>', '')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3805948/783888086.py:9: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Initialized with lang={lang}, total words={len(words)}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:36 - 0:06:56 at 783888086.py:9 - [Trie] Initialized with lang=greek, total words=12070\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:40: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Weight built: shape={self._weight.shape}, nnz={self._weight._nnz()}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:37 - 0:06:57 at 783888086.py:40 - [Trie] Weight built: shape=torch.Size([12070, 627]), nnz=97507\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:74: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Sampled submatrix shape={weight.shape}, nnz={weight._nnz()}, max_len={self._eff_max_length}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:38 - 0:06:58 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "/tmp/ipykernel_3805948/783888086.py:87: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(f\"[Trie] Analyzing with log_probs shape={log_probs.shape}, eff_weight shape={self._eff_weight.shape}\")\n",
      "\u001b[33mWARNING - 06/29/25 19:27:38 - 0:06:58 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:39 - 0:06:59 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:39 - 0:06:59 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:40 - 0:07:00 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:40 - 0:07:00 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:41 - 0:07:01 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:41 - 0:07:01 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:42 - 0:07:02 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:42 - 0:07:02 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:43 - 0:07:03 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:43 - 0:07:03 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1911]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:44 - 0:07:04 at 783888086.py:74 - [Trie] Sampled submatrix shape=torch.Size([12070, 627]), nnz=97507, max_len=19\u001b[0m\n",
      "\u001b[33mWARNING - 06/29/25 19:27:44 - 0:07:04 at 783888086.py:87 - [Trie] Analyzing with log_probs shape=torch.Size([19, 33, 1842]), eff_weight shape=torch.Size([12070, 627])\u001b[0m\n",
      "\u001b[37mDEBUG - 06/29/25 19:27:44 - 0:07:04 at 207726662.py:19 - Having 13308 translations\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = man.model\n",
    "trie = Trie(args[\"known_lang\"])\n",
    "model.trie = trie\n",
    "\n",
    "for batch in data_loader:\n",
    "    model_ret = model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds = almt.get_best()\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        if k.form not in translations:\n",
    "            if type(v) == list:\n",
    "                translations[k.form] = \"|\".join([word.form for word in v])\n",
    "            else:\n",
    "                translations[k.form] = v.form\n",
    "            translations[k.form] += f\"|{reconstructed_dict[k.form].replace('<EOW>', '')}\"\n",
    "    \n",
    "logging.debug(f\"Having {len(translations)} translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[\"ke-se-da-o-ne\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntranslations = {}\\nscores = {}\\nfor batch in data_loader:\\n    model_ret = man.model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\\n\\n    almt = model_ret.valid_log_probs\\n    preds,score = almt.get_best(topk=100, return_scores=True)\\n    #logging.critical(preds)\\n    for k, v in preds.items():\\n        if k.form not in translations:\\n                translations[k.form] = \"|\".join([word.form for word in v])\\n                scores[k.form] = score[k]\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "translations = {}\n",
    "scores = {}\n",
    "for batch in data_loader:\n",
    "    model_ret = man.model(batch, mode=\"mle\", num_cognates=13308, edit=False, capacity=None)\n",
    "\n",
    "    almt = model_ret.valid_log_probs\n",
    "    preds,score = almt.get_best(topk=100, return_scores=True)\n",
    "    #logging.critical(preds)\n",
    "    for k, v in preds.items():\n",
    "        if k.form not in translations:\n",
    "                translations[k.form] = \"|\".join([word.form for word in v])\n",
    "                scores[k.form] = score[k]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>', '')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"ke-se-da-o-ne\"\n",
    "reconstructed_dict[w], translations[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trie._word2rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - a-mi-ni-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - wa-na-ka\tf\tf<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ko-no-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - to-so\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ke-se-da-o-ne\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - ka-ra-ja\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - di-pi-zo\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - o-wi-to-wo\t\tf<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n",
      "\u001b[37mDEBUG - 07/09/25 17:57:48 - 0:12:09 at 3490677880.py:3 - pa-ka-ma\t\t<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_words = [\"a-mi-ni-so\", \"wa-na-ka\", \"ko-no-so\", \"to-so\", \"ke-se-da-o-ne\", \"ka-ra-ja\", \"di-pi-zo\", \"o-wi-to-wo\", \"pa-ka-ma\"] # last is invalid\n",
    "for word in check_words:\n",
    "    logging.debug(f\"{word}\\t{translations[word]}\\t{reconstructed_dict[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW><EOW>',\n",
       " '')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_dict[\"ka-ma-to\"], translations[\"ka-ma-to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2mD8AD1X89w",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Auxiliary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBEOQMEWYCom",
    "outputId": "e259eec7-72b7-40d9-fe89-1051108e3556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m TASKS:\n\u001b[1;32m      3\u001b[0m   classifier \u001b[38;5;241m=\u001b[39m train_classifier(task, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m   words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mtranslations\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      5\u001b[0m   labels \u001b[38;5;241m=\u001b[39m use_classifier(classifier, words)\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translations' is not defined"
     ]
    }
   ],
   "source": [
    "classifications =  defaultdict(list)\n",
    "for task in TASKS:\n",
    "  classifier = train_classifier(task, max_iter=2000)\n",
    "  words = list(translations.keys())\n",
    "  labels = use_classifier(classifier, words)\n",
    "  for word in labels.keys():\n",
    "    classifications[word].append(labels[word])\n",
    "\n",
    "logging.debug(classifications[\"wa-na-ka-te\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYEuAhe5ahUG",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Sequence Transliterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "NOE2ZVniK6RH"
   },
   "outputs": [],
   "source": [
    "@has_properties('dataset', 'translations', 'vocabulary')\n",
    "class SequenceTransliterator:\n",
    "    def __init__(self, dataset, translations, vocabulary, classifications):\n",
    "        self.linb_versions = defaultdict(dict)\n",
    "\n",
    "        for s in dataset:\n",
    "            seq = s.missing.split(\" \")\n",
    "            unk = s.sequence.unknown\n",
    "\n",
    "            for word_idx in unk:\n",
    "                if len(unk[word_idx]) > 0:\n",
    "                    seq[word_idx] = \"BRUTE_ME\"\n",
    "            #logging.critical(f\"{s}, {s.sequence.idx}\")\n",
    "            self.linb_versions[s.sequence.idx][\"word_list\"] = seq\n",
    "            self.linb_versions[s.sequence.idx][\"classifications\"] = []\n",
    "            self.linb_versions[s.sequence.idx][\"brute_classifications\"] = []\n",
    "            for word in self.linb_versions[s.sequence.idx][\"word_list\"]:\n",
    "                if word == \"BRUTE_ME\":\n",
    "                    self.linb_versions[s.sequence.idx][\"classifications\"].append([])\n",
    "                else:\n",
    "                    self.linb_versions[s.sequence.idx][\"classifications\"].append(classifications[word])\n",
    "\n",
    "\n",
    "        input_file = os.path.join(prefix_path, \"linb_words.tsv\")\n",
    "        inverted = {}\n",
    "        with open(input_file, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    logging.debug(f\"Error with word: {parts[0]}\")\n",
    "                    continue\n",
    "                word = parts[0]\n",
    "                id_list = ast.literal_eval(parts[2])  # Convert string \"[1633]\" to list\n",
    "                id_list = list(set([int(seq_id) for (seq_id, prob) in id_list if prob != 1]))\n",
    "                #logging.critical(id_list)\n",
    "                for id_num in id_list:\n",
    "                    if word not in set(self.linb_versions[id_num][\"word_list\"]):\n",
    "                        if \"brute\" in self.linb_versions[id_num]:\n",
    "                            self.linb_versions[id_num][\"brute\"].append(word)\n",
    "                        else:\n",
    "                            self.linb_versions[id_num][\"brute\"] = [word]\n",
    "                        self.linb_versions[id_num][\"brute_classifications\"].append(classifications[word])\n",
    "                    #else:\n",
    "                    #    print(word, \"PORCODIO\" ,dataset[id_num].sequence.missing_form) VERIFY THIS\n",
    "        self.produce_greek_versions()\n",
    "\n",
    "    def produce_greek_versions(self):\n",
    "        self.greek_versions = defaultdict(dict)\n",
    "        for seq_id, dic in self.linb_versions.items():\n",
    "            self.greek_versions[seq_id][\"word_list\"] = []\n",
    "            self.greek_versions[seq_id][\"full_word_list\"] = []\n",
    "            self.greek_versions[seq_id][\"brute\"] = []\n",
    "            self.greek_versions[seq_id][\"solutions\"] = []\n",
    "            sequence = self.dataset[seq_id].sequence\n",
    "            unknown = sequence.unknown\n",
    "            for word_idx in unknown:\n",
    "                if len(unknown[word_idx]) > 0:\n",
    "                    unknown_word = sequence.form.split(\" \")[word_idx]\n",
    "                    self.greek_versions[seq_id][\"solutions\"].append(self.lb2greek(unknown_word))\n",
    "            count = 0\n",
    "            for word in dic[\"word_list\"]:\n",
    "                if word == \"BRUTE_ME\":\n",
    "                    self.greek_versions[seq_id][\"word_list\"].append(word)\n",
    "                    self.greek_versions[seq_id][\"full_word_list\"].append(self.greek_versions[seq_id][\"solutions\"][count])\n",
    "                    count += 1\n",
    "                else:\n",
    "                    self.greek_versions[seq_id][\"word_list\"].append(self.lb2greek(word))\n",
    "                    self.greek_versions[seq_id][\"full_word_list\"].append(self.lb2greek(word))\n",
    "            if \"brute\" in dic:\n",
    "                for word in dic[\"brute\"]:\n",
    "                        self.greek_versions[seq_id][\"brute\"].append(self.lb2greek(word))\n",
    "            #logging.critical(f\"{dic}, {self.greek_versions[seq_id]}\" )\n",
    "\n",
    "    def lb2greek(self, word):\n",
    "        if word in self.translations:\n",
    "            return self.translations[word]\n",
    "        else:\n",
    "            return self.vocabulary.logogram_vocab.get(word, word)\n",
    "            #return self.vocabulary.logogram_vocab[word]\n",
    "\n",
    "#HAve to import Cognate matching imports and phonetic embeddings\n",
    "phonetic_embeddings = recover_phonetic_embeddings(loaded_embeddings, missing)\n",
    "seq_dataset = SequenceDataset(sequences, missing, unknown, phonetic_embeddings, idxs, our_vocabulary)\n",
    "st = SequenceTransliterator(seq_dataset, translations, our_vocabulary, classifications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBR9NEuhv3Jw",
    "outputId": "559cb38d-a2d8-478c-ffdb-86ae5ffc6e18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'word_list': ['BRUTE_ME', 'CROC', 'N', '1'],\n",
       "  'classifications': [[], [], [], []],\n",
       "  'brute_classifications': [[], [], [], [], []],\n",
       "  'brute': ['e-a-na-wo',\n",
       "   'e-ko-na-wo',\n",
       "   'e-po-na-wo',\n",
       "   'e-re-na-wo',\n",
       "   'e-ri-na-wo']},\n",
       " {'word_list': ['BRUTE_ME', '', 'N', '1'],\n",
       "  'full_word_list': ['f', '', 'N', '1'],\n",
       "  'brute': ['e-a-na-wo',\n",
       "   'e-ko-na-wo',\n",
       "   'e-po-na-wo',\n",
       "   'e-re-na-wo',\n",
       "   'e-ri-na-wo'],\n",
       "  'solutions': ['f']},\n",
       " {'sequence': Sequence idx=13 form='e-u-na-wo CROC N 1' missing_form='e-?-na-wo CROC N 1',\n",
       "  'x': [1, 6, 3, 30, 71, 2, 194, 2, 233, 2, 167],\n",
       "  'y': [6, 9, 30, 71, 2, 194, 2, 233, 2, 167, 4],\n",
       "  'lang': 'transliterated_linear_b',\n",
       "  'form': 'e-u-na-wo CROC N 1',\n",
       "  'missing': 'e-?-na-wo CROC N 1',\n",
       "  'phonetic': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [-0.52298063,  0.56957954, -0.63964498, ...,  0.41888806,\n",
       "          -0.5710628 , -0.21507236],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]])},\n",
       " 'e-u-na-wo')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 13\n",
    "for missing_word_idx in seq_dataset[n].sequence.unknown:\n",
    "    if len(seq_dataset[n].sequence.unknown[missing_word_idx]) > 0:\n",
    "        break\n",
    "st.linb_versions[n], st.greek_versions[n], seq_dataset[n], seq_dataset[n].sequence.form.split(\" \")[missing_word_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pfmoWZU1xSB4",
    "outputId": "82ece903-d2fc-460a-ad63-0be3a87c8e84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRUTE_ME  N 1'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(st.greek_versions[n][\"word_list\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW1q-mW1dp7V",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Gemini initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Djc5XL_Wea6N",
    "outputId": "7b84f2d6-6bfb-4259-fb5c-61626cb44e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dotenv in ./.local/lib/python3.10/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in ./.local/lib/python3.10/site-packages (from dotenv) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google.generativeai in ./.local/lib/python3.10/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in ./.local/lib/python3.10/site-packages (from google.generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.172.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./.local/lib/python3.10/site-packages (from google.generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.10/site-packages (from google.generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in ./.local/lib/python3.10/site-packages (from google.generativeai) (1.10.22)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.local/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./.local/lib/python3.10/site-packages (from google-api-core->google.generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.10/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2020.6.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./.local/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google.generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client->google.generativeai) (0.20.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.local/lib/python3.10/site-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.local/lib/python3.10/site-packages (from google-api-python-client->google.generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (2.4.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv\n",
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "alb_8Nriduu9"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "1g9HFh3yd9o0"
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(prefix_path,\".env\"))\n",
    "n_keys = 13\n",
    "# Retrieve the API key\n",
    "api_keys = []\n",
    "for i in range(1, n_keys+1):\n",
    "    api_keys.append(os.getenv(f\"GOOGLE_API_KEY_{i}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Gk7GsjCPYq16",
    "outputId": "82bd42b5-951d-45e0-c701-d97dc6a8d28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat session started. Type 'exit' or 'quit' to end the conversation.\n",
      "-------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Exiting chat session. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "# Corrected import for GenerationConfig\n",
    "from google.generativeai.types import GenerationConfig\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace 'YOUR_API_KEY' with your actual Gemini API key.\n",
    "# For security, consider loading this from an environment variable.\n",
    "# Example: api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY_10\") # Placeholder: Please replace with your actual API key\n",
    "\n",
    "# Configure the genai library with your API key\n",
    "try:\n",
    "    genai.configure(api_key=api_key)\n",
    "    # The 'client' object is no longer directly used for chat creation in this way.\n",
    "    # We will directly initialize the model.\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Generative AI: {e}\")\n",
    "    print(\"Please ensure your API key is correct and you have network access.\")\n",
    "    exit()\n",
    "\n",
    "# --- Chat Initialization ---\n",
    "try:\n",
    "    # Initialize the GenerativeModel directly\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\", # Specify the model to use\n",
    "        generation_config=GenerationConfig(temperature=0.6) # Configure generation parameters\n",
    "    )\n",
    "\n",
    "    # Start a chat session with initial history\n",
    "    # The history format is a list of dictionaries with 'role' and 'parts'\n",
    "    chat = model.start_chat(\n",
    "        history=[{'role': 'user', 'parts': [{'text': 'Learn these Linear B to ancient greek correspondances from my json object: '}]},\n",
    "                 {'role': 'model', 'parts': [{'text': 'Okay, I got it.'}]} # Add a model response to establish the system instruction\n",
    "                ]\n",
    "    )\n",
    "    print(\"Chat session started. Type 'exit' or 'quit' to end the conversation.\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating chat session: {e}\")\n",
    "    print(\"Please check your model name and configuration.\")\n",
    "    exit()\n",
    "\n",
    "# --- Interactive Loop ---\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \") # Prompt the user for input\n",
    "\n",
    "        # Check for exit commands\n",
    "        if user_input.lower().strip() in {\"exit\", \"quit\"}:\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            print(\"Exiting chat session. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Send the user's message to the Gemini model\n",
    "        # The SDK automatically appends this message to the chat history\n",
    "        # and sends the full history with each subsequent call to maintain state.\n",
    "        response = chat.send_message(user_input)\n",
    "        #logging.critical(chat.history)\n",
    "        # Print the model's response\n",
    "        if response.text:\n",
    "            print(\"Gemini:\", response.text)\n",
    "        else:\n",
    "            print(\"Gemini: (No response text received)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the chat: {e}\")\n",
    "        print(\"Please try again or type 'exit' to quit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "V4vms_JcfFAu",
    "outputId": "81a071dc-f258-4303-a7ef-36b289043837",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;66;03m# Make sure json is imported if it's not already\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m linear_b_correspondances \u001b[38;5;241m=\u001b[39m \u001b[43mtranslations\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m linear_b_correspondances\u001b[38;5;241m.\u001b[39mupdate(our_vocabulary\u001b[38;5;241m.\u001b[39mlogogram_vocab)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fix: Use ensure_ascii=False to prevent Unicode escaping\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translations' is not defined"
     ]
    }
   ],
   "source": [
    "import json # Make sure json is imported if it's not already\n",
    "\n",
    "linear_b_correspondances = translations.copy()\n",
    "linear_b_correspondances.update(our_vocabulary.logogram_vocab)\n",
    "# Fix: Use ensure_ascii=False to prevent Unicode escaping\n",
    "str_linear_b_correspondances = json.dumps(linear_b_correspondances, ensure_ascii=False)\n",
    "\n",
    "str_linear_b_correspondances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0WyWPwanHDu",
    "outputId": "9cd64d42-a42d-450b-ca81-5c6836d7d975"
   },
   "outputs": [],
   "source": [
    "\n",
    "syllabograms_matching = {\n",
    "  \"a\": [\"\"],\n",
    "  \"e\": [\"\", \"\"],\n",
    "  \"i\": [\"\"],\n",
    "  \"o\": [\"\", \"\"],\n",
    "  \"u\": [\"\"],\n",
    "  \"da\": [\"\", \"\"],\n",
    "  \"de\": [\"\", \"\", \"\"],\n",
    "  \"di\": [\"\", \"\"],\n",
    "  \"do\": [\"\", \"\", \"\"],\n",
    "  \"du\": [\"\", \"\"],\n",
    "  \"dwe\": [\"\", \"f\", \"\"],\n",
    "  \"dwo\": [\"\", \"f\", \"\"],\n",
    "  \"ja\": [\"\", \"\"],\n",
    "  \"je\": [\"\", \"\", \"\"],\n",
    "  \"jo\": [\"\", \"\", \"\"],\n",
    "  \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ma\": [\"\", \"\", \"\"],\n",
    "  \"me\": [\"\", \"\", \"\"],\n",
    "  \"mi\": [\"\", \"\"],\n",
    "  \"mo\": [\"\", \"\"],\n",
    "  \"mu\": [\"\", \"\"],\n",
    "  \"na\": [\"\", \"\"],\n",
    "  \"ne\": [\"\", \"\", \"\"],\n",
    "  \"ni\": [\"\", \"\"],\n",
    "  \"no\": [\"\", \"\", \"\"],\n",
    "  \"nu\": [\"\", \"\"],\n",
    "  \"nwa\": [\"f\", \"\", \"\"],\n",
    "  \"pa\": [\"\", \"\", \"\"],\n",
    "  \"pe\": [\"\", \"\", \"\"],\n",
    "  \"pi\": [\"\", \"\", \"\"],\n",
    "  \"po\": [\"\", \"\", \"\"],\n",
    "  \"pu\": [\"\", \"\", \"\"],\n",
    "  \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "  \"phu\": [\"\", \"\"],\n",
    "  \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "  \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "  \"re\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ri\": [\"\", \"\", \"\"],\n",
    "  \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ru\": [\"\", \"\", \"\"],\n",
    "  \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "  \"rai\": [\"\", \"\", \"\"],\n",
    "  \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "  \"sa\": [\"\", \"\"],\n",
    "  \"se\": [\"\", \"\", \"\"],\n",
    "  \"si\": [\"\", \"\"],\n",
    "  \"so\": [\"\", \"\", \"\"],\n",
    "  \"su\": [\"\", \"\"],\n",
    "  \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "  \"te\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ti\": [\"\", \"\", \"\"],\n",
    "  \"to\": [\"\", \"\", \"\", \"\"],\n",
    "  \"tu\": [\"\", \"\", \"\"],\n",
    "  \"tya\": [\"\", \"\", \"\"],\n",
    "  \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "  \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "  \"wa\": [\"\", \"f\", \"\"],\n",
    "  \"we\": [\"\", \"f\", \"\"],\n",
    "  \"wi\": [\"\", \"f\"],\n",
    "  \"wo\": [\"\", \"f\", \"\"],\n",
    "  \"za\": [\"\", \"\", \"\", \"\"],\n",
    "  \"ze\": [\"\", \"\", \"\"],\n",
    "  \"zo\": [\"\", \"\", \"\"],\n",
    "  \"ha\": [\"\", \"h\"],\n",
    "  \"ai\": [\"\", \"\"],\n",
    "  \"au\": [\"\", \"\"],\n",
    "  \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "  \"*64\": [\"\", \"\", \"f\"],\n",
    "  \"*65\": [\"\", \"\"],\n",
    "  \"*79\": [\"\", \"\"],\n",
    "  \"*82\": [\"\", \"\", \"f\"]\n",
    "}\n",
    "str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "len(str_syllabogram_matching), str_syllabogram_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvq9k3Rj1Z0U",
    "outputId": "d751c339-34c6-436d-ed5b-d5106af499a7"
   },
   "outputs": [],
   "source": [
    "suffixes = 'The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -) -> AMBIGUOUS\\n\\\n",
    "3. -de: can be either:\\n\\\n",
    "   - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "   - Allative/demonstrative suffix (equivalent to Greek -) -> AMBIGUOUS\\n\\\n",
    "4. -pi: instrumental/locative suffix'\n",
    "#Prefixes\n",
    "prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "3. we- : f- numerical prefix meaning \"six\" -> AMBIGUOUS\\n\\\n",
    "4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "6. jo- :  - relative/comparative prefix meaning \"as, like, how\"  -> AMBIGUOUS\\n'\n",
    "prefixes_and_suffixes = prefixes + suffixes\n",
    "print(prefixes_and_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1U_i9xj2WDv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Prompt for infilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Your problematic JSON string\n",
    "json_string = '''{\n",
    "    \"chosen_sequence\": \"Option 4\",\n",
    "    \"reasoning\": \"The Linear B sequence describes various items, including a 'rose-colored table' (`wo-de-wi-jo to-pe-za`) and several instances of `o-u-ki-te-mi` and `o-u-te-mi` (likely proper nouns or specific items). The final part of the sequence is `BRUTE_ME i-ku-wo-i-pi`. The word `i-ku-wo-i-pi` (Ancient Greek: ff) is classified as a common noun, thematic in -o, and notably ends with the instrumental/locative suffix `-pi`, indicating 'by means of horses' or 'among horses'.\n",
    "\n",
    "Let's evaluate the options:\n",
    "*   **Option 1 (`e-ki`):** Classified as an anthroponym/animal name/theonym noun. While grammatically possible as another item in a list, its Ancient Greek translation `` (echi) doesn't form a clear semantic connection with 'horses' in an inventory context.\n",
    "*   **Option 2 (`e-pa`):** Classified as a common noun, thematic in -a. Its Ancient Greek translation `` (ephas, 'speech' or 'daybreak') does not semantically fit with 'horses' in a practical inventory.\n",
    "*   **Option 3 (`e-pe`):** Classified as a common noun, thematic in -o. However, its Ancient Greek translation `` (epo) is a verb ('to say', 'to follow'), which directly contradicts its classification as a noun. This makes it grammatically inconsistent with the provided information.\n",
    "*   **Option 4 (`e-pi`):** Classified as a common adverb. Its Ancient Greek translation `` (epi) is a highly common preposition/adverb meaning 'on', 'at', 'upon', 'for', or 'in addition to'. This word perfectly complements the instrumental/locative `i-ku-wo-i-pi`. The phrase ` ff` (epi ikkuwoihi) translates to 'on/for horses' or 'with horses', which makes excellent semantic and grammatical sense in an administrative or inventory context, describing items associated with horses.\n",
    "*   **Option 5 (`e-ra`):** Classified as an anthroponym/animal name/theonym noun, thematic in -a. Its Ancient Greek translation `` (era, 'earth' or 'Hera') does not form a coherent semantic unit with 'horses' in this context.\n",
    "\n",
    "Therefore, `e-pi` () is the most semantically coherent and grammatically consistent choice. It functions as an adverb/preposition that logically connects the preceding items or the general context to the 'horses', which is highly plausible for Linear B administrative texts.\"\n",
    "}'''\n",
    "\n",
    "# Solution 1: Clean the JSON by escaping backticks\n",
    "def clean_json_backticks(json_str):\n",
    "    \"\"\"Escape backticks in JSON string values\"\"\"\n",
    "    # This is a simple approach - replace backticks with escaped backticks\n",
    "    return json_str.replace('`', '\\\\`')\n",
    "\n",
    "# Solution 2: Aggressive JSON cleaning\n",
    "def clean_json_thoroughly(json_str):\n",
    "    \"\"\"Thoroughly clean JSON string for parsing\"\"\"\n",
    "    # Remove or replace control characters\n",
    "    cleaned = ''.join(char for char in json_str if unicodedata.category(char) != 'Cc' or char in '\\n\\r\\t')\n",
    "    \n",
    "    # Remove backticks (code formatting)\n",
    "    cleaned = re.sub(r'`([^`]*)`', r'\\1', cleaned)\n",
    "    \n",
    "    # Remove bold formatting\n",
    "    cleaned = re.sub(r'\\*\\*([^*]*)\\*\\*', r'\\1', cleaned)\n",
    "    \n",
    "    # Remove bullet points\n",
    "    cleaned = re.sub(r'^\\s*\\*\\s+', '', cleaned, flags=re.MULTILINE)\n",
    "    \n",
    "    # Fix common escape sequence issues\n",
    "    cleaned = cleaned.replace('\\\\', '\\\\\\\\')  # Escape backslashes\n",
    "    cleaned = cleaned.replace('\\\\\"', '\"')    # Fix escaped quotes\n",
    "    cleaned = cleaned.replace('\\\\\\\\\"', '\\\\\"') # Fix double-escaped quotes\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Solution 3: Extract JSON from markdown-wrapped content\n",
    "def extract_json_from_markdown(text):\n",
    "    \"\"\"Extract JSON from markdown code blocks\"\"\"\n",
    "    # Look for JSON in markdown code blocks\n",
    "    json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    match = re.search(json_pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return text\n",
    "\n",
    "# Solution 4: Ultra-robust JSON parsing\n",
    "def parse_gemini_json_robust(json_str):\n",
    "    \"\"\"Parse JSON with extensive preprocessing for Gemini responses\"\"\"\n",
    "    \n",
    "    # Strategy 1: Try direct parsing first\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Extract from markdown if present\n",
    "    if '```json' in json_str:\n",
    "        json_str = extract_json_from_markdown(json_str)\n",
    "    \n",
    "    # Strategy 3: Clean thoroughly\n",
    "    try:\n",
    "        cleaned = clean_json_thoroughly(json_str)\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: Character-by-character reconstruction\n",
    "    try:\n",
    "        # Find JSON boundaries\n",
    "        start = json_str.find('{')\n",
    "        end = json_str.rfind('}') + 1\n",
    "        if start >= 0 and end > start:\n",
    "            json_portion = json_str[start:end]\n",
    "            cleaned = clean_json_thoroughly(json_portion)\n",
    "            return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 5: Manual field extraction as fallback\n",
    "    try:\n",
    "        return extract_fields_manually(json_str)\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"All parsing strategies failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_fields_manually(text):\n",
    "    \"\"\"Extract fields manually when JSON parsing fails\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Extract chosen_sequence\n",
    "    match = re.search(r'\"chosen_sequence\":\\s*\"([^\"]*)\"', text)\n",
    "    if match:\n",
    "        result['chosen_sequence'] = match.group(1)\n",
    "    \n",
    "    # Extract reasoning (more complex due to multiline)\n",
    "    match = re.search(r'\"reasoning\":\\s*\"(.*?)\"(?=\\s*})', text, re.DOTALL)\n",
    "    if match:\n",
    "        reasoning = match.group(1)\n",
    "        # Clean up the reasoning text\n",
    "        reasoning = reasoning.replace('\\\\\"', '\"')\n",
    "        reasoning = re.sub(r'`([^`]*)`', r'\\1', reasoning)  # Remove backticks\n",
    "        reasoning = re.sub(r'\\*\\*([^*]*)\\*\\*', r'\\1', reasoning)  # Remove bold\n",
    "        reasoning = re.sub(r'^\\s*\\*\\s+', '', reasoning, flags=re.MULTILINE)  # Remove bullets\n",
    "        result['reasoning'] = reasoning.strip()\n",
    "    \n",
    "    return result if result else None\n",
    "\n",
    "# Test the solutions\n",
    "logging.debug(\"=== Testing Robust Solutions ===\")\n",
    "\n",
    "\n",
    "# Test robust parsing\n",
    "try:\n",
    "    parsed = parse_gemini_json_robust(json_string)\n",
    "    if parsed:\n",
    "        logging.debug(\" Robust parsing worked!\")\n",
    "        logging.debug(f\"Chosen sequence: {parsed['chosen_sequence']}\")\n",
    "        logging.debug(f\"Reasoning preview: {parsed['reasoning']}\")\n",
    "    else:\n",
    "        logging.debug(\" Robust parsing failed\")\n",
    "except Exception as e:\n",
    "    logging.debug(f\" Exception in robust parsing: {e}\")\n",
    "\n",
    "# Simple utility function for your use case\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Quick parsing function for Gemini JSON responses\"\"\"\n",
    "    \n",
    "    # Handle markdown-wrapped JSON\n",
    "    if '```json' in response_text:\n",
    "        response_text = extract_json_from_markdown(response_text)\n",
    "    \n",
    "    # Try robust parsing\n",
    "    result = parse_gemini_json_robust(response_text)\n",
    "    \n",
    "    if result:\n",
    "        return result\n",
    "    else:\n",
    "        logging.debug(\"Failed to parse JSON response\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def make_infill_selection_prompt(linear_b_data, greek_data, api_key):\n",
    "    \"\"\"\n",
    "    This is a prompt for selecting the most semantically coherent Greek translation\n",
    "    of a Linear B sequence with an unknown syllable to infill.\n",
    "\n",
    "    Args:\n",
    "        linear_b_sequence: str - Linear B sequence with unknown syllable marked\n",
    "        greek_translations: list - 5 possible Greek translations\n",
    "        word_classifiers: list - Logic role classifiers for each word in Linear B script\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with chosen sequence, reasoning and index of sequence within the list of translations\n",
    "    \"\"\"\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"## HISTORICAL CONTEXT \\n Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\\n\\\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\"\n",
    "\n",
    "    syll_matching_context = '## LINEAR B SYLLABOGRAMS TO GREEK CHARACTERS MAPPING \\n Consider this mapping between Linear B syllabograms and Ancient Greek\\'s characters.' + str_syllabogram_matching + \"\\n\"\n",
    "    syll_matching_context += 'Be careful with the following aspect: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> '\n",
    "\n",
    "    suffixes = '## SUFFIXES AND PREFIXES INFORMATION \\n The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "    CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "    Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "    1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "    2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); **AMBIGUOUS** \\n\\\n",
    "    3. -de: can be either:\\n\\\n",
    "       - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); **AMBIGUOUS** \\n\\\n",
    "    4. -pi: instrumental/locative suffix'\n",
    "\n",
    "    #Prefixes\n",
    "    prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "    They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "    1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "    2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "    3. we- : f- numerical prefix meaning \"six\"; **AMBIGUOUS** \\n\\\n",
    "    4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "    5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "    6. jo- :  - relative/comparative prefix meaning \"as, like, how\"; **AMBIGUOUS** \\n'\n",
    "    prefixes_and_suffixes = prefixes + suffixes\n",
    "\n",
    "    prompt = \"\"\n",
    "    # Task overview\n",
    "    task_overview = \"## TASK OVERVIEW \\n You are presented with a Linear B sequence containing an unknown word that needs to be infilled.\\n\\\n",
    "    We have generated 5 possible infillings, with corresponding Ancient Greek translations of this sequence, where only the word with the unknown syllable varies between translations.\\n\\\n",
    "    For logograms, rely on their ancient greek translations.\\n\\\n",
    "    BRUTE_ME is the placeholder for the word that you need to fill in.\\n\\\n",
    "    Your task is to select the single translation that makes the most semantic and contextual sense, considering the grammatical roles and meaning coherence of the entire sequence.\\n\\\n",
    "    The input is encoded using markdown.\\n\\n\"\n",
    "    #CRITICAL CONSIDERATION: Linear B is a very old form of greek, with little defined grammar and very ancient words. Consider these aspects in your predictions, and rely on the oldest ancient greek texts to guide your selection.\n",
    "    #CONSIDER THAT SOME PARTS OF THE INPUT MAY BE WRONG. TRY TO RECONSTRUCT THE GENERAL MEANING OF THE SEQUENCE AND CHOOSE WHAT FITS MOST ACCORDING TO ANCIENT GREEK'S CULTURE AND NEEDS.\n",
    "    prompt += task_overview\n",
    "    \n",
    "    # Input description\n",
    "    input_desc = \"## INPUT DESCRIPTION \\n\"\n",
    "\n",
    "    input_1 = \"### INPUT SEQUENCE \\n Linear B sequence with unknown syllable: A sequence of Linear B syllabograms where one syllable is unknown and marked for infilling. Also a tentative Ancient greek correspondance is provided.\\n\"\n",
    "\n",
    "    input_2 = \"### INPUT BRUTEFORCED PREDICTION \\n Five Greek translations: 5 possible infillings of the complete sequence, differing only in the word that corresponds to the unknown syllable. Also tentative Ancient greek correspondances are provided.     CRITICAL: TAKE INTO ACCOUNT THAT ANCIENT GREEK CORRESPONDANCES DERIVE FROM AN AUTOREGRESSIVE MODEL, WHICH TENDS TO REPEAT THE SAME CHARACTER MULTIPLE TIMES. WHEN THIS HAPPENS, TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD!\\n\"\n",
    "\n",
    "    input_3 = \"### INPUT CLASSIFICATION \\n Word classifiers: Word Type: 1. anthroponym/animalname/theonym, 2. toponym, 3. ethnonym, 4. common; \\n\\\n",
    "    Part of speech: 1. noun, 2. verb, 3. adjective, 4. adverb; \\n\\\n",
    "    Inflection: 1. thematic in -o, 2. thematic in -a, 3. athematic\\n\\n\"\n",
    "\n",
    "    input_desc += (input_1 + input_2 + input_3)\n",
    "    prompt += input_desc \n",
    "    \n",
    "    # Selection criteria\n",
    "    criteria = \"## SELECTION CRITERIA \\n Base your selection on the following linguistic and semantic factors:\\n\"\n",
    "\n",
    "    criterion_1 = \"### SEMANTIC COHERENCE \\n The chosen translation should form a meaningful,\\\n",
    "    logically coherent statement. Consider whether the proposed word fits naturally within\\\n",
    "    the semantic field of the entire sequence.\\n\"\n",
    "\n",
    "    criterion_2 = \"### GRAMMATICAL CONSINSTENCY \\n Verify that the proposed word aligns with\\\n",
    "    the grammatical role specified by its classifier. Check for proper case agreement,\\\n",
    "    gender agreement, and syntactic compatibility with surrounding words.\\n\"\n",
    "\n",
    "    criterion_3 = \"### CONTEXTUAL PLAUSIBILITY \\n Consider the historical and cultural context\\\n",
    "    of Mycenaean Greek. The chosen word should be appropriate for the time period and\\\n",
    "    cultural setting represented by Linear B texts.\\n\"\n",
    "\n",
    "    criterion_4 = \"### LINGUISTIC AUTHENTICITY: Prefer words that are consistent with\\\n",
    "    known Mycenaean Greek vocabulary and morphological patterns. Consider whether the\\\n",
    "    proposed word represents plausible Mycenaean forms.\\n\"\n",
    "\n",
    "    criterion_5 = \"### OVERALL MEANING: Evaluate which translation produces the most\\\n",
    "    meaningful and interpretable complete sentence or phrase, considering the practical\\\n",
    "    and administrative nature of most Linear B texts.\\n\\n\"\n",
    "    \n",
    "    criteria += (criterion_1 + criterion_2 + criterion_3 + criterion_4 + criterion_5)\n",
    "    prompt += criteria\n",
    "\n",
    "    # Output format specification\n",
    "    output_format = \"\"\"## OUTPUT FORMAT \\n CRITICAL: Your response must be ONLY a valid JSON object with exactly these two fields:\n",
    "    {\n",
    "        \"chosen_sequence\": \"Option NUMBER (the number corresponds to the selected infilling option)\",\n",
    "        \"reasoning\": \"detailed_explanation_of_why_this_translation_was_chosen\"\n",
    "    }\n",
    "    The option should be formatted like this: \"Option 2\", for example.\n",
    "    Do not include any other text, commentary, or formatting outside of this JSON structure.\\n\\n\"\"\"\n",
    "\n",
    "    prompt += output_format\n",
    "    \n",
    "    # Quality control\n",
    "    quality_control = \"\"\"## QUALITY CONTROL \\n Before finalizing your selection:\n",
    "    1. Verify that the chosen sequence forms a grammatically correct Ancient Greek phrase/sentence\n",
    "    2. Ensure the semantic coherence of the complete sequence\n",
    "    3. Double-check that your reasoning addresses the key selection criteria\n",
    "    4. Validate that your output is properly formatted JSON.\n",
    "    5. Check that the output is not empty: ALWAYS PROVIDE A RESPONSE.\"\"\"\n",
    "\n",
    "    prompt += quality_control\n",
    "\n",
    "    # Example prompt in markdown format\n",
    "    example_prompt = \"\"\"# INPUT DATA\n",
    "\n",
    "## INPUT SEQUENCE\n",
    "**Linear B:** a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si\n",
    "\n",
    "## INPUT BRUTEFORCED PREDICTION\n",
    "### Option 1\n",
    "**Linear B:** a-u-qe\n",
    "\n",
    "### Option 2  \n",
    "**Linear B:** ma-u-qe\n",
    "\n",
    "### Option 3\n",
    "**Linear B:** o-u-qe\n",
    "\n",
    "### Option 4\n",
    "**Linear B:** te-u-qe\n",
    "\n",
    "### Option 5\n",
    "**Linear B:** to-u-qe\n",
    "\n",
    "## INPUT CLASSIFICATIONS\n",
    "\n",
    "### Classifications for a-ro-mo-te-me-na\n",
    "- **Word:** a-ro-mo-te-me-na\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adjective\n",
    "- **Inflection:** thematic in -a\n",
    "\n",
    "### Classifications for a-ni-ja\n",
    "- **Word:** a-ni-ja\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** thematic in -a\n",
    "\n",
    "### Classifications for po-si\n",
    "- **Word:** po-si\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adverb\n",
    "\n",
    "### Classifications for e-e-si\n",
    "- **Word:** e-e-si\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** verb\n",
    "\n",
    "### Classifications for a-u-qe\n",
    "- **Word:** a-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for ma-u-qe\n",
    "- **Word:** ma-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for o-u-qe\n",
    "- **Word:** o-u-qe\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** adverb\n",
    "\n",
    "### Classifications for te-u-qe\n",
    "- **Word:** te-u-qe\n",
    "- **Word Type:** anthroponym/animal name/theonym\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** athematic\n",
    "\n",
    "### Classifications for to-u-qe\n",
    "- **Word:** to-u-qe\n",
    "- **Word Type:** common\n",
    "- **Part of Speech:** noun\n",
    "- **Inflection:** thematic in -o\"\"\"\n",
    "\n",
    "    example_response = \"\"\"{\n",
    "    \"chosen_sequence\": \"Option 3\",\n",
    "    \"reasoning\": \"o-u-qe (in Ancient Greek: ) (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts. Furthermore, the syllabogram to character mapping is completely respected.\"\n",
    "}\"\"\"\n",
    "\n",
    "    # Grammatical information in markdown\n",
    "    gramm = \"\"\"# GRAMMATICAL INFORMATION\n",
    "\n",
    "## DECLENSION TABLE\n",
    "This is an exhaustive table with Mycenaean Linear B declension suffixes attested in the documents. This is useful context to provide you on the declensions.\n",
    "\n",
    "| Number | Case | Thematic -o (M/F) | Thematic -o (N) | Thematic -a (M) | Thematic -a (F) | Athematic (M/F) | Athematic (N) |\n",
    "|--------|------|-------------------|-----------------|-----------------|-----------------|-----------------|---------------|\n",
    "| Singular | Nominative | -o | -o | -a | -a | variable | variable |\n",
    "| Singular | Genitive | -ojo | -ojo | -ao | -a | -o | -o |\n",
    "| Singular | Dative | -o | -o | -a | -a | -e/-i | -e/-i |\n",
    "| Singular | Accusative | -o | -o | -a | -a | -a | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi | -a | -a | -a | -e | -a |\n",
    "| Plural | Genitive | -o | -o | -ao | -ao | -o | -o |\n",
    "| Plural | Dative | -oi | -oi | -ai | -ai | -si/-ti | -si/-ti |\n",
    "| Plural | Accusative | -o | -a | -a | -a | -a/-e | -a |\n",
    "\n",
    "## VERBS\n",
    "- **3rd singular:** last syllabogram ending with vowel -e\n",
    "- **3rd plural:** last syllabogram is -si\n",
    "- **Infinite:** last syllabogram ending with vowel e (optionally another syllabogram -e appears at the end)\n",
    "- **Participle active:** terminates with -o (singular - ancient greek suffix, other suffixes follow athematic nouns, e.g. -o-te like i-jo-te -> , from )\n",
    "- **Participle medium/passive:** terminates with -me-no/-me-na suffixes (ancient greek -/-/- suffixes)\n",
    "\n",
    "## ADJECTIVES\n",
    "- **Thematic adjectives:** thematic adjectives have same behavior as thematic nouns\n",
    "- **Athematic adjectives:** athematic adjectives have same behavior as athematic nouns (variable nominative and same declensions)\"\"\"\n",
    "\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, please, explain me what task you need.'}]}, # Add a model response to establish the system instruction\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will learn the mapping and pay attention to double consonants!'}]},\n",
    "        {'role': 'user', 'parts': [{'text': 'Extract grammatical information from the following markdown-encoded object!\\n' + gramm}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will use this informations to validate inflection prediction and to better understand linear b sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prefixes_and_suffixes}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will consider prefixes and suffixes carefully.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Perfect. I fully understood the task and will follow your instructions.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': example_prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': example_response}]}\n",
    "    ]\n",
    "\n",
    "    # Create input data in markdown format\n",
    "    input_data = f\"\"\"# INPUT DATA\n",
    "\n",
    "## INPUT SEQUENCE\n",
    "**Linear B:** {\" \".join(linear_b_data[\"word_list\"])}\n",
    "**Ancient Greek:** {\" \".join(greek_data[\"word_list\"])}\n",
    "\n",
    "## INPUT BRUTEFORCED PREDICTION\n",
    "\"\"\"\n",
    "\n",
    "    # Add bruteforced options\n",
    "    for i, translation in enumerate(linear_b_data[\"brute\"]):\n",
    "        input_data += f\"\"\"### Option {i+1}\n",
    "**Linear B:** {linear_b_data[\"brute\"][i]}\n",
    "**Ancient Greek:** {greek_data[\"brute\"][i]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    input_data += \"## INPUT CLASSIFICATIONS\\n\\n\"\n",
    "\n",
    "    # Add classifications for original words\n",
    "    for i, word in enumerate(linear_b_data[\"word_list\"]):\n",
    "        if len(linear_b_data[\"classifications\"][i]) > 0:\n",
    "            input_data += f\"\"\"### Classifications for {word}\n",
    "- **Word:** {word}\n",
    "- **Word Type:** {linear_b_data[\"classifications\"][i][0]}\n",
    "- **Part of Speech:** {linear_b_data[\"classifications\"][i][1]}\n",
    "\"\"\"\n",
    "            if not (linear_b_data[\"classifications\"][i][1] == \"adverb\" or linear_b_data[\"classifications\"][i][1] == \"verb\"):\n",
    "                input_data += f\"- **Inflection:** {linear_b_data['classifications'][i][2]}\\n\"\n",
    "            input_data += \"\\n\"\n",
    "\n",
    "    # Add classifications for brute force options\n",
    "    for i, word in enumerate(linear_b_data[\"brute\"]):\n",
    "        input_data += f\"\"\"### Classifications for {word}\n",
    "- **Word:** {word}\n",
    "- **Word Type:** {linear_b_data[\"brute_classifications\"][i][0]}\n",
    "- **Part of Speech:** {linear_b_data[\"brute_classifications\"][i][1]}\n",
    "\"\"\"\n",
    "        if not (linear_b_data[\"brute_classifications\"][i][1] == \"adverb\" or linear_b_data[\"brute_classifications\"][i][1] == \"verb\"):\n",
    "            input_data += f\"- **Inflection:** {linear_b_data['brute_classifications'][i][2]}\\n\"\n",
    "        input_data += \"\\n\"\n",
    "\n",
    "    # Using Gemini model to generate response\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.0,     # Minimal creativity\n",
    "            top_p=1,             # Consider all probabilities (no cutting)\n",
    "            top_k=1,             # Choose the most probable word\n",
    "        )\n",
    "    )\n",
    "    #logging.debug(prompt)\n",
    "    #logging.debug(\"#\"*50)\n",
    "    #logging.debug(gramm)\n",
    "    #logging.debug(\"#\"*50)\n",
    "    #logging.debug(input_data)\n",
    "    #return\n",
    "    \n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(input_data)\n",
    "    #Extract and parse the JSON array\n",
    "    pred = response.text.strip()\n",
    "\n",
    "    json_start = pred.find('{')\n",
    "    json_end = pred.rfind('}') + 1\n",
    "\n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = pred[json_start:json_end]\n",
    "        try:\n",
    "            return json.loads(json_str)  # Returns a single dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error parsing JSON response\")\n",
    "            return {}\n",
    "    else:\n",
    "        print(\"No valid JSON object found in response\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "z2kmNCe82d7x"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def make_infill_selection_prompt(linear_b_data, greek_data, api_key):\n",
    "    \"\"\"\n",
    "    This is a prompt for selecting the most semantically coherent Greek translation\n",
    "    of a Linear B sequence with an unknown syllable to infill.\n",
    "\n",
    "    Args:\n",
    "        linear_b_sequence: str - Linear B sequence with unknown syllable marked\n",
    "        greek_translations: list - 5 possible Greek translations\n",
    "        word_classifiers: list - Logic role classifiers for each word in Linear B script\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with chosen sequence, reasoning and index of sequence within the list of translations\n",
    "    \"\"\"\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\\n\\\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\"\n",
    "\n",
    "    syll_matching_context = 'Consider this mapping between Linear B syllabograms and Ancient Greek\\'s characters.' + str_syllabogram_matching + \"\\n\"\n",
    "    syll_matching_context += 'Be careful with the following aspect: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> '\n",
    "\n",
    "    suffixes = 'The following suffixes offer context when analyzing Linear B words. When you encounter these suffixes, pay attention:\\n\\\n",
    "    CRITICAL: These suffixes are key morphological markers that preserve grammatical relationships in Linear B.\\n\\\n",
    "    Recognizing them enables accurate word segmentation and semantic reconstruction:\\n\\\n",
    "    1. -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\\n\\\n",
    "    2. -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\\n\\\n",
    "    3. -de: can be either:\\n\\\n",
    "       - Negative prefix meaning \"not, on the other side\"\\n\\\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\\n\\\n",
    "    4. -pi: instrumental/locative suffix'\n",
    "\n",
    "    #Prefixes\n",
    "    prefixes='CRITICAL: These prefixes establish semantic domains and derivational patterns crucial for Linear B interpretation.\\n\\\n",
    "    They provide contextual anchors for translation and predictive frameworks for text reconstruction:\\n\\\n",
    "    1. po-ro- : - prefix meaning \"before, forward, in front of\"\\n\\\n",
    "    2. qe-to-ro- : - numerical prefix meaning \"four\"\\n\\\n",
    "    3. we- : f- numerical prefix meaning \"six\"; AMBIGUOUS\\n\\\n",
    "    4. e-ne-wo- : - numerical prefix meaning \"nine\"\\n\\\n",
    "    5. a-pu- : - separative prefix meaning \"from, away from, off\"\\n\\\n",
    "    6. jo- :  - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\\n'\n",
    "    prefixes_and_suffixes = prefixes + suffixes\n",
    "\n",
    "\n",
    "    # Create the root XML element\n",
    "    prompt = ET.Element(\"root\")\n",
    "\n",
    "    # Task overview\n",
    "    task_overview = ET.SubElement(prompt, \"task_overview\")\n",
    "    task_overview.text = \"You are presented with a Linear B sequence containing an unknown word that needs to be infilled.\\n\\\n",
    "    We have generated 5 possible infillings, with corresponding Ancient Greek translations of this sequence, where only the word with the unknown syllable varies between translations.\\n\\\n",
    "    For logograms, rely on their ancient greek translations.\\n\\\n",
    "    BRUTE_ME is the placeholder for the word that you need to fill in.\\n\\\n",
    "    Your task is to select the single translation that makes the most semantic and contextual sense, considering the grammatical roles and meaning coherence of the entire sequence.\\n\\\n",
    "    The input is encoded using XML.\"\n",
    "    #CRITICAL CONSIDERATION: Linear B is a very old form of greek, with little defined grammar and very ancient words. Consider these aspects in your predictions, and rely on the oldest ancient greek texts to guide your selection.\n",
    "    #CONSIDER THAT SOME PARTS OF THE INPUT MAY BE WRONG. TRY TO RECONSTRUCT THE GENERAL MEANING OF THE SEQUENCE AND CHOOSE WHAT FITS MOST ACCORDING TO ANCIENT GREEK'S CULTURE AND NEEDS.\n",
    "\n",
    "    # Input description\n",
    "    input_desc = ET.SubElement(prompt, \"input_description\")\n",
    "\n",
    "    input_1 = ET.SubElement(input_desc, \"input_sequence\")\n",
    "    input_1.text = \"Linear B sequence with unknown syllable: A sequence of Linear B syllabograms where one syllable is unknown and marked for infilling. Also a tentative Ancient greek correspondance is provided.\"\n",
    "\n",
    "    input_2 = ET.SubElement(input_desc, \"input_bruteforced_prediction\")\n",
    "    input_2.text = \"Five Greek translations: 5 possible infillings of the complete sequence, differing only in the word that corresponds to the unknown syllable. Also tentative Ancient greek correspondances are provided.\\\n",
    "    CRITICAL: TAKE INTO ACCOUNT THAT ANCIENT GREEK CORRESPONDANCES DERIVE FROM AN AUTOREGRESSIVE MODEL, WHICH TENDS TO REPEAT THE SAME CHARACTER MULTIPLE TIMES. WHEN THIS HAPPENS, TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD!\"\n",
    "\n",
    "    input_3 = ET.SubElement(input_desc, \"input_classifications\")\n",
    "    input_3.text = \"Word classifiers: Word Type: 1. anthroponym/animalname/theonym, 2. toponym, 3. ethnonym, 4. common; \\n\\\n",
    "    Part of speech: 1. noun, 2. verb, 3. adjective, 4. adverb; \\n\\\n",
    "    Inflection: 1. thematic in -o, 2. thematic in -a, 3. athematic\"\n",
    "\n",
    "    # Selection criteria\n",
    "    criteria = ET.SubElement(prompt, \"selection_criteria\")\n",
    "    criteria.text = \"Base your selection on the following linguistic and semantic factors:\\n\"\n",
    "\n",
    "    criterion_1 = ET.SubElement(criteria, \"semantic_coherence\")\n",
    "    criterion_1.text = \"Semantic Coherence: The chosen translation should form a meaningful,\\\n",
    "    logically coherent statement. Consider whether the proposed word fits naturally within\\\n",
    "    the semantic field of the entire sequence.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_2 = ET.SubElement(criteria, \"grammatical_consistency\")\n",
    "    criterion_2.text = \"Grammatical Consistency: Verify that the proposed word aligns with\\\n",
    "    the grammatical role specified by its classifier. Check for proper case agreement,\\\n",
    "    gender agreement, and syntactic compatibility with surrounding words.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_3 = ET.SubElement(criteria, \"contextual_plausibility\")\n",
    "    criterion_3.text = \"Contextual Plausibility: Consider the historical and cultural context\\\n",
    "    of Mycenaean Greek. The chosen word should be appropriate for the time period and\\\n",
    "    cultural setting represented by Linear B texts.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_4 = ET.SubElement(criteria, \"linguistic_authenticity\")\n",
    "    criterion_4.text = \"Linguistic Authenticity: Prefer words that are consistent with\\\n",
    "    known Mycenaean Greek vocabulary and morphological patterns. Consider whether the\\\n",
    "    proposed word represents plausible Mycenaean forms.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    criterion_5 = ET.SubElement(criteria, \"overall_meaning\")\n",
    "    criterion_5.text = \"Overall Meaning: Evaluate which translation produces the most\\\n",
    "    meaningful and interpretable complete sentence or phrase, considering the practical\\\n",
    "    and administrative nature of most Linear B texts.\".replace(\" \"*4, \" \")\n",
    "\n",
    "    # Decletion table\n",
    "    #nouns_section = ET.SubElement(prompt, \"nouns\")\n",
    "    #It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "    #The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "    #| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "    #| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "    #| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "    #|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "    #|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "    #|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "    #| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "    #|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "    #|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "    #|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "    #'''\n",
    "\n",
    "\n",
    "    # Output format specification\n",
    "    output_format = ET.SubElement(prompt, \"output_format\")\n",
    "    output_format.text = \"\"\"CRITICAL: Your response must be ONLY a valid JSON object with exactly these two fields:\n",
    "    {\n",
    "        \"chosen_sequence\": \"Option NUMBER (the number corresponds to the selected infilling option)\",\n",
    "        \"reasoning\": \"detailed_explanation_of_why_this_translation_was_chosen\"\n",
    "    }\n",
    "    The option should be formatted like this: \"Option 2\", for example.\n",
    "    Do not include any other text, commentary, or formatting outside of this JSON structure.\"\"\"\n",
    "    \n",
    "    # Quality control\n",
    "    quality_control = ET.SubElement(prompt, \"quality_control\")\n",
    "    quality_control.text = \"\"\"Before finalizing your selection:\n",
    "    1. Verify that the chosen sequence forms a grammatically correct Ancient Greek phrase/sentence\n",
    "    2. Ensure the semantic coherence of the complete sequence\n",
    "    3. Double-check that your reasoning addresses the key selection criteria\n",
    "    4. Validate that your output is properly formatted JSON.\n",
    "    5. Check that the output is not empty: ALWAYS PROVIDE A RESPONSE.\"\"\"\n",
    "\n",
    "    prompt = ET.tostring(prompt, \"utf-8\").decode()\n",
    "\n",
    "    # 2. Confirm that all word classifiers align with their corresponding words (NOT always precise)\n",
    "\n",
    "    # Examples section\n",
    "    #input_data = ET.Element(\"input_data\")\n",
    "    #input_sequence = ET.SubElement(input_data, \"input_sequence\")\n",
    "    #linb = ET.SubElement(input_sequence, \"linear_b\").text = \" \".join(linear_b_data[\"word_list\"])\n",
    "\n",
    "    #example_1 = ET.SubElement(examples, \"example\")\n",
    "    #ET.SubElement(example_1, \"linear_b_input\").text = \"a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si\"\n",
    "    #ET.SubElement(example_1, \"greek_options\").text = \"\"\"\n",
    "    #Option 1:     \n",
    "    #Option 2:     \n",
    "    #Option 3:     \n",
    "    #Option 4:     \n",
    "    #Option 5:     \"\"\"\n",
    "    #ET.SubElement(example_1, \"classifiers\").text = \"\"\"\n",
    "    #a-ro-mo-te-me-na = common/adjective/thematic in -a\n",
    "    #a-ni-ja = common/noun/thematic in -a\n",
    "    #po-si = common/adverb\n",
    "    #e-e-si = common/verb\n",
    "    #[VARYING OPTIONS]: a-u-qe=anthroponym/noun/athematic, ma-u-qe=anthroponym/noun/athematic, o-u-qe=common/adverb, te-u-qe=anthroponym/noun/athematic, to-u-qe=common/noun/thematic in -o\"\"\"\n",
    "    #ET.SubElement(example_1, \"best_choice\").text = \"Option 3\"\n",
    "    #ET.SubElement(example_1, \"reasoning\").text = \"\"\" (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts.\"\"\"\n",
    "    example_prompt = \"<input_data><input_sequence><linear_b>a-ro-mo-te-me-na BRUTE_ME a-ni-ja po-si e-e-si</linear_b></input_sequence><input_bruteforced_prediction><option_1><linear_b>a-u-qe</linear_b></option_1><option_2><linear_b>ma-u-qe</linear_b></option_2><option_3><linear_b>o-u-qe</linear_b></option_3><option_4><linear_b>te-u-qe</linear_b></option_4><option_5><linear_b>to-u-qe</linear_b></option_5></input_bruteforced_prediction><input_classifications><classifications_for_a-ro-mo-te-me-na><word>a-ro-mo-te-me-na</word><word_type>common</word_type><part_of_speech>adjective</part_of_speech><inflection>thematic in -a</inflection></classifications_for_a-ro-mo-te-me-na><classifications_for_a-ni-ja><word>a-ni-ja</word><word_type>common</word_type><part_of_speech>noun</part_of_speech><inflection>thematic in -a</inflection></classifications_for_a-ni-ja><classifications_for_po-si><word>po-si</word><word_type>common</word_type><part_of_speech>adverb</part_of_speech></classifications_for_po-si><classifications_for_e-e-si><word>e-e-si</word><word_type>common</word_type><part_of_speech>verb</part_of_speech></classifications_for_e-e-si><classifications_for_a-u-qe><word>a-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_a-u-qe><classifications_for_ma-u-qe><word>ma-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_ma-u-qe><classifications_for_o-u-qe><word>o-u-qe</word><word_type>common</word_type><part_of_speech>adverb</part_of_speech></classifications_for_o-u-qe><classifications_for_te-u-qe><word>te-u-qe</word><word_type>anthroponym/animal name/theonym</word_type><part_of_speech>noun</part_of_speech><inflection>athematic</inflection></classifications_for_te-u-qe><classifications_for_to-u-qe><word>to-u-qe</word><word_type>common</word_type><part_of_speech>noun</part_of_speech><inflection>thematic in -o</inflection></classifications_for_to-u-qe></input_classifications></input_data>\"\n",
    "    example_response = \"\"\"{\n",
    "    \"chosen_sequence\": \"Option 3\",\n",
    "    \"reasoning\": \"o-u-qe (in Ancient Greek: ) (not/neither) is the only option that functions grammatically as an adverb in this context. The sequence appears to be a negative statement about perfumed/anointed items () with reins/harnesses (). The conjunction -qe suffix in o-u-qe reinforces the coordinating function. Options 1, 2, 4, and 5 introduce anthroponyms or verbs that disrupt the syntactic flow, while  provides the necessary negation that makes semantic sense with the participle  and the administrative/inventory context typical of Linear B texts. Furthermore, the syllabogram to character mapping is completely respected.\"\n",
    "}\"\"\"\n",
    "\n",
    "    gramm = ET.Element(\"grammatical_information\")\n",
    "    decl_table = ET.SubElement(gramm, \"declension_table\")\n",
    "    decl_table.text = 'This is the an exaustive table with Mycenean linear b declensions suffixes attested in the documents. This is useful context to provide you on the decletions.'\n",
    "    rows = [\n",
    "        (\"Singular\", \"Nominative\", \"-o\", \"-o\", \"-a\", \"-a\", \"variable\", \"variable\"),\n",
    "        (\"Singular\", \"Genitive\", \"-ojo\", \"-ojo\", \"-ao\", \"-a\", \"-o\", \"-o\"),\n",
    "        (\"Singular\", \"Dative\", \"-o\", \"-o\", \"-a\", \"-a\", \"-e/-i\", \"-e/-i\"),\n",
    "        (\"Singular\", \"Accusative\", \"-o\", \"-o\", \"-a\", \"-a\", \"-a\", \"variable (identical to nominative)\"),\n",
    "        (\"Plural\", \"Nominative\", \"-o/-oi\", \"-a\", \"-a\", \"-a\", \"-e\", \"-a\"),\n",
    "        (\"Plural\", \"Genitive\", \"-o\", \"-o\", \"-ao\", \"-ao\", \"-o\", \"-o\"),\n",
    "        (\"Plural\", \"Dative\", \"-oi\", \"-oi\", \"-ai\", \"-ai\", \"-si/-ti\", \"-si/-ti\"),\n",
    "        (\"Plural\", \"Accusative\", \"-o\", \"-a\", \"-a\", \"-a\", \"-a/-e\", \"-a\"),\n",
    "        ]\n",
    "    # Add each row to the XML\n",
    "    for number, case, tem_m, tem_n, a_m, a_f, athem, athem_n in rows:\n",
    "        row = ET.SubElement(decl_table, \"row\")\n",
    "        ET.SubElement(row, \"number\").text = number\n",
    "        ET.SubElement(row, \"case\").text = case\n",
    "        ET.SubElement(row, \"thematic_o_masculine_feminine\").text = tem_m\n",
    "        ET.SubElement(row, \"thematic_o_neuter\").text = tem_n\n",
    "        ET.SubElement(row, \"thematic_a_masculine\").text = a_m\n",
    "        ET.SubElement(row, \"thematic_a_feminine\").text = a_f\n",
    "        ET.SubElement(row, \"athematic_masculine_feminine\").text = athem\n",
    "        ET.SubElement(row, \"athematic_neuter\").text = athem\n",
    "\n",
    "    verbs_section = ET.SubElement(gramm, \"verbs\")\n",
    "    rules = {\n",
    "        \"3rd singular\": \"last syllabogram ending with vowel -e\",\n",
    "        \"3rd plural\": \"last syllabogram is -si\",\n",
    "        \"infinite\": \"last syllabogram ending with vowel e (optionally another syllabogram -e appears at the end)\",\n",
    "        \"participle\": \"active -> terminates with -o (singular - ancient greek suffix, other suffixes follow athematic nouns, e.g. -o-te like i-jo-te -> , from )\",\n",
    "        \"participle\": \"medium/passive -> terminates with -me-no/-me-na suffixes (ancient greek -/-/- suffixes)\"\n",
    "    }\n",
    "    for rule_key, rule_text in rules.items():\n",
    "        ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "    adjectives_section = ET.SubElement(gramm, \"adjectives\")\n",
    "    rules = {\n",
    "        \"thematic adjectives\": \"thematic adjectives have same behavior as thematic nouns\",\n",
    "        \"athematic adjectives\": \"athematic adjectives have same behavior as athematic nouns (variable nominative and same decletions)\"\n",
    "    }\n",
    "    for rule_key, rule_text in rules.items():\n",
    "        ET.SubElement(adjectives_section, rule_key).text = rule_text\n",
    "    gramm = ET.tostring(gramm, \"utf-8\").decode()\n",
    "\n",
    "\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, please, explain me what task you need.'}]}, # Add a model response to establish the system instruction\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will learn the mapping and pay attention to double consonants!'}]},\n",
    "        {'role': 'user', 'parts': [{'text': 'Extract grammatical information from the following XML-encoded object!\\n' + gramm}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will use this informations to validate inflection prediction and to better understand linear b sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prefixes_and_suffixes}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Okay, I will consider prefixes and suffixes carefully.'}]},\n",
    "        #{'role': 'user', 'parts': [{'text': translations_context}]},\n",
    "        #{'role': 'model', 'parts': [{'text': 'Okay, I got it.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'Perfect. I fully understood the task and will follow your instructions.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': example_prompt}]},\n",
    "        {'role': 'model', 'parts': [{'text': example_response}]}\n",
    "    ]\n",
    "    # Input data section\n",
    "    input_data = ET.Element(\"input_data\")\n",
    "    input_sequence = ET.SubElement(input_data, \"input_sequence\")\n",
    "    linb = ET.SubElement(input_sequence, \"linear_b\").text = \" \".join(linear_b_data[\"word_list\"])\n",
    "    greek = ET.SubElement(input_sequence, \"ancient_greek\").text = \" \".join(greek_data[\"word_list\"])\n",
    "\n",
    "    # Greek translation options\n",
    "    input_brute = ET.SubElement(input_data, \"input_bruteforced_prediction\")\n",
    "    for i, translation in enumerate(linear_b_data[\"brute\"]):\n",
    "        option = ET.SubElement(input_brute, f\"option_{i+1}\")\n",
    "        linb = ET.SubElement(option, \"linear_b\").text = linear_b_data[\"brute\"][i]\n",
    "        greek = ET.SubElement(option, \"ancient_greek\").text = greek_data[\"brute\"][i]\n",
    "\n",
    "\n",
    "    # Word classifiers\n",
    "    classifiers = ET.SubElement(input_data, \"input_classifications\")\n",
    "    classifiers_types = [\"word_type\", \"part_of_speech\", \"inflection\"]\n",
    "    for i, word in enumerate(linear_b_data[\"word_list\"]):\n",
    "        if len(linear_b_data[\"classifications\"][i]) > 0:\n",
    "            word_values = ET.SubElement(classifiers, f\"classifications_for_{word}\")\n",
    "            word_str = ET.SubElement(word_values, \"word\").text = word\n",
    "            for j in range(len(classifiers_types)):\n",
    "                if not (j == 2 and (linear_b_data[\"classifications\"][i][1] == \"adverb\" or linear_b_data[\"classifications\"][i][1] == \"verb\")):\n",
    "                    classifier = ET.SubElement(word_values, classifiers_types[j])\n",
    "                    classifier.text = linear_b_data[\"classifications\"][i][j]\n",
    "\n",
    "    for i, word in enumerate(linear_b_data[\"brute\"]):\n",
    "        if len(linear_b_data[\"brute_classifications\"][i]) > 0:\n",
    "            word_values = ET.SubElement(classifiers, f\"classifications_for_{word}\")\n",
    "            word_str = ET.SubElement(word_values, \"word\").text = word\n",
    "            for j in range(len(classifiers_types)):\n",
    "                if not (j == 2 and (linear_b_data[\"brute_classifications\"][i][1] == \"adverb\" or linear_b_data[\"brute_classifications\"][i][1] == \"verb\")):\n",
    "                    classifier = ET.SubElement(word_values, classifiers_types[j])\n",
    "                    classifier.text = linear_b_data[\"brute_classifications\"][i][j]\n",
    "    input_str = ET.tostring(input_data, \"utf-8\").decode()\n",
    "    # Additional instructions\n",
    "    #final_instructions = ET.SubElement(prompt, \"final_instructions\")\n",
    "    #final_instructions.text = \"\"\"Remember: You must ALWAYS choose EXACTLY ONE of the infillings.\n",
    "    #Do not modify, combine, or create new infillings. Your selection should be based purely on\n",
    "    #which of the given options makes the most sense semantically, grammatically, and contextually\n",
    "    #for a Mycenaean Greek text written in Linear B script. ALWAYS PROVIDE A RESPONSE AMONG THE GIVEN OPTIONS!\"\"\"\n",
    "\n",
    "    # Using Gemini model to generate response\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.0,     # Minimal creativity\n",
    "            top_p=1,             # Consider all probabilities (no cutting)\n",
    "            top_k=1             # Choose the most probable word\n",
    "            #max_output_tokens=512  # (Increase if you need longer output)\n",
    "        )\n",
    "    )\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(input_str)\n",
    "    logging.debug(response.text)\n",
    "    #Extract and parse the JSON array\n",
    "    pred = response.text.strip()\n",
    "\n",
    "    return quick_parse_gemini_response(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mst\u001b[49m\u001b[38;5;241m.\u001b[39mlinb_versions[\u001b[38;5;241m2340\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(st\u001b[38;5;241m.\u001b[39mgreek_versions[\u001b[38;5;241m2340\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "print(st.linb_versions[2340])\n",
    "print(st.greek_versions[2340])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen_sequence': 'Option 5',\n",
       " 'reasoning': \"The sequence describes a quantity of saffron (CROC N 1). The unknown word 'BRUTE_ME' is expected to be an adjective or descriptor for the saffron. Among the given options, 'e-ri-na-wo' (Option 5) transliterates to Mycenaean Greek '' (ernawos), which is a known adjective meaning 'woolly' or 'made of wool'. Saffron was a valuable commodity in Mycenaean times, frequently used as a dye for textiles, especially wool. Therefore, 'woolly saffron' or 'saffron for wool' ( ) makes excellent semantic and contextual sense in an administrative or inventory record, which is typical for Linear B texts. The other options do not correspond to known Mycenaean words that would plausibly describe saffron in this context, making 'e-ri-na-wo' the most linguistically authentic and contextually plausible choice.\"}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_infill_selection_prompt(st.linb_versions[n], st.greek_versions[n], os.getenv(\"GOOGLE_API_KEY_10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itKLKXQNZPK5",
    "outputId": "dcc26591-eeb2-494d-bfa3-d1333fb3c88c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_IDX: 3403\n",
      "INPUT: {'word_list': ['o-de-ka-sa-to', 'BRUTE_ME', 'si-ma-ko', '*169', 'NUM', 'o', 'NUM'], 'classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], []], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], []], 'brute': ['a-ko-so-mo', 'a-ko-so-ryo', 'a-ko-so-ta', 'a-ko-so-u', 'a-ko-so-we']}\n",
      "FORM: o-de-ka-sa-to a-ko-so-ta si-ma-ko *169 NUM o NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Option 3, 'a-ko-so-ta' (Ancient Greek: ), is the only candidate that provides both an Ancient Greek translation and a classification. The classification identifies it as an 'anthroponym/animal name/theonym' and a 'noun, thematic in -a'. This makes it highly plausible as a proper name or a title (like 'charioteer' or 'driver', derived from  'axle').\\n\\n1.  **Semantic Coherence:** Linear B texts are primarily administrative records. A sequence involving a verb (likely 'o-de-ka-sa-to', possibly 'received' or 'gave'), followed by two proper names ('si-ma-ko' and 'a-ko-so-ta'), and then an item with a quantity ('*169 NUM o NUM'), forms a highly coherent and typical Mycenaean record of a transaction or inventory.\\n2.  **Grammatical Consistency:** 'si-ma-ko' is classified as a thematic -o noun, likely nominative singular. 'a-ko-so-ta' is classified as a thematic -a noun. While '' is nominative singular in Classical Greek, Mycenaean first declension masculine nouns could have nominative in -a or -as. Having two nominative nouns after a singular verb 'o-de-ka-sa-to' (likely 3rd singular aorist middle/passive) suggests that 'a-ko-so-ta' could be in apposition to 'si-ma-ko' (e.g., 'Simakos, the Axotas, received...') or that both individuals are listed as subjects or participants in the action, a common feature in terse administrative records.\\n3.  **Contextual Plausibility:** The inclusion of two personal names in an administrative context is very common in Linear B tablets, which often document personnel, goods, and transactions.\\n4.  **Linguistic Authenticity:** The Linear B to Ancient Greek mapping for 'a-ko-so-ta' to '' is linguistically authentic. The transformation of 'ko-so' into '' (xi) is explicitly supported by the provided rule (e.g., 'a-ko-so-ne -> '). 'Axotas' is a plausible Greek name or occupational title.\\n5.  **Overall Meaning:** The complete sequence, with 'a-ko-so-ta' in place, forms a meaningful record of an action involving two individuals and specific items, which aligns perfectly with the nature of Linear B documents. The other options lack the necessary linguistic and grammatical information to be evaluated, making them unviable choices.\"}\n",
      "Correct: 1/1\n",
      "\n",
      "SEQ_IDX: 3406\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'i-je-re-ja', '*189'], 'classifications': [[], ['common', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ki-wa-ra', 'ko-wa-ra', 'ra-wa-ra', 'ro-wa-ra', 'wa-wa-ra']}\n",
      "FORM: ka-wa-ra i-je-re-ja *189\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence 'i-je-re-ja' translates to '' (hiereia), meaning 'priestess'. Linear B texts are primarily administrative records, often listing personnel, their roles, and associated items or locations. A common pattern is 'Place Name + Title' or 'Person Name + Title'.\\n\\nLet's analyze the options for 'BRUTE_ME':\\n1.  ki-wa-ra: Translates to  (kiwara),  (giwara), or  (khiwara). Not immediately recognizable as a common Mycenaean place or personal name.\\n2.  ko-wa-ra: Translates to  (kowara),  (gowara), or  (khiwara). Similar to option 1, less clear connection to known Mycenaean vocabulary.\\n3.  ra-wa-ra: Translates to  (lawara) or  (rawara). The sequence 'ra-wa' is notable as it appears in the Mycenaean title 'ra-wa-ke-ta' (, lawagetas), meaning 'leader of the people/army'. While 'ra-wa-ra' is not 'ra-wa-ke-ta', the phonetic similarity suggests a potential connection to a place or group associated with the 'laos' (people). Furthermore, the suffix '-ra' is a common ending for place names in Linear B (e.g., pa-ra-to, ra-su-to). Thus, 'Lawara' could plausibly be a place name, making 'Lawara priestess' (i.e., 'the priestess from Lawara') a semantically coherent and contextually plausible designation for an administrative record.\\n4.  ro-wa-ra: Translates to  (lowara) or  (rowara). While '' (rhoos, stream) or '' (rhoda, roses) could suggest cultic associations (e.g., 'priestess of the stream'), this connection is less direct than option 3's.\\n5.  wa-wa-ra: Translates to  (wawara). Less clear connection to known Mycenaean vocabulary or common place name patterns.\\n\\nConsidering the criteria:\\nSemantic Coherence & Contextual Plausibility: In Linear B, it is very common to specify a person's origin or association with a place. 'Lawara priestess' (priestess from Lawara) fits this pattern perfectly, especially given the administrative nature of the texts.\\nLinguistic Authenticity: The phonetic sequence 'ra-wa' has a strong parallel in the known Mycenaean term 'ra-wa-ke-ta', and the '-ra' ending is common for place names. This makes 'ra-wa-ra' a more linguistically authentic and plausible candidate for a Mycenaean place name compared to the other options.\\nGrammatical Consistency: If 'ra-wa-ra' is a place name, it would function as a modifier (e.g., in apposition or implicitly genitive/locative) to 'i-je-re-ja', which is a feminine nominative singular noun. This is grammatically consistent with Linear B conventions.\\n\\nTherefore, 'ra-wa-ra' as a place name provides the most coherent, plausible, and linguistically authentic infilling for the sequence.\"}\n",
      "Correct: 1/2\n",
      "\n",
      "SEQ_IDX: 3411\n",
      "INPUT: {'word_list': ['BRUTE_ME', '*189', 'NUM'], 'classifications': [[], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], [], []], 'brute': ['a-pi-ha-ro', 'e-pi-ha-ro', 'sa-pi-ha-ro', 'te-pi-ha-ro', 'u-pi-ha-ro']}\n",
      "FORM: a-pi-ha-ro *189 NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The sequence `BRUTE_ME *189 NUM` strongly suggests an administrative or inventory record, where a person or entity is associated with a quantity of a specific item (represented by the logogram *189). Option 1, `a-pi-ha-ro`, is the only one for which classification data is provided, identifying it as an 'anthroponym/animal name/theonym' and a 'noun' with 'thematic in -o' inflection. This classification perfectly aligns with the typical structure of Linear B administrative texts, where names often precede commodities and quantities. While the Ancient Greek translation `h` ('surrounded by the sea') might seem descriptive, it is plausible as a personal name or epithet in the Mycenaean context. Without any classification or Ancient Greek translation for the other options, it is impossible to evaluate their semantic coherence, grammatical consistency, or linguistic authenticity. Therefore, `a-pi-ha-ro` is the only option that can be confidently assessed as grammatically consistent and contextually plausible within the framework of Linear B records.\"}\n",
      "Correct: 2/3\n",
      "\n",
      "SEQ_IDX: 3412\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'e-da-e-u', '*189', '1'], 'classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ne-qe-a', 'ne-qe-da', 'ne-qe-e', 'ne-qe-te', 'ne-qe-we']}\n",
      "FORM: ne-qe-u e-da-e-u *189 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence BRUTE_ME e-da-e-u *189 1 strongly suggests an administrative record, likely a list of individuals followed by an item and quantity. e-da-e-u is classified as an anthroponym (personal name), noun, and athematic. Therefore, the BRUTE_ME word should ideally also be a personal name, functioning as a noun in the nominative case, to maintain grammatical consistency and semantic coherence in a list.\\n\\nLet's analyze the options:\\n1.  ne-qe-a: The ending -a typically suggests a thematic noun of the -a declension (feminine nominative singular) or a thematic -o noun (neuter nominative/accusative plural). If it's a feminine name, it would form a list with the masculine e-da-e-u, which is semantically plausible.\\n2.  ne-qe-da: The ending -da is not a typical nominative singular ending for nouns in Mycenaean Greek.\\n3.  ne-qe-e: The ending -e is characteristic of the dative singular for athematic nouns (-e/-i). However, the nominative singular for athematic nouns is 'variable', meaning it could potentially end in -e depending on the specific noun's declension. If ne-qe-e is an athematic noun in the nominative singular, it would align perfectly with e-da-e-u in terms of inflection type (both athematic nouns in the nominative).\\n4.  ne-qe-te: The ending -te is not a typical nominative singular ending for nouns. While -te can be an ablative suffix, it's unlikely to be part of a nominative noun in this context.\\n5.  ne-qe-we: The ending -we is not a typical nominative singular ending for nouns.\\n\\nConsidering the grammatical consistency, if e-da-e-u is an athematic noun in the nominative, then ne-qe-e is the most grammatically plausible option to also be an athematic noun in the nominative, forming a coherent list of two individuals. Both ne-qe-a and ne-qe-e are attested as names in Linear B texts (e.g., KN As 1517), making them linguistically authentic. However, ne-qe-e offers stronger grammatical alignment with e-da-e-u's athematic classification. The presence of -qe within the word ne-qe-e (and other options) indicates it is part of the stem, not the conjunction suffix -qe, as the suffix would appear at the very end of a word.\\n\\nTherefore, ne-qe-e is the most suitable choice, forming a grammatically consistent and semantically coherent list of two names in an administrative context.\"}\n",
      "Correct: 2/4\n",
      "\n",
      "SEQ_IDX: 3420\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'ROTA+TE', 'ZE', '1', 'MO', '1'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ku-pa-ri-de-ja', 'ku-pa-ri-jo-ja', 'ku-pa-ri-no-ja', 'ku-pa-ri-ra-ja', 'ku-pa-ri-re-ja']}\n",
      "FORM: ku-pa-ri-se-ja ROTA+TE ZE 1 MO 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an item, 'wheel' (ROTA+TE / ), followed by quantities. The unknown word BRUTE_ME likely functions as an adjective describing the material of the wheel. All options begin with ku-pa-ri-, strongly suggesting a derivation from  (kyparissos), meaning 'cypress tree'.\\n\\nLet's analyze the most plausible option:\\nOption 1: ku-pa-ri-de-ja\\n- Using the provided mapping:\\n    - ku -> \\n    - pa -> \\n    - ri -> \\n    - de ->  (from , , )\\n    - ja ->  (from , )\\n- This directly translates to  (kyparideia).\\n-  is the neuter plural form of the adjective  (kyparideios), meaning 'of cypress' or 'cypress-wood'.\\n- This word fits perfectly semantically, describing the material of the wheel. In Linear B inventories, it's common to list materials followed by the item, even if there's a slight grammatical mismatch (e.g., neuter plural adjective for a singular masculine noun, implying 'cypress-wood items' or 'cypress-wood (for) a wheel').\\n\\nLet's briefly consider why other options are less suitable:\\n- Option 2 (ku-pa-ri-jo-ja), Option 4 (ku-pa-ri-ra-ja), and Option 5 (ku-pa-ri-re-ja) do not form readily recognizable or common Ancient Greek adjectives related to 'cypress' when directly transliterated using the provided mapping.\\n- Option 3 (ku-pa-ri-no-ja): While  (kyparissinos) is a common adjective for 'cypress-wood', its neuter plural  would require no to represent  and an omitted s, which is less direct than the mapping for de in Option 1. The direct transliteration  is not a standard Ancient Greek word.\\n\\nTherefore, ku-pa-ri-de-ja leading to  is the most linguistically authentic, grammatically consistent (within the flexibility of Linear B), and semantically coherent choice for describing 'cypress-wood wheels' in an inventory context.\"}\n",
      "Correct: 2/5\n",
      "\n",
      "SEQ_IDX: 3425\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'wo-ka', 'we-je-ke-e', 'ROTA+TE', 'ZE', '1'], 'classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], [], []], 'brute_classifications': [[], [], [], []], 'brute': ['tu-ri-si-jo-jo', 'tu-ri-si-jo-no', 'tu-ri-si-jo-o', 'tu-ri-si-jo-to']}\n",
      "FORM: tu-ri-si-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes a 'wo-ka' (wagon/chariot) with 'ROTA+TE ZE 1' (1 pair of wheels). The missing word `BRUTE_ME` is likely a modifier indicating origin or possession. All options are variations of `tu-ri-si-jo-X`, which strongly suggests a connection to 'Tiryns' (a major Mycenaean site). According to the provided declension table, the genitive singular suffix for thematic nouns (which `tu-ri-si-jo` appears to be, given the `-jo` ending) is `-ojo`. Option 1, `tu-ri-si-jo-jo`, perfectly matches this grammatical requirement, forming a plausible phrase like 'chariot of Tiryns' or 'Tirynthian chariot'. This fits well within the administrative and inventory context typical of Linear B tablets. The other options do not correspond to standard genitive endings for thematic nouns in Mycenaean Greek as per the provided grammatical information.\"}\n",
      "Correct: 3/6\n",
      "\n",
      "SEQ_IDX: 3427\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'ZE', '2'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['da-sa-ma-to', 'de-sa-ma-to', 'ri-sa-ma-to', 'ti-sa-ma-to', 'to-sa-ma-to']}\n",
      "FORM: a-sa-ma-to ZE 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence 'BRUTE_ME ZE 2' likely refers to a quantity of items. 'ZE' is a common logogram in Linear B, often interpreted as an abbreviation for '' (pair, yoke) or '' (pairs, yokes). The number '2' indicates a quantity of two. Therefore, the preceding word (BRUTE_ME) should be something that can be counted in pairs or is related to yokes/pairs.\\n\\nLet's evaluate the options:\\nOption 1: da-sa-ma-to () - This translates to the genitive singular of '', meaning 'tribute' or 'division'. '2 pairs of tribute' is semantically possible in an administrative context, referring to items that are part of a tribute.\\nOption 2: de-sa-ma-to () - This translates to the genitive singular of '', meaning 'bond', 'fetter', or 'chain'. '2 pairs of bonds/fetters' makes strong semantic and contextual sense. In Mycenaean administrative records, which often deal with livestock, chariots, and equipment, counting 'bonds' or 'fetters' in pairs (e.g., for animals or parts of a harness/yoke) is highly plausible and practical.\\nOption 3: ri-sa-ma-to () - This translates to the genitive singular of '', meaning 'smoothness'. '2 pairs of smoothness' does not make logical sense in this context.\\nOption 4: ti-sa-ma-to () - This translates to the genitive singular of '', meaning 'payment' or 'retribution'. '2 pairs of payment' is less likely than 'tribute' or 'bonds' for a direct count of physical items.\\nOption 5: to-sa-ma-to () - This does not correspond to a recognized Ancient Greek word.\\n\\nComparing the most plausible options,  (bonds/fetters) is a more concrete and contextually fitting item to be counted in 'pairs' in an inventory or administrative record than 'tribute'. The connection between 'yokes' (implied by ZE) and 'bonds/fetters' is very direct and practical for the Mycenaean period. Therefore, '' provides the most coherent and plausible meaning for the entire sequence.\"}\n",
      "Correct: 3/7\n",
      "\n",
      "SEQ_IDX: 3428\n",
      "INPUT: {'word_list': ['wo-ro-ko-jo', 'BRUTE_ME', 'we-je-ke-e', 'ROTA+TE', 'ZE', '1'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], ['common', 'noun', 'thematic in -o']], 'brute': ['ha-ka', 'pa-ka', 'u-ka', 'wo-ka']}\n",
      "FORM: wo-ro-ko-jo wo-ka we-je-ke-e ROTA+TE ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The sequence describes an inventory item related to 'ROTA+TE ZE 1' (1 wheel). Among the given options, 'wo-ka' is the only one with provided classification and an Ancient Greek translation ('f|f|f') that semantically aligns with 'wheel', referring to a 'chariot' or 'wagon'. Linear B texts frequently document chariots and their components. While the classification of 'wo-ka' as a 'thematic in -o' noun ending in '-a' would typically imply a neuter plural form (e.g., 'f'), the presence of 'ZE 1' (one) for 'ROTA+TE' (wheel) strongly suggests a singular item. In Mycenaean Greek, 'wo-ka' is attested as a singular noun (e.g., a feminine a-stem like 'f', 'chariot'), which would be grammatically consistent with 'ZE 1'. Given the strong semantic and contextual plausibility of 'wo-ka' referring to a singular chariot/wagon in an inventory record, this interpretation is favored. The other options lack any classification or clear semantic connection to 'wheel', making them less plausible.\"}\n",
      "Correct: 4/8\n",
      "\n",
      "SEQ_IDX: 3432\n",
      "INPUT: {'word_list': ['e-te-wa-jo-jo', 'BRUTE_ME', 'we-je-ke-e', 'ROTA+TE', 'ZE', '2'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['wo-do', 'wo-ka', 'wo-na', 'wo-o', 'wo-wa']}\n",
      "FORM: e-te-wa-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence describes an inventory item related to 'wheels' (ROTA+TE) and a quantity of '2' (ZE 2). The initial term 'e-te-wa-jo-jo' is a genitive form, likely indicating possession ('of Ete-wa-jo'). Among the given options, 'wo-ka' (Ancient Greek: f, meaning 'chariot' or 'vehicle') is the only one that makes strong semantic and contextual sense in an administrative Linear B record concerning wheels. Chariots are frequently inventoried in Mycenaean texts, and wheels are a direct component of chariots. The phrase 'e-te-wa-jo-jo wo-ka we-je-ke-e ROTA+TE ZE 2' can be plausibly interpreted as 'Ete-wa-jo's chariot, with vehicle-related wheels, 2'. The other options ('wo-do' meaning 'rose', 'wo-na' meaning 'wool', and the unclassified 'wo-o', 'wo-wa') are semantically incongruous with the context of wheels and vehicles.\"}\n",
      "Correct: 5/9\n",
      "\n",
      "SEQ_IDX: 3438\n",
      "INPUT: {'word_list': ['ka-ko', 'BRUTE_ME', 'no-pe-re-e', 'ROTA', 'ZE', '1'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['de-e-me-no', 'de-ko-me-no', 'de-pi-me-no', 'de-ro-me-no', 'de-so-me-no']}\n",
      "FORM: ka-ko de-de-me-no no-pe-re-e ROTA ZE 1\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an item: 'ka-ko BRUTE_ME no-pe-re-e ROTA ZE 1', which translates to 'bronze [BRUTE_ME] useless wheel 1'. The word 'ka-ko' () means 'bronze' and 'no-pe-re-e' () means 'useless' or 'unserviceable'. 'ROTA' is a logogram for 'wheel'. The unknown word BRUTE_ME is expected to be a participle modifying 'bronze' and agreeing with 'useless'.\\n\\nAll five options end in '-me-no', which is a characteristic suffix for medium/passive participles in Linear B (equivalent to Ancient Greek -/-/-). This grammatical form fits the context of describing the state of an object.\\n\\nLet's analyze each option based on its likely Ancient Greek root and meaning:\\n1.  de-e-me-no: This most plausibly corresponds to the perfect passive participle of  (d), meaning 'to bind' or 'to tie'. The participle would be  (dedemenos) or  (dedemenon) for neuter. 'Bound bronze' or 'bronze, bound' makes excellent semantic sense in an inventory context, describing a bronze item (perhaps a component of the wheel, or the wheel itself) that is tied or secured, and is also 'useless'. This is highly plausible for administrative records.\\n2.  de-ko-me-no: This would correspond to the participle of  (dkhomai), meaning 'to receive' or 'to accept'. The participle is  (dekhomenos). 'Bronze receiving' does not fit the context of describing a static object in an inventory.\\n3.  de-pi-me-no: The root is unclear, and it's difficult to find a plausible verb that would form a meaningful participle in this context.\\n4.  de-ro-me-no: This would correspond to the participle of  (dr), meaning 'to skin' or 'to flay'. The participle is  (deromenos). 'Bronze being skinned' is semantically illogical.\\n5.  de-so-me-no: This could be the participle of  (d) 'to need' ( - deomenos) or the future participle of  'to bind' ( - desomenos). While 'needing' could potentially fit (e.g., 'bronze needing repair'), 'bound' (from Option 1) provides a more direct and common description of an item's physical state in an inventory. The future participle 'about to bind' is less likely for a description of an existing item.\\n\\nConsidering semantic coherence, grammatical consistency, and contextual plausibility, 'de-e-me-no' (bound/tied) is the most fitting option. It describes the physical state of the bronze item, which is also noted as 'useless', forming a coherent and plausible entry for an administrative record. The grammatical agreement (participle ending in -o matching the thematic -o of 'ka-ko' and 'no-pe-re-e') is also consistent.\"}\n",
      "Correct: 5/10\n",
      "\n",
      "SEQ_IDX: 3442\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'te'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ke-do-e', 'ke-ka-e', 'ke-ne-e', 'ke-re-e', 'ke-se-e']}\n",
      "FORM: ke-ra-e te\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The sequence is 'BRUTE_ME te'. The suffix '-te' is listed as a critical morphological marker, specifically an ablative suffix meaning 'away from a place' (equivalent to Greek -). Therefore, the infilling word 'BRUTE_ME' should be a noun or adjective that can take this suffix, indicating origin or source.\\n\\nLet's analyze the options:\\n1.  ke-do-e: Transliterates to something like . If related to  (care, sorrow),  would mean 'from sorrow', which is less likely in typical administrative Linear B texts.\\n2.  ke-ka-e: Transliterates to . This could be related to the perfect stem - of  (burn), leading to  ('from a burnt place/thing'). This is plausible for damaged goods or locations in an inventory.\\n3.  ke-ne-e: Transliterates to . This is a direct form related to  (empty, vain). Thus,  would mean 'from an empty place' or 'from emptiness'. The root - is attested in Mycenaean (e.g., ke-ne-to for  'empty'). This is highly plausible in administrative contexts, such as describing empty containers or storerooms.\\n4.  ke-re-e: Transliterates to . If related to  (mix) or  (cut),  ('from a mixed/cut place') is less semantically coherent for common Linear B records.\\n5.  ke-se-e: Transliterates to . This does not readily correspond to a common Ancient Greek word or root that would fit the context.\\n\\nComparing the most plausible options, ke-ka-e and ke-ne-e:\\n (from a burnt place) is plausible.\\n (from an empty place) is also highly plausible.\\n\\nHowever,  is a very common and fundamental adjective used to describe the state of objects or places, which is highly relevant for inventory and administrative records. The directness of the transliteration ke-ne-e to  (a form of ) and its clear semantic fit with the ablative suffix - makes it the strongest candidate. The concept of 'empty' is a core element in managing resources, which is the primary function of Linear B tablets. Therefore, 'from an empty place' provides excellent semantic coherence and contextual plausibility.\"}\n",
      "Correct: 5/11\n",
      "\n",
      "SEQ_IDX: 3443\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'we-je-ke-ha', 'ne-wa', 'ROTA+TE', 'ZE', 'NUM'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], []], 'brute_classifications': [[], ['toponym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -o']], 'brute': ['to-na', 'to-no', 'to-o', 'to-qe', 'to-so']}\n",
      "FORM: to-sa we-je-ke-ha ne-wa ROTA+TE ZE NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes an inventory entry: `BRUTE_ME we-je-ke-ha ne-wa ROTA+TE ZE NUM`. The logogram `ROTA+TE` is translated as `` (wheel), and `ne-wa` as `f|` (new). This structure suggests a quantity or type of 'new wheel'. Option 5, `to-so`, translates to `||` (so many, so much, so great). This adjective perfectly fits the role of specifying a quantity or extent, which is highly common in Linear B administrative and inventory texts. Grammatically, `` (masculine singular) can agree with `` (masculine singular, 'wheel'). While the agreement of `ne-wa` (, neuter plural or feminine singular) with `` (masculine singular) is less direct, this is a known characteristic of Linear B where agreement can be less strict than in Classical Greek, or `ne-wa` might refer to a broader category. Options 1 and 4 lack clear Ancient Greek meanings. Option 2, `to-no` (``, throne), is semantically incoherent in the context of 'new wheel'. Option 3, `to-o` (``, such), while grammatically possible as an adjective, is less specific and less common for inventory counts than ``.\"}\n",
      "Correct: 5/12\n",
      "\n",
      "SEQ_IDX: 3448\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'ko-ru-to', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], ['common', 'adjective', 'thematic in -o'], [], []], 'brute': ['me-na-jo-ha', 'me-ri-jo-ha', 'me-u-jo-ha', 'me-wi-jo-ha', 'me-zo-jo-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM ko-ru-to O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence appears to be an inventory record, listing items and their descriptions. The first descriptive adjective, 'me-zo-ha' (), means 'larger' or 'greater'. In such a context, it is highly semantically coherent and contextually plausible to have a contrasting adjective meaning 'smaller' or 'lesser'. Option 3, 'me-u-jo-ha' (fh|), translates to 'smaller' or 'lesser', providing a perfect antonym to 'me-zo-ha'. This creates a logical pair of descriptions for different sizes of the 'ARM' (weapons) being cataloged. The grammatical classification of 'me-u-jo-ha' as an adjective also aligns with the role of 'me-zo-ha'. The other options do not provide such a clear and logical semantic relationship within an inventory context.\"}\n",
      "Correct: 6/13\n",
      "\n",
      "SEQ_IDX: 3449\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'KO', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], [], ['common', 'adjective', 'thematic in -o'], []], 'brute': ['me-*86-jo-ha', 'me-mu-jo-ha', 'me-na-jo-ha', 'me-u-jo-ha', 'me-wi-jo-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM KO O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an inventory, starting with 'ARM 1 me-zo-ha' (arms, 1, larger). In such administrative contexts, it is common to list items by contrasting qualities, such as size. Option 4, 'me-u-jo-ha', corresponds to Ancient Greek 'fh|' (meuioha/meiona), which translates to 'smaller' or 'lesser' (from *). This creates a perfect semantic contrast with 'me-zo-ha' (larger), indicating different sizes of arms being inventoried. Grammatically, both 'me-zo-ha' and 'me-u-jo-ha' are classified as adjectives, fitting the descriptive role. The presence of the digamma (f) in 'me-u-jo-ha' is also consistent with Mycenaean phonology. The other options do not provide a clear Ancient Greek correspondence or a semantically coherent meaning that fits the inventory context as well as 'smaller' does.\"}\n",
      "Correct: 7/14\n",
      "\n",
      "SEQ_IDX: 3455\n",
      "INPUT: {'word_list': ['we-ne-e-ne-mi', 'i-je-re-ja', 'ko-wa', 'se-re-ne', 'BRUTE_ME'], 'classifications': [[], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], ['common', 'noun', 'athematic'], [], [], []], 'brute': ['e-do-si', 'e-ko-si', 'e-o-si', 'e-pe-si', 'e-po-si']}\n",
      "FORM: we-ne-e-ne-mi i-je-re-ja ko-wa se-re-ne e-ko-si\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence describes a list of entities: 'i-je-re-ja' (priestess), 'ko-wa' (girl/maiden), and 'se-re-ne' (Selene, the goddess). These three nouns, when combined, form a plural subject. The missing word 'BRUTE_ME' ends with the syllabogram '-si', which, according to the provided grammatical information, is the characteristic ending for 3rd plural verbs in Linear B. Among the given options, 'e-ko-si' is translated into Ancient Greek as '' (echousi), meaning 'they have'. This is a highly plausible verb in the context of Linear B administrative texts, which often record possessions or inventories. While the classification for 'e-ko-si' in the input data states 'noun', the explicit Ancient Greek translation '' is unequivocally a 3rd plural verb, and the instructions prioritize the Ancient Greek correspondence for understanding the word. The other options, 'e-do-si' ('they give') and 'e-o-si' ('they are'), are also grammatically possible 3rd plural verbs, but their Ancient Greek translations are not explicitly provided beyond their Linear B transliteration, suggesting less certainty or commonality compared to ''. Therefore, 'e-ko-si' provides the most semantically coherent, grammatically consistent (as a verb), and contextually plausible completion for the sentence, forming a statement like '[something] priestess, girl, Selene have/possess'.\"}\n",
      "Correct: 8/15\n",
      "\n",
      "SEQ_IDX: 3458\n",
      "INPUT: {'word_list': ['ARM', '1', 'me-zo-ha', 'O', 'NUM', 'BRUTE_ME', 'O', 'NUM', 'ko-ru-to', 'O', 'NUM', 'PA', '2'], 'classifications': [[], [], ['common', 'adjective', 'thematic in -a'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute': ['me-u-da-ha', 'me-u-jo-ha', 'me-u-no-ha', 'me-u-re-ha', 'me-u-to-ha']}\n",
      "FORM: ARM 1 me-zo-ha O NUM me-u-jo-ha O NUM ko-ru-to O NUM PA 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence describes an inventory of 'arms' (ARM) and 'helmets' (ko-ru-to). The word 'me-zo-ha' () means 'larger'. In an inventory context, it is highly semantically coherent and contextually plausible to have a contrasting adjective like 'smaller'. Option 2, 'me-u-jo-ha', translates to 'fh|' (h/), which means 'smaller' or 'lesser'. This creates a logical pair of descriptions ('larger' and 'smaller') for items in an administrative record. The other options do not provide a clear or plausible Ancient Greek translation, making them less suitable for semantic coherence and linguistic authenticity. Grammatically, both 'me-zo-ha' and 'me-u-jo-ha' are classified as adjectives, fitting the descriptive role in the sequence. The slight discrepancy in thematic inflection (-a vs. -o) is mitigated by the fact that they are separated by 'O NUM', suggesting they might modify different implied nouns or categories of items, and the Ancient Greek translation provided for 'me-u-jo-ha' also ends in -a, which is consistent with neuter plural, similar to 'me-zo-ha'.\"}\n",
      "Correct: 9/16\n",
      "\n",
      "SEQ_IDX: 3464\n",
      "INPUT: {'word_list': ['qe-to', '*203VAS', 'NUM', 'di-pa', 'me-zo-e', 'qe-to-ro-we', '*202VAS', '1', 'di-pa-e', 'me-zo-e', 'BRUTE_ME', '*202VAS', '2', 'di-pa', 'me-wi-jo', 'qe-to-ro-we', '*202VAS', '1', 'di-pa', 'me-wi-jo', 'ti-ri-jo-we', '*202VAS', '1', 'di-pa', 'me-wi-jo', 'a-no-we', '*202VAS', '1'], 'classifications': [['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], ['common', 'adjective', 'athematic'], [], [], [], ['common', 'adjective', 'thematic in -o'], [], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'athematic'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ti-*56-o-we-e', 'ti-da-o-we-e', 'ti-de-o-we-e', 'ti-ja-o-we-e', 'ti-te-o-we-e']}\n",
      "FORM: qe-to *203VAS NUM di-pa me-zo-e qe-to-ro-we *202VAS 1 di-pa-e me-zo-e ti-ri-o-we-e *202VAS 2 di-pa me-wi-jo qe-to-ro-we *202VAS 1 di-pa me-wi-jo ti-ri-jo-we *202VAS 1 di-pa me-wi-jo a-no-we *202VAS 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence is an inventory list detailing various types of di-pa (cups/goblets) by their size and number of handles. The pattern observed is di-pa [size] [handle_type] [quantity]. We have qe-to-ro-we (four-handled), ti-ri-jo-we (three-handled), and a-no-we (handle-less) cups listed with their respective quantities. The BRUTE_ME placeholder appears in the context of di-pa-e me-zo-e BRUTE_ME *202VAS 2, meaning 'larger cups, BRUTE_ME, 2 units'. Given that the quantity is '2', the most semantically and contextually plausible infilling for BRUTE_ME is an adjective meaning 'two-handled'.\\n\\nWhile the standard Mycenaean form for 'two-handled' would typically involve the di- or du-wo- syllabograms (e.g., di-wo-we), none of the provided options contain these. All options begin with ti-. However, among the given choices, ti-de-o-we-e is the most plausible candidate, assuming a less common or dialectal phonetic representation where ti-de might correspond to the prefix for 'two' (/-). Despite the phonetic mapping challenges (as ti typically maps to , ,  and de to , , , making a direct - mapping difficult), the strong contextual and numerical evidence points to 'two-handled' as the intended meaning. The suffix -o-we-e aligns with the pattern of other handle descriptors (-o-we). The final -e is consistent with the plural form di-pa-e and me-zo-e.\\n\\nTherefore, prioritizing semantic coherence, grammatical consistency within the established pattern of the inventory list, and contextual plausibility over a strict one-to-one phonetic mapping for this specific problematic segment, Option 3 provides the most logical completion of the sequence.\"}\n",
      "Correct: 9/17\n",
      "\n",
      "SEQ_IDX: 3466\n",
      "INPUT: {'word_list': ['to-pe-za', 'ra-e-ja', 'me-no-e-ja', 'e-re-pa-te', 'a-ja-me-na', 'qe-qi-no-to', 'au-de-pi', 'ko-ru-pi-qe', '1', 'e-ne-wo', 'pe-za', 'to-pe-za', 'ra-e-ja', 'a-pi-qo-to', 'e-re-pa-te-jo', 'BRUTE_ME', 'e-ka-ma-te-qe', 'qe-qi-no-to-qe', 'to-qi-de'], 'classifications': [['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -o'], [], [], [], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'athematic'], [], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['po-ke', 'po-ku', 'po-pi', 'po-po', 'po-ro']}\n",
      "FORM: to-pe-za ra-e-ja me-no-e-ja e-re-pa-te a-ja-me-na qe-qi-no-to au-de-pi ko-ru-pi-qe 1 e-ne-wo pe-za to-pe-za ra-e-ja a-pi-qo-to e-re-pa-te-jo po-pi e-ka-ma-te-qe qe-qi-no-to-qe to-qi-de\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes various types of tables and their features. The phrase in question is e-re-pa-te-jo BRUTE_ME e-ka-ma-te-qe qe-qi-no-to-qe to-qi-de. Based on the provided classifications:\\n    - e-re-pa-te-jo is classified as a common noun, thematic in -o, meaning 'ivory' or 'an ivory thing'.\\n    - e-ka-ma-te-qe is classified as a common adjective, athematic, with '-qe' meaning 'and'.\\n    - qe-qi-no-to-qe is classified as a common adjective, thematic in -o, with '-qe' meaning 'and'. Its Ancient Greek translation  means 'decorated'.\\n    - to-qi-de is classified as a common noun, thematic in -o, meaning 'keel/base'.\\n\\n    The structure suggests that e-re-pa-te-jo BRUTE_ME forms a noun phrase, followed by conjoined adjectives (e-ka-ma-te and qe-qi-no-to) modifying the subsequent noun to-qi-de.\\n\\n    Let's evaluate the options for BRUTE_ME:\\n    - Option 3: po-pi (): Classified as an anthroponym/animal name/theonym, noun, athematic. While it's a noun, its classification as a proper name or animal/theonym makes it semantically implausible to be in apposition to 'an ivory thing' in a description of table parts. The -pi suffix indicates instrumental/locative, which would make it function adverbially ('by means of po' or 'at po'), disrupting the noun phrase structure.\\n    - Option 5: po-ro (): Classified as a common noun, thematic in -o. The Ancient Greek translation  means 'callus, passage, means'. This is highly plausible in the context of describing a table's materials or features. e-re-pa-te-jo po-ro could be interpreted as 'an ivory thing, a callus/passage' or 'an ivory knob/protuberance/part'. This fits perfectly as a descriptive noun in apposition to e-re-pa-te-jo.\\n\\n    The other options (po-ke, po-ku, po-po) lack classification and Ancient Greek translations, making them impossible to evaluate against the criteria.\\n\\n    Therefore, po-ro is the most semantically coherent and grammatically consistent choice, forming a plausible description of a feature of an ivory table within an inventory-like text.\"}\n",
      "Correct: 9/18\n",
      "\n",
      "SEQ_IDX: 3472\n",
      "INPUT: {'word_list': ['o-wi-de', 'phu-ke-qi-ri', 'o-te', 'BRUTE_ME', 'te-ke', 'au-ke-wa', 'da-mo-ko-ro', 'qe-ra-na', 'wa-na-se-wi-ja', 'qo-u-ka-ra', 'ko-ki-re-ja', '*204VAS', '1', 'qe-ra-na', 'a-mo-te-wi-ja', 'ko-ro-no-we-sa', 'qe-ra-na', 'wa-na-se-wi-ja', 'ku-na-ja', 'qo-u-ka-ra', '1', 'to-qi-de-we-sa', '*204VAS', '1'], 'classifications': [['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], ['common', 'adverb', 'athematic'], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'adjective', 'thematic in -a'], [], []], 'brute_classifications': [[], [], [], ['toponym', 'noun', 'thematic in -o'], []], 'brute': ['wa-na-de', 'wa-na-jo', 'wa-na-ko', 'wa-na-so', 'wa-na-to']}\n",
      "FORM: o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro qe-ra-na wa-na-se-wi-ja qo-u-ka-ra ko-ki-re-ja *204VAS 1 qe-ra-na a-mo-te-wi-ja ko-ro-no-we-sa qe-ra-na wa-na-se-wi-ja ku-na-ja qo-u-ka-ra 1 to-qi-de-we-sa *204VAS 1\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence describes an event where 'Phugeqiris saw when BRUTE_ME placed Augeuas, the damokoros', followed by a detailed inventory of vessels. The missing word (BRUTE_ME) must function as the subject of the verb 'te-ke' (placed). Among the given options, 'wa-na-ko' is the most plausible candidate. While 'wa-na-ka' is the more common nominative form for 'wanax' (king) in Linear B, 'wa-na-ko' is a known variant or case form (e.g., genitive). However, in the context of an administrative record where an official ('damokoros') is being 'placed' or appointed, the 'wanax' (king) is the most logical and contextually appropriate agent. Options 1, 2, and 5 ('wa-na-de', 'wa-na-jo', 'wa-na-to') do not readily correspond to a suitable noun or agent in Mycenaean Greek that could perform the action of 'placing' an official. Option 4, 'wa-na-so', is classified as a toponym (place name), which cannot grammatically or semantically be the subject of 'te-ke'. Therefore, 'wa-na-ko' as the 'king' provides the most semantically coherent, grammatically plausible, and contextually authentic interpretation for this administrative record.\"}\n",
      "Correct: 9/19\n",
      "\n",
      "SEQ_IDX: 3476\n",
      "INPUT: {'word_list': ['pa-sa-ro', 'BRUTE_ME', 'a-pi', 'to-ni-jo', '2', 'wa-o', '*232', '2', 'qi-si-pe-e', '*234', '2'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'athematic'], [], [], [], [], [], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ku-ru-e', 'ku-ru-no', 'ku-ru-ta', 'ku-ru-te', 'ku-ru-wa']}\n",
      "FORM: pa-sa-ro ku-ru-so a-pi to-ni-jo 2 wa-o *232 2 qi-si-pe-e *234 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence appears to be an inventory list, a common type of text found in Mycenaean archives. The initial word pa-sa-ro is translated as  (psalion), meaning 'bridle' or 'bit'. Although pa-sa-ro is classified as an 'anthroponym/animal name/theonym', the provided Ancient Greek translation  strongly suggests it functions as a common noun referring to an item. In Linear B inventory contexts, it is common to list an item followed by the name of its owner or a descriptor.\\n\\nAmong the given options for BRUTE_ME, ku-ru-no is the most plausible. It can be transliterated as  (Krounos), a known personal name in Ancient Greek. The construction   (bridle Krounos) is a common Mycenaean way to indicate possession, where the owner's name (Krounos) is in the nominative case, implying 'Krounos's bridle'. This fits perfectly within the administrative and inventory nature of Linear B texts.\\n\\nGrammatically, pa-sa-ro is classified as 'thematic in -o'. If ku-ru-no is also interpreted as a thematic -o noun (a common declension for personal names), it maintains grammatical consistency with the preceding word, even though its specific classification is not provided. The other options (ku-ru-e, ku-ru-ta, ku-ru-te, ku-ru-wa) are less readily identifiable as plausible personal names or common nouns that would fit this specific inventory context and grammatical pattern. For instance, ku-ru-e could be a dative form, but it would imply 'for Krounos', which is also plausible but less direct for indicating ownership in such lists compared to the nominative form. However, the dative of a thematic -o noun is typically -o, not -e, suggesting ku-ru-e would need to be an athematic noun, which introduces more assumptions.\\n\\nTherefore, ku-ru-no provides the best semantic coherence, grammatical consistency (as a thematic -o noun in apposition or implying possession), and contextual plausibility for an inventory record.\"}\n",
      "Correct: 9/20\n",
      "\n",
      "SEQ_IDX: 3477\n",
      "INPUT: {'word_list': ['ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'BRUTE_ME', 'ka-ru-we-qe', '*220', '1', 'ta-ra-nu-we', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', 'to-qi-de-qe', '*220', 'NUM', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', '*220', '1', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', 'so-we-no-qe', '*220', '1', 'ta-ra-nu', 'a-ja-me-no', 'e-re-pa-te-jo', 'au-de-pi', '*220', '1'], 'classifications': [['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'athematic'], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['common', 'noun', 'athematic'], ['common', 'adjective', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-qi-de-qe', 'i-qi-de-qe', 'o-qi-de-qe', 'ro-qi-de-qe', 'wo-qi-de-qe']}\n",
      "FORM: ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi to-qi-de-qe ka-ru-we-qe *220 1 ta-ra-nu-we a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe to-qi-de-qe *220 NUM ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe *220 1 ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi so-we-no-qe *220 1 ta-ra-nu a-ja-me-no e-re-pa-te-jo au-de-pi *220 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes 'thrones (ta-ra-nu) inlaid (a-ja-me-no) with ivory (e-re-pa-te-jo) and other materials'. The placeholder BRUTE_ME is followed by 'ka-ru-we-qe' (, 'nut/kernel' + 'and'), indicating that BRUTE_ME should also be a material or decorative element, likely in the dative/instrumental case, similar to 'au-de-pi' (, 'with ornaments/handles'). The subsequent entries in the text list other inlay materials like 'so-we-no-qe' (, 'wedges/pins') and 'to-qi-de-qe' (, 'small panels/inlays').\\n\\nAmong the given options, 'a-qi-de-qe' and 'ro-qi-de-qe' are the most semantically plausible.\\n- 'a-qi-de-qe' could correspond to Ancient Greek '' (akides), meaning 'points, splinters, needles'.\\n- 'ro-qi-de-qe' could correspond to Ancient Greek '' (rokides), meaning 'splinters, chips, fragments'.\\n\\nBoth '' and '' are plausible as small decorative elements or inlay materials for furniture. However, '' (chips/fragments) is slightly more specific and fitting for the concept of 'inlay' which typically involves small pieces of material. Both terms are attested in Linear B in contexts related to furniture.\\n\\nWhile the exact dative plural ending (-si) is not explicitly present in 'qi-de' for athematic nouns, Linear B often simplifies or omits final consonants, and the context strongly implies a list of materials used for inlay, which would be in the dative/instrumental case. The classification of 'ro-qi-de-qe' as an athematic noun is consistent with ''.\\n\\nTherefore, 'ro-qi-de-qe' () provides the most coherent semantic fit within the context of describing a throne inlaid with various materials, aligning with the administrative nature of Linear B texts.\"}\n",
      "Correct: 9/21\n",
      "\n",
      "SEQ_IDX: 3478\n",
      "INPUT: {'word_list': ['ta-ra-nu', 'BRUTE_ME', 'e-re-pa-te-jo', 'a-to-ro-qo', 'i-qo-qe', 'po-ru-po-de-qe', 'po-ni-ke-qe', '*220', '1', 'ta-ra-nu'], 'classifications': [['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], ['common', 'noun', 'athematic'], [], [], ['common', 'noun', 'athematic']], 'brute_classifications': [['common', 'adjective', 'thematic in -o'], [], [], [], []], 'brute': ['a-ja-me-no', 'a-ke-me-no', 'a-nu-me-no', 'a-re-me-no', 'a-u-me-no']}\n",
      "FORM: ta-ra-nu a-ja-me-no e-re-pa-te-jo a-to-ro-qo i-qo-qe po-ru-po-de-qe po-ni-ke-qe *220 1 ta-ra-nu\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence describes a 'ta-ra-nu' (footstool/throne) that is 'e-re-pa-te-jo' (ivory) and decorated with 'a-to-ro-qo' (human), 'i-qo-qe' (horse-and), 'po-ru-po-de-qe' (octopus/many-footed-and), and 'po-ni-ke-qe' (phoenix/palm-tree-and) figures. The unknown word 'BRUTE_ME' is classified as a common adjective, thematic in -o, suggesting it describes the footstool's condition or state.\\n\\n1.  Elimination of Options:\\nOption 1: a-ja-me-no ( - lamented, grieved). This meaning is semantically incoherent for describing a footstool.\\nOption 4: a-re-me-no (likely related to  - to please, satisfied). This meaning is semantically incoherent for a footstool.\\nOption 5: a-u-me-no (likely related to  - to increase, grown). This meaning is semantically incoherent for a footstool.\\n\\n2.  Comparison of Plausible Options: This leaves Option 2 (a-ke-me-no) and Option 3 (a-nu-me-no). Both are grammatically consistent as perfect middle/passive participles (ending in -me-no) and fit the thematic -o adjective classification.\\nOption 2: a-ke-me-no (). This participle can derive from two main verbs:\\n (to break, shatter), yielding 'broken'.\\n (to lead, bring), yielding 'led' or 'brought'.\\nOption 3: a-nu-me-no (). This participle derives from  (to accomplish, finish, complete), yielding 'finished' or 'completed'.\\n\\n3.  Contextual and Semantic Analysis:\\nLinear B texts are primarily administrative records, often detailing inventories of goods, their materials, and their condition.\\nThe item described is a valuable 'ivory footstool' with intricate 'human, horse, octopus, and phoenix' decorations.\\nNoting the condition of such a valuable item is crucial for inventory management.\\nBoth 'broken' and 'finished' are plausible descriptors for an item in an inventory.\\nHowever, a-ke-me-no () meaning 'broken' is a very common and significant descriptor in Linear B for damaged items (e.g., chariots, furniture). Explicitly stating that a valuable item is 'broken' provides critical information about its usability, value, and potential need for repair or replacement. While 'finished' is also relevant, 'broken' denotes a specific state of defect or damage that is highly pertinent to administrative records.\\n\\nTherefore, a-ke-me-no (broken) provides the most specific and contextually relevant information for an inventory record of a valuable, decorated ivory footstool, making it the most semantically coherent and plausible choice.\"}\n",
      "Correct: 9/22\n",
      "\n",
      "SEQ_IDX: 3481\n",
      "INPUT: {'word_list': ['po-ro-wi-to-jo', 'i-je-to-qe', 'pa-ki-ja-si', 'do-ra-qe', 'pe-re', 'po-re-na-qe', 'pu-ro', 'BRUTE_ME', 'po-ti-ni-ja', 'AUR', '*215VAS', '1', 'MUL', '1', 'ma-na-sa', 'AUR', '*213VAS', '1', 'MUL', '1', 'po-si-da-e-ja', 'ARG', '*213VAS', '1', 'MUL', '1', 'ti-ri-se-ro-e', 'AUR', '*216VAS', '1', 'do-po-ta', 'ARG', '*215VAS', '1', 'pu-ro'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'verb', 'athematic'], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], []], 'brute': ['a-me', 'a-ra', 'a-re', 'a-sa', 'a-ta']}\n",
      "FORM: po-ro-wi-to-jo i-je-to-qe pa-ki-ja-si do-ra-qe pe-re po-re-na-qe pu-ro a-ke po-ti-ni-ja AUR *215VAS 1 MUL 1 ma-na-sa AUR *213VAS 1 MUL 1 po-si-da-e-ja ARG *213VAS 1 MUL 1 ti-ri-se-ro-e AUR *216VAS 1 do-po-ta ARG *215VAS 1 pu-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence describes a list of offerings to various deities and figures, including Potnia, Manasa, Poseidaia, Trisheros, and Despota. The placeholder BRUTE_ME appears within this list, specifically after 'pu-ro' (Pylos or Puros) and before 'po-ti-ni-ja' (Potnia). This strongly suggests that the missing word is another recipient of offerings, likely a deity or a significant figure. Option 3, 'a-re', is classified as an 'anthroponym/animal name/theonym' and maps directly to the Ancient Greek '' (Ares), a prominent Greek deity. Including Ares in a list of divine recipients is highly semantically coherent and contextually plausible for a Mycenaean administrative record of religious offerings. The grammatical classification as an athematic noun also fits perfectly within such a list. The other options lack clear Ancient Greek equivalents that fit the context or are not classified, making them less suitable.\"}\n",
      "Correct: 9/23\n",
      "\n",
      "SEQ_IDX: 3483\n",
      "INPUT: {'word_list': ['ko-ma-we-te-ja', 'i-je-to-qe', 'pe-re-*82-jo', 'i-pe-me-de-ja-qe', 'di-u-ja-jo-qe', 'do-ra-qe', 'pe-re-po-re-na-qe', 'a', 'pe-re-*82', 'AUR', '*213VAS', '1', 'MUL', '1', 'i-pe-me-de-ja', 'ARG', '*213VAS', '1', 'di-u-ja', 'AUR', '*213VAS', '1', 'MUL', '1', 'pu-ro', 'e-ma-ha', 'a-re-ja', 'AUR', '*216VAS', '1', 'VIR', '1', 'i-je-to-qe', 'di-u-jo', 'do-ra-qe', 'BRUTE_ME', 'po-re-na-qe', 'a-ke', 'di-we', 'ARG', '*213VAS', '1', 'VIR', '1', 'e-ra', 'ARG', '*213VAS', '1', 'MUL', '1', 'di-ri-mi-jo', 'di-wo', 'i-je-we', 'ARG', '*213VAS', '1', 'pu-ro', 'pu-ro'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'verb', 'athematic'], ['toponym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], ['toponym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], [], [], ['common', 'verb', 'athematic'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['e-re', 'o-re', 'pte-re', 'te-re']}\n",
      "FORM: ko-ma-we-te-ja i-je-to-qe pe-re-*82-jo i-pe-me-de-ja-qe di-u-ja-jo-qe do-ra-qe pe-re-po-re-na-qe a pe-re-*82 AUR *213VAS 1 MUL 1 i-pe-me-de-ja ARG *213VAS 1 di-u-ja AUR *213VAS 1 MUL 1 pu-ro e-ma-ha a-re-ja AUR *216VAS 1 VIR 1 i-je-to-qe di-u-jo do-ra-qe pe-re po-re-na-qe a-ke di-we ARG *213VAS 1 VIR 1 e-ra ARG *213VAS 1 MUL 1 di-ri-mi-jo di-wo i-je-we ARG *213VAS 1 pu-ro pu-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 4'}\n",
      "Correct: 9/24\n",
      "\n",
      "SEQ_IDX: 3490\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'e-te-wa-jo', 'O'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-si-ja-du-ja', 'a-si-ja-ni-ja', 'a-si-ja-qi-ja', 'a-si-ja-si-ja', 'a-si-ja-wi-ja']}\n",
      "FORM: a-si-ja-ti-ja e-te-wa-jo O\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence 'BRUTE_ME e-te-wa-jo O' likely represents an administrative record, a common type of Linear B text. 'e-te-wa-jo' (Etewajo) is a known masculine personal name in Mycenaean Greek, and 'O' is a logogram commonly interpreted as 'oil' (). The most semantically and contextually plausible interpretation of the sequence is 'Place, Person, Commodity' or 'Commodity from Place for Person'. All options for BRUTE_ME begin with 'a-si-ja-', strongly suggesting a connection to 'Asia' (). Among the given options, 'a-si-ja-ni-ja' () is the most linguistically authentic and plausible form for a place name or a feminine ethnonym derived from 'Asia' in Mycenaean Greek. This fits the common administrative pattern of specifying the origin of goods or the association of a person with a place. For example, 'Asiania, Etewajo, Oil' would mean 'Oil from Asiania for Etewajo' or 'Etewajo from Asiania's oil'. The other options are less common or less plausible as place names or ethnonyms in Mycenaean Greek (e.g., 'a-si-ja-si-ja' with its reduplication is less likely for a proper noun).\"}\n",
      "Correct: 9/25\n",
      "\n",
      "SEQ_IDX: 3498\n",
      "INPUT: {'word_list': ['au-ke-i-ja-te-we', 'o-pi-de-so-mo', 'ka-tu-ryo', 'di-pte-ra', 'NUM', 'ka-ne-ja', 'wo-ro-ma-ta', 'NUM', 'me-ti-ja-no', 'to-pa', 'ru-de-ha', 'di-pte-ra', '1', 'a-re-se-si', 'e-ru-ta-ra', 'di-pte-ra', 'NUM', 'BRUTE_ME', 'pe-di-ra', '2', 'we-e-wi-ja', 'di-pte-ra', 'NUM', 'wi-ri-no', 'we-ru-ma-ta', 'ti-ri-si', 'ze-u-ke-si', '1', 'wi-ri-no', 'pe-di-ro', 'e-ma-ta', 'NUM', 'e-ra-pe-ja', 'e-pi-u-ru-te-we', 'E', '2', 'a-pe-i-ja', 'u-po', 'ka-ro', 'we-e-wi-ja', '1', 'u-po', 'we-e-wi-ja', 'e-ra-pe-ja', 'E', '1', 'mu-te-we', 'we-re-ne-ja', 'ku'], 'classifications': [[], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], [], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'adjective', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -o'], [], ['common', 'noun', 'athematic'], ['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'athematic'], [], ['common', 'noun', 'thematic in -a'], [], [], [], [], ['common', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -a'], [], [], [], ['common', 'noun', 'thematic in -a'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['wo-di-je-ja', 'wo-di-je-re', 'wo-di-je-ro', 'wo-di-je-u', 'wo-di-je-we']}\n",
      "FORM: au-ke-i-ja-te-we o-pi-de-so-mo ka-tu-ryo di-pte-ra NUM ka-ne-ja wo-ro-ma-ta NUM me-ti-ja-no to-pa ru-de-ha di-pte-ra 1 a-re-se-si e-ru-ta-ra di-pte-ra NUM wo-di-je-ja pe-di-ra 2 we-e-wi-ja di-pte-ra NUM wi-ri-no we-ru-ma-ta ti-ri-si ze-u-ke-si 1 wi-ri-no pe-di-ro e-ma-ta NUM e-ra-pe-ja e-pi-u-ru-te-we E 2 a-pe-i-ja u-po ka-ro we-e-wi-ja 1 u-po we-e-wi-ja e-ra-pe-ja E 1 mu-te-we we-re-ne-ja ku\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an inventory of items, specifically mentioning 'di-pte-ra' (hides/leather) and 'pe-di-ra' (sandals). The phrase 'e-ru-ta-ra di-pte-ra NUM BRUTE_ME pe-di-ra 2' suggests that 'BRUTE_ME' is an adjective or noun describing the 'pe-di-ra' (sandals). The word 'pe-di-ra' is a neuter plural noun (thematic in -a). Among the given options, 'wo-di-je-ja' (Option 1) ends in '-ja', which can correspond to the Ancient Greek suffix '-' or '-'. This suffix is commonly used to form neuter plural adjectives (e.g., ), which would grammatically agree with 'pe-di-ra'. This interpretation ('sandal-like' or 'for sandals') makes the most semantic sense in the context of an inventory listing types of sandals. The other options (wo-di-je-re, wo-di-je-ro, wo-di-je-u, wo-di-je-we) do not form grammatically consistent adjectives or nouns that would agree with 'pe-di-ra' in number and gender, or fit the descriptive context as well.\"}\n",
      "Correct: 10/26\n",
      "\n",
      "SEQ_IDX: 3499\n",
      "INPUT: {'word_list': ['1', 'BRUTE_ME', 'di-pte-ra', 'ai-za', 'pe-di-ro-i', '1'], 'classifications': [[], [], ['common', 'noun', 'thematic in -a'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], ['common', 'noun', 'thematic in -o'], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['mu-te-jo', 'mu-te-o', 'mu-te-pi', 'mu-te-re', 'mu-te-wo']}\n",
      "FORM: 1 mu-te-we di-pte-ra ai-za pe-di-ro-i 1\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The Linear B sequence describes an inventory item: '1 BRUTE_ME leather goat sandals 1'. The known words are di-pte-ra (leather/hide, feminine singular), ai-za (goat, feminine singular), and pe-di-ro-i (sandals, dative plural). The numbers '1' at the beginning and end indicate a count.\\n\\nOption 5, mu-te-wo, is classified as a 'common noun' and 'thematic in -o'. The suffix -wo in Linear B frequently represents the genitive singular ending for thematic nouns (e.g., corresponding to Ancient Greek - or -, often with a preserved digamma). If mu-te-wo is a genitive singular, it would translate to 'of Mutewos' (a person, place, or specific type/material).\\n\\nThis interpretation provides the most semantically coherent and grammatically consistent meaning: '1 (pair of) sandals of Mutewos (person/place/material) made of goat leather'. This structure is highly plausible for an administrative or inventory record in Mycenaean Greek, indicating ownership, origin, or a specific type of item.\\n\\nLet's briefly consider why other options are less suitable:\\n- Option 1 (mu-te-jo): While jo can form nouns or adjectives, its use as a genitive is less common than -wo in this context, and if it's a neuter nominative/accusative, it's less specific in meaning.\\n- Option 2 (mu-te-o): Could be nominative/accusative singular or genitive plural. If nominative/accusative, it's less clear how it modifies the phrase. If genitive plural, 'of Mutewos (plural)', it's possible but less likely than a singular possessor/origin for a single item.\\n- Option 3 (mu-te-pi): The suffix -pi indicates instrumental or locative case. Placing an instrumental/locative noun directly after the count '1' and before the descriptive nouns (di-pte-ra ai-za) makes the grammatical structure awkward and semantically less plausible for an inventory entry.\\n- Option 4 (mu-te-re): Classified as an 'anthroponym/animal name/theonym' and 'athematic noun'. While it could indicate ownership, the genitive case (as in Option 5) is a more direct and common way to express possession in Linear B inventory contexts. The ending -re for an athematic nominative singular anthroponym is less typical for indicating ownership compared to a genitive.\\n\\nTherefore, mu-te-wo as a genitive singular noun best fits the context of an inventory record, providing a clear and plausible relationship (e.g., ownership or origin) for the described item.\"}\n",
      "Correct: 10/27\n",
      "\n",
      "SEQ_IDX: 3503\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'TELA1+TE'], 'classifications': [[], []], 'brute_classifications': [[], [], ['common', 'noun', 'thematic in -o'], [], []], 'brute': ['ka-di-wi-po-ro', 'ka-je-wi-po-ro', 'ka-ra-wi-po-ro', 'ka-si-wi-po-ro', 'ka-te-wi-po-ro']}\n",
      "FORM: ka-ra-wi-po-ro TELA1+TE\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence is 'BRUTE_ME TELA1+TE', where TELA1+TE translates to '' (fabric/cloth). We need to find the most semantically and grammatically coherent infilling for BRUTE_ME.\\n\\nLet's analyze the options by transliterating them using the provided mapping, paying close attention to the rule for double consonants (, ):\\n\\n1.  Option 1: ka-di-wi-po-ro\\nTransliteration: f or f. This does not correspond to a readily recognizable Ancient Greek word.\\n\\n2.  Option 2: ka-je-wi-po-ro\\nTransliteration: f or f. This also does not correspond to a readily recognizable Ancient Greek word.\\n\\n3.  Option 3: ka-ra-wi-po-ro\\nProvided Ancient Greek: f||f. The most common and plausible interpretation is  (kleidophoros), meaning 'key-bearer'. If this is the case, 'key-bearer fabric' ( ) is semantically possible (e.g., fabric for a key-bearer's garment), but perhaps less direct than other possibilities. The classification provided for this option is 'common noun, thematic in -o', which fits .\\n\\n4.  Option 4: ka-si-wi-po-ro\\nTransliteration: Applying the rule for double consonants, ka-si ( + ) strongly suggests  (xi). Thus, ka-si-wi-po-ro transliterates to f (xiphophoros), meaning 'sword-bearer'. This is a well-attested Ancient Greek word.\\nSemantic Coherence: 'Sword-bearer fabric' (f ) makes excellent semantic sense in the context of Mycenaean administrative records, which frequently list items related to military personnel, equipment, or specific roles. This could refer to fabric for a sword-bearer's uniform or a fabric with a sword motif.\\nGrammatical Consistency: f is a common noun, thematic in -o, which aligns with the grammatical patterns expected for such a word.\\n\\n5.  Option 5: ka-te-wi-po-ro\\nTransliteration: f or f. This does not correspond to a readily recognizable Ancient Greek word.\\n\\nConclusion:\\nOption 4, ka-si-wi-po-ro, leads to the Ancient Greek word f (sword-bearer), which forms a highly coherent and plausible phrase (f  - sword-bearer fabric) within the typical administrative and military context of Linear B tablets. The transliteration is robust, adhering to the specific rule for double consonants. While Option 3 provides a plausible word (), its semantic connection to 'fabric' is less direct and compelling than 'sword-bearer'. The instruction to be critical of autoregressive model translations and rely on the mapping reinforces the choice of Option 4, as its derivation is clear and the resulting word is highly relevant.\"}\n",
      "Correct: 10/28\n",
      "\n",
      "SEQ_IDX: 3508\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'O', '1', 'pa-de-we', 'O', '1', 'ka-ru-ke', 'PE', '2', 'KA', '1', 'O', 'NUM', 'te-qi-jo-ne', 'O', '1', 'a-ke-ti-ri-ja-i', 'KA', '1', 'a-ti-mi-te', 'O', '1', 'da-ko-ro-i', 'E', '1', 'di-pte-ra-po-ro', 'RA', '1', 'O', 'NUM', 'ko-ro'], 'classifications': [[], [], [], [], [], [], ['common', 'noun', 'athematic'], [], [], [], [], [], [], [], [], [], ['common', 'noun', 'thematic in -a'], [], [], ['toponym', 'noun', 'athematic'], [], [], [], [], [], ['common', 'noun', 'thematic in -o'], [], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['da-de-we', 'di-de-we', 'me-de-we', 'ta-de-we']}\n",
      "FORM: pa-de-we O 1 pa-de-we O 1 ka-ru-ke PE 2 KA 1 O NUM te-qi-jo-ne O 1 a-ke-ti-ri-ja-i KA 1 a-ti-mi-te O 1 da-ko-ro-i E 1 di-pte-ra-po-ro RA 1 O NUM ko-ro\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The Linear B sequence is an administrative list, typical of Mycenaean texts, enumerating various individuals or items with associated quantities. The placeholder BRUTE_ME appears at the beginning of this list, followed by a logogram and a number (O 1). This structure suggests that BRUTE_ME is likely a noun, specifically an anthroponym (personal name) or a common noun representing an item.\\n\\nAll provided options (da-de-we, di-de-we, me-de-we, ta-de-we) share the common ending -de-we. In Mycenaean Greek, the suffix -we is attested in personal names (e.g., e-we-de-we, a-ka-sa-no-we), often indicating a masculine nominative singular form. This makes all options grammatically plausible as names.\\n\\nHowever, when considering linguistic authenticity and plausibility, 'me-de-we' stands out. The root 'med-' (from which names like Medon, , derive) is well-established in Ancient Greek, meaning 'to rule' or 'to protect'. A name like Medon would be a very plausible anthroponym to appear in an administrative record. While the other options could also be names, 'me-de-we' has a stronger etymological connection to known Greek naming conventions, making it the most contextually and linguistically authentic choice for a personal name in this type of document. The sequence then reads as a list starting with 'Medon' (or a similar name) followed by other entries like 'heralds', 'female practitioners', 'Artemis' (or a place name), and 'leather-carriers', all consistent with an inventory.\"}\n",
      "Correct: 10/29\n",
      "\n",
      "SEQ_IDX: 3512\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'MA', '1', 'KO', '1', 'ME'], 'classifications': [[], [], [], [], [], []], 'brute_classifications': [[], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], []], 'brute': ['ai-wa-ke-ta', 'jo-wa-ke-ta', 'ko-wa-ke-ta', 'ra-wa-ke-ta', 'wo-wa-ke-ta']}\n",
      "FORM: ra-wa-ke-ta MA 1 KO 1 ME\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence `BRUTE_ME MA 1 KO 1 ME` strongly suggests an administrative or inventory record, common in Mycenaean texts. The placeholder `BRUTE_ME` is followed by what appear to be logograms or abbreviations (`MA`, `KO`, `ME`) and numbers (`1`). Among the given options, `ra-wa-ke-ta` is the most plausible infilling because it directly corresponds to the well-attested Mycenaean title `lawagetas` (f), a high-ranking official or military leader. This title fits perfectly within the context of administrative records. The provided Ancient Greek translation `f|` for `ra-wa-ke-ta` further confirms this identification, as the mapping of `ra` to `` and `wa` to `f` (digamma) is consistent with the Linear B syllabary. The classification of `ra-wa-ke-ta` as an 'anthroponym/animal name/theonym' and 'noun' with 'thematic in -a' inflection aligns with the known grammatical behavior of `lawagetas`. The other options do not correspond to known Mycenaean titles or words that would fit as coherently into this administrative context.\"}\n",
      "Correct: 11/30\n",
      "\n",
      "SEQ_IDX: 3516\n",
      "INPUT: {'word_list': ['ka-pa-ti-ja', 'HORD', '1', 'BRUTE_ME', 'GRA', '1', 'LANA', 'NUM'], 'classifications': [['toponym', 'noun', 'thematic in -a'], [], [], [], [], [], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -a'], [], [], [], []], 'brute': ['te-i-ja', 'te-mi-ja', 'te-se-ja', 'te-ti-ja', 'te-wi-ja']}\n",
      "FORM: ka-pa-ti-ja HORD 1 te-ri-ja GRA 1 LANA NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence appears to be an administrative record or inventory, listing commodities associated with 'ka-pa-ti-ja' (Karpathos or a related entity). The structure is 'ka-pa-ti-ja HORD 1 BRUTE_ME GRA 1 LANA NUM', which translates to 'Karpathos: Barley 1, [BRUTE_ME], Wheat 1, Wool NUM'. This suggests that the unknown word BRUTE_ME should either be another commodity or a descriptor/recipient related to the listed items.\\n\\nOption 1, te-i-ja, is translated as  (theia) in Ancient Greek. This word means 'divine', 'goddess', or 'aunt'. The classification provided for te-i-ja is 'anthroponym/animal name/theonym' and 'noun', 'thematic in -a'. This classification strongly supports the interpretation of te-i-ja as a proper noun, likely referring to a deity or a person.\\n\\nIn the context of Mycenaean administrative texts, it is common to find records of offerings to deities or allocations for specific individuals. Therefore, 'Karpathos: Barley 1, (for) Theia, Wheat 1, Wool NUM' makes perfect semantic and contextual sense. 'Theia' could be the recipient of the listed goods, or the goods could be designated 'for Theia' (e.g., an offering). Grammatically, if 'Theia' is a recipient, it would be in the dative case. A thematic -a noun in the dative singular ends in -a, which is consistent with te-i-ja.\\n\\nThe other options (ma-u-qe, o-u-qe, te-u-qe, to-u-qe) are not provided with clear Ancient Greek translations by the model (they are just transliterated), which, according to the critical instruction, suggests they are less likely to be the intended words compared to an option with a recognized Ancient Greek equivalent. This makes Option 1 the most linguistically authentic choice.\\n\\nTherefore, Option 1 provides the most coherent and plausible interpretation for the complete sequence, fitting the typical content and structure of Linear B administrative tablets.\"}\n",
      "Correct: 11/31\n",
      "\n",
      "SEQ_IDX: 3519\n",
      "INPUT: {'word_list': ['ta-we-si-jo-jo', 'BRUTE_ME', 'VIR', 'NUM'], 'classifications': [[], [], [], []], 'brute_classifications': [[], [], [], ['common', 'noun', 'thematic in -a'], []], 'brute': ['ke-ro-i-ja', 'ke-ro-ne-ja', 'ke-ro-pi-ja', 'ke-ro-si-ja', 'ke-ro-wi-ja']}\n",
      "FORM: ta-we-si-jo-jo ke-ro-si-ja VIR NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence `ta-we-si-jo-jo BRUTE_ME VIR NUM` strongly suggests an administrative record counting men. The term `ta-we-si-jo-jo` ends in the genitive singular suffix `-ojo`, indicating possession or association. Option 4, `ke-ro-si-ja`, is classified as a common noun, thematic in -a, and is translated as `|` (gerousia), meaning 'council of elders' or 'senate'. This fits perfectly within the administrative context of Linear B tablets, where records of personnel and groups were common. The phrase can be interpreted as 'X men of the gerousia of Ta-we-si-jo' or 'Ta-we-si-jo's gerousia: X men'. This interpretation is grammatically consistent, semantically coherent, and contextually plausible for Mycenaean Greek. The other options lack a clear Ancient Greek translation or a plausible semantic fit within this specific context of counting 'men'.\"}\n",
      "Correct: 12/32\n",
      "\n",
      "SEQ_IDX: 3531\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'pa-ma-ko', 'jo-qi'], 'classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], [], []], 'brute': ['ai-wa-ra-ka-na-o', 'ko-wa-ra-ka-na-o', 'ro-wa-ra-ka-na-o', 'wa-wa-ra-ka-na-o']}\n",
      "FORM: a-wa-ra-ka-na-o pa-ma-ko jo-qi\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence is BRUTE_ME pa-ma-ko jo-qi.\\n    1.  Interpretation of pa-ma-ko and jo-qi: The prompt provides conflicting information for pa-ma-ko and jo-qi. Their classifications state they are 'anthroponym/animal name/theonym, noun, thematic in -o'. However, the provided Ancient Greek translations are  (pharmakon, 'drug/poison') for pa-ma-ko and | (hoti/hostis, 'that/which/whoever') for jo-qi. Crucially, the jo- prefix is explicitly listed as a 'CRITICAL' relative/comparative prefix (-). The prompt also warns that Ancient Greek correspondences are from an autoregressive model and may repeat characters, implying that the *meaning* or *function* derived from the Linear B and other critical information (like prefixes) should be prioritized over potentially misleading classifications or literal character-by-character translations. Therefore, I interpret pa-ma-ko as a common noun (pharmakon, likely neuter nominative/accusative singular) and jo-qi as a relative pronoun/conjunction.\\n\\n    2.  Grammatical Role of BRUTE_ME: Given pa-ma-ko as 'drug' (a noun), BRUTE_ME would most logically function as a possessive modifier (genitive case) or an adjective. The options for BRUTE_ME all end in -o. According to the provided declension table, an athematic noun in the singular genitive case ends in -o. This fits the structure of the BRUTE_ME options, suggesting they are proper nouns (names) in the genitive case.\\n\\n    3.  Semantic Coherence: The resulting structure [Name in Genitive] [Drug] [Relative Pronoun] (e.g., 'X's drug, which...') forms a semantically coherent phrase, highly plausible for an administrative record in Linear B, which often detail ownership or origin of goods.\\n\\n    4.  Linguistic Authenticity and Selection: All four options for BRUTE_ME (ai-wa-ra-ka-na-o, ko-wa-ra-ka-na-o, ro-wa-ra-ka-na-o, wa-wa-ra-ka-na-o) are plausible as Mycenaean names in the genitive case. Without further specific contextual clues or frequency data for Mycenaean names, distinguishing between them is challenging. However, ko- is a very common initial syllable in attested Mycenaean names (e.g., Ko-ma-we, Ko-to-no). While this is a subtle preference, it provides a slight edge in terms of linguistic authenticity compared to the other options, which are equally plausible but less demonstrably common in their initial syllable. Therefore, Option 2 (ko-wa-ra-ka-na-o) is chosen as the most fitting option.\"}\n",
      "Correct: 12/33\n",
      "\n",
      "SEQ_IDX: 3533\n",
      "INPUT: {'word_list': ['PA', 'NUM', 'BRUTE_ME', 'GRA', 'HA', '1', 'WO', '2', 're-u-ko-to', 'GRA', 'HA', '2', 'PA', '1', 'E', 'NUM', 'a-ro-ka', 'E', 'NUM'], 'classifications': [[], [], [], [], [], [], [], [], ['common', 'adjective', 'thematic in -o'], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [['common', 'noun', 'athematic'], [], [], [], []], 'brute': ['e-ri-ka-we-e', 'pa-ri-ka-we-e', 'po-ri-ka-we-e', 'we-ri-ka-we-e', 'wi-ri-ka-we-e']}\n",
      "FORM: PA NUM e-ri-ka-we-e GRA HA 1 WO 2 re-u-ko-to GRA HA 2 PA 1 E NUM a-ro-ka E NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence appears to be an administrative record or inventory, listing quantities of various commodities, specifically 'GRA' (grain/wheat) and 're-u-ko-to GRA' (white grain). The unknown word 'BRUTE_ME' precedes the first instance of 'GRA', suggesting it functions as a descriptor or type of grain. Option 1, 'e-ri-ka-we-e', is the only candidate for which an Ancient Greek translation, '|' (helices, spirals, or a type of plant/tree), is provided. This translation, referring to a type of plant or a specific form/quality, makes semantic sense in the context of an inventory where different varieties or preparations of grain would be listed. For example, 'helical grain' or 'grain from a helical plant' is a plausible entry. The other options lack Ancient Greek translations, making it impossible to assess their semantic coherence or contextual plausibility. Therefore, 'e-ri-ka-we-e' is the only option that provides a meaningful and grammatically consistent fit within the overall inventory context.\"}\n",
      "Correct: 13/34\n",
      "\n",
      "SEQ_IDX: 3536\n",
      "INPUT: {'word_list': ['A', 'NUM', 'pa-ro', 'BRUTE_ME', 'A', '2', 'pa-ro', 'ru-na', 'A', '1'], 'classifications': [[], [], ['common', 'noun', 'thematic in -o'], [], [], [], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], []], 'brute': ['e-u-ka-jo', 'e-u-ka-mo', 'e-u-ka-ra', 'e-u-ka-ro', 'e-u-ka-ta']}\n",
      "FORM: A NUM pa-ro e-u-ka-no A 2 pa-ro ru-na A 1\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence 'A NUM pa-ro BRUTE_ME A 2 pa-ro ru-na A 1' strongly suggests an administrative or inventory record, a common type of Linear B text. The phrase 'pa-ro X' typically indicates 'from/by X', where X is often a person's name or a place. The presence of 'pa-ro ru-na' (likely 'from/by Runas', a personal name) further supports this interpretation. Among the given options, 'e-u-ka-ro' is explicitly classified as an 'anthroponym/animal name/theonym' and a 'noun' with 'thematic in -o' inflection. This classification perfectly aligns with the grammatical and semantic requirements for a personal name in this context. While the Ancient Greek correspondence '' provided for 'e-u-ka-ro' by the autoregressive model is problematic (as it's an adjective and doesn't map perfectly syllabically), the *classification* of 'e-u-ka-ro' as an anthroponym is the critical piece of information. A plausible Ancient Greek name for 'e-u-ka-ro' consistent with the Linear B mapping rules and thematic -o inflection is '' (Eukharos), meaning 'gracious' or 'charming'. This makes Option 4 the most semantically coherent, grammatically consistent, and contextually plausible choice for infilling the unknown word as a personal name in an administrative record.\"}\n",
      "Correct: 13/35\n",
      "\n",
      "SEQ_IDX: 3542\n",
      "INPUT: {'word_list': ['BRUTE_ME', '*146', 'GRA', 'NUM', 'we'], 'classifications': [[], [], [], [], []], 'brute_classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['re-ke-to', 're-po-to', 're-ra-to', 're-sa-to', 're-u-to']}\n",
      "FORM: re-po-to *146 GRA NUM we\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence involves the logogram 'GRA' (grain/wheat) followed by 'NUM' (number), indicating an administrative record of quantities of grain. Option 2, 're-po-to', translates to '' or '' in Ancient Greek, meaning 'thin', 'fine', or 'light'. When combined with '' (grain), ' ' (fine grain) makes perfect semantic sense in the context of an inventory or record distinguishing different qualities of grain. This is highly plausible for Linear B administrative texts. The classification of 're-po-to' as a 'common noun' (thematic in -o) is consistent with '' functioning as an adjective modifying 'grain' or as a substantivized adjective referring to 'fine grain'. The other options do not provide clear Ancient Greek translations or classifications, making them less semantically coherent and less likely to be linguistically authentic in this context.\"}\n",
      "Correct: 14/36\n",
      "\n",
      "SEQ_IDX: 3548\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'o'], 'classifications': [[], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [[], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['pu-de', 'pu-no', 'pu-ro', 'pu-so', 'pu-ta']}\n",
      "FORM: pu-ro o\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The input sequence BRUTE_ME o with the Ancient Greek hint BRUTE_ME  suggests that the word to be infilled, BRUTE_ME, should be a Linear B word whose Ancient Greek translation ends in '-'. The o in the Linear B sequence and  in the Ancient Greek hint likely refer to the final syllabogram and its corresponding Ancient Greek ending, respectively.\\n\\nLet's evaluate the options based on this:\\n1.  Option 1: pu-de\\nLinear B: pu-de. Ancient Greek: pu-de. This does not end in '-'.\\n2.  Option 2: pu-no\\nLinear B: pu-no. Ancient Greek: pu-no. If this corresponds to  (a type of wood/tree), it ends in '-'. No classification is provided for pu-no.\\n3.  Option 3: pu-ro\\nLinear B: pu-ro. Ancient Greek: |. Both  (Pylos) and  (wheat/fire) end in '-'.\\nClassification for pu-ro: 'anthroponym/animal name/theonym', 'noun', 'thematic in -o'. This classification perfectly matches  (Pylos), which is a proper noun, masculine, and thematic in -o (e.g., Nominative Singular ). Pylos was a major Mycenaean center, making this word highly contextually plausible in Linear B texts.\\n4.  Option 4: pu-so\\nLinear B: pu-so. Ancient Greek: pu-so. If this corresponds to  (a type of plant), it ends in '-'. No classification is provided for pu-so.\\n5.  Option 5: pu-ta\\nLinear B: pu-ta. Ancient Greek: |. Neither  (plant, neuter, ends in -) nor  (plants, neuter plural, ends in -) ends in '-'.\\nClassification for pu-ta: 'common', 'noun', 'thematic in -o'. This is contradictory, as pu-ta ends in 'a', suggesting a thematic in -a declension, not -o.\\n\\nComparing the plausible options (2, 3, and 4), Option 3 (pu-ro) is the strongest. It has a clear and highly relevant Ancient Greek translation ( or ) that fits the '-' ending. Furthermore, the provided classification for pu-ro (thematic in -o) is perfectly consistent with  (Pylos), a common and significant noun in Mycenaean contexts. The other options lack specific Ancient Greek translations or consistent classifications, or are less contextually prominent.\"}\n",
      "Correct: 15/37\n",
      "\n",
      "SEQ_IDX: 3557\n",
      "INPUT: {'word_list': ['e-ke-i-ja', 'NUM', 'pe-di-je-wi-ja', 'NUM', 'BRUTE_ME', '2'], 'classifications': [['common', 'noun', 'thematic in -a'], [], ['common', 'noun', 'thematic in -a'], [], [], []], 'brute_classifications': [[], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], [], []], 'brute': ['a-ka-so-ne', 'a-ko-so-ne', 'a-na-so-ne', 'a-ri-so-ne', 'a-wi-so-ne']}\n",
      "FORM: e-ke-i-ja NUM pe-di-je-wi-ja NUM a-ko-so-ne 2\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': 'The Linear B sequence appears to be an inventory list, typical of Mycenaean texts. The known words e-ke-i-ja (spear) and pe-di-je-wi-ja (foot-strap) are common items, followed by a numerical quantity. Therefore, the missing word BRUTE_ME should logically be another countable item.\\n\\nOption 2, a-ko-so-ne, translates to  (axones), meaning \\'axles\\'. This translation is highly plausible for several reasons:\\n1.  Linguistic Authenticity: The conversion from a-ko-so-ne to  perfectly matches the critical rule provided: \\'Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! E.g. a-ko-so-ne -> \\'. Here, ko-so forms  (ks), leading to . This is a strong indicator of its correctness.\\n2.  Semantic Coherence: \\'Axles\\' are countable items that fit naturally within an inventory alongside \\'spears\\' and \\'foot-straps\\'. The quantity \\'2\\' for axles is also very common, as axles typically come in pairs for vehicles.\\n3.  Contextual Plausibility: Linear B tablets are predominantly administrative records, including inventories of goods and equipment. Listing items like spears, foot-straps, and axles with their quantities is entirely consistent with this historical context.\\n4.  Grammatical Consistency: While the provided classification for a-ko-so-ne lists it as an \\'anthroponym/animal name/theonym\\', the Ancient Greek translation  is clearly a common noun (plural of ). Given the instruction to \\'TAKE ALSO INTO ACCOUNT THE ORIGINAL LINEAR B SEQUENCE UNDERSTAND THE RIGHT WORD\\' when the autoregressive model might repeat characters or provide potentially misleading classifications, the strong linguistic derivation and semantic fit of  as a common noun override the classification discrepancy. It functions grammatically as a plural noun, which is consistent with being counted.\\n\\nThe other options (a-ka-so-ne, a-na-so-ne, a-ri-so-ne, a-wi-so-ne) do not provide clear Ancient Greek translations that fit the context as well, nor do they demonstrate the specific linguistic transformation rule for  as clearly as a-ko-so-ne does.'}\n",
      "Correct: 16/38\n",
      "\n",
      "SEQ_IDX: 3567\n",
      "INPUT: {'word_list': ['1', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [['common', 'noun', 'thematic in -o'], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['ne-o', 'no-o', 'nu-o', 'ta-o', 'te-o']}\n",
      "FORM: 1 wa-o\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The sequence '1 BRUTE_ME' indicates a numeral followed by a noun, likely an item or entity being counted or listed. Option 5, 'te-o', translates to Ancient Greek '' (god), '' (god, acc.), or '' (to gods, dat. pl.). In Linear B tablets, 'te-o' (god) is a very common term, frequently appearing in administrative records related to offerings or allocations to deities. The presence of the numeral '1' before 'te-o' strongly suggests 'one god' or 'one (item) for a god', which is highly consistent with the content and purpose of Mycenaean Linear B texts.\\n\\nIn contrast, Option 1, 'ne-o', translates to '' (temple/ship, dative singular). While '1 temple' or '1 ship' could be plausible in an inventory, the dative case '' ('to a temple/ship') makes it grammatically less direct for a simple count following '1' compared to the nominative/accusative forms of ''/''. Options 2, 3, and 4 lack Ancient Greek translations and classifications, making them impossible to evaluate for semantic and grammatical consistency. Therefore, 'te-o' provides the most semantically coherent, grammatically consistent, and contextually plausible interpretation for this Linear B sequence.\"}\n",
      "Correct: 16/39\n",
      "\n",
      "SEQ_IDX: 3577\n",
      "INPUT: {'word_list': ['o-ze-to', 'ke-sa-do-ro', '*34-to-pi', 'pa-ro', 'a-ke-ha', 'me-ta-pa', 'pe-ri-te', '1', 'a-pi-no-e-wi-jo', 'pa-ro', 'e-ru-si-jo', '1', 'a-pi-no-e-wi-jo', 'pa-ro', 'ai-ki-e-we', 'NUM', 'e-na-po-ro', 'pa-ro', 'wa-do-me-no', 'NUM', 'sa-ri-no-te', 'pa-ro', 'o-wo-to', 'NUM', 'pa-ki-ja-si', 'pa-ro', 'a-ta-no-re', 'NUM', 'ka-ra-do-ro', 'pa-ro', 'to-ro-wo', '1', 'pa-ki-ja-si', 'pa-ro', 'e-ri-we-ro', 'NUM', 'e-wi-te-wi-jo', 'pa-ro', 'BRUTE_ME', '1', 'me-te-to'], 'classifications': [['common', 'verb', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o'], [], ['toponym', 'noun', 'thematic in -a'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['toponym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], [], ['common', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], ['toponym', 'noun', 'athematic'], ['common', 'noun', 'thematic in -o'], [], [], ['ethnonym', 'noun', 'thematic in -o'], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute_classifications': [[], ['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], [], []], 'brute': ['i-sa-to', 'ka-sa-to', 'ko-sa-to', 'ri-sa-to', 'ti-sa-to']}\n",
      "FORM: o-ze-to ke-sa-do-ro *34-to-pi pa-ro a-ke-ha me-ta-pa pe-ri-te 1 a-pi-no-e-wi-jo pa-ro e-ru-si-jo 1 a-pi-no-e-wi-jo pa-ro ai-ki-e-we NUM e-na-po-ro pa-ro wa-do-me-no NUM sa-ri-no-te pa-ro o-wo-to NUM pa-ki-ja-si pa-ro a-ta-no-re NUM ka-ra-do-ro pa-ro to-ro-wo 1 pa-ki-ja-si pa-ro e-ri-we-ro NUM e-wi-te-wi-jo pa-ro wi-sa-to 1 me-te-to\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The Linear B sequence is an administrative record, likely an inventory or list of contributions/allocations. The recurring pattern is 'pa-ro [NAME/PLACE] NUM [ITEM/DESCRIPTION]'. The BRUTE_ME placeholder appears in the context 'pa-ro BRUTE_ME 1 me-te-to'. This strongly suggests that BRUTE_ME should be an anthroponym (person's name) or a toponym (place name), fitting the pattern of the preceding entries like 'pa-ro ai-ki-e-we' (Aigeus) or 'pa-ro a-ta-no-re' (Atanor).\\n\\nOption 2, ka-sa-to, is classified as an 'anthroponym/animal name/theonym' (word type) and 'verb' (part of speech). While the 'verb' classification is contradictory to its likely role as a name, the 'anthroponym' classification aligns perfectly with the contextual pattern. The Ancient Greek translation provided for ka-sa-to is  (Xanthos), which is a well-attested Greek proper name. The Linear B to Ancient Greek mapping ka-sa-to ->  is linguistically plausible, as ka followed by sa can form  ( +  + ).\\n\\nThe other options (i-sa-to, ko-sa-to, ri-sa-to, ti-sa-to) do not have corresponding Ancient Greek translations provided, making it impossible to verify their linguistic authenticity or semantic coherence as names or relevant terms in this context. Without this crucial information, they cannot be confidently selected.\\n\\nTherefore, ka-sa-to (Xanthos) is the most semantically coherent, grammatically consistent (as an anthroponym in this list structure), contextually plausible, and linguistically authentic choice, despite the minor inconsistency in its provided part-of-speech classification.\"}\n",
      "Correct: 16/40\n",
      "\n",
      "SEQ_IDX: 3578\n",
      "INPUT: {'word_list': ['pa-ro', 'BRUTE_ME', 'NUM', 'ro-u-so', 'NUM', 'me-te-to'], 'classifications': [['common', 'noun', 'thematic in -o'], [], [], ['toponym', 'noun', 'thematic in -o'], [], []], 'brute_classifications': [['anthroponym/animal name/theonym', 'verb', 'thematic in -o'], [], [], [], []], 'brute': ['di-do', 'e-do', 'pu-do', 'se-do', 'we-do']}\n",
      "FORM: pa-ro ko-do NUM ro-u-so NUM me-te-to\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence describes an administrative transaction, which is typical for Mycenaean texts. The presence of 'NUM' (numbers) and 'ro-u-so' (a toponym) strongly suggests a record of quantities being moved or assigned to a specific location. Option 1, 'di-do', is classified as a verb and translated as forms of 'to give' (, , ). This fits perfectly into an administrative context. If 'pa-ro' is interpreted as a personal name (as its classification as a 'noun, thematic in -o' suggests, e.g., 'Paros'), then the phrase 'Paros gives [quantity] to Lousos' makes excellent semantic and grammatical sense. The other options (e-do, pu-do, se-do, we-do) lack any provided Ancient Greek translation or classification, making it impossible to assess their semantic coherence or grammatical fit within the sentence. Therefore, 'di-do' is the only option that provides a plausible and coherent meaning for this administrative record.\"}\n",
      "Correct: 16/41\n",
      "\n",
      "SEQ_IDX: 3588\n",
      "INPUT: {'word_list': ['1', 'wa-na-si-ja-ke', '1', 'sa-nu-we', '1', 'wi-ri-ke-ja', '1', 'BRUTE_ME', '1', 'e-ti-ri-ja', '1', 'te-do-ne-ja', '1', 'e-ti-je-ja', '1', 'ne-ka-ta-ta', '1', 'ta-zo-te-ja'], 'classifications': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ko-to-wo-we-i', 'me-to-wo-we-i', 'po-to-wo-we-i', 'ri-to-wo-we-i', 'wi-to-wo-we-i']}\n",
      "FORM: 1 wa-na-si-ja-ke 1 sa-nu-we 1 wi-ri-ke-ja 1 o-to-wo-we-i 1 e-ti-ri-ja 1 te-do-ne-ja 1 e-ti-je-ja 1 ne-ka-ta-ta 1 ta-zo-te-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence presents a list of items, each preceded by the numeral '1', which is characteristic of administrative or inventory records. The surrounding words like `wi-ri-ke-ja` (possibly related to 'weaver') and `wa-na-si-ja-ke` (related to 'queen' or 'lady') appear to be designations of individuals or types of people. Among the given options for `BRUTE_ME`, `ko-to-wo-we-i` (Option 1) is the most semantically and contextually plausible. The Linear B term `ko-to-wo-we` () is well-attested and refers to 'land-holders' or 'possessors of koto-land'. The form `ko-to-wo-we-i` would be a plausible dative singular inflection, meaning 'to the land-holder' or 'for the land-holder'. Including '1 land-holder' in a list of individuals or their roles is highly consistent with the administrative nature of Linear B texts. The other options (`me-to-wo-we-i`, `po-to-wo-we-i`, `ri-to-wo-we-i`, `wi-to-wo-we-i`) do not have clear connections to known Mycenaean vocabulary that would make them suitable designations in such a list, nor do they offer the same level of contextual fit as 'land-holder'.\"}\n",
      "Correct: 16/42\n",
      "\n",
      "SEQ_IDX: 3593\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'NUM', 'ko-ni-ti-ja-ja', 'pe-*65-ka', 'NUM', 'e-to-ki-ja', 'qa-ra-de-ro', 'NUM', 'pa-ke-te-re', 'qa-ra-de-ro', 'NUM'], 'classifications': [[], [], [], [], [], ['common', 'noun', 'thematic in -a'], [], [], ['common', 'noun', 'athematic'], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['pe-ke-ka', 'pe-ko-ka', 'pe-nu-ka', 'pe-re-ka', 'pe-ri-ka']}\n",
      "FORM: pe-*65-ka NUM ko-ni-ti-ja-ja pe-*65-ka NUM e-to-ki-ja qa-ra-de-ro NUM pa-ke-te-re qa-ra-de-ro NUM\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an inventory of items, indicated by the presence of 'NUM' (numerical logogram) and common nouns like 'e-to-ki-ja' (, 'fixtures') and 'pa-ke-te-re' (, 'fasteners' or 'pegs'). In this context, the unknown word 'BRUTE_ME' is expected to be a noun, likely referring to a tool or material.\\n\\nOption 4, 'pe-re-ka', can be plausibly transliterated into Ancient Greek as '' (pelekas), the accusative plural of '' (pelekys), meaning 'axe'. Axes were common tools and frequently listed in Mycenaean administrative records. This interpretation fits perfectly within the semantic field of an inventory of tools and fittings.\\n\\nLet's analyze the full sequence with this choice:\\n' NUM ko-ni-ti-ja-ja  NUM  qa-ra-de-ro NUM  qa-ra-de-ro NUM'\\nThis translates to something like: 'Axes [quantity], ko-ni-ti-ja-ja (possibly 'stone' or 'lime' related) pine wood [quantity], fixtures of type qa-ra-de-ro [quantity], fasteners of type qa-ra-de-ro [quantity].'\\n\\nThis interpretation provides strong semantic coherence and contextual plausibility. The other options ('pe-ke-ka', 'pe-ko-ka', 'pe-nu-ka', 'pe-ri-ka') do not readily correspond to known Ancient Greek words that fit the inventory context as well as 'axe' does. 'pe-ri-ka' could potentially be related to '' (pelix, 'bowl'), but 'axes' aligns more strongly with 'fixtures' and 'fasteners' as practical equipment. The linguistic authenticity is high, as 'pe-re-ka' is a recognized Linear B form for 'axe'.\"}\n",
      "Correct: 16/43\n",
      "\n",
      "SEQ_IDX: 3596\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'jo-pi-do-ja'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['ki-wa-ra-e-ke', 'ki-wa-ra-e-ra', 'ki-wa-ra-e-ri', 'ki-wa-ra-e-ta', 'ki-wa-ra-e-we']}\n",
      "FORM: ki-wa-ra-e-ru jo-pi-do-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence is 'BRUTE_ME jo-pi-do-ja'. Based on research, 'jo-pi-do-ja' is a known toponym (place name) in Linear B, likely referring to 'Opidoia' or 'Ophidoia'. Therefore, the preceding word 'BRUTE_ME' is most likely an item or commodity associated with this place. Among the given options, 'ki-wa-ra-e-ta' can be transliterated as '' (kibarrata). '' (kibarra) is a well-attested Mycenaean Greek word referring to a type of garment or textile. The suffix '-ata' is a common neuter plural ending in Ancient Greek, making '' a grammatically plausible form for 'garments' or 'textiles'. This interpretation ('garments/textiles from/at Opidoia') fits perfectly within the administrative and inventory-keeping context typical of Linear B tablets, demonstrating strong semantic coherence, grammatical consistency, and linguistic authenticity. The other options do not form recognizable or grammatically plausible Mycenaean words that would fit this context as well.\"}\n",
      "Correct: 16/44\n",
      "\n",
      "SEQ_IDX: 3598\n",
      "INPUT: {'word_list': ['NUM', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['a-ta-jo', 'ku-ta-jo', 'me-ta-jo', 'mu-ta-jo', 'u-ta-jo']}\n",
      "FORM: NUM pa-ta-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The sequence `NUM BRUTE_ME` strongly suggests that `BRUTE_ME` should be a noun, likely a proper noun (an anthroponym) or a common noun representing an item being counted. Among the given options, only 'u-ta-jo' (Option 5) is explicitly associated with plausible Ancient Greek names: '' or ''. Linear B tablets are primarily administrative records, and the structure 'NUMBER + NAME' (e.g., 'X units for person Y') is extremely common and contextually plausible. The other options (a-ta-jo, ku-ta-jo, me-ta-jo, mu-ta-jo) are presented as direct transliterations without corresponding known Ancient Greek words, making them less semantically coherent or linguistically authentic in this context. Therefore, 'u-ta-jo' provides the most meaningful and interpretable complete phrase, fitting perfectly within the typical content of Mycenaean texts.\"}\n",
      "Correct: 16/45\n",
      "\n",
      "SEQ_IDX: 3610\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], ['common', 'noun', 'thematic in -o'], [], [], []], 'brute': ['de-mi-na-jo', 'de-mi-ni-jo', 'de-mi-nu-jo', 'de-mi-ri-jo', 'de-mi-ti-jo']}\n",
      "FORM: de-mi-ni-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"Option 2, 'de-mi-ni-jo', is the only choice that provides a clear and well-attested Ancient Greek translation, '' (demnion), meaning 'bed' or 'couch'. This word is a common noun and fits the 'thematic in -o' inflection, consistent with the provided classification. Linear B texts are primarily administrative records, and items like beds or couches are frequently inventoried, making '' contextually plausible and linguistically authentic for Mycenaean Greek. The other options lack a specific Ancient Greek translation or classification, making it impossible to verify their semantic coherence, grammatical consistency, or linguistic authenticity.\"}\n",
      "Correct: 17/46\n",
      "\n",
      "SEQ_IDX: 3614\n",
      "INPUT: {'word_list': ['VIN', 'BRUTE_ME'], 'classifications': [[], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['e-pa-wa-i', 'e-te-wa-i', 'e-wa-wa-i', 'e-we-wa-i', 'e-wo-wa-i']}\n",
      "FORM: VIN e-ti-wa-i\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The sequence 'VIN BRUTE_ME' translates to 'wine [something]'. In Linear B administrative texts, wine entries are frequently followed by a descriptor indicating its origin, type, or destination. Among the given options, 'e-te-wa-i' is a known and attested Linear B word that appears in contexts related to wine (e.g., in the Knossos tablets, specifically KN Fp 1.1). It is generally interpreted as a place name or an adjective derived from a place name, likely in the dative or locative case, meaning 'at Etewa' or 'for Etewa'. This interpretation provides excellent semantic coherence ('wine for/from Etewa') and contextual plausibility for an administrative record. The other options ('e-pa-wa-i', 'e-wa-wa-i', 'e-we-wa-i', 'e-wo-wa-i') do not correspond to known Mycenaean wine descriptors or place names in this specific context, making them less linguistically authentic and contextually plausible. The grammatical function of 'e-te-wa-i' as an adverbial modifier (locative/dative) to 'VIN' is consistent with Mycenaean syntax.\"}\n",
      "Correct: 17/47\n",
      "\n",
      "SEQ_IDX: 3623\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], ['common', 'noun', 'thematic in -o']], 'brute': ['o-ko', 'pa-ko', 'pu-ko', 'ti-ko', 'to-ko']}\n",
      "FORM: to-ko\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"The task requires selecting the most semantically and contextually sensible translation. Among the given options, only 'to-ko' (Option 5) is provided with a classification (common noun, thematic in -o) and a plausible Ancient Greek translation (''). '' (tokos) means 'birth, offspring, interest' and is a well-attested word in Ancient Greek, fitting the common administrative and economic contexts of Linear B texts. Without any surrounding Linear B sequence, the choice must rely on the inherent plausibility and provided linguistic information of the infilling options. The other options lack any such information, making it impossible to assess their semantic or grammatical coherence. Therefore, 'to-ko' is the only option that can be evaluated as a linguistically authentic and plausible Mycenaean word.\"}\n",
      "Correct: 18/48\n",
      "\n",
      "SEQ_IDX: 3625\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ke-re-te-u-jo-no', 'ke-re-te-u-ko-no', 'ke-re-te-u-ne-no', 'ke-re-te-u-ta-no', 'ke-re-te-u-wo-no']}\n",
      "FORM: ke-re-te-u-ti-no\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"Without any contextual information, grammatical classifications, or surrounding words, it is impossible to apply the criteria of semantic coherence, grammatical consistency, contextual plausibility, or overall meaning. All five options represent syntactically valid Linear B sequences. Therefore, the selection must rely solely on the general linguistic authenticity and plausibility of the Linear B sequence itself. Option 1, 'ke-re-te-u-jo-no', is chosen as the most plausible due to the common occurrence of the 'jo' syllabogram in Mycenaean Greek, often forming patronymics or derivatives (e.g., -). While this is a weak justification without further context, it represents the most reasonable choice given the severe lack of information.\"}\n",
      "Correct: 18/49\n",
      "\n",
      "SEQ_IDX: 3630\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'o'], 'classifications': [[], ['common', 'noun', 'thematic in -o']], 'brute_classifications': [['toponym', 'noun', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -a'], ['common', 'adjective', 'thematic in -o']], 'brute': ['to-no', 'to-qe', 'to-ro', 'to-sa', 'to-so']}\n",
      "FORM: to-sa o\n",
      "ANSWER: {'chosen_sequence': 'Option 5'}\n",
      "Correct: 18/50\n",
      "\n",
      "SEQ_IDX: 3634\n",
      "INPUT: {'word_list': ['BRUTE_ME', 'da-phu-ra-zo', 'wa'], 'classifications': [[], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['da-*22-ja', 'da-*22-ma', 'da-*22-ro', 'da-*22-to', 'da-*22-we']}\n",
      "FORM: da-*22-to da-phu-ra-zo wa\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': 'Due to the absence of Ancient Greek translations and word classifications for the infilling options, it is impossible to apply the primary selection criteria of semantic coherence, grammatical consistency, contextual plausibility, and linguistic authenticity as instructed. The syllabogram *22 is also not present in the provided mapping, preventing even a basic transliteration.\\n\\nTherefore, the selection is based on a very limited morphological observation: the ending -ro (as in da-*22-ro) is a common final syllabogram in Linear B words, often found in nouns or adjectives (e.g., forming agent nouns or descriptive adjectives). While this is a weak justification without further context, it represents a plausible morphological structure within the known patterns of Linear B. Without the crucial Ancient Greek translations and classifications, a definitive and well-reasoned choice according to the specified criteria cannot be made.'}\n",
      "Correct: 18/51\n",
      "\n",
      "SEQ_IDX: 3640\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], []], 'brute': ['ku-da-ni', 'ku-do-ni', 'ku-mi-ni', 'ku-pu-ni']}\n",
      "FORM: ku-ja-ni\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Given that the input sequence consists solely of the unknown word 'BRUTE_ME' and no surrounding context, the selection must be based on the linguistic plausibility and known vocabulary of Mycenaean Greek. Among the provided options, 'ku-mi-ni' (Option 3) is the most compelling. When transliterated into Ancient Greek using the provided mapping, 'ku-mi-ni' strongly corresponds to '' (kymni), which is the dative singular form of '' (kyminon), meaning 'cumin'. Cumin was a widely used spice and commodity in the ancient world, and its presence in administrative Linear B texts is well-attested. Similarly, 'ku-do-ni' (Option 2) could correspond to '' (kydni), the dative singular of '' (kydnion, 'quince'), or relate to 'Cydonia' (a place name). While 'quince' and 'Cydonia' are also plausible, 'cumin' is a very direct and common commodity found in such records. Options 1 ('ku-da-ni') and 4 ('ku-pu-ni') do not readily map to well-known Mycenaean Greek words or commodities with the same degree of certainty. Therefore, 'ku-mi-ni' offers the strongest linguistic authenticity and contextual plausibility as a commodity name in a Linear B record.\"}\n",
      "Correct: 18/52\n",
      "\n",
      "SEQ_IDX: 3643\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['a-ja-ni', 'i-ja-ni', 'po-ja-ni', 'ra-ja-ni', 'ri-ja-ni']}\n",
      "FORM: ku-ja-ni\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The task requires selecting the most semantically and contextually plausible word for the unknown Linear B sequence, especially in the absence of surrounding context or grammatical classifications for the options. Linear B texts are predominantly administrative records, frequently listing proper names (anthroponyms or toponyms).\\n\\n1.  Linguistic Authenticity and Plausibility:\\na-ja-ni is a highly plausible Linear B sequence. When transliterated into Ancient Greek, a-ja-ni can correspond to  (Aiani). The syllabogram ja often represents the sound ia in Mycenaean Greek (e.g., a-ja-ko for ).  is the dative singular form of the well-known Greek hero's name  (Ajax). The dative case is common in administrative records to indicate recipients or beneficiaries.\\nThe other options (i-ja-ni, po-ja-ni, ra-ja-ni, ri-ja-ni) do not readily correspond to commonly attested Mycenaean words or prominent Ancient Greek proper nouns or common nouns that would stand alone meaningfully in an administrative context. While some might be theoretically possible,  is a much stronger and more recognizable candidate.\\n\\n2.  Contextual Plausibility: In the absence of any other words, a proper noun (like a personal name) is one of the most common types of entries found as standalone words or initial entries in Linear B tablets. This makes a-ja-ni a very strong candidate.\\n\\n3.  Semantic Coherence: As a standalone word,  (to/for Ajax) is semantically coherent as a record entry. The other options do not offer such clear or common interpretations as standalone words in this context.\\n\\nTherefore, a-ja-ni is the most linguistically authentic and contextually plausible infilling for the unknown word.\"}\n",
      "Correct: 18/53\n",
      "\n",
      "SEQ_IDX: 3655\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], [], []], 'brute': ['a-ka-to', 'a-na-to', 'a-qi-to', 'a-ra-to', 'a-sa-to']}\n",
      "FORM: a-nu-to\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"Option 2, 'a-na-to' corresponding to Ancient Greek '' (blameless), is the most suitable choice. This option perfectly aligns with its provided classification as a 'common' 'adjective' with 'thematic in -o' inflection. '' is indeed a thematic adjective (neuter singular of ). Options 3, 4, and 5 lack Ancient Greek translations and classifications, making them impossible to evaluate against the criteria. Option 1, 'a-ka-to' (), while a plausible word, is classified as an 'anthroponym/animal name/theonym' and 'noun', which is less direct for '' (primarily an adjective) compared to the precise fit of '' as an adjective. Therefore, Option 2 demonstrates the strongest grammatical consistency and linguistic authenticity based on the provided data.\"}\n",
      "Correct: 18/54\n",
      "\n",
      "SEQ_IDX: 3657\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], ['common', 'noun', 'athematic'], []], 'brute': ['a-ru', 'e-ru', 'i-ru', 'ko-ru', 'ro-ru']}\n",
      "FORM: i-ru\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"Option 4, 'ko-ru', is the only choice that provides a clear and meaningful Ancient Greek translation: '' (helmet) or '' (with helmets, instrumental/locative plural). This word is a common noun, and its athematic inflection aligns with the classification provided. The potential form '' also demonstrates the application of the '-pi' instrumental/locative suffix, which is explicitly mentioned as a critical morphological marker in the provided context. Given that Linear B texts often deal with inventories and military equipment, 'helmet' is a highly plausible and contextually appropriate word. The other options ('a-ru', 'e-ru', 'i-ru', 'ro-ru') lack specific Ancient Greek translations or classifications, making them semantically ambiguous and grammatically unverified in this context. Therefore, 'ko-ru' is the most linguistically authentic, semantically coherent, and contextually plausible choice.\"}\n",
      "Correct: 18/55\n",
      "\n",
      "SEQ_IDX: 3658\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ru-ki', 'ru-ro', 'ru-to', 'ru-wo', 'ru-zo']}\n",
      "FORM: ru-i\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The input sequence consists of a single unknown word. To select the best option, I must evaluate which of the proposed Linear B syllabograms forms the most plausible and meaningful Ancient Greek word when transliterated.\\n\\n    1.  Option 1: ru-ki can be transliterated to '' or ''. This could correspond to proper nouns like '' (Lyke) or '' (Lykios), or related to '' (wolf). This is a plausible option.\\n    2.  Option 2: ru-ro can be transliterated to '' or ''. This combination does not immediately suggest a common or well-attested Ancient Greek word.\\n    3.  Option 3: ru-to can be transliterated to '' or '' (or with theta). The most prominent Ancient Greek word derived from '' is '' (lytos), meaning 'loosed, released, soluble'. This is a very common and grammatically sound adjective/noun in Ancient Greek.\\n    4.  Option 4: ru-wo can be transliterated to 'f' or 'f'. The 'f' (digamma) is present in Mycenaean Greek. This could correspond to verbal forms related to '' (lyo, 'I loose') or '' (rheo, 'I flow'), or nouns derived from them. This is also a plausible option.\\n    5.  Option 5: ru-zo can be transliterated to '' or ''. This combination does not immediately suggest a common or well-attested Ancient Greek word.\\n\\n    Comparing the plausible options (1, 3, 4), '' (from ru-to) stands out as a particularly clear, common, and complete word that can function meaningfully on its own as an adjective or a nominalized adjective. Without further context, a descriptive adjective like 'loosed' or 'released' is a very strong candidate for a standalone word in administrative or descriptive texts, which are common in Linear B. The other plausible options are either proper nouns (which are context-dependent) or verbal forms (which often require more context to fully interpret their tense/mood/voice). Therefore, 'ru-to' forming '' offers the most robust semantic and linguistic authenticity as a standalone word.\"}\n",
      "Correct: 18/56\n",
      "\n",
      "SEQ_IDX: 3660\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ta-*22-de-du', 'ta-*22-de-pi', 'ta-*22-de-qa', 'ta-*22-de-qe', 'ta-*22-de-so']}\n",
      "FORM: ta-*22-de-so\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': \"The input sequence consists solely of the unknown word. Therefore, the selection must be based on the internal structure of the proposed infillings, particularly the presence of known and critical suffixes. Option 2, 'ta-*22-de-pi', ends with the suffix '-pi'. According to the provided grammatical information, '-pi' is an instrumental/locative suffix, which is explicitly marked as a 'CRITICAL' morphological marker. This suffix provides a clear grammatical role (instrumental or locative case) to the word, making it a complete and grammatically functional unit on its own. In contrast, options 1, 3, and 5 end with suffixes ('-du', '-qa', '-so') that are not listed as critical morphological markers, making their grammatical function less clear in isolation. Option 4, 'ta-*22-de-qe', ends with '-qe', which is also a critical suffix meaning 'and'. While grammatically valid, a word ending in a conjunction typically implies a connection to another element, making it less of a standalone, self-contained grammatical unit compared to a word with a case ending like '-pi'. Given the lack of external context, the option that provides the most specific and complete grammatical function to the word itself is preferred for its linguistic authenticity and potential for semantic reconstruction.\"}\n",
      "Correct: 18/57\n",
      "\n",
      "SEQ_IDX: 3661\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], []], 'brute': ['ta-*22-de-me', 'ta-*22-de-qe', 'ta-*22-de-ro', 'ta-*22-de-so', 'ta-*22-de-to']}\n",
      "FORM: ta-*22-de-so\n",
      "ANSWER: {'chosen_sequence': 'Option 2', 'reasoning': 'The input sequence consists solely of the placeholder \\'BRUTE_ME\\', and no surrounding context or grammatical classifications for any of the options are provided. This makes it impossible to apply the criteria of semantic coherence, grammatical consistency, contextual plausibility, or linguistic authenticity in a meaningful way. However, the task requires a choice and a reasoning. Among the given options, \\'Option 2: ta-*22-de-qe\\' is the only one that ends with the suffix \\'-qe\\'. The instructions explicitly state that \\'-qe\\' is a \\'CRITICAL\\' suffix, a \\'key morphological marker\\' that is a \\'conjunction suffix meaning \"and\"\\'. While the meaning of the root \\'ta-*22-de-\\' remains unknown, the presence of this critical suffix suggests a clear grammatical function (conjunction) that is plausible within the structure of Linear B texts, which often involve lists or coordinated elements. Without any other distinguishing information, the presence of a recognized and critical morphological marker provides the most defensible, albeit limited, basis for selection.'}\n",
      "Correct: 18/58\n",
      "\n",
      "SEQ_IDX: 3669\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['common', 'adjective', 'thematic in -o'], [], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o']], 'brute': ['a-ka-to', 'a-ko-to', 'a-na-to', 'a-ne-to', 'a-nu-to']}\n",
      "FORM: a-nu-to\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The task requires selecting the most semantically and contextually plausible single word. Let's evaluate each option based on the provided mapping rules and classifications:\\n\\n1.  Option 1: a-ka-to ->  (agathos)\\nMapping: a -> , ka ->  (from ka's possible characters , , ), to ->  (from to's possible characters , , , ). The final  is typically omitted in Linear B. This mapping is excellent and phonetically very close.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'good' and can function as a proper name (Agathos) or a substantive ('the good one'). This is highly plausible for a Linear B record, which often lists names or describes items/people.\\n\\n2.  Option 2: a-ko-to -> | (aktor/achthon)\\nMapping: a -> , ko ->  (from ko's possible characters , , , , ), to ->  (from to's possible characters , , , ). The final  or  is omitted. This mapping is also excellent.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'leader' or 'commander', a very plausible name or title in an administrative context.  ('burden') is less likely as a name.\\n\\n3.  Option 3: a-na-to ->  (anaiton)\\nMapping: a -> , na -> , , to -> , . The  in  is not directly represented by any syllabogram in a-na-to. While Linear B can be ambiguous, a missing vowel like this makes the mapping less direct compared to other options. a-na-to would more directly map to .\\nClassification: Common, adjective, thematic in -o.  means 'blameless'. While a common adjective is plausible, the mapping is weaker.\\n\\n4.  Option 4: a-ne-to -> a-ne-to\\nMapping: No Ancient Greek translation or classification is provided, making it impossible to evaluate against the criteria. This option is immediately disadvantaged.\\n\\n5.  Option 5: a-nu-to ->  (anytos)\\nMapping: a -> , nu -> , , to -> , . The final  is omitted. This mapping is excellent.\\nClassification: Anthroponym/animal name/theonym, noun, thematic in -o.  means 'unaccomplished' or 'useless'. While technically possible as a name, its negative connotation makes it less likely to be a common anthroponym compared to 'good' or 'leader'.\\n\\nConclusion:\\nOptions 3 and 4 are less plausible due to mapping issues (Option 3) or lack of information (Option 4). This leaves Options 1, 2, and 5. Among these, Option 5 ( - 'useless') is less likely as a common name given its meaning.\\n\\nBoth Option 1 ( - 'good') and Option 2 ( - 'leader') are highly plausible as names or titles in Linear B administrative contexts, and both have excellent phonetic mappings. However,  is a very fundamental and broadly applicable concept ('good', 'noble') and a common root for names, making it a highly versatile and contextually plausible single word entry in an administrative record.  is also strong, but  feels slightly more general and universally applicable in the absence of further context. Therefore, a-ka-to corresponding to  is chosen as the most semantically and contextually plausible option.\"}\n",
      "Correct: 18/59\n",
      "\n",
      "SEQ_IDX: 3670\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], ['common', 'noun', 'thematic in -a'], [], []], 'brute': ['e-ke-ja', 'e-ni-ja', 'e-ra-ja', 'e-ta-ja', 'e-wi-ja']}\n",
      "FORM: e-ko-ja\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"Option 3, 'e-ra-ja' (Ancient Greek: ), is the only option that provides a recognized and meaningful Ancient Greek word. The other options ('e-ke-ja', 'e-ni-ja', 'e-ta-ja', 'e-wi-ja') are presented with Ancient Greek translations that are merely transliterations of their Linear B forms, suggesting they do not correspond to commonly recognized Ancient Greek words in this context. '' (eraia) means 'lovely', 'desirable', or 'beloved', which is a plausible word to appear in Mycenaean texts, often dealing with descriptions of items or individuals. Although classified as a noun, '' is an adjective that can function substantively (e.g., 'the beloved one'), and its thematic -a inflection is consistent with the provided grammatical information. This makes it the most linguistically authentic and semantically coherent choice among the given options, as it is the only one that forms a meaningful unit in Ancient Greek.\"}\n",
      "Correct: 18/60\n",
      "\n",
      "SEQ_IDX: 3671\n",
      "INPUT: {'word_list': ['ra-tu-ko', '1', 'BRUTE_ME'], 'classifications': [[], [], []], 'brute_classifications': [['common', 'noun', 'thematic in -a'], ['common', 'adverb', 'athematic'], [], [], []], 'brute': ['o-pa', 'o-pi', 'o-ru', 'o-ti', 'o-tu']}\n",
      "FORM: ra-tu-ko 1 o-wo\n",
      "ANSWER: {'chosen_sequence': 'Option 1', 'reasoning': \"The Linear B sequence 'ra-tu-ko 1 BRUTE_ME' strongly suggests an inventory or list, where 'ra-tu-ko' is an item or person, followed by a quantity '1'. In such contexts, the 'BRUTE_ME' position is most likely filled by another noun representing an item or a descriptor. Option 1, 'o-pa', is classified as a common noun and its Ancient Greek translation '|' (meaning 'tools, weapons, equipment' or 'oil-flask') fits perfectly into an administrative inventory context. The structure 'Item A 1, Item B' is common in Linear B tablets. Options 3, 4, and 5 lack Ancient Greek translations and grammatical classifications, making them impossible to evaluate for semantic and grammatical coherence. Option 2, 'o-pi', is classified as an adverb ('' - 'on, upon'), which does not fit grammatically or semantically after a numeral and a noun in an inventory list. Therefore, 'o-pa' is the only option that provides a plausible noun to complete the inventory entry, maintaining semantic coherence and grammatical consistency within the typical context of Linear B texts.\"}\n",
      "Correct: 18/61\n",
      "\n",
      "SEQ_IDX: 3672\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], [], [], [], ['toponym', 'noun', 'thematic in -o']], 'brute': ['wi-na-jo', 'wi-na-o', 'wi-na-qe', 'wi-na-so', 'wi-na-to']}\n",
      "FORM: wi-na-jo\n",
      "ANSWER: {'chosen_sequence': 'Option 5', 'reasoning': \"Option 5, 'wi-na-to' (f), is the only choice that provides a recognized and grammatically classified Ancient Greek word. The classification provided for 'wi-na-to' as a 'toponym' (place name), 'noun', and 'thematic in -o' aligns perfectly with the form 'f', which is a plausible Mycenaean place name. Linear B texts frequently consist of lists of toponyms, making this a highly contextually plausible option. The initial 'f' (digamma) in 'f' is consistent with the 'wi' syllabogram in Linear B, reflecting the preservation of this sound in Mycenaean Greek. The other options provide only direct transliterations of the Linear B syllabograms into Ancient Greek characters, without forming a recognized or classified word, which suggests they are less likely to be correct infillings based on the provided instructions about the autoregressive model's output.\"}\n",
      "Correct: 18/62\n",
      "\n",
      "SEQ_IDX: 3676\n",
      "INPUT: {'word_list': ['di-wi-jo-de', 'di-we', 'ME+RI', '*209VAS+A', '1', 'BRUTE_ME', 'ME+RI', '2'], 'classifications': [['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'athematic'], [], [], [], [], [], []], 'brute_classifications': [[], [], [], [], []], 'brute': ['di-ja-nu-so', 'di-ni-nu-so', 'di-su-nu-so', 'di-u-nu-so', 'di-wi-nu-so']}\n",
      "FORM: di-wi-jo-de di-we ME+RI *209VAS+A 1 di-wo-nu-so ME+RI 2\n",
      "ANSWER: {'chosen_sequence': 'Option 4', 'reasoning': \"The Linear B sequence describes an offering, likely to deities, given the presence of 'di-wi-jo-de' (to the divine/Zeus) and 'di-we' (to Zeus) followed by quantities of 'ME+RI' (honey) in '*209VAS+A' (vessels). The structure 'X 1 BRUTE_ME Y 2' strongly suggests that 'BRUTE_ME' is another recipient of an offering, parallel to Zeus. Among the given options, 'di-u-nu-so' is a well-attested Linear B spelling for the deity Dionysus (). This makes the entire sequence semantically coherent as an offering to Zeus and Dionysus. While the provided classification for 'di-u-nu-so' as 'athematic' might seem to conflict with Dionysus being a thematic noun in Classical Greek, Linear B spellings can sometimes simplify or vary, and the form 'di-u-nu-so' is a known Mycenaean transcription. The strong contextual fit and linguistic authenticity of 'di-u-nu-so' as Dionysus outweigh this minor discrepancy in classification, especially considering the administrative nature of Linear B texts where names in lists might not always strictly adhere to expected case endings if the context is clear. Options 1, 2, 3, and 5 are not standard or attested spellings for Dionysus in Linear B, making them less plausible.\"}\n",
      "Correct: 18/63\n",
      "\n",
      "SEQ_IDX: 3681\n",
      "INPUT: {'word_list': ['BRUTE_ME'], 'classifications': [[]], 'brute_classifications': [[], ['common', 'adjective', 'thematic in -o'], ['anthroponym/animal name/theonym', 'noun', 'thematic in -o'], [], ['common', 'noun', 'thematic in -o']], 'brute': ['i-no', 'me-no', 'o-no', 'wa-no', 'wo-no']}\n",
      "FORM: ko-no\n",
      "ANSWER: {'chosen_sequence': 'Option 3', 'reasoning': \"The task requires selecting the most semantically and grammatically coherent option, while also verifying the Ancient Greek correspondences against the provided Linear B mapping.\\n\\n1.  Elimination of Options 1 and 4: Options 1 (i-no) and 4 (wa-no) lack Ancient Greek translations and classifications, making it impossible to assess their semantic or grammatical fit. They are therefore the weakest candidates.\\n\\n2.  Evaluation of Option 5 (wo-no): The proposed Ancient Greek translation is f (wine). However, when checking this against the Linear B mapping:\\nwo maps to , f, .\\nno maps to , , .\\nThe Ancient Greek word f contains the character . This character  is not present in the possible mappings for either wo or no. This significant discrepancy makes f an inaccurate translation for wo-no based on the provided mapping, despite wine being a plausible commodity in Linear B texts.\\n\\n3.  Evaluation of Option 2 (me-no): The proposed Ancient Greek translations are ||.\\nme maps to , , .\\nno maps to , , .\\nFor  (month/moon):  from me,  from me,  from no. This is a perfect character match.\\nHowever, the classification for me-no is adjective.  (month/moon) is primarily a noun. While  (might/spirit) is also a noun, it would require a final  which is not directly mapped by no. This creates a grammatical inconsistency between the proposed word and its classification.\\n\\n4.  Evaluation of Option 3 (o-no): The proposed Ancient Greek translations are |.\\no maps to , .\\nno maps to , , .\\nFor  (donkey):  from o,  from no,  from no. The final  is a common omission in Linear B transcriptions, as Linear B is a syllabic script that typically does not write final consonants. This is a well-attested Mycenaean word.\\nThe classification for o-no is anthroponym/animal name/theonym, noun, thematic in -o.  (donkey) perfectly fits the 'animal name' and 'noun' classifications, and it is a thematic -o noun. This option demonstrates strong grammatical consistency and linguistic authenticity.\\n\\nConclusion: Option 3 (o-no) is the most consistent choice. Its Ancient Greek translation  (donkey) aligns perfectly with its classification as an 'animal name' and 'noun', and the character mapping is accurate (accounting for the typical omission of final consonants in Linear B). This makes it the most plausible and grammatically sound infilling for a single word in a Mycenaean context, which often deals with inventories of animals or goods.\"}\n",
      "Correct: 18/64\n",
      "\n",
      "Correct: 18/64\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "output_file_path = os.path.join(prefix_path, \"infilling_answers.csv\")\n",
    "correct = 0\n",
    "total = 0\n",
    "prec_idx = -1\n",
    "fail_count = 0\n",
    "switch_key_interval = 10\n",
    "TEST_ONLY = True\n",
    "\n",
    "already_done = set()\n",
    "file_exists = os.path.isfile(output_file_path)\n",
    "\n",
    "# Read existing rows to populate already_done\n",
    "if file_exists:\n",
    "    with open(output_file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            already_done.add(int(row[\"idx\"]))\n",
    "\n",
    "# Prepare test index filtering\n",
    "if TEST_ONLY:\n",
    "    sorted_test_idxs = sorted(test_idxs)\n",
    "    test_idx_set = set(sorted_test_idxs)\n",
    "\n",
    "with open(output_file_path, mode='a', newline='', encoding='utf-8') as output_file:\n",
    "    fieldnames = [\"idx\", \"original_form\", \"correct_word\", \"predicted_word\", \"reasoning\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if file is new\n",
    "    if not file_exists or os.stat(output_file_path).st_size == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i, seq in enumerate(seq_dataset):\n",
    "        idx = seq.sequence.idx\n",
    "\n",
    "        # Skip if already processed\n",
    "        if idx in already_done:\n",
    "            continue\n",
    "\n",
    "        # Skip if TEST_ONLY and not in test set\n",
    "        if TEST_ONLY and idx not in test_idx_set:\n",
    "            continue\n",
    "\n",
    "        lin_b = st.linb_versions[i]\n",
    "        greek = st.greek_versions[i]\n",
    "\n",
    "        # Determine which API key to use\n",
    "        if i % 300 == 0 and i != 0:\n",
    "            time.sleep(60)\n",
    "        key_idx = (i % (switch_key_interval * len(api_keys))) // switch_key_interval\n",
    "        api_key = api_keys[key_idx]\n",
    "\n",
    "        # Delay if the API key changes\n",
    "        if key_idx != prec_idx:\n",
    "            time.sleep(5)\n",
    "        prec_idx = key_idx\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                gemini_answer = make_infill_selection_prompt(lin_b, greek, api_key)\n",
    "                logging.debug(f\"SEQ_IDX: {seq.sequence.idx}\")\n",
    "                logging.debug(f\"INPUT: {lin_b}\")\n",
    "                logging.debug(f\"FORM: {seq.sequence.form}\")\n",
    "                logging.debug(f\"ANSWER: {gemini_answer}\")\n",
    "                print(f\"SEQ_IDX: {seq.sequence.idx}\")\n",
    "                print(f\"INPUT: {lin_b}\")\n",
    "                print(f\"FORM: {seq.sequence.form}\")\n",
    "                print(f\"ANSWER: {gemini_answer}\")\n",
    "\n",
    "                if not gemini_answer or \"chosen_sequence\" not in gemini_answer or not gemini_answer[\"chosen_sequence\"].startswith(\"Option\"):\n",
    "                    continue\n",
    "\n",
    "                chosen_option = int(gemini_answer[\"chosen_sequence\"].split()[1]) - 1\n",
    "                if chosen_option < 0 or chosen_option >= len(lin_b[\"brute\"]):\n",
    "                    continue\n",
    "\n",
    "                for missing_word_idx in seq.sequence.unknown:\n",
    "                    if len(seq.sequence.unknown[missing_word_idx]) > 0:\n",
    "                        break\n",
    "\n",
    "                correct_form = seq.sequence.form.split(\" \")[missing_word_idx]\n",
    "                predicted_form = lin_b[\"brute\"][chosen_option]\n",
    "\n",
    "                if correct_form == predicted_form:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                logging.debug(f\"Correct: {correct}/{total}\\n\")\n",
    "                print(f\"Correct: {correct}/{total}\\n\")\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"idx\": idx,\n",
    "                    \"original_form\": seq.sequence.form,\n",
    "                    \"correct_word\": correct_form,\n",
    "                    \"predicted_word\": predicted_form,\n",
    "                    \"reasoning\": gemini_answer.get(\"reasoning\", \"\")\n",
    "                })\n",
    "\n",
    "                if i % 20 == 0:\n",
    "                    output_file.flush()\n",
    "                fail_count = 0\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if fail_count >= switch_key_interval:\n",
    "                    break\n",
    "                fail_count += 1\n",
    "                logging.debug(\"DEBUG:\", seq.form)\n",
    "                logging.debug(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                print(\"DEBUG: \", seq.form)\n",
    "                print(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                time.sleep(65)\n",
    "\n",
    "logging.debug(f\"Correct: {correct}/{total}\")\n",
    "print(f\"Correct: {correct}/{total}\")\n",
    "if total > 0:\n",
    "    logging.debug(f\"Accuracy: {correct / total:.2%}\")\n",
    "    with open(os.path.join(prefix_path, \"final_accuracy_textInfillingPipeline.txt\"), 'a') as f:\n",
    "        f.write(f\"Correct: {correct}/{total}\")\n",
    "        f.write(f\"Accuracy: {correct / total:.2%}\")\n",
    "else:\n",
    "    logging.debug(\"No sequences processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 20,\n",
       " 22,\n",
       " 30,\n",
       " 32,\n",
       " 43,\n",
       " 50,\n",
       " 60,\n",
       " 61,\n",
       " 63,\n",
       " 72,\n",
       " 74,\n",
       " 80,\n",
       " 84,\n",
       " 107,\n",
       " 113,\n",
       " 115,\n",
       " 124,\n",
       " 125,\n",
       " 127,\n",
       " 132,\n",
       " 134,\n",
       " 135,\n",
       " 149,\n",
       " 150,\n",
       " 154,\n",
       " 160,\n",
       " 164,\n",
       " 169,\n",
       " 176,\n",
       " 180,\n",
       " 202,\n",
       " 211,\n",
       " 212,\n",
       " 217,\n",
       " 223,\n",
       " 227,\n",
       " 229,\n",
       " 238,\n",
       " 240,\n",
       " 247,\n",
       " 253,\n",
       " 255,\n",
       " 257,\n",
       " 265,\n",
       " 269,\n",
       " 275,\n",
       " 290,\n",
       " 292,\n",
       " 296,\n",
       " 313,\n",
       " 316,\n",
       " 318,\n",
       " 319,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 326,\n",
       " 333,\n",
       " 336,\n",
       " 338,\n",
       " 342,\n",
       " 350,\n",
       " 352,\n",
       " 355,\n",
       " 356,\n",
       " 359,\n",
       " 368,\n",
       " 375,\n",
       " 379,\n",
       " 382,\n",
       " 385,\n",
       " 405,\n",
       " 439,\n",
       " 445,\n",
       " 453,\n",
       " 455,\n",
       " 459,\n",
       " 463,\n",
       " 467,\n",
       " 468,\n",
       " 475,\n",
       " 485,\n",
       " 500,\n",
       " 509,\n",
       " 510,\n",
       " 516,\n",
       " 518,\n",
       " 519,\n",
       " 526,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 538,\n",
       " 539,\n",
       " 551,\n",
       " 552,\n",
       " 556,\n",
       " 563,\n",
       " 565,\n",
       " 568,\n",
       " 575,\n",
       " 576,\n",
       " 580,\n",
       " 584,\n",
       " 585,\n",
       " 588,\n",
       " 599,\n",
       " 600,\n",
       " 609,\n",
       " 610,\n",
       " 612,\n",
       " 617,\n",
       " 622,\n",
       " 623,\n",
       " 628,\n",
       " 633,\n",
       " 635,\n",
       " 645,\n",
       " 649,\n",
       " 657,\n",
       " 658,\n",
       " 662,\n",
       " 675,\n",
       " 676,\n",
       " 680,\n",
       " 683,\n",
       " 684,\n",
       " 688,\n",
       " 692,\n",
       " 696,\n",
       " 705,\n",
       " 724,\n",
       " 725,\n",
       " 739,\n",
       " 744,\n",
       " 746,\n",
       " 748,\n",
       " 751,\n",
       " 753,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 766,\n",
       " 771,\n",
       " 773,\n",
       " 774,\n",
       " 782,\n",
       " 788,\n",
       " 790,\n",
       " 793,\n",
       " 808,\n",
       " 810,\n",
       " 811,\n",
       " 817,\n",
       " 834,\n",
       " 836,\n",
       " 838,\n",
       " 840,\n",
       " 846,\n",
       " 861,\n",
       " 870,\n",
       " 874,\n",
       " 878,\n",
       " 879,\n",
       " 881,\n",
       " 885,\n",
       " 892,\n",
       " 894,\n",
       " 897,\n",
       " 900,\n",
       " 901,\n",
       " 905,\n",
       " 910,\n",
       " 915,\n",
       " 916,\n",
       " 920,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 949,\n",
       " 960,\n",
       " 962,\n",
       " 975,\n",
       " 978,\n",
       " 980,\n",
       " 989,\n",
       " 993,\n",
       " 997,\n",
       " 1000,\n",
       " 1003,\n",
       " 1019,\n",
       " 1022,\n",
       " 1023,\n",
       " 1024,\n",
       " 1026,\n",
       " 1036,\n",
       " 1040,\n",
       " 1052,\n",
       " 1055,\n",
       " 1056,\n",
       " 1071,\n",
       " 1078,\n",
       " 1081,\n",
       " 1082,\n",
       " 1084,\n",
       " 1095,\n",
       " 1099,\n",
       " 1104,\n",
       " 1109,\n",
       " 1110,\n",
       " 1114,\n",
       " 1117,\n",
       " 1118,\n",
       " 1122,\n",
       " 1124,\n",
       " 1130,\n",
       " 1137,\n",
       " 1152,\n",
       " 1173,\n",
       " 1174,\n",
       " 1176,\n",
       " 1180,\n",
       " 1183,\n",
       " 1186,\n",
       " 1188,\n",
       " 1193,\n",
       " 1195,\n",
       " 1200,\n",
       " 1201,\n",
       " 1203,\n",
       " 1204,\n",
       " 1205,\n",
       " 1210,\n",
       " 1218,\n",
       " 1223,\n",
       " 1224,\n",
       " 1229,\n",
       " 1231,\n",
       " 1232,\n",
       " 1236,\n",
       " 1246,\n",
       " 1248,\n",
       " 1251,\n",
       " 1253,\n",
       " 1257,\n",
       " 1262,\n",
       " 1269,\n",
       " 1275,\n",
       " 1280,\n",
       " 1297,\n",
       " 1299,\n",
       " 1303,\n",
       " 1304,\n",
       " 1315,\n",
       " 1316,\n",
       " 1327,\n",
       " 1328,\n",
       " 1334,\n",
       " 1336,\n",
       " 1338,\n",
       " 1351,\n",
       " 1352,\n",
       " 1360,\n",
       " 1370,\n",
       " 1380,\n",
       " 1383,\n",
       " 1388,\n",
       " 1389,\n",
       " 1390,\n",
       " 1393,\n",
       " 1395,\n",
       " 1399,\n",
       " 1401,\n",
       " 1402,\n",
       " 1413,\n",
       " 1415,\n",
       " 1416,\n",
       " 1419,\n",
       " 1427,\n",
       " 1428,\n",
       " 1438,\n",
       " 1439,\n",
       " 1447,\n",
       " 1449,\n",
       " 1452,\n",
       " 1455,\n",
       " 1457,\n",
       " 1459,\n",
       " 1468,\n",
       " 1474,\n",
       " 1475,\n",
       " 1480,\n",
       " 1489,\n",
       " 1494,\n",
       " 1497,\n",
       " 1502,\n",
       " 1509,\n",
       " 1513,\n",
       " 1528,\n",
       " 1530,\n",
       " 1535,\n",
       " 1536,\n",
       " 1542,\n",
       " 1544,\n",
       " 1552,\n",
       " 1554,\n",
       " 1561,\n",
       " 1562,\n",
       " 1565,\n",
       " 1572,\n",
       " 1575,\n",
       " 1581,\n",
       " 1588,\n",
       " 1589,\n",
       " 1597,\n",
       " 1598,\n",
       " 1599,\n",
       " 1605,\n",
       " 1606,\n",
       " 1611,\n",
       " 1623,\n",
       " 1627,\n",
       " 1638,\n",
       " 1641,\n",
       " 1643,\n",
       " 1650,\n",
       " 1651,\n",
       " 1652,\n",
       " 1662,\n",
       " 1664,\n",
       " 1667,\n",
       " 1673,\n",
       " 1675,\n",
       " 1678,\n",
       " 1681,\n",
       " 1682,\n",
       " 1687,\n",
       " 1695,\n",
       " 1699,\n",
       " 1701,\n",
       " 1726,\n",
       " 1727,\n",
       " 1730,\n",
       " 1735,\n",
       " 1739,\n",
       " 1743,\n",
       " 1747,\n",
       " 1749,\n",
       " 1751,\n",
       " 1757,\n",
       " 1760,\n",
       " 1761,\n",
       " 1766,\n",
       " 1768,\n",
       " 1776,\n",
       " 1791,\n",
       " 1793,\n",
       " 1802,\n",
       " 1803,\n",
       " 1811,\n",
       " 1814,\n",
       " 1817,\n",
       " 1821,\n",
       " 1828,\n",
       " 1829,\n",
       " 1834,\n",
       " 1838,\n",
       " 1841,\n",
       " 1856,\n",
       " 1859,\n",
       " 1860,\n",
       " 1863,\n",
       " 1865,\n",
       " 1870,\n",
       " 1871,\n",
       " 1874,\n",
       " 1878,\n",
       " 1886,\n",
       " 1888,\n",
       " 1894,\n",
       " 1900,\n",
       " 1901,\n",
       " 1910,\n",
       " 1934,\n",
       " 1939,\n",
       " 1941,\n",
       " 1942,\n",
       " 1945,\n",
       " 1949,\n",
       " 1950,\n",
       " 1953,\n",
       " 1964,\n",
       " 1965,\n",
       " 1968,\n",
       " 1970,\n",
       " 1982,\n",
       " 1994,\n",
       " 1996,\n",
       " 2006,\n",
       " 2010,\n",
       " 2016,\n",
       " 2028,\n",
       " 2036,\n",
       " 2038,\n",
       " 2046,\n",
       " 2047,\n",
       " 2048,\n",
       " 2068,\n",
       " 2069,\n",
       " 2072,\n",
       " 2076,\n",
       " 2078,\n",
       " 2080,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 2094,\n",
       " 2095,\n",
       " 2096,\n",
       " 2100,\n",
       " 2104,\n",
       " 2105,\n",
       " 2109,\n",
       " 2114,\n",
       " 2119,\n",
       " 2129,\n",
       " 2133,\n",
       " 2136,\n",
       " 2139,\n",
       " 2140,\n",
       " 2150,\n",
       " 2158,\n",
       " 2166,\n",
       " 2168,\n",
       " 2169,\n",
       " 2178,\n",
       " 2185,\n",
       " 2190,\n",
       " 2193,\n",
       " 2200,\n",
       " 2206,\n",
       " 2217,\n",
       " 2218,\n",
       " 2228,\n",
       " 2234,\n",
       " 2238,\n",
       " 2242,\n",
       " 2247,\n",
       " 2249,\n",
       " 2265,\n",
       " 2268,\n",
       " 2269,\n",
       " 2280,\n",
       " 2282,\n",
       " 2284,\n",
       " 2293,\n",
       " 2294,\n",
       " 2295,\n",
       " 2297,\n",
       " 2316,\n",
       " 2319,\n",
       " 2322,\n",
       " 2323,\n",
       " 2331,\n",
       " 2332,\n",
       " 2333,\n",
       " 2340,\n",
       " 2342,\n",
       " 2346,\n",
       " 2352,\n",
       " 2354,\n",
       " 2357,\n",
       " 2359,\n",
       " 2369,\n",
       " 2370,\n",
       " 2374,\n",
       " 2375,\n",
       " 2379,\n",
       " 2399,\n",
       " 2401,\n",
       " 2402,\n",
       " 2404,\n",
       " 2418,\n",
       " 2425,\n",
       " 2435,\n",
       " 2446,\n",
       " 2448,\n",
       " 2450,\n",
       " 2459,\n",
       " 2469,\n",
       " 2473,\n",
       " 2477,\n",
       " 2478,\n",
       " 2483,\n",
       " 2484,\n",
       " 2494,\n",
       " 2502,\n",
       " 2505,\n",
       " 2514,\n",
       " 2520,\n",
       " 2521,\n",
       " 2523,\n",
       " 2527,\n",
       " 2531,\n",
       " 2541,\n",
       " 2552,\n",
       " 2561,\n",
       " 2565,\n",
       " 2572,\n",
       " 2573,\n",
       " 2575,\n",
       " 2588,\n",
       " 2596,\n",
       " 2597,\n",
       " 2626,\n",
       " 2628,\n",
       " 2629,\n",
       " 2631,\n",
       " 2634,\n",
       " 2636,\n",
       " 2641,\n",
       " 2642,\n",
       " 2644,\n",
       " 2655,\n",
       " 2658,\n",
       " 2661,\n",
       " 2664,\n",
       " 2668,\n",
       " 2676,\n",
       " 2704,\n",
       " 2706,\n",
       " 2720,\n",
       " 2721,\n",
       " 2733,\n",
       " 2736,\n",
       " 2742,\n",
       " 2743,\n",
       " 2744,\n",
       " 2747,\n",
       " 2760,\n",
       " 2762,\n",
       " 2786,\n",
       " 2792,\n",
       " 2796,\n",
       " 2797,\n",
       " 2805,\n",
       " 2817,\n",
       " 2831,\n",
       " 2832,\n",
       " 2836,\n",
       " 2841,\n",
       " 2846,\n",
       " 2854,\n",
       " 2856,\n",
       " 2857,\n",
       " 2859,\n",
       " 2860,\n",
       " 2862,\n",
       " 2864,\n",
       " 2875,\n",
       " 2876,\n",
       " 2884,\n",
       " 2888,\n",
       " 2902,\n",
       " 2906,\n",
       " 2916,\n",
       " 2918,\n",
       " 2921,\n",
       " 2931,\n",
       " 2941,\n",
       " 2945,\n",
       " 2956,\n",
       " 2960,\n",
       " 2962,\n",
       " 2964,\n",
       " 2968,\n",
       " 2970,\n",
       " 2974,\n",
       " 2979,\n",
       " 2983,\n",
       " 2991,\n",
       " 2993,\n",
       " 3008,\n",
       " 3013,\n",
       " 3022,\n",
       " 3024,\n",
       " 3026,\n",
       " 3030,\n",
       " 3033,\n",
       " 3034,\n",
       " 3039,\n",
       " 3044,\n",
       " 3047,\n",
       " 3065,\n",
       " 3068,\n",
       " 3072,\n",
       " 3082,\n",
       " 3085,\n",
       " 3086,\n",
       " 3088,\n",
       " 3090,\n",
       " 3098,\n",
       " 3101,\n",
       " 3102,\n",
       " 3103,\n",
       " 3105,\n",
       " 3106,\n",
       " 3109,\n",
       " 3112,\n",
       " 3114,\n",
       " 3115,\n",
       " 3118,\n",
       " 3132,\n",
       " 3137,\n",
       " 3141,\n",
       " 3153,\n",
       " 3158,\n",
       " 3161,\n",
       " 3182,\n",
       " 3184,\n",
       " 3185,\n",
       " 3186,\n",
       " 3190,\n",
       " 3191,\n",
       " 3199,\n",
       " 3213,\n",
       " 3219,\n",
       " 3223,\n",
       " 3226,\n",
       " 3237,\n",
       " 3242,\n",
       " 3243,\n",
       " 3245,\n",
       " 3247,\n",
       " 3253,\n",
       " 3255,\n",
       " 3260,\n",
       " 3261,\n",
       " 3264,\n",
       " 3265,\n",
       " 3267,\n",
       " 3270,\n",
       " 3271,\n",
       " 3279,\n",
       " 3284,\n",
       " 3286,\n",
       " 3292,\n",
       " 3294,\n",
       " 3300,\n",
       " 3305,\n",
       " 3315,\n",
       " 3320,\n",
       " 3323,\n",
       " 3324,\n",
       " 3330,\n",
       " 3350,\n",
       " 3356,\n",
       " 3357,\n",
       " 3369,\n",
       " 3370,\n",
       " 3371,\n",
       " 3375,\n",
       " 3379,\n",
       " 3380,\n",
       " 3385,\n",
       " 3386,\n",
       " 3388,\n",
       " 3389,\n",
       " 3395,\n",
       " 3399,\n",
       " 3403,\n",
       " 3406,\n",
       " 3411,\n",
       " 3412,\n",
       " 3420,\n",
       " 3425,\n",
       " 3427,\n",
       " 3428,\n",
       " 3432,\n",
       " 3438,\n",
       " 3442,\n",
       " 3443,\n",
       " 3448,\n",
       " 3449,\n",
       " 3455,\n",
       " 3458,\n",
       " 3464,\n",
       " 3466,\n",
       " 3472,\n",
       " 3476,\n",
       " 3477,\n",
       " 3478,\n",
       " 3481,\n",
       " 3483,\n",
       " 3490,\n",
       " 3498,\n",
       " 3499,\n",
       " 3503,\n",
       " 3508,\n",
       " 3512,\n",
       " 3516,\n",
       " 3519,\n",
       " 3531,\n",
       " 3533,\n",
       " 3536,\n",
       " 3542,\n",
       " 3548,\n",
       " 3557,\n",
       " 3567,\n",
       " 3577,\n",
       " 3578,\n",
       " 3588,\n",
       " 3593,\n",
       " 3596,\n",
       " 3598,\n",
       " 3610,\n",
       " 3614,\n",
       " 3623,\n",
       " 3625,\n",
       " 3630,\n",
       " 3634,\n",
       " 3640,\n",
       " 3643,\n",
       " 3655,\n",
       " 3657,\n",
       " 3658,\n",
       " 3660,\n",
       " 3661,\n",
       " 3669,\n",
       " 3670,\n",
       " 3671,\n",
       " 3672,\n",
       " 3676,\n",
       " 3681]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_classifications(vocab, tasks, model_cls=LinearSVC, max_iter=2000):\n",
    "    classifications =  defaultdict(list)\n",
    "    for task in tasks:\n",
    "        classifier = train_classifier(task, model_cls=model_cls, max_iter=max_iter)\n",
    "        words = [w.form for w in vocab.get_words]\n",
    "        labels = use_classifier(classifier, words)\n",
    "        for word in labels.keys():\n",
    "            classifications[word].append(labels[word])\n",
    "    return classifications\n",
    "    \n",
    "classifications = get_words_classifications(my_voc, TASKS)\n",
    "classifications[\"wa-na-ka-te\"], classifications[\"a-mi-ni-so\"], len(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationsGatherer:\n",
    "    def __init__(self, data_path=os.path.join(prefix_path, \"cognates_final_translation.cog\")):\n",
    "        self.translations = defaultdict(dict)\n",
    "        self.extract_dict_from_tsv(data_path)\n",
    "        \n",
    "        transfer_exp_name = \"final_results_zero_long_term\"\n",
    "        transfer_args = self.luo_model_args(transfer_exp_name, data_path)\n",
    "        loader = self.luo_model_initialize_data(transfer_args)\n",
    "        self.luo_model_run(transfer_args, loader, transfer=True, test=False)\n",
    "\n",
    "        newdata_exp_name = \"final_results_translation_zero_long_term\"\n",
    "        newdata_args = self.luo_model_args(newdata_exp_name, data_path)\n",
    "        self.luo_model_run(transfer_args, loader, transfer=False, test=False)\n",
    "        \n",
    "    def extract_dict_from_tsv(self, tsv_path):\n",
    "        assert os.path.exists(tsv_path), \"The file does not exist\"\n",
    "        with open(tsv_path, newline='', encoding='utf-8') as tsvfile:\n",
    "            reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "            reader.fieldnames = [field.strip() for field in reader.fieldnames]  # Clean headers\n",
    "            assert \"transliterated_linear_b\" in reader.fieldnames, \"The file does not contain the Linear B field\"\n",
    "            assert \"greek\" in reader.fieldnames, \"The file does not contain the Ancient Greek field\"\n",
    "            for row in reader:\n",
    "                key = row[\"transliterated_linear_b\"]\n",
    "                value = row[\"greek\"]\n",
    "                if key and value:\n",
    "                    self.translations[key.strip()][\"dataset\"] = value.strip()\n",
    "\n",
    "    def luo_model_args(self, exp_name, data_path):\n",
    "        log_dir = os.path.join(prefix_path, \"repo_cinese\")\n",
    "        log_dir = os.path.join(log_dir, exp_name)\n",
    "        saved_path = os.path.join(log_dir, \"saved.latest\")\n",
    "        num_lost_words = len(self.translations)\n",
    "        num_known_words = len(set(sum([self.translations[w]['dataset'].split(\"|\") for w in self.translations], [])))\n",
    "        \n",
    "        args = {\n",
    "            \"num_rounds\" : 80, # 25\n",
    "            \"num_epochs_per_M_step\" : 100, # 100\n",
    "            \"saved_path\" : saved_path, #SAVED_PATH,\n",
    "            \"learning_rate\" : 5e-3,\n",
    "            \"num_cognates\" : num_lost_words,\n",
    "            \"inc\" : 75, # changed\n",
    "            \"warm_up_steps\" : 5,\n",
    "            \"capacity\" : (3, ),\n",
    "            \"save_all\" : False,\n",
    "            \"eval_interval\" : 10, # 10\n",
    "            \"check_interval\" : 10,\n",
    "            \"cog_path\" : data_path,\n",
    "            \"char_emb_dim\" : 400, # changed\n",
    "            \"hidden_size\" : 200, # changed\n",
    "            \"num_layers\" : 16, # changed\n",
    "            \"dropout\" : 0.2, # changed\n",
    "            \"universal_charset_size\" : 400,#200,\n",
    "            \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "            \"known_lang\" : \"greek\",#greek\n",
    "            \"norms_or_ratios\" : (1.0, 0.2),#, 0.2),\n",
    "            \"control_mode\" : \"relative\",\n",
    "            \"residual\" : True,\n",
    "            \"reg_hyper\" : 0.5,\n",
    "            \"batch_size\" : num_known_words, # changed\n",
    "            \"momentum\" : 0.9,\n",
    "            \"random\" : False,\n",
    "            \"seed\" : 17,\n",
    "            \"log_level\" : \"DEBUG\",\n",
    "            \"n_similar\" : 10, # changed\n",
    "            \"log_dir\": log_dir,\n",
    "            \"gpu\" : '0',#\"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "            \"evaluation\": False\n",
    "        }\n",
    "\n",
    "        if args[\"gpu\"] is not None:\n",
    "            torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "        \n",
    "        if not args[\"random\"]:\n",
    "            random.seed(args[\"seed\"])\n",
    "            np.random.seed(args[\"seed\"])\n",
    "            torch.manual_seed(args[\"seed\"])\n",
    "        \n",
    "        return args\n",
    "\n",
    "    def luo_model_initialize_data(self, args):\n",
    "        clear_stages()\n",
    "        clear_vocabs()\n",
    "    \n",
    "        build_vocabs(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"])    \n",
    "        loader = LostKnownDataLoader(args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"])\n",
    "        return loader        \n",
    "    \n",
    "    def luo_model_run(self, args, loader, transfer, test=False):\n",
    "        trie = Trie(args[\"known_lang\"])\n",
    "        model = DecipherModelWithFlow(trie, args[\"char_emb_dim\"], args[\"hidden_size\"], args[\"num_layers\"], args[\"dropout\"], args[\"universal_charset_size\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"norms_or_ratios\"], args[\"control_mode\"], args[\"residual\"], args[\"n_similar\"])\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES', False):\n",
    "            model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        ckpt = torch.load(args[\"saved_path\"], weights_only=False)\n",
    "        try:\n",
    "            model.load_state_dict(ckpt['model'])\n",
    "            logging.info(\"Loaded model's checkpoint\")\n",
    "        except RuntimeError as e:\n",
    "            logging.error(e)\n",
    "        \n",
    "        modes = [[\"mle\", None], [\"flow\", False], [\"flow\", True]]\n",
    "        charset = [PAD, SOW, EOW, UNK, EOS] + get_charset(\"greek\")._CHARS \n",
    "        data = loader.entire_batch\n",
    "        dict_key = \"luo_model\"\n",
    "        if transfer:\n",
    "            dict_key += \"_transfer\"\n",
    "            \n",
    "        for i, (mode, edit) in enumerate(modes):\n",
    "            capacity = args[\"capacity\"][0] if mode != \"mle\" else None\n",
    "            model_ret = model(data, mode=mode, num_cognates=args[\"num_cognates\"], edit=edit, capacity=capacity)\n",
    "            \n",
    "            if i == 0:\n",
    "                log_probs = model_ret.log_probs\n",
    "                log_probs = log_probs.permute(2, 0, 1)  # From (19, 33, 1911) to (1911, 19, 33)\n",
    "                predicted_ids = log_probs.argmax(dim=-1)  # shape: (batch_size, seq_len)\n",
    "                reconstructed_dict = {}\n",
    "                for i, seq in enumerate(predicted_ids):\n",
    "                    decoded = \"\".join([charset[idx] for idx in seq if idx != EOW_ID])\n",
    "                    key = data.lost.forms[i]\n",
    "                    self.translations[key][f\"{dict_key}_raw\"] = decoded\n",
    "\n",
    "                    \n",
    "                \n",
    "            output = model_ret.valid_log_probs if mode == 'mle' else model_ret.flow\n",
    "            preds = output.get_best()\n",
    "            for lb_word, gr_word in preds.items():\n",
    "                self.translations[lb_word.form][f\"{dict_key}_{mode}_{edit}\"] = gr_word.form\n",
    "\n",
    "            if test:\n",
    "                test_words = [\"wa-na-ka\", \"wa-na-ka-te\", \"a-mi-ni-so\", \"ko-no-so\"]\n",
    "                for test in test_words:\n",
    "                    print(self.translations[test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tg = TranslationsGatherer()\n",
    "translations = tg.translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for k, v in translations.items():\n",
    "    words = set()\n",
    "    for src, out in v.items():\n",
    "        out = out.split(\"|\")\n",
    "        for w in out:\n",
    "            words.add(w)\n",
    "    res[k] = sorted(list(words))\n",
    "translations = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in my_voc.get_logograms:\n",
    "    log = log.form\n",
    "    translations[log] = [my_voc.logogram_vocab.get(log, log)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.restore_numerals(nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = [250, 5057, 5072, 568, 454, 4901, 5350, 4726, 5344, 4959, 4088, 4611, 4684, 5385]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0nPynQ3avWF"
   },
   "source": [
    "#### Translation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(os.path.join(prefix_path,\".env\"))\n",
    "n_keys = 13\n",
    "# Retrieve the API key\n",
    "api_keys = []\n",
    "for i in range(1, n_keys+1):\n",
    "    api_keys.append(os.getenv(f\"GOOGLE_API_KEY_{i}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 250\n",
    "doc = corpus.get_document(doc_idx)\n",
    "\n",
    "def document_input(document, translations=translations, classifications=classifications):\n",
    "    seq = document.word_list\n",
    "    trans_doc = \" \".join([\"|\".join(translations[s.form]) for s in seq])\n",
    "    class_doc = []\n",
    "    for w in seq:\n",
    "        word_obj = {\"word\": w.form, \"completeness level\": w.completeness.name}\n",
    "        if w.form in classifications:\n",
    "            classes = classifications[w.form]\n",
    "            pos = classes[1]\n",
    "            word_obj[\"part of speech\"] = pos\n",
    "            if pos in {\"noun\", \"adjective\"}:\n",
    "                word_obj[\"noun type\"] = classes[0]\n",
    "                word_obj[\"inflection\"] = classes[2]\n",
    "        class_doc.append(word_obj)\n",
    "    return document.form, trans_doc, class_doc\n",
    "\n",
    "def sequence_input(sequence, translations=translations, classifications=classifications, vocab=my_voc):\n",
    "    seq = sequence.split(\" \")\n",
    "    trans_seq = \" \".join([\"|\".join(translations[s]) for s in seq])\n",
    "    class_seq = []\n",
    "    for w in seq:\n",
    "        w = vocab.get_word(vocab.get_form_idx(w))\n",
    "        word_obj = {\"word\": w.form, \"completeness level\": w.completeness.name}\n",
    "        if w.form in classifications:\n",
    "            classes = classifications[w.form]\n",
    "            pos = classes[1]\n",
    "            word_obj[\"part of speech\"] = pos\n",
    "            if pos in {\"noun\", \"adjective\"}:\n",
    "                word_obj[\"noun type\"] = classes[0]\n",
    "                word_obj[\"inflection\"] = classes[2]\n",
    "        class_seq.append(word_obj)\n",
    "    return trans_seq, class_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation_prompt(linear_b_document, translations, out_classifiers, api_key):\n",
    "    \"\"\"\n",
    "    Prompt for translating complete Linear B documents into Ancient Greek and English.\n",
    "\n",
    "    Args:\n",
    "        linear_b_document: str - Complete Linear B document text\n",
    "        translations: str - Pipe-separated translation options for each word\n",
    "        out_classifiers: list - List of dicts with word classifications and completeness\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with Ancient Greek translation, English translation, and reasoning\n",
    "    \"\"\"\n",
    "\n",
    "    # You'll need to define this mapping between Linear B syllabograms and Greek characters\n",
    "    syllabograms_matching = {\n",
    "      \"a\": [\"\"],\n",
    "      \"e\": [\"\", \"\"],\n",
    "      \"i\": [\"\"],\n",
    "      \"o\": [\"\", \"\"],\n",
    "      \"u\": [\"\"],\n",
    "      \"da\": [\"\", \"\"],\n",
    "      \"de\": [\"\", \"\", \"\"],\n",
    "      \"di\": [\"\", \"\"],\n",
    "      \"do\": [\"\", \"\", \"\"],\n",
    "      \"du\": [\"\", \"\"],\n",
    "      \"dwe\": [\"\", \"f\", \"\"],\n",
    "      \"dwo\": [\"\", \"f\", \"\"],\n",
    "      \"ja\": [\"\", \"\"],\n",
    "      \"je\": [\"\", \"\", \"\"],\n",
    "      \"jo\": [\"\", \"\", \"\"],\n",
    "      \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ma\": [\"\", \"\", \"\"],\n",
    "      \"me\": [\"\", \"\", \"\"],\n",
    "      \"mi\": [\"\", \"\"],\n",
    "      \"mo\": [\"\", \"\"],\n",
    "      \"mu\": [\"\", \"\"],\n",
    "      \"na\": [\"\", \"\"],\n",
    "      \"ne\": [\"\", \"\", \"\"],\n",
    "      \"ni\": [\"\", \"\"],\n",
    "      \"no\": [\"\", \"\", \"\"],\n",
    "      \"nu\": [\"\", \"\"],\n",
    "      \"nwa\": [\"f\", \"\", \"\"],\n",
    "      \"pa\": [\"\", \"\", \"\"],\n",
    "      \"pe\": [\"\", \"\", \"\"],\n",
    "      \"pi\": [\"\", \"\", \"\"],\n",
    "      \"po\": [\"\", \"\", \"\"],\n",
    "      \"pu\": [\"\", \"\", \"\"],\n",
    "      \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "      \"phu\": [\"\", \"\"],\n",
    "      \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "      \"re\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ri\": [\"\", \"\", \"\"],\n",
    "      \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ru\": [\"\", \"\", \"\"],\n",
    "      \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "      \"rai\": [\"\", \"\", \"\"],\n",
    "      \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "      \"sa\": [\"\", \"\"],\n",
    "      \"se\": [\"\", \"\", \"\"],\n",
    "      \"si\": [\"\", \"\"],\n",
    "      \"so\": [\"\", \"\", \"\"],\n",
    "      \"su\": [\"\", \"\"],\n",
    "      \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "      \"te\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ti\": [\"\", \"\", \"\"],\n",
    "      \"to\": [\"\", \"\", \"\", \"\"],\n",
    "      \"tu\": [\"\", \"\", \"\"],\n",
    "      \"tya\": [\"\", \"\", \"\"],\n",
    "      \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"wa\": [\"\", \"f\", \"\"],\n",
    "      \"we\": [\"\", \"f\", \"\"],\n",
    "      \"wi\": [\"\", \"f\"],\n",
    "      \"wo\": [\"\", \"f\", \"\"],\n",
    "      \"za\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ze\": [\"\", \"\", \"\"],\n",
    "      \"zo\": [\"\", \"\", \"\"],\n",
    "      \"ha\": [\"\", \"h\"],\n",
    "      \"ai\": [\"\", \"\"],\n",
    "      \"au\": [\"\", \"\"],\n",
    "      \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "      \"*64\": [\"\", \"\", \"f\"],\n",
    "      \"*65\": [\"\", \"\"],\n",
    "      \"*79\": [\"\", \"\"],\n",
    "      \"*82\": [\"\", \"\", \"f\"]\n",
    "    }\n",
    "    str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"\"\"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\n",
    "    Linear B texts are primarily administrative records found at palatial sites like Knossos, Pylos, and Thebes.\"\"\"\n",
    "\n",
    "    # Syllabogram matching context\n",
    "    syll_matching_context = f\"\"\"Consider this mapping between Linear B syllabograms and Ancient Greek characters: {str_syllabogram_matching}\n",
    "\n",
    "    CRITICAL: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! \n",
    "    Example: a-ko-so-ne   (axes)\"\"\"\n",
    "\n",
    "    # Morphological markers\n",
    "    morphological_markers = \"\"\"CRITICAL MORPHOLOGICAL MARKERS:\n",
    "    \n",
    "    SUFFIXES - Key grammatical markers that preserve relationships:\n",
    "     -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\n",
    "     -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\n",
    "     -de: can be either:\n",
    "       - Negative prefix meaning \"not, on the other side\"\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\n",
    "     -pi: instrumental/locative suffix\n",
    "\n",
    "    PREFIXES - Establish semantic domains and derivational patterns:\n",
    "     po-ro-: - prefix meaning \"before, forward, in front of\"\n",
    "     qe-to-ro-: - numerical prefix meaning \"four\"\n",
    "     we-: - numerical prefix meaning \"six\"; AMBIGUOUS\n",
    "     e-ne-wo-: - numerical prefix meaning \"nine\"\n",
    "     a-pu-: - separative prefix meaning \"from, away from, off\"\n",
    "     jo-: - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\"\"\"\n",
    "\n",
    "    # Create comprehensive grammatical information\n",
    "    def create_grammatical_info():\n",
    "        gramm = ET.Element(\"grammatical_information\")\n",
    "        \n",
    "        # Enhanced declension table\n",
    "        decl_section = ET.SubElement(gramm, \"declension_system\")\n",
    "        decl_intro = ET.SubElement(decl_section, \"introduction\")\n",
    "        decl_intro.text = \"\"\"Mycenaean Linear B declension system based on Ancient Greek patterns. \n",
    "        Columns 1-2: Ancient Greek second declension (thematic -o stems)\n",
    "        Columns 3-4: Ancient Greek first declension (thematic -a stems) \n",
    "        Columns 5-6: Ancient Greek third declension (athematic stems)\n",
    "        Same patterns apply to adjectives: first four columns for first-class adjectives, last two for second-class.\"\"\"\n",
    "        \n",
    "        decl_table = ET.SubElement(decl_section, \"declension_table\")\n",
    "        decl_table.text = '''It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "'''\n",
    "        # Verb system\n",
    "        verbs_section = ET.SubElement(gramm, \"verb_system\")\n",
    "        verb_rules = {\n",
    "            \"third_person_singular\": \"Final syllabogram ending with vowel -e\",\n",
    "            \"third_person_plural\": \"Final syllabogram is -si\",\n",
    "            \"infinitive\": \"Final syllabogram ending with vowel -e (optionally additional -e)\",\n",
    "            \"active_participle\": \"Terminates with -o (singular - suffix in Greek, plural follows athematic pattern, e.g., -o-te   from )\",\n",
    "            \"middle_passive_participle\": \"Terminates with -me-no/-me-na suffixes (Greek -/-/-)\"\n",
    "        }\n",
    "        \n",
    "        for rule_key, rule_text in verb_rules.items():\n",
    "            ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "        # Adjective system\n",
    "        adj_section = ET.SubElement(gramm, \"adjective_system\")\n",
    "        ET.SubElement(adj_section, \"thematic_adjectives\").text = \"Follow same declension patterns as thematic nouns\"\n",
    "        ET.SubElement(adj_section, \"athematic_adjectives\").text = \"Follow athematic noun patterns with variable nominative forms\"\n",
    "        \n",
    "        return ET.tostring(gramm, encoding=\"unicode\")\n",
    "\n",
    "    # Create the main prompt structure\n",
    "    def create_main_prompt():\n",
    "        prompt = ET.Element(\"translation_task\")\n",
    "\n",
    "        # Task description\n",
    "        task_desc = ET.SubElement(prompt, \"task_description\")\n",
    "        task_desc.text = \"\"\"You are translating complete Linear B documents composed of words and logographic signs.\n",
    "Each Linear B word has possible Ancient Greek translation options (separated by |) and grammatical classifications:\n",
    " part_of_speech: grammatical function in {noun, adjective, verb, adverb} (participles labeled as adjectives)\n",
    " noun_type: logical function in {proper, toponym, ethnonym, common}\n",
    " inflection: morphological pattern in {thematic in -o, thematic in -a, athematic}\n",
    " completeness_level: preservation state in {COMPLETE, MOSTLY_COMPLETE, UNCERTAIN, MOSTLY_INCOMPLETE, INCOMPLETE}\n",
    "Your task is to produce complete translations into both Ancient Greek and modern English with detailed linguistic analysis.\n",
    "Please note that the Linear B document can be fragmentary. Fragments will be indicated by a '[...]' separator. Nearby words can be incomplete.\"\"\"\n",
    "\n",
    "        # Selection methodology\n",
    "        methodology = ET.SubElement(prompt, \"selection_methodology\")\n",
    "        \n",
    "        ET.SubElement(methodology, \"grammatical_analysis\").text = \"\"\"Analyze each word's grammatical function as nouns, adjectives, verbs, adverbs, prepositions, etc. Consider morphological patterns and declensional endings.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"syntactic_analysis\").text = \"\"\"Identify sentence structure: subject, predicate, complements. Perform logical analysis of each word's role in the sentence.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"discourse_analysis\").text = \"\"\"Analyze document structure, distinguish different sentences, examine subordinate clauses, and establish logical connections for accurate translation.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"linguistic_authenticity\").text = \"\"\"Prioritize words consistent with attested Mycenaean Greek vocabulary and morphological patterns. Verify plausibility of proposed Mycenaean forms.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"semantic_coherence\").text = \"\"\"Evaluate which translation produces the most meaningful complete sentence, considering the administrative and practical nature of most Linear B texts.\"\"\"\n",
    "\n",
    "        # Output requirements\n",
    "        output_req = ET.SubElement(prompt, \"output_requirements\")\n",
    "        output_req.text = \"\"\"MANDATORY: Respond ONLY with valid JSON containing exactly these fields:\n",
    "        {\n",
    "            \"Ancient Greek translation\": \"complete document translated into Ancient Greek with proper diacritics and error recovery\",\n",
    "            \"English translation\": \"complete document translated into modern English\",\n",
    "            \"reasoning\": \"detailed explanation covering grammatical analysis, logical/syntactic analysis, and subordinate clause analysis that justifies the translation decisions\"\n",
    "        }\n",
    "        No additional text, commentary, or formatting permitted.\"\"\"\n",
    "\n",
    "        # Quality assurance\n",
    "        qa_section = ET.SubElement(prompt, \"quality_assurance\")\n",
    "        qa_section.text = \"\"\"Before finalizing translation:\n",
    "        1. Verify grammatical correctness of Ancient Greek translation with proper case agreements\n",
    "        2. Ensure semantic coherence and contextual appropriateness\n",
    "        3. Confirm reasoning addresses grammatical, logical, and syntactic criteria\n",
    "        4. Validate proper JSON formatting with all required fields\n",
    "        5. Check that both translations capture the administrative/documentary nature of Linear B texts\n",
    "        6. ALWAYS provide complete translations - never leave empty or partial responses\"\"\"\n",
    "\n",
    "        return ET.tostring(prompt, encoding=\"unicode\")\n",
    "\n",
    "    # Training examples section with complete translations\n",
    "    def create_examples_section():\n",
    "        examples = ET.Element(\"examples_section\")\n",
    "        \n",
    "        # Example 1: PY Eb 895+906\n",
    "        example_1 = ET.SubElement(examples, \"example_1\")\n",
    "        sequence = \"a-i-qe-u e-ke-qe ke-ke-me-na ko-to-na ko-to-no-o-ko to-so-de pe-mo GRA T 6\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_1, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_1, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_1, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_1, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"          6\",\n",
    "    \"English translation\": \"Aigeus, the plot owner, who owns a communal plot; so much grain: 6 'T' units of wheat.\",\n",
    "    \"reasoning\": \"Grammatical analysis: a-i-qe-u () is a proper noun in nominative case, anthroponym with thematic inflection. e-ke-qe ( ) contains the verb 'to have' in 3rd person singular plus conjunction 'and'. ke-ke-me-na () is a perfect passive participle agreeing with ko-to-na () in accusative plural. ko-to-no-o-ko () is a compound noun meaning 'plot-holder'. Logical analysis: Subject = Aigeus, Verb = has, Objects = plots (communal) and grain. Syntactic structure follows standard administrative record format typical of Linear B land tenure documents.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 2: PY Ta 711\n",
    "        example_2 = ET.SubElement(examples, \"example_2\")\n",
    "        sequence = \"o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_2, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_2, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_2, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_2, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"-      \",\n",
    "    \"English translation\": \"Phygekles witnessed when the king appointed Augeus as damoklos.\",\n",
    "    \"reasoning\": \"Grammatical analysis: o-wi-de (-) is 3rd person singular aorist of  meaning 'witnessed/saw'. phu-ke-qi-ri () is anthroponym in nominative. o-te () is temporal conjunction 'when'. wa-na-ka () is title 'king' in nominative. te-ke () is 3rd person singular aorist of  'placed/appointed'. au-ke-wa () is anthroponym in accusative. da-mo-ko-ro () is title/office in accusative. Logical analysis: Main clause has Phygekles as subject witnessing; subordinate temporal clause shows king appointing Augeus to office. Subordinate analysis reveals administrative appointment record with witness attestation.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 3: PY An 657\n",
    "        example_3 = ET.SubElement(examples, \"example_3\")\n",
    "        sequence = \"me-ta-qe pe-i e-qe-ta ke-ki-jo\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_3, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_3, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_3, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_3, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"    \",\n",
    "    \"English translation\": \"And with them Kerkios, the epetas (follower/companion).\",\n",
    "    \"reasoning\": \"Grammatical analysis: me-ta-qe ( ) contains preposition 'with' plus conjunction 'and'. pe-i () is 3rd person plural pronoun in dative 'them'. e-qe-ta () is military/administrative title in accusative, derived from  'to follow'. ke-ki-jo () is anthroponym in nominative. Logical analysis: Prepositional phrase with pronoun reference, followed by personal name and title in apposition. Syntactic structure indicates military or administrative personnel listing, common in Linear B records. The conjunction -qe links this entry to previous entries in the document series.\"\n",
    "}\"\"\"\n",
    "        \n",
    "        return ET.tostring(examples, encoding=\"unicode\")\n",
    "\n",
    "    examples_xml = create_examples_section()\n",
    "\n",
    "    # Build conversation history with comprehensive examples\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the historical context of Linear B and Mycenaean Greek.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will apply the syllabogram-to-character mapping carefully, especially for double consonants.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Extract grammatical information from this XML:\\n{create_grammatical_info()}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will use this declension and grammatical information to validate predictions and analyze Linear B sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': morphological_markers}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will carefully consider these prefixes and suffixes as morphological markers.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': create_main_prompt()}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the translation task methodology and output requirements.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Study these translation examples:\\n{examples_xml}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I have studied the examples and understand the expected format for Ancient Greek translation, English translation, and detailed reasoning that includes grammatical, logical, and syntactic analysis. I will do the same on your input.'}]}\n",
    "    ]\n",
    "\n",
    "    # Generate input data XML for your actual data structure\n",
    "    def create_input_data():\n",
    "        input_data = ET.Element(\"input_data\")\n",
    "        \n",
    "        # Document text\n",
    "        document = ET.SubElement(input_data, \"document\")\n",
    "        ET.SubElement(document, \"linear b\").text = linear_b_document\n",
    "        ET.SubElement(document, \"ancient greek translations\").text = translations\n",
    "        ET.SubElement(document, \"classifiers info\").text = json.dumps(out_classifiers, ensure_ascii=False)\n",
    "        \n",
    "        return ET.tostring(input_data, encoding=\"unicode\")\n",
    "\n",
    "    # Configure and use Gemini\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.1,     # Slightly increased for better reasoning\n",
    "            top_p=0.95,          # Slightly more focused\n",
    "            top_k=40,             # Allow more vocabulary options\n",
    "            response_mime_type=\"application/json\"  # <- enforce JSON\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(create_input_data())\n",
    "    logging.debug(f\"Gemini response: {response.text}\")\n",
    "    \n",
    "    return quick_parse_gemini_response(response.text.strip())\n",
    "\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Helper function to parse Gemini JSON response\"\"\"\n",
    "    # Clean up the response if needed\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:-3]\n",
    "    elif response_text.startswith('```'):\n",
    "        response_text = response_text[3:-3]\n",
    "        \n",
    "    return json.loads(response_text)\n",
    "\n",
    "# Call the function:\n",
    "# s, t, c = document_input(corpus.get_document_no(0))\n",
    "# make_translation_prompt(s, t, c, api_keys[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation_prompt(linear_b_document, translations, out_classifiers, api_key):\n",
    "    \"\"\"\n",
    "    Prompt for translating complete Linear B documents into Ancient Greek and English.\n",
    "\n",
    "    Args:\n",
    "        linear_b_document: str - Complete Linear B document text\n",
    "        translations: str - Pipe-separated translation options for each word\n",
    "        out_classifiers: list - List of dicts with word classifications and completeness\n",
    "        api_key: str - API key for Gemini\n",
    "\n",
    "    Returns:\n",
    "        dict - JSON with Ancient Greek translation, English translation, and reasoning\n",
    "    \"\"\"\n",
    "\n",
    "    # You'll need to define this mapping between Linear B syllabograms and Greek characters\n",
    "    syllabograms_matching = {\n",
    "      \"a\": [\"\"],\n",
    "      \"e\": [\"\", \"\"],\n",
    "      \"i\": [\"\"],\n",
    "      \"o\": [\"\", \"\"],\n",
    "      \"u\": [\"\"],\n",
    "      \"da\": [\"\", \"\"],\n",
    "      \"de\": [\"\", \"\", \"\"],\n",
    "      \"di\": [\"\", \"\"],\n",
    "      \"do\": [\"\", \"\", \"\"],\n",
    "      \"du\": [\"\", \"\"],\n",
    "      \"dwe\": [\"\", \"f\", \"\"],\n",
    "      \"dwo\": [\"\", \"f\", \"\"],\n",
    "      \"ja\": [\"\", \"\"],\n",
    "      \"je\": [\"\", \"\", \"\"],\n",
    "      \"jo\": [\"\", \"\", \"\"],\n",
    "      \"ka\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ke\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ki\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ko\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ku\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ma\": [\"\", \"\", \"\"],\n",
    "      \"me\": [\"\", \"\", \"\"],\n",
    "      \"mi\": [\"\", \"\"],\n",
    "      \"mo\": [\"\", \"\"],\n",
    "      \"mu\": [\"\", \"\"],\n",
    "      \"na\": [\"\", \"\"],\n",
    "      \"ne\": [\"\", \"\", \"\"],\n",
    "      \"ni\": [\"\", \"\"],\n",
    "      \"no\": [\"\", \"\", \"\"],\n",
    "      \"nu\": [\"\", \"\"],\n",
    "      \"nwa\": [\"f\", \"\", \"\"],\n",
    "      \"pa\": [\"\", \"\", \"\"],\n",
    "      \"pe\": [\"\", \"\", \"\"],\n",
    "      \"pi\": [\"\", \"\", \"\"],\n",
    "      \"po\": [\"\", \"\", \"\"],\n",
    "      \"pu\": [\"\", \"\", \"\"],\n",
    "      \"pte\": [\"\", \"\", \"\", \"\"],\n",
    "      \"phu\": [\"\", \"\"],\n",
    "      \"qa\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qe\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qi\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"qo\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "      \"ra\": [\"\", \"\", \"\", \"\"],\n",
    "      \"re\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ri\": [\"\", \"\", \"\"],\n",
    "      \"ro\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ru\": [\"\", \"\", \"\"],\n",
    "      \"rya\": [\"\", \"\", \"\", \"\"],\n",
    "      \"rai\": [\"\", \"\", \"\"],\n",
    "      \"ryo\": [\"\", \"\", \"\", \"\"],\n",
    "      \"sa\": [\"\", \"\"],\n",
    "      \"se\": [\"\", \"\", \"\"],\n",
    "      \"si\": [\"\", \"\"],\n",
    "      \"so\": [\"\", \"\", \"\"],\n",
    "      \"su\": [\"\", \"\"],\n",
    "      \"ta\": [\"\", \"\", \"\", \"\"],\n",
    "      \"te\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ti\": [\"\", \"\", \"\"],\n",
    "      \"to\": [\"\", \"\", \"\", \"\"],\n",
    "      \"tu\": [\"\", \"\", \"\"],\n",
    "      \"tya\": [\"\", \"\", \"\"],\n",
    "      \"twe\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"two\": [\"\", \"f\", \"\", \"\"],\n",
    "      \"wa\": [\"\", \"f\", \"\"],\n",
    "      \"we\": [\"\", \"f\", \"\"],\n",
    "      \"wi\": [\"\", \"f\"],\n",
    "      \"wo\": [\"\", \"f\", \"\"],\n",
    "      \"za\": [\"\", \"\", \"\", \"\"],\n",
    "      \"ze\": [\"\", \"\", \"\"],\n",
    "      \"zo\": [\"\", \"\", \"\"],\n",
    "      \"ha\": [\"\", \"h\"],\n",
    "      \"ai\": [\"\", \"\"],\n",
    "      \"au\": [\"\", \"\"],\n",
    "      \"*56\": [\"\", \"\", \"\", \"\"],\n",
    "      \"*64\": [\"\", \"\", \"f\"],\n",
    "      \"*65\": [\"\", \"\"],\n",
    "      \"*79\": [\"\", \"\"],\n",
    "      \"*82\": [\"\", \"\", \"f\"]\n",
    "    }\n",
    "    str_syllabogram_matching = json.dumps(syllabograms_matching, ensure_ascii=False)\n",
    "\n",
    "    # Historical and linguistic context\n",
    "    historical_context = \"\"\"Linear B is a syllabic script that was used to write Mycenaean Greek, the earliest attested form of Greek, dating from approximately 1450-1200 BCE.\n",
    "    This script predates the Greek alphabet by several centuries and represents a crucial link in understanding the evolution of the Greek language from its Mycenaean origins to Classical Ancient Greek.\n",
    "    Linear B texts are primarily administrative records found at palatial sites like Knossos, Pylos, and Thebes.\"\"\"\n",
    "\n",
    "    # Syllabogram matching context\n",
    "    syll_matching_context = f\"\"\"Consider this mapping between Linear B syllabograms and Ancient Greek characters: {str_syllabogram_matching}\n",
    "\n",
    "    CRITICAL: Ancient Greek double consonants like  and  may derive from two consecutive syllabograms, where the second one begins with \"s\"! \n",
    "    Example: a-ko-so-ne   (axes)\"\"\"\n",
    "\n",
    "    # Morphological markers\n",
    "    morphological_markers = \"\"\"CRITICAL MORPHOLOGICAL MARKERS:\n",
    "    \n",
    "    SUFFIXES - Key grammatical markers that preserve relationships:\n",
    "     -qe: conjunction suffix meaning \"and\" (equivalent to Latin -que)\n",
    "     -te: ablative suffix meaning \"away from a place\" (equivalent to Greek -); AMBIGUOUS\n",
    "     -de: can be either:\n",
    "       - Negative prefix meaning \"not, on the other side\"\n",
    "       - Allative/demonstrative suffix (equivalent to Greek -); AMBIGUOUS\n",
    "     -pi: instrumental/locative suffix\n",
    "\n",
    "    PREFIXES - Establish semantic domains and derivational patterns:\n",
    "     po-ro-: - prefix meaning \"before, forward, in front of\"\n",
    "     qe-to-ro-: - numerical prefix meaning \"four\"\n",
    "     we-: - numerical prefix meaning \"six\"; AMBIGUOUS\n",
    "     e-ne-wo-: - numerical prefix meaning \"nine\"\n",
    "     a-pu-: - separative prefix meaning \"from, away from, off\"\n",
    "     jo-: - relative/comparative prefix meaning \"as, like, how\"; AMBIGUOUS\"\"\"\n",
    "\n",
    "    # Create comprehensive grammatical information\n",
    "    def create_grammatical_info():\n",
    "        gramm = ET.Element(\"grammatical_information\")\n",
    "        \n",
    "        # Enhanced declension table\n",
    "        decl_section = ET.SubElement(gramm, \"declension_system\")\n",
    "        decl_intro = ET.SubElement(decl_section, \"introduction\")\n",
    "        decl_intro.text = \"\"\"Mycenaean Linear B declension system based on Ancient Greek patterns. \n",
    "        Columns 1-2: Ancient Greek second declension (thematic -o stems)\n",
    "        Columns 3-4: Ancient Greek first declension (thematic -a stems) \n",
    "        Columns 5-6: Ancient Greek third declension (athematic stems)\n",
    "        Same patterns apply to adjectives: first four columns for first-class adjectives, last two for second-class.\"\"\"\n",
    "        \n",
    "        decl_table = ET.SubElement(decl_section, \"declension_table\")\n",
    "        decl_table.text = '''It is based on ancient greek decletions: the first two coluns correspond to Ancient greek's second decletion, the second two columns correspond to the first decletions, while the last two correspond to the third decletion in ancient greek.\n",
    "The same suffixes are also used for the adjectives of the first class (the first four columns), and for those of the second class (the last two columns).\n",
    "| Number | Case       | Fless. Tem. (M.) | Fless. Tem. (N.) | Fless. in -a (M.) | Fless. in -a (F.) | Fless. atem. (M./F.)  | Fless. atem. (N.) (?)              |\n",
    "| ------ | ---------- | -----------      | -----------      | ----------------- | ----------------- | --------------        | --------------                     |\n",
    "| Sing.  | Nominative | -o               | -o               | -a                | -a                | variable              | variable                           |\n",
    "|        | Genitive   | -ojo             | -ojo             | -ao               | -a                | -o                    | -o                                 |\n",
    "|        | Dative     | -o               | -o               | -a                | -a                | -e/-i                 | -e/-i                              |\n",
    "|        | Accusative | -o               | -o               | -a                | -a                | -a                    | variable (identical to nominative) |\n",
    "| Plural | Nominative | -o/-oi           | -a               | -a                | -a                | -e                    | -a                                 |\n",
    "|        | Genitive   | -o               | -o               | -ao               | -ao               | -o                    | -o                                 |\n",
    "|        | Dative     | -oi              | -oi              | -ai               | -ai               | -si/-ti               | -si/-ti                            |\n",
    "|        | Accusative | -o               | -a               | -a                | -a                | -a/-e                 | -a                                 |\n",
    "'''\n",
    "        # Verb system\n",
    "        verbs_section = ET.SubElement(gramm, \"verb_system\")\n",
    "        verb_rules = {\n",
    "            \"third_person_singular\": \"Final syllabogram ending with vowel -e\",\n",
    "            \"third_person_plural\": \"Final syllabogram is -si\",\n",
    "            \"infinitive\": \"Final syllabogram ending with vowel -e (optionally additional -e)\",\n",
    "            \"active_participle\": \"Terminates with -o (singular - suffix in Greek, plural follows athematic pattern, e.g., -o-te   from )\",\n",
    "            \"middle_passive_participle\": \"Terminates with -me-no/-me-na suffixes (Greek -/-/-)\"\n",
    "        }\n",
    "        \n",
    "        for rule_key, rule_text in verb_rules.items():\n",
    "            ET.SubElement(verbs_section, rule_key).text = rule_text\n",
    "\n",
    "        # Adjective system\n",
    "        adj_section = ET.SubElement(gramm, \"adjective_system\")\n",
    "        ET.SubElement(adj_section, \"thematic_adjectives\").text = \"Follow same declension patterns as thematic nouns\"\n",
    "        ET.SubElement(adj_section, \"athematic_adjectives\").text = \"Follow athematic noun patterns with variable nominative forms\"\n",
    "        \n",
    "        return ET.tostring(gramm, encoding=\"unicode\")\n",
    "\n",
    "    # Create the main prompt structure\n",
    "    def create_main_prompt():\n",
    "        prompt = ET.Element(\"translation_task\")\n",
    "\n",
    "        # Task description\n",
    "        task_desc = ET.SubElement(prompt, \"task_description\")\n",
    "        task_desc.text = \"\"\"You are translating complete Linear B documents composed of words and logographic signs.\n",
    "Each Linear B word has possible Ancient Greek translation options (separated by |) and grammatical classifications:\n",
    " part_of_speech: grammatical function in {noun, adjective, verb, adverb} (participles labeled as adjectives)\n",
    " noun_type: logical function in {proper, toponym, ethnonym, common}\n",
    " inflection: morphological pattern in {thematic in -o, thematic in -a, athematic}\n",
    " completeness_level: preservation state in {COMPLETE, MOSTLY_COMPLETE, UNCERTAIN, MOSTLY_INCOMPLETE, INCOMPLETE}\n",
    "Your task is to produce complete translations into both Ancient Greek and modern English with detailed linguistic analysis.\n",
    "Please note that the Linear B document can be fragmentary. Fragments are divided by a '[...]' separator. Nearby words can be incomplete.\n",
    "Notice that within each document, similar patterns occur. If you are sure about how a sentence is built, you can try to extend its structure to nearby sentences.\"\"\"\n",
    "\n",
    "        # Selection methodology\n",
    "        methodology = ET.SubElement(prompt, \"selection_methodology\")\n",
    "        \n",
    "        ET.SubElement(methodology, \"grammatical_analysis\").text = \"\"\"Analyze each word's grammatical function as nouns, adjectives, verbs, adverbs, prepositions, etc. Consider morphological patterns and declensional endings.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"syntactic_analysis\").text = \"\"\"Identify sentence structure: subject, predicate, complements. Perform logical analysis of each word's role in the sentence.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"discourse_analysis\").text = \"\"\"Analyze document structure, distinguish different sentences, examine subordinate clauses, and establish logical connections for accurate translation.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"linguistic_authenticity\").text = \"\"\"Prioritize words consistent with attested Mycenaean Greek vocabulary and morphological patterns. Verify plausibility of proposed Mycenaean forms.\"\"\"\n",
    "        \n",
    "        ET.SubElement(methodology, \"semantic_coherence\").text = \"\"\"Evaluate which translation produces the most meaningful complete sentence, considering the administrative and practical nature of most Linear B texts.\"\"\"\n",
    "\n",
    "        # Output requirements\n",
    "        output_req = ET.SubElement(prompt, \"output_requirements\")\n",
    "        output_req.text = \"\"\"MANDATORY: Respond ONLY with valid JSON containing exactly these fields:\n",
    "        {\n",
    "            \"Ancient Greek translation\": \"complete document translated into Ancient Greek with proper diacritics and error recovery\",\n",
    "            \"English translation\": \"complete document translated into modern English\",\n",
    "            \"reasoning\": \"detailed explanation covering grammatical analysis, logical/syntactic analysis, and subordinate clause analysis that justifies the translation decisions\"\n",
    "        }\n",
    "        No additional text, commentary, or formatting permitted.\"\"\"\n",
    "\n",
    "        # Quality assurance\n",
    "        qa_section = ET.SubElement(prompt, \"quality_assurance\")\n",
    "        qa_section.text = \"\"\"Before finalizing translation:\n",
    "1. Verify grammatical correctness of Ancient Greek translation with proper case agreements\n",
    "2. Ensure semantic coherence and contextual appropriateness\n",
    "3. Confirm reasoning addresses grammatical, logical, and syntactic criteria\n",
    "4. Validate proper JSON formatting with all required fields\n",
    "5. Check that both translations capture the administrative/documentary nature of Linear B texts\n",
    "6. Use the Chain Of Thoughts to capture all logical patterns inside the document. Reiterate translations grammatical, logical, syntactical, and translation steps until a clear and correct result is obtained. Do not try to translate units of measures, indicated by isolated uppercase letters.\n",
    "7. ALWAYS provide complete translations - never leave empty or partial responses\"\"\"\n",
    "\n",
    "        return ET.tostring(prompt, encoding=\"unicode\")\n",
    "\n",
    "    # Training examples section with complete translations\n",
    "    def create_examples_section():\n",
    "        examples = ET.Element(\"examples_section\")\n",
    "        \n",
    "        # Example 1: PY Eb 895+906\n",
    "        example_1 = ET.SubElement(examples, \"example_1\")\n",
    "        sequence = \"a-i-qe-u e-ke-qe ke-ke-me-na ko-to-na ko-to-no-o-ko to-so-de pe-mo GRA T 6\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_1, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_1, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_1, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_1, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"          6\",\n",
    "    \"English translation\": \"Aigeus, the plot owner, who owns a communal plot; so much grain: 6 'T' units of wheat.\",\n",
    "    \"reasoning\": \"Grammatical analysis: a-i-qe-u () is a proper noun in nominative case, anthroponym with thematic inflection. e-ke-qe ( ) contains the verb 'to have' in 3rd person singular plus conjunction 'and'. ke-ke-me-na () is a perfect passive participle agreeing with ko-to-na () in accusative plural. ko-to-no-o-ko () is a compound noun meaning 'plot-holder'. Logical analysis: Subject = Aigeus, Verb = has, Objects = plots (communal) and grain. Syntactic structure follows standard administrative record format typical of Linear B land tenure documents.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 2: PY Ta 711\n",
    "        example_2 = ET.SubElement(examples, \"example_2\")\n",
    "        sequence = \"o-wi-de phu-ke-qi-ri o-te wa-na-ka te-ke au-ke-wa da-mo-ko-ro\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_2, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_2, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_2, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_2, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \"-      \",\n",
    "    \"English translation\": \"Phygekles witnessed when the king appointed Augeus as damoklos.\",\n",
    "    \"reasoning\": \"Grammatical analysis: o-wi-de (-) is 3rd person singular aorist of  meaning 'witnessed/saw'. phu-ke-qi-ri () is anthroponym in nominative. o-te () is temporal conjunction 'when'. wa-na-ka () is title 'king' in nominative. te-ke () is 3rd person singular aorist of  'placed/appointed'. au-ke-wa () is anthroponym in accusative. da-mo-ko-ro () is title/office in accusative. Logical analysis: Main clause has Phygekles as subject witnessing; subordinate temporal clause shows king appointing Augeus to office. Subordinate analysis reveals administrative appointment record with witness attestation.\"\n",
    "}\"\"\"\n",
    "\n",
    "        # Example 3: PY Ae 303\n",
    "        example_3 = ET.SubElement(examples, \"example_3\")\n",
    "        sequence = \"i-je-ro-jo pu-ro i-je-re-ja do-e-ra e-ne-ka ku-ru-so-jo MUL 14\"\n",
    "        transl, classif = sequence_input(sequence)\n",
    "        ET.SubElement(example_3, \"linear b\").text = sequence\n",
    "        ET.SubElement(example_3, \"ancient greek translations\").text = transl\n",
    "        ET.SubElement(example_3, \"classifiers info\").text = json.dumps(classif, ensure_ascii=False)\n",
    "        ET.SubElement(example_3, \"expected_response\").text = \"\"\"{\n",
    "    \"Ancient Greek translation\": \":       14\",\n",
    "    \"English translation\": \"Pylos: slaves of the priestess for the sake of sacred gold: 14 women.\",\n",
    "    \"reasoning\": \"Grammatical analysis: pu-ro is a topical header (Pylos). do-e-ra =  (fem. pl.) 'female slaves'; i-je-re-ja () provides genitival possession ('of the priestess'). e-ne-ka () governs the genitive ku-ru-so-jo ( 'of gold'), with i-je-ro-jo ( 'sacred') as attributive genitive  'for the sake of sacred gold'. MUL 14 is the logogram and tally for women (14). Logical/Syntactic analysis: nominal, paratactic inventory entry with structure [Topic] : [Possessed group] [Purpose PP] [Logographic count]; no finite verb required. Subordinate clause analysis: none present. Notes: respects administrative roster style, uses -jo genitives and +GEN, treats logogram as a counted NP head.\"\n",
    "}\"\"\"\n",
    "        \n",
    "        return ET.tostring(examples, encoding=\"unicode\")\n",
    "\n",
    "    examples_xml = create_examples_section()\n",
    "\n",
    "    # Build conversation history with comprehensive examples\n",
    "    history = [\n",
    "        {'role': 'user', 'parts': [{'text': historical_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the historical context of Linear B and Mycenaean Greek.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': syll_matching_context}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will apply the syllabogram-to-character mapping carefully, especially for double consonants.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Extract grammatical information from this XML:\\n{create_grammatical_info()}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will use this declension and grammatical information to validate predictions and analyze Linear B sequences.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': morphological_markers}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I will carefully consider these prefixes and suffixes as morphological markers.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': create_main_prompt()}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I understand the translation task methodology and output requirements.'}]},\n",
    "        {'role': 'user', 'parts': [{'text': f'Study these translation examples:\\n{examples_xml}'}]},\n",
    "        {'role': 'model', 'parts': [{'text': 'I have studied the examples and understand the expected format for Ancient Greek translation, English translation, and detailed reasoning that includes grammatical, logical, and syntactic analysis. I will do the same on your input.'}]}\n",
    "    ]\n",
    "\n",
    "    # Generate input data XML for your actual data structure\n",
    "    def create_input_data():\n",
    "        input_data = ET.Element(\"input_data\")\n",
    "        \n",
    "        # Document text\n",
    "        document = ET.SubElement(input_data, \"document\")\n",
    "        ET.SubElement(document, \"linear b\").text = linear_b_document\n",
    "        ET.SubElement(document, \"ancient greek translations\").text = translations\n",
    "        ET.SubElement(document, \"classifiers info\").text = json.dumps(out_classifiers, ensure_ascii=False)\n",
    "        \n",
    "        return ET.tostring(input_data, encoding=\"unicode\")\n",
    "\n",
    "    # Configure and use Gemini\n",
    "    genai.configure(api_key=api_key)\n",
    "    gemini_model = genai.GenerativeModel(\n",
    "        model_name='models/gemini-2.5-flash',\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=0.1,     # Slightly increased for better reasoning\n",
    "            top_p=0.95,          # Slightly more focused\n",
    "            top_k=40,             # Allow more vocabulary options\n",
    "            response_mime_type=\"application/json\"  # <- enforce JSON\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chat = gemini_model.start_chat(history=history)\n",
    "    response = chat.send_message(create_input_data())\n",
    "    logging.debug(f\"Gemini response: {response.text}\")\n",
    "    \n",
    "    return quick_parse_gemini_response(response.text.strip())\n",
    "\n",
    "def quick_parse_gemini_response(response_text):\n",
    "    \"\"\"Helper function to parse Gemini JSON response\"\"\"\n",
    "    # Clean up the response if needed\n",
    "    if response_text.startswith('```json'):\n",
    "        response_text = response_text[7:-3]\n",
    "    elif response_text.startswith('```'):\n",
    "        response_text = response_text[3:-3]\n",
    "        \n",
    "    return json.loads(response_text)\n",
    "\n",
    "# Call the function:\n",
    "# s, t, c = document_input(corpus.get_document_no(0))\n",
    "# make_translation_prompt(s, t, c, api_keys[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file_path = os.path.join(prefix_path, \"translation_answers.csv\")\n",
    "TEST_ONLY = False\n",
    "\n",
    "already_done = set()\n",
    "file_exists = os.path.isfile(output_file_path)\n",
    "random.shuffle(api_keys)\n",
    "\n",
    "# Read existing rows to populate already_done\n",
    "if file_exists:\n",
    "    with open(output_file_path, mode='r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            already_done.add(int(row[\"idx\"]))\n",
    "\n",
    "# Prepare test index filtering\n",
    "if TEST_ONLY:\n",
    "    todo = set(translated)    \n",
    "\n",
    "switch_key_interval = 10\n",
    "prec_idx = 0\n",
    "fail_count = 0\n",
    "with open(output_file_path, mode='a', newline='', encoding='utf-8') as output_file:\n",
    "    fieldnames = [\"idx\", \"original_document\", \"greek_translation\", \"english_translation\", \"reasoning\"]\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if file is new\n",
    "    if not file_exists or os.stat(output_file_path).st_size == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    for i in range(corpus.num_documents):\n",
    "        doc = corpus.get_document_no(i)\n",
    "        idx = doc.idx\n",
    "        # Skip if already processed or TEST_ONLY and the index is not for testing\n",
    "        if idx in already_done or TEST_ONLY and idx not in todo:\n",
    "            continue\n",
    "        print(f\"Current Document: {idx}\")\n",
    "\n",
    "        seq, trans, clas = document_input(doc)\n",
    "\n",
    "        # Determine which API key to use\n",
    "        if i % 300 == 0 and i != 0:\n",
    "            time.sleep(60)\n",
    "        key_idx = (i % (switch_key_interval * len(api_keys))) // switch_key_interval\n",
    "        api_key = api_keys[key_idx]\n",
    "\n",
    "        # Delay if the API key changes\n",
    "        if key_idx != prec_idx:\n",
    "            time.sleep(5)\n",
    "        prec_idx = key_idx\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                gemini_answer = make_translation_prompt(seq, trans, clas, api_key)\n",
    "                print(f\"document: {seq}\")\n",
    "                print(f\"answer: {gemini_answer}\")\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"idx\": idx,\n",
    "                    \"original_document\": seq,\n",
    "                    \"greek_translation\": gemini_answer[\"Ancient Greek translation\"],\n",
    "                    \"english_translation\": gemini_answer[\"English translation\"],\n",
    "                    \"reasoning\": gemini_answer[\"reasoning\"]\n",
    "                })\n",
    "\n",
    "                if i % 20 == 0:\n",
    "                    output_file.flush()\n",
    "                fail_count = 0\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if fail_count >= len(api_keys) * switch_key_interval:\n",
    "                    break\n",
    "                elif fail_count >= switch_key_interval:\n",
    "                    api_key = api_keys[(key_idx + 1) % len(api_keys)]\n",
    "                    prec_idx = key_idx\n",
    "                    key_idx = (key_idx + 1) % len(api_keys)\n",
    "                \n",
    "                fail_count += 1\n",
    "                print(\"DEBUG:\", seq)\n",
    "                print(f\"Error occurred: {e}. Retrying in 65 seconds...\")\n",
    "                time.sleep(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUW1774iLcV3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### BOH Esperimenti credo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[\"e-ke-si-qe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wz_J6HoTrh87"
   },
   "outputs": [],
   "source": [
    "brnn_args = args\n",
    "brnn_args[\"saved_path\"] = os.path.join(brnn_args[\"log_dir\"], \"saved.latest\")\n",
    "clear_stages()\n",
    "tim = TextInfillerManager(data, lang, batch_size, brnn_args)\n",
    "\n",
    "brnn_args[\"saved_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RcRcei3607A"
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"testing_hypers1\"\n",
    "LOG_DIR = os.path.join(prefix_path, \"repo_cinese\")\n",
    "LOG_DIR = os.path.join(LOG_DIR, EXP_NAME)\n",
    "SAVED_PATH = os.path.join(LOG_DIR, \"saved.latest\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "COG_PATH = os.path.join(prefix_path, \"transliterated_linear_b-greek.cog\") #transliterated_linear_b-greek.cog or linear_b-greek.cog\n",
    "\n",
    "args = {\n",
    "    \"num_rounds\" : 25, # 25\n",
    "    \"num_epochs_per_M_step\" : 100, # 100\n",
    "    \"saved_path\" : None,\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"num_cognates\" : 919,\n",
    "    \"inc\" : 50, # changed\n",
    "    \"warm_up_steps\" : 5,\n",
    "    \"capacity\" : (3, ),\n",
    "    \"save_all\" : False,\n",
    "    \"eval_interval\" : 10, # 10\n",
    "    \"check_interval\" : 10,\n",
    "    \"cog_path\" : f\"{COG_PATH}\",\n",
    "    \"char_emb_dim\" : 256, # changed\n",
    "    \"hidden_size\" : 256, # changed\n",
    "    \"num_layers\" : 8, # changed\n",
    "    \"dropout\" : 0.2, # changed\n",
    "    \"universal_charset_size\" : 200,\n",
    "    \"lost_lang\" : \"transliterated_linear_b\",#transliterated_linear_b or linear_b as parameters\n",
    "    \"known_lang\" : \"greek\",#greek\n",
    "    \"norms_or_ratios\" : (1.0, 0.7), # PORCODIO HAI CAMBIATO QUI OCHO\n",
    "    \"control_mode\" : \"relative\",\n",
    "    \"residual\" : True,\n",
    "    \"reg_hyper\" : 0.5,\n",
    "    \"batch_size\" : 256, # changed\n",
    "    \"momentum\" : 0.9,\n",
    "    \"gpu\" : None,\n",
    "    \"random\" : False,\n",
    "    \"seed\" : 17,\n",
    "    \"log_level\" : \"DEBUG\",\n",
    "    \"n_similar\" : 10, # changed\n",
    "    \"log_dir\": LOG_DIR,\n",
    "    \"gpu\" : \"0\"  # Set the first GPU (Colab typically provides one GPU)\n",
    "}\n",
    "\n",
    "if args[\"gpu\"] is not None:\n",
    "    torch.cuda.set_device(int(args[\"gpu\"]))  # HACK\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args[\"gpu\"]\n",
    "if not args[\"random\"]:\n",
    "    random.seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "#CAMBIAMENTI FATTI IN: MAGIC_TENSOR\n",
    "args[\"saved_path\"] = SAVED_PATH\n",
    "manager = Manager(args[\"cog_path\"], args[\"lost_lang\"], args[\"known_lang\"], args[\"batch_size\"], args)\n",
    "model = manager.get_trained_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bxm8ZGt0mfo"
   },
   "outputs": [],
   "source": [
    "#additional_words = set([word.form for word in v.get_words])\n",
    "words = v.get_words\n",
    "clear_vocabs()\n",
    "clear_stages()\n",
    "cog_path = \"/content/drive/MyDrive/DMPROJECT/transliterated_linear_b-greek.cog\"\n",
    "lost_lang = \"transliterated_linear_b\"\n",
    "known_lang = \"greek\"\n",
    "build_vocabs(cog_path, lost_lang, known_lang)#, additional_words=additional_words)\n",
    "data_loader = LostKnownDataLoader(lost_lang, known_lang, batch_size, cognate_only=False)\n",
    "\n",
    "dataset = data_loader.entire_batch\n",
    "model.eval()\n",
    "model_ret = model(dataset, mode=\"flow\", num_cognates=args[\"num_cognates\"], edit=False, capacity=args[\"capacity\"])\n",
    "# Magic tensor to the rescue!\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fi0YywR9Oqh"
   },
   "outputs": [],
   "source": [
    "greek_sequences = model_ret.log_probs\n",
    "greek_sequences = greek_sequences.permute(2, 0, 1)\n",
    "greek_sequences = torch.argmax(greek_sequences, dim=-1).cpu().numpy()\n",
    "greek_sequences = get_charset(\"greek\").id2char(greek_sequences)\n",
    "almt = model_ret.valid_log_probs\n",
    "preds = almt.get_best()\n",
    "preds_flow = model_ret.flow.get_best()\n",
    "idx = 9\n",
    "greek_sequences[idx], model_ret.valid_log_probs.row_words[idx], preds[model_ret.valid_log_probs.row_words[idx]],preds_flow[model_ret.valid_log_probs.row_words[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNenqn9M0w9Y"
   },
   "outputs": [],
   "source": [
    "boh = [word.form for word in get_words(lost_lang)]\n",
    "for w in additional_words:\n",
    "  if w not in boh:\n",
    "      print(w, \"wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzRAy1H75ymQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eZVPovYqqVT",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_JGjDSQQcRJ"
   },
   "source": [
    "### Get corpuses and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMzmz-ksOdrL"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_documents(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_sequences_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "    documents = {}\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sequence_number']).groupby('id')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        sequences = []\n",
    "        for w in group['sequence']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                sequences.append(\"NUM\")\n",
    "            else:\n",
    "                sequences.append(w)\n",
    "        documents[document_name] = list(zip(sequences, group['complete']))\n",
    "\n",
    "    return documents\n",
    "\n",
    "#Creates sequences in order to be then split into datasets\n",
    "def create_sequence_dataset(documents):\n",
    "    sequences = []\n",
    "    curr_seq = []\n",
    "    for doc in documents.values():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if len(curr_seq) > 0:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq:\n",
    "                         curr_seq.append(seq)\n",
    "                     sequences.append(\" \".join(curr_seq))\n",
    "                     curr_seq = []\n",
    "        if len(curr_seq) > 0:\n",
    "            sequences.append(\" \".join(curr_seq))\n",
    "            curr_seq = []\n",
    "    return sequences\n",
    "\n",
    "def create_missing_dataset(sequences):\n",
    "    res = []\n",
    "    for seq in sequences:\n",
    "        seq = seq.split(\" \")\n",
    "\n",
    "        # Collect indexes of actual words in the sequence\n",
    "        positions = [i for i, w in enumerate(seq) if \"-\" in w]\n",
    "\n",
    "        # Determine how many words to modify\n",
    "        wrong_seq = min(wrong_per_sequence, len(positions))\n",
    "\n",
    "        # Choose random positions to modify\n",
    "        if positions:\n",
    "            chosen = np.random.choice(positions, wrong_seq, replace=False)\n",
    "        else:\n",
    "            chosen = []\n",
    "\n",
    "        for pos in chosen:\n",
    "            length = seq[pos].count(\"-\") + 1\n",
    "\n",
    "            # Determine which dashes to replace with '?'\n",
    "            to_rem = np.random.choice(range(length), min(wrong_per_word, length), replace=False)\n",
    "\n",
    "            # Modify the word\n",
    "            sequence = seq[pos].split(\"-\")\n",
    "            for pos2 in to_rem:\n",
    "                sequence[pos2] = \"?\"\n",
    "            seq[pos] = \"-\".join(sequence)\n",
    "\n",
    "        res.append(\" \".join(seq))\n",
    "\n",
    "    return res\n",
    "\n",
    "# drop all sequences with only logograms and numerals: no sign can be removed from the sequence\n",
    "def clean_datasets(seq, mis):\n",
    "    for j in range(len(mis) - 1, -1, -1):  # Iterate from last to first\n",
    "        if \"?\" not in mis[j]:\n",
    "            seq.pop(j)\n",
    "            mis.pop(j)\n",
    "    return seq, mis\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(missing, sequences):\n",
    "    assert len(missing) == len(sequences), \"Both lists must have the same length\"\n",
    "\n",
    "    # Create a shuffled index array\n",
    "    indices = np.random.permutation(len(missing))\n",
    "\n",
    "    # Apply the shuffled indices to both lists\n",
    "    missing_shuffled = [missing[i] for i in indices]\n",
    "    sequences_shuffled = [sequences[i] for i in indices]\n",
    "\n",
    "    # Determine split sizes\n",
    "    train_size = int(0.9 * len(missing))  # 90% for training and LOOCV\n",
    "    test_size = len(missing) - train_size  # 10% for testing\n",
    "\n",
    "    # Split the datasets\n",
    "    train_x, test_x = missing_shuffled[:train_size], missing_shuffled[train_size:]\n",
    "    train_y, test_y = sequences_shuffled[:train_size], sequences_shuffled[train_size:]\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    tensor_sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "    return padded_sequences\n",
    "\n",
    "def pad_sequences_to_length(sequences, target_length=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a batch of sequences to the max sequence length or a specified target length.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of lists): List of tokenized sequences (each sequence is a list of integers).\n",
    "        target_length (int, optional): If specified, all sequences will be padded to this length.\n",
    "        padding_value (int, optional): Value used for padding (default is 0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor of shape (batch_size, max_length)\n",
    "    \"\"\"\n",
    "    tensor_sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    # Determine max length: either given target_length or max sequence length in batch\n",
    "    max_length = target_length if target_length is not None else max(len(seq) for seq in tensor_sequences)\n",
    "    # Pad sequences using pad_sequence, ensuring all sequences are target_length long\n",
    "    padded_sequences = pad_sequence(tensor_sequences, batch_first=True, padding_value=padding_value)\n",
    "    # If target_length is specified and some sequences are shorter, add extra padding manually\n",
    "    if target_length is not None and padded_sequences.shape[1] < target_length:\n",
    "        padding_needed = target_length - padded_sequences.shape[1]\n",
    "        padded_sequences = torch.nn.functional.pad(padded_sequences, (0, padding_needed), value=padding_value)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qGjdDrCuaVn"
   },
   "outputs": [],
   "source": [
    "def reconstruct_LB_signs(replace_numerals=False):\n",
    "    # Load the sequences_LB.csv file\n",
    "    sequences_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "    sequences_df = pd.read_csv(sequences_path)\n",
    "\n",
    "    documents = {}\n",
    "\n",
    "    # Sort and group by document_name\n",
    "    grouped_sequences = sequences_df.sort_values(by=['id', 'sign_number']).groupby('id')\n",
    "    # Iterate over each group\n",
    "    for document_name, group in grouped_sequences:\n",
    "        signs = []\n",
    "        for w in group['sign']:\n",
    "            # preserves singular and dual distinctiviness\n",
    "            if w.isdigit() and w != \"1\" and w != \"2\" and replace_numerals:\n",
    "                signs.append(\"NUM\")\n",
    "            elif \"?\" not in w:\n",
    "                signs.append(w)\n",
    "        documents[document_name] = signs\n",
    "\n",
    "    return documents\n",
    "\n",
    "def create_documents_dict():\n",
    "    filename = os.path.join(prefix_path, \"LIBER_documents.csv\")\n",
    "    documents_dict = {}\n",
    "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            document_id = int(row['id'])\n",
    "            documents_dict[document_id] = {key: value for key, value in row.items() if key != 'id'}\n",
    "    return documents_dict\n",
    "\n",
    "docs_info = create_documents_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10eHUq9O08Oy"
   },
   "outputs": [],
   "source": [
    "def preprocess_doc_info(doc_info):\n",
    "    doc_processed_info = {}\n",
    "    find_places = {}\n",
    "    counter = 0\n",
    "    chrono_dict = {\n",
    "        '': 0,\n",
    "        \"LM III B\": 1250,\n",
    "        \"LM II-III A\": 1400,\n",
    "        \"LH III C\": 1125,\n",
    "        \"LH III A2\": 1340,\n",
    "        \"LH III B2-C\": 1200,\n",
    "        \"LH III B2\": 1225,\n",
    "        \"LH III A2-B\": 1300,\n",
    "        \"LH III B1\": 1275,\n",
    "        \"LH III B\":  1250,\n",
    "        \"LM III B1\": 1275,\n",
    "        \"LH III A\": 1350,\n",
    "        \"LM III A2\": 1340,\n",
    "        'LH III A2-B1': 1320,\n",
    "        'LH III A2-C': 1213,\n",
    "        'LM III C': 1125,\n",
    "        'LM III B2-C1': 1210,\n",
    "        'LM III B2': 1225,\n",
    "        'LH III B-C': 1175\n",
    "    }\n",
    "\n",
    "    # gpt reconstructed place coorinates\n",
    "    places_coordinates = {\n",
    "        # Widely attested sites with confirmed data:\n",
    "        'KN': {\n",
    "            'name': 'Knossos (Crete)',\n",
    "            'coordinates': (35.2989, 25.1597)  # cite:WikipediaKnossos2025\n",
    "        },\n",
    "        'MY': {\n",
    "            'name': 'Mycenae (Argolid)',\n",
    "            'coordinates': (37.7300, 22.7500)  # cite:WikipediaMycenae2025\n",
    "        },\n",
    "        'TI': {\n",
    "            'name': 'Tiryns (Argolid)',\n",
    "            'coordinates': (37.5710, 22.7460)  # cite:WikipediaTiryns2025\n",
    "        },\n",
    "        'PY': {\n",
    "            'name': 'Pylos (Peloponnese)',\n",
    "            'coordinates': (36.8819, 21.6794)  # cite:WikipediaPylos2025\n",
    "        },\n",
    "        'TH': {\n",
    "            'name': 'Thebes (Boeotia)',\n",
    "            'coordinates': (38.3250, 23.3180)  # cite:WikipediaThebesBoeotia2025\n",
    "        },\n",
    "        'MAM': {\n",
    "            'name': 'Malia (Crete)',\n",
    "            'coordinates': (35.7514, 25.9329)  # cite:WikipediaMalia2025\n",
    "        },\n",
    "        'EL': {\n",
    "            'name': 'Eleusis',\n",
    "            'coordinates': (38.0425, 23.5367)  # cite:WikipediaEleusis2025\n",
    "        },\n",
    "        'OR': {\n",
    "            'name': 'Orchomenos (Boeotia)',\n",
    "            'coordinates': (38.3170, 22.7670)  # cite:WikipediaOrchomenos2025\n",
    "        },\n",
    "        'IK': {\n",
    "            'name': 'Ikaria',\n",
    "            'coordinates': (37.7500, 26.3000)  # cite:WikipediaIkaria2025\n",
    "        },\n",
    "        'VOL': {\n",
    "            'name': 'Volos',\n",
    "            'coordinates': (39.3667, 22.9500)  # cite:WikipediaVolos2025\n",
    "        },\n",
    "        'MA': {\n",
    "            'name': 'Marathon',\n",
    "            'coordinates': (38.1170, 23.9670)  # cite:WikipediaMarathon2025\n",
    "        },\n",
    "        'KH': {\n",
    "            'name': 'Khania (Chania, Crete)',\n",
    "            'coordinates': (35.5110, 24.0150)  # cite:WikipediaChania2025\n",
    "        },\n",
    "        'KR': {\n",
    "            'name': 'Kritsa (Crete)',\n",
    "            'coordinates': (35.2167, 25.1833)  # cite:WikipediaKritsa2025\n",
    "        },\n",
    "\n",
    "        # Tentative identifications (scholarly debate remains):\n",
    "        'MI': {\n",
    "            'name': 'Midea? (Argolid)',\n",
    "            'coordinates': (37.7220, 22.7660)  # Approximate; identification not definitively confirmed\n",
    "        },\n",
    "        'GLA': {\n",
    "            'name': 'Gla? (Boeotia)',\n",
    "            'coordinates': (38.3100, 22.6600)  # Approximate; based on limited archaeological surveys\n",
    "        },\n",
    "        'DI': {\n",
    "            'name': 'Dilos? (tentative)',\n",
    "            'coordinates': (38.0000, 23.5000)  # Coordinates provided as a rough estimate\n",
    "        },\n",
    "        'MED': {\n",
    "            'name': 'Medeon? (tentative)',\n",
    "            'coordinates': (38.0500, 23.6000)  # Tentative identification; data from limited sources\n",
    "        },\n",
    "        'ARM': {\n",
    "            'name': 'Armi? (tentative)',\n",
    "            'coordinates': (38.1000, 21.7000)  # Tentative; identification remains debated\n",
    "        },\n",
    "        'PR': {\n",
    "            'name': 'Prinias? (Crete)',\n",
    "            'coordinates': (35.2500, 24.0000)  # Approximation based on archaeological records\n",
    "        },\n",
    "        'SI': {\n",
    "            'name': 'Sicyon',\n",
    "            'coordinates': (38.0000, 22.9500)  # Approximate; consistent with regional data\n",
    "        }\n",
    "    }\n",
    "    id_count = 0\n",
    "    for doc_id, doc_data in doc_info.items():\n",
    "        find_place = doc_data[\"document_name\"].split()[0]\n",
    "        #associate an integer to any (new) find place\n",
    "        if find_place not in find_places:\n",
    "            find_places[find_place] = counter\n",
    "            counter += 1\n",
    "        # Regex to match initial digits\n",
    "        result = re.match(r'^\\d+', doc_data[\"scribe\"])\n",
    "        if result:\n",
    "            scribe = int(result.group())\n",
    "        else:\n",
    "            scribe = 0\n",
    "        doc_processed_info[doc_id] = {\n",
    "            \"id\": id_count,\n",
    "            \"site\": find_places[find_place],\n",
    "            \"scribe\": scribe,\n",
    "            \"time\": chrono_dict[doc_data[\"chronology\"]],\n",
    "            \"coordinates\": places_coordinates[find_place][\"coordinates\"],\n",
    "            \"words\": int(doc_data[\"words\"])\n",
    "        }\n",
    "        id_count += 1\n",
    "\n",
    "    return doc_processed_info\n",
    "\n",
    "def invert_mapping(signs_by_doc):\n",
    "    signs = defaultdict(lambda: set())\n",
    "    for doc_id, signs_list in signs_by_doc.items():\n",
    "        for pos, s in signs_list:\n",
    "            signs[s].add((pos, doc_id))\n",
    "    res = {}\n",
    "    for s in signs:\n",
    "        res[s] = sorted(list(signs[s]))\n",
    "    return res\n",
    "\n",
    "docs_info_preprocessed = preprocess_doc_info(docs_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLXP2UgY2Dv"
   },
   "outputs": [],
   "source": [
    "#Get all sequences of linear B from the corpus\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing = create_missing_dataset(sequences)\n",
    "sequences, missing = clean_datasets(sequences, missing)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(missing, sequences)\n",
    "\n",
    "#Get input and output tokens to use for the encoding: input has all signs + <SOS> and <PAD>, output has <EOS> and <PAD>, all numbers different from 1 and 2 become  <NUM> for simplicity reasons.Space < > also needs to be encoded\n",
    "signs_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "df_lb = pd.read_csv(signs_path)\n",
    "signs = df_lb['sign'].unique()\n",
    "\n",
    "tokens = set()\n",
    "for s in signs:\n",
    "    if not s.isdigit() or s == \"1\" or s == \"2\":\n",
    "        if not \"?\" in s and not \"separatum\" in s:\n",
    "            tokens.add(s)\n",
    "    else:\n",
    "        tokens.add(\"NUM\")\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "input_tokens = [\" \"] + tokens\n",
    "output_tokens = [\" \"] + tokens\n",
    "input_tokens.sort()\n",
    "output_tokens.sort()\n",
    "# last one is the unknown symbol\n",
    "input_tokens = [\"PAD\", \"SOS\", \"EOS\"] + input_tokens\n",
    "output_tokens = [\"PAD\", \"SOS\", \"EOS\"] + output_tokens\n",
    "\n",
    "input_tokens.extend([\"SYL\", \"LOG\", \"NUMERAL\",\"?\"])\n",
    "\n",
    "def label_dictionary(tokens):\n",
    "    mapping = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        mapping[t] = i\n",
    "    return mapping\n",
    "\n",
    "#Get mapping of every symbol to its encoding\n",
    "input_mapping = label_dictionary(input_tokens)\n",
    "output_mapping = label_dictionary(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PYetgzDWVs0J"
   },
   "outputs": [],
   "source": [
    "#Get dictionary of anthroponyms and toponyms\n",
    "LB_names = { \"anthroponyms\" : [], \"toponyms\" : []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "biPFBn-YAV9X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa-pa-ro-jo o-ka ne-wo-ki-to di-wi-je-u e-ri-ko-wo ha-di-je-u a-ki-wo-ni-jo wa-ka-ti-ja-ta ke-ki-de sa-pi-da me-ta-qe pe-i e-qe-ta pe-re-qo-ni-jo a-re-i-jo ne-wo-ki-to wo-wi-ja ko-ro-ku-ra-i-jo VIR NUM me-ta-qe pe-i e-qe-ta di-wi-je-u du-wo-jo-jo o-ka a-ke-re-wa ha-ku-ni-jo pe-ri-me-de phu-ti-ja a-phu-ka-ne ke-ki-de po-ra-i VIR NUM me-ta-qe pe-i e-qe-ta di-ko-na-ro a-da-ra-ti-jo u-wa-si ke-ki-de ne-wo VIR NUM me-ta-qe pe-i pe-re-u-ro-ni-jo e-qe-ta a-ke-re-wa ko-ro-ku-ra-i-jo VIR NUM me-ta-qe pe-i e-qe-ta ka-e-sa-me-no a-phu-ka\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i]) > 500:\n",
    "        print(sequences[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqSJBCKr3g3Q",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_CKSAe9bzcA9"
   },
   "outputs": [],
   "source": [
    "#Get all sequences of linear B from the corpus\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "sequences = create_sequence_dataset(corpus_LB)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(sequences, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrEO8WDvcLJR"
   },
   "outputs": [],
   "source": [
    "def create_signs_dataset_with_positions(documents, add_SOS=True):\n",
    "    sequences = {}\n",
    "    curr_seq = ['SOS'] if add_SOS else []\n",
    "    for doc_id, doc in documents.items():\n",
    "        for seq, complete in doc:\n",
    "            if complete and seq != \"separatum\":\n",
    "                curr_seq.append(seq)\n",
    "            else:\n",
    "                 if not add_SOS and len(curr_seq) > 0 or add_SOS and len(curr_seq) > 1:\n",
    "                     if seq != \"separatum\" and not \"?\" in seq:\n",
    "                         curr_seq.append(seq)\n",
    "                     signs = []\n",
    "                     sign_pos = 0\n",
    "                     for i, w in enumerate(curr_seq):\n",
    "                        signs.extend([(sign_pos+j, sign) for j, sign in enumerate(w.split(\"-\"))])\n",
    "                        sign_pos = len(signs)\n",
    "                        if i != len(curr_seq) - 1:\n",
    "                          signs.append(((sign_pos,\" \"))) # add spaces\n",
    "                          sign_pos += 1\n",
    "\n",
    "                     # normalize by length\n",
    "                     for i, (pos, sign) in enumerate(signs):\n",
    "                        signs[i] = (pos/(len(signs)-1+1e-10), sign)\n",
    "\n",
    "                     sequences[doc_id] = signs\n",
    "                     curr_seq = ['SOS'] if add_SOS else []\n",
    "        if not add_SOS and len(curr_seq) > 0 or add_SOS and len(curr_seq) > 1:\n",
    "            signs = []\n",
    "            sign_pos = 0\n",
    "            for i, w in enumerate(curr_seq):\n",
    "               signs.extend([(sign_pos+j, sign) for j, sign in enumerate(w.split(\"-\"))])\n",
    "               sign_pos = len(signs)\n",
    "               if i != len(curr_seq) - 1:\n",
    "                 signs.append(((sign_pos,\" \"))) # add spaces\n",
    "                 sign_pos += 1\n",
    "\n",
    "            for i, (pos, sign) in enumerate(signs):\n",
    "                signs[i] = (pos/(len(signs)-1+1e-10), sign)\n",
    "\n",
    "            sequences[doc_id] = signs\n",
    "            curr_seq = ['SOS'] if add_SOS else []\n",
    "\n",
    "    return sequences\n",
    "\n",
    "corpus_LB = reconstruct_LB_documents(replace_numerals=True)\n",
    "signs_info = create_signs_dataset_with_positions(corpus_LB)\n",
    "signs_info = invert_mapping(signs_info)\n",
    "signs = {input_mapping[k]: v for k, v in signs_info.items()} # tokenized view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ej-QMpmhkt3g"
   },
   "outputs": [],
   "source": [
    "# Recover features of documents where signs appear\n",
    "'''\n",
    "def define_token_features(signs_info, docs_info, input_tokens=input_tokens):\n",
    "    signs_list = [0 for _ in range(len(input_tokens))]\n",
    "    for i in range(len(signs_list)):\n",
    "        info_vec = []\n",
    "        docs = signs_info.get(i, [])\n",
    "        for (pos, key) in docs:\n",
    "            info_vec.append([pos, docs_info[key]['id'], docs_info[key]['site'], docs_info[key]['coordinates'][0],docs_info[key]['coordinates'][1], docs_info[key]['time'], docs_info[key]['scribe']])\n",
    "        signs_list[i] = info_vec\n",
    "    return signs_list\n",
    "\n",
    "def pad_signs_list(signs_list):\n",
    "    final_shape = max([len(signs_list[i]) for i in range(len(signs_list))])\n",
    "    for info_vec in signs_list:\n",
    "        info_vec.extend([torch.zeros(7) for _ in range(final_shape - len(info_vec))])\n",
    "    return signs_list\n",
    "\n",
    "signs_list = define_token_features(signs, docs_info_preprocessed)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH5RJI3Xv-aG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def aggregate_document_info(signs_list):\n",
    "    features = [0 for _ in range(len(signs_list))]\n",
    "    number_of_sites = 1\n",
    "    for s in signs_list:\n",
    "        for doc_info in s:\n",
    "            number_of_sites = max(number_of_sites, doc_info[2]+1)\n",
    "\n",
    "    for i, s in enumerate(signs_list):\n",
    "        if len(s) == 0:\n",
    "            features[i] = [0] * 7 + [0] * number_of_sites\n",
    "            continue\n",
    "        avg_pos = sum([doc_info[0] for doc_info in s]) / len(s)\n",
    "        num_docs = len(set([doc_info[1] for doc_info in s])) # n. of distinct documents where the token appears at least once\n",
    "        sites = set([doc_info[2] for doc_info in s])\n",
    "        one_hot_sites = [0 for _ in range(number_of_sites)]\n",
    "        for site in sites: one_hot_sites[site] = 1\n",
    "        avg_lat = sum([doc_info[3] for doc_info in s]) / len(s)\n",
    "        avg_long = sum([doc_info[4] for doc_info in s]) / len(s)\n",
    "        first_appearance = max([doc_info[5] for doc_info in s])\n",
    "        last_appearance = min([doc_info[5] if doc_info[5] != 0 else 1000000000 for doc_info in s]) # big number to avoid 0s encoding None dates\n",
    "        # remove scribe with key 0 (placeholder for no scribe)\n",
    "        num_scribes =  len(set([doc_info[6] for doc_info in s])) - 1 if 0 in set([doc_info[6] for doc_info in s]) else len(set([doc_info[6] for doc_info in s]))\n",
    "        features[i] = [avg_pos, num_docs, avg_lat, avg_long, first_appearance, last_appearance, num_scribes] + one_hot_sites\n",
    "    return features\n",
    "\n",
    "node_features = torch.tensor(aggregate_document_info(signs_list))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o03MhF2UwPis"
   },
   "outputs": [],
   "source": [
    "def define_nodes_features(docs_info, num_input_tokens=len(input_tokens)):\n",
    "    features = []\n",
    "    for _ in range(num_input_tokens):\n",
    "        features.append(torch.randn(6))\n",
    "    docs_info = sorted([(doc_id, doc_info) for doc_id, doc_info in docs_info.items()], key=lambda x: x[0])\n",
    "    for doc_id, doc_info in docs_info:\n",
    "        features.append(torch.tensor([doc_info['site'], doc_info['coordinates'][0], doc_info['coordinates'][1], doc_info['scribe'], doc_info['time'], doc_info[\"words\"]]))\n",
    "    return torch.stack(features)\n",
    "\n",
    "node_features = define_nodes_features(docs_info_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6EAy-xG15eN"
   },
   "outputs": [],
   "source": [
    "len(docs_info_preprocessed) + len(input_tokens), node_features.shape, len(docs_info_preprocessed), len(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5pgT1dC3vtj"
   },
   "outputs": [],
   "source": [
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_6zyHpx1I7F"
   },
   "outputs": [],
   "source": [
    "def define_edges_document_token(docs_info, signs_dict, num_input_tokens=len(input_tokens)):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "    edges = set()\n",
    "    for sign in signs_dict.keys():\n",
    "        #filter out PAD, SOS, EOS\n",
    "        if sign > 2:\n",
    "            for pos, doc_key in signs_dict[sign]:\n",
    "                node_token_idx = sign\n",
    "                node_doc_idx = num_input_tokens + docs_info[doc_key]['id']\n",
    "                if (node_doc_idx, node_token_idx) not in edges:\n",
    "                    edges_src.extend([node_doc_idx, node_token_idx])\n",
    "                    edges_tgt.extend([node_token_idx, node_doc_idx])\n",
    "                    edges.add((node_doc_idx, node_token_idx))\n",
    "\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "\n",
    "    return edge_index\n",
    "edges_document_token = define_edges_document_token(docs_info_preprocessed, signs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jby_FGS-yY7"
   },
   "outputs": [],
   "source": [
    "def define_edges_token_token(input_tokens):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "    edges = set()\n",
    "    for i in range(len(input_tokens)):\n",
    "        for j in range(len(input_tokens)):\n",
    "            if i != j:\n",
    "                #syllabograms\n",
    "                if (input_tokens[i][0] == input_tokens[j][0] or input_tokens[i][-1] == input_tokens[j][-1]) and input_tokens[i].islower() and input_tokens[j].islower():\n",
    "                    if (i,j) not in edges:\n",
    "                        edges_src.extend([i, j])\n",
    "                        edges_tgt.extend([j, i])\n",
    "                        edges.add((i, j))\n",
    "                #logograms\n",
    "                if (not input_tokens[i].islower() and not input_tokens[j].islower()):\n",
    "                    pref1 = input_tokens[i].split(\"+\")[0] if \"+\" in input_tokens[i] else input_tokens[i]\n",
    "                    pref2 = input_tokens[j].split(\"+\")[0] if \"+\" in input_tokens[j] else input_tokens[j]\n",
    "                    suff1 = input_tokens[i].split(\"+\")[-1] if \"+\" in input_tokens[i] else input_tokens[i]\n",
    "                    suff2 = input_tokens[j].split(\"+\")[-1] if \"+\" in input_tokens[j] else input_tokens[j]\n",
    "                    if not input_tokens[i] == \"1\" and not input_tokens[j] == \"1\" and not input_tokens[i] == \"2\" and not input_tokens[j] == \"2\" and not input_tokens[i] == \"?\" and not input_tokens[j] == \"?\":\n",
    "                        if ((pref1 == pref2 or suff1 == suff2 or (len(input_tokens[i])==1 and len(input_tokens[j])==1)) or \"VAS\" in pref1 and \"VAS\" in pref2) and (i,j) not in edges:\n",
    "                            edges_src.extend([i, j])\n",
    "                            edges_tgt.extend([j, i])\n",
    "                            edges.add((i, j))\n",
    "                    #numerals only between them\n",
    "                    if (input_tokens[i] == \"NUM\" or input_tokens[i] == \"1\" or input_tokens[i] == \"2\") and (input_tokens[j] == \"NUM\" or input_tokens[j] == \"1\" or input_tokens[j] == \"2\"):\n",
    "                        if (i,j) not in edges:\n",
    "                            edges_src.extend([i, j])\n",
    "                            edges_tgt.extend([j, i])\n",
    "                            edges.add((i, j))\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "    return edge_index\n",
    "edges_token_token = define_edges_token_token(input_tokens)\n",
    "edges_document_token.shape, edges_token_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rYbguv0d4Nt"
   },
   "outputs": [],
   "source": [
    "def define_meta_paths_token_token(signs, num_input_tokens=len(input_tokens)):\n",
    "    edges_src = []  # First type of edges\n",
    "    edges_tgt = []\n",
    "\n",
    "    edges_doc_tok = set()\n",
    "\n",
    "    sign_docs = defaultdict(lambda: set())\n",
    "    for sign, tup_list in signs.items():\n",
    "        for pos, doc in tup_list:\n",
    "            sign_docs[sign].add(doc)\n",
    "\n",
    "    # Create token-token edges based on shared document connections\n",
    "    for tok_i in range(num_input_tokens):\n",
    "        for tok_j in range(num_input_tokens):\n",
    "            if len(sign_docs[tok_i] & sign_docs[tok_j]) > 0:  # Ensure both tokens share the same document\n",
    "                edges_src.extend([tok_i, tok_j])\n",
    "                edges_tgt.extend([tok_j, tok_i])\n",
    "\n",
    "    edge_index = torch.tensor([\n",
    "        edges_src,  # source nodes\n",
    "        edges_tgt   # target nodes\n",
    "    ], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "meta_paths = define_meta_paths_token_token(signs)\n",
    "meta_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3NJY-NLKD22"
   },
   "outputs": [],
   "source": [
    "!pip install torch-cluster\n",
    "!pip install torch_geometric\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0X0OnDrUoBjh"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Combine the two types of edges\n",
    "edges_src = torch.cat([edges_document_token[0], edges_token_token[0]], dim=0)\n",
    "edges_tgt = torch.cat([edges_document_token[1], edges_token_token[1]], dim=0)\n",
    "pos_edges = torch.stack([edges_src, edges_tgt], dim=0)\n",
    "\n",
    "# Define edge types\n",
    "edge_types = torch.cat([\n",
    "    torch.zeros(edges_document_token.shape[1], dtype=torch.long),   # Document-Token edges\n",
    "    torch.ones(edges_token_token.shape[1], dtype=torch.long)       # Token-Token edges\n",
    "])\n",
    "\n",
    "# Graph Data\n",
    "#graph_data = Data(x=node_features, edge_index=edge_index, edge_type=edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6dFbhBuyILo"
   },
   "outputs": [],
   "source": [
    "edge_types[:pos_edges.shape[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPGIhvnAwDrm"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx, negative_sampling, degree\n",
    "\n",
    "def sample_links(pos_edges, edge_types):\n",
    "    idxs = (edge_types[:pos_edges.shape[1]]) == 1\n",
    "    num_samples = pos_edges[:, idxs].shape[1]\n",
    "\n",
    "    pos_samples_tok_tok = pos_edges[:, idxs]\n",
    "    neg_samples_tok_tok = negative_sampling (\n",
    "                    edge_index=pos_samples_tok_tok,\n",
    "                    num_nodes=len(input_tokens),\n",
    "                    num_neg_samples=num_samples,\n",
    "                )\n",
    "\n",
    "    idxs = (edge_types[:pos_edges.shape[1]]) == 0\n",
    "    doc_tok_edges = pos_edges[:, idxs]\n",
    "    doc_tok_edges_set = set()\n",
    "\n",
    "    for i in range(doc_tok_edges.shape[1]):\n",
    "        e = doc_tok_edges[:, i]\n",
    "        doc_tok_edges_set.add((e[0], e[1]))\n",
    "        doc_tok_edges_set.add((e[1], e[0]))\n",
    "\n",
    "\n",
    "    def sample_negative_doc_tok_links(num_samples, num_tokens, num_docs, doc_tok_edges_set):\n",
    "        \"\"\"\n",
    "        Efficiently samples negative doc-token links while avoiding existing edges.\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of negative samples to generate.\n",
    "            num_tokens (int): Number of token nodes.\n",
    "            num_docs (int): Number of document nodes.\n",
    "            doc_tok_edges_set (set): Existing doc-token edges to avoid.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled negative edges of shape (2, num_samples).\n",
    "        \"\"\"\n",
    "        neg_edges = []\n",
    "        attempts = 0\n",
    "        max_attempts = num_samples * 10  # Avoid infinite loops\n",
    "\n",
    "        while len(neg_edges) < num_samples and attempts < max_attempts:\n",
    "            # Randomly sample token and document indices\n",
    "            tok_idx = torch.randint(0, num_tokens, (num_samples,))\n",
    "            doc_idx = torch.randint(num_tokens, num_tokens + num_docs, (num_samples,))\n",
    "\n",
    "            # Create candidate edges\n",
    "            candidate_edges = list(zip(tok_idx.tolist(), doc_idx.tolist()))\n",
    "\n",
    "            # Filter out existing edges\n",
    "            new_edges = [e for e in candidate_edges if e not in doc_tok_edges_set]\n",
    "\n",
    "            # Add to negative edges list\n",
    "            neg_edges.extend(new_edges)\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        # Convert to tensor\n",
    "        neg_edges = torch.tensor(neg_edges[:num_samples], dtype=torch.long).T  # Shape: (2, num_samples)\n",
    "\n",
    "        return neg_edges\n",
    "\n",
    "    neg_samples_doc_tok = sample_negative_doc_tok_links(num_samples, len(input_tokens), node_features.shape[0]-len(input_tokens), doc_tok_edges_set)\n",
    "\n",
    "    # Randomly sample `num_samples` positive edges from `doc_tok_edges`\n",
    "    perm = torch.randperm(doc_tok_edges.shape[1])[:num_samples]  # Random indices\n",
    "    pos_samples_doc_tok = doc_tok_edges[:, perm]  # Select random positives\n",
    "\n",
    "    edge_index = torch.cat([pos_samples_tok_tok, pos_samples_doc_tok, neg_samples_tok_tok, neg_samples_doc_tok], dim=1)\n",
    "    labels = torch.cat([torch.ones(num_samples*2), torch.zeros(num_samples*2)])\n",
    "    return edge_index, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khj9gfHLJ8-T"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "#Combine the two types of edges\n",
    "edges_src = torch.cat([edges_document_token[0], edges_token_token[0], meta_paths[0]], dim=0)\n",
    "edges_tgt = torch.cat([edges_document_token[1], edges_token_token[1], meta_paths[1]], dim=0)\n",
    "edge_index = torch.stack([edges_src, edges_tgt], dim=0)\n",
    "\n",
    "#Define edge types\n",
    "edge_types = torch.cat([\n",
    "    torch.zeros(edges_document_token.shape[1], dtype=torch.long),   #Document-Token edges\n",
    "    torch.ones(edges_token_token.shape[1], dtype=torch.long),       #Token-Token edges\n",
    "    torch.ones(meta_paths.shape[1], dtype=torch.long) * 2           #Meta-Path edges\n",
    "])\n",
    "\n",
    "# Graph Data\n",
    "graph_data = Data(x=node_features, edge_index=edge_index, edge_type=edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5l4FHaEmrKa"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Node2Vec\n",
    "from torch.optim import Adam, SparseAdam\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Node2VecEmbeddings:\n",
    "    def __init__(self, data, params):\n",
    "        self.params = params\n",
    "        # initialize Node2Vec model with specified parameters\n",
    "        self.node2vec = Node2Vec(\n",
    "            data.edge_index,\n",
    "            embedding_dim=params['embedding_dim'],\n",
    "            walk_length=params.get('walk_length', 20),\n",
    "            context_size=params.get('context_size', 10),\n",
    "            walks_per_node=params.get('walks_per_node', 1),\n",
    "            num_negative_samples=params.get('num_negative_samples', 1),\n",
    "            p=params['p'],\n",
    "            q=params['q'],\n",
    "            sparse=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # prepare the data loader for Node2Vec training\n",
    "        self.loader = self.node2vec.loader(batch_size=params['batch_size'], shuffle=True)\n",
    "        self.optimizer_node2vec = SparseAdam(list(self.node2vec.parameters()), lr=params['node2vec_lr'])\n",
    "\n",
    "    def train_node2vec(self):\n",
    "        self.node2vec.train()\n",
    "        total_loss = 0\n",
    "        # iterate through the data loader and train Node2Vec\n",
    "        for pos_rw, neg_rw in self.loader:\n",
    "            self.optimizer_node2vec.zero_grad()\n",
    "\n",
    "            # compute the loss for the current batch\n",
    "            loss = self.node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            self.optimizer_node2vec.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(self.loader)\n",
    "        return avg_loss\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.node2vec.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.node2vec()\n",
    "        return embeddings.detach()\n",
    "\n",
    "params = {\n",
    "    'embedding_dim': 128,          # test different embedding dimensions\n",
    "    'p': 0.1,               # tests different values for bias towards recent nodes\n",
    "    'q': 10.0,               # tests different values for bias towards outward and inward walks\n",
    "    'node2vec_lr': 0.01,        # learning rate for Node2Vec\n",
    "    'epochs_n2v': 200,        # number of epochs for training Node2Vec: smaller grid to avoid excessive training time\n",
    "    'batch_size': 128,              # batch size for training Node2Vec\n",
    "    'walk_length': 4,\n",
    "    'context_size': 3\n",
    "}\n",
    "\n",
    "n2v = Node2VecEmbeddings(graph_data, params)\n",
    "for i in range(params[\"epochs_n2v\"]):\n",
    "    loss = n2v.train_node2vec()\n",
    "    if not (i % 10):\n",
    "        print(f\"Epoch {i}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYb7m9FdjyDq"
   },
   "outputs": [],
   "source": [
    "edge_index.shape, edge_types.shape, node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pydncW4C2KH"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Custom Message Passing: Document-to-Token  #\n",
    "#############################################\n",
    "from torch_geometric.nn import MessagePassing, GATConv\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "class DocToTokenAttention(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DocToTokenAttention, self).__init__(aggr='add')  # We'll sum the messages.\n",
    "        # Linear transformation for both document and token features.\n",
    "        self.lin = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        # Learnable attention: takes concatenated [token, document] features.\n",
    "        self.att_lin = nn.Linear(2 * out_channels, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: [num_nodes, in_channels] node features.\n",
    "        edge_index: [2, num_edges] for document-to-token edges.\n",
    "                    We assume that for each edge, source is a document and target is a token.\n",
    "        \"\"\"\n",
    "        # First, linearly transform all node features.\n",
    "        x_trans = self.lin(x)  # Now shape is [num_nodes, out_channels]\n",
    "        # Propagate messages along edges.\n",
    "        out = self.propagate(edge_index, x=x_trans)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, index):\n",
    "        \"\"\"\n",
    "        x_j: features of the source nodes (documents).\n",
    "        x_i: features of the target nodes (tokens).\n",
    "        index: target node indices for softmax aggregation.\n",
    "        \"\"\"\n",
    "        # Compute attention score from the concatenation of target (token) and source (document) features.\n",
    "        cat = torch.cat([x_i, x_j], dim=-1)  # shape: [num_edges, 2*out_channels]\n",
    "        alpha = self.att_lin(cat)  # shape: [num_edges, 1]\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        # Normalize the attention scores over all incoming edges for each target token.\n",
    "        alpha = softmax(alpha, index)\n",
    "        # Multiply the document features by the attention weight.\n",
    "        return x_j * alpha\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # Simply return the aggregated message.\n",
    "        return aggr_out\n",
    "\n",
    "#############################################\n",
    "# Overall Model: Documents influence tokens #\n",
    "#############################################\n",
    "\n",
    "class DocTokenGNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):#, num_tokens):\n",
    "        \"\"\"\n",
    "        in_channels: Dimension of input features.\n",
    "        out_channels: Dimension of output token embeddings.\n",
    "        num_tokens: Number of token nodes (assumed to be the first num_tokens in x).\n",
    "        \"\"\"\n",
    "        super(DocTokenGNN, self).__init__()\n",
    "        #self.num_tokens = num_tokens\n",
    "        #\n",
    "        #self.feature_transform = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "        # Branch for document-to-token message passing using learnable distance function.\n",
    "        self.doc2token = DocToTokenAttention(in_channels, out_channels)\n",
    "        # Branch for token-to-token interactions (using standard GAT).\n",
    "        self.token2token = GATConv(in_channels, out_channels, heads=1)\n",
    "        # Optional final linear layer to fuse the two branches.\n",
    "        self.final_lin = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        \"\"\"\n",
    "        x: [num_nodes, in_channels] input features for all nodes.\n",
    "           The first `num_tokens` rows correspond to tokens.\n",
    "           The remaining rows correspond to documents.\n",
    "        edge_index_doc2token: [2, num_edges_doc2token] edges from documents (source) to tokens (target).\n",
    "        edge_index_token2token: [2, num_edges_token2token] edges among tokens.\n",
    "        \"\"\"\n",
    "        mask_document_token = (edge_type == 0)\n",
    "        mask_token_token = (edge_type == 1)\n",
    "\n",
    "        edge_index_doc2token = edge_index[:, mask_document_token]\n",
    "\n",
    "        # Get the token-token edge index\n",
    "        edge_index_token2token = edge_index[:, mask_token_token]\n",
    "\n",
    "        # Compute messages from document nodes to tokens.\n",
    "        doc2token_out = self.doc2token(x, edge_index_doc2token)\n",
    "        # Compute messages among tokens.\n",
    "        token2token_out = self.token2token(x, edge_index_token2token)\n",
    "        # Combine both branches.\n",
    "        tokens_updated = doc2token_out + token2token_out\n",
    "        tokens_updated = self.final_lin(tokens_updated)\n",
    "        #out = self.feature_transform(x)[self.num_tokens:]\n",
    "        #\n",
    "        #out = torch.cat([tokens_updated[:self.num_tokens], out], dim=0)\n",
    "\n",
    "        return tokens_updated\n",
    "\n",
    "model = DocTokenGNN(in_channels=node_features.shape[1], out_channels=8)#, num_tokens=len(input_tokens))\n",
    "output = model(node_features, edge_index, edge_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr3ZfhZKVr6X"
   },
   "outputs": [],
   "source": [
    "# inspired by https://arxiv.org/abs/1903.07293\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class HAGN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads):\n",
    "        super(HAGN, self).__init__()\n",
    "\n",
    "        self.gat_doc_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_token_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_meta_path = GATConv(in_channels, out_channels, heads)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.semantic_transform = nn.Linear(out_channels * heads, out_channels)  # MLP Transformation (W)\n",
    "        self.semantic_attention_vector = nn.Parameter(torch.randn(out_channels))  # Attention vector (q)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.final_proj = nn.Linear(out_channels * heads, out_channels)\n",
    "        #self.reconstruct = nn.Linear(out_channels, in_channels)\n",
    "        #self.embeddings = None\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        edge_doc_tok = edge_index[:, (edge_type == 0)]\n",
    "        edge_token_tok = edge_index[:, (edge_type == 1)]\n",
    "        edge_meta_path = edge_index[:, (edge_type == 2)]\n",
    "        doc_tok = self.sigmoid(self.gat_doc_token(x, edge_doc_tok))\n",
    "        tok_tok = self.sigmoid(self.gat_token_token(x, edge_token_tok))\n",
    "        meta_tok = self.sigmoid(self.gat_meta_path(x, edge_meta_path))\n",
    "\n",
    "        # Stack node-level embeddings to a tensor of shape [num_nodes, num_meta_paths, hidden_size]\n",
    "        node_level_embeddings = torch.stack([doc_tok, tok_tok, meta_tok], dim=1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "\n",
    "        # Apply the semantic transformation (MLP) and tanh across all embeddings at once\n",
    "        transformed = self.semantic_transform(node_level_embeddings.view(-1, node_level_embeddings.size(-1)))  # [num_nodes * num_meta_paths, hidden_size]\n",
    "        transformed = transformed.view(node_level_embeddings.size(0), node_level_embeddings.size(1), -1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "        transformed = self.tanh(transformed)\n",
    "\n",
    "        # Compute attention scores in parallel for all meta-paths\n",
    "        attention_scores = torch.matmul(transformed, self.semantic_attention_vector)  # [num_nodes, num_meta_paths]\n",
    "\n",
    "        # Compute the average importance score across all nodes for each meta-path\n",
    "        meta_path_importance = attention_scores.mean(dim=0)  # [num_meta_paths]\n",
    "\n",
    "        # Apply softmax to get the final meta-path weights\n",
    "        meta_path_weights = self.softmax(meta_path_importance)  # [num_meta_paths]\n",
    "\n",
    "        # Expand meta_path_weights to match the dimensions of node_level_embeddings for broadcasting\n",
    "        meta_path_weights = meta_path_weights.view(1, -1, 1)  # Shape: [1, num_meta_paths, 1]\n",
    "\n",
    "        # Aggregate the final embeddings by weighting the node embeddings\n",
    "        z = torch.sum(meta_path_weights * node_level_embeddings, dim=1)  # [num_nodes, hidden_size]\n",
    "\n",
    "        z = self.final_proj(z)\n",
    "        #self.embeddings = z\n",
    "        #z = self.reconstruct(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "model = HAGN(in_channels=node_features.shape[1], out_channels=256, heads=16)\n",
    "output = model(node_features, edge_index, edge_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwSN2wRKt0FL"
   },
   "outputs": [],
   "source": [
    "class HAGN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads):\n",
    "        super(HAGN, self).__init__()\n",
    "\n",
    "        self.gat_doc_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_token_token = GATConv(in_channels, out_channels, heads)\n",
    "        self.gat_meta_path = GATConv(in_channels, out_channels, heads)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.semantic_transform = nn.Linear(out_channels * heads, out_channels)  # MLP Transformation (W)\n",
    "        self.semantic_attention_vector = nn.Parameter(torch.randn(out_channels))  # Attention vector (q)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.final_proj = nn.Linear(out_channels * heads, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_type):\n",
    "        edge_doc_tok = edge_index[:, (edge_type == 0)]\n",
    "        edge_token_tok = edge_index[:, (edge_type == 1)]\n",
    "        edge_meta_path = edge_index[:, (edge_type == 2)]\n",
    "        doc_tok = self.sigmoid(self.gat_doc_token(x, edge_doc_tok))\n",
    "        tok_tok = self.sigmoid(self.gat_token_token(x, edge_token_tok))\n",
    "        meta_tok = self.sigmoid(self.gat_meta_path(x, edge_meta_path))\n",
    "\n",
    "        # Stack node-level embeddings to a tensor of shape [num_nodes, num_meta_paths, hidden_size]\n",
    "        node_level_embeddings = torch.stack([doc_tok, tok_tok, meta_tok], dim=1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "\n",
    "        # Apply the semantic transformation (MLP) and tanh across all embeddings at once\n",
    "        transformed = self.semantic_transform(node_level_embeddings.view(-1, node_level_embeddings.size(-1)))  # [num_nodes * num_meta_paths, hidden_size]\n",
    "        transformed = transformed.view(node_level_embeddings.size(0), node_level_embeddings.size(1), -1)  # [num_nodes, num_meta_paths, hidden_size]\n",
    "        transformed = self.tanh(transformed)\n",
    "\n",
    "        # Compute attention scores in parallel for all meta-paths\n",
    "        attention_scores = torch.matmul(transformed, self.semantic_attention_vector)  # [num_nodes, num_meta_paths]\n",
    "\n",
    "        # Compute the average importance score across all nodes for each meta-path\n",
    "        meta_path_importance = attention_scores.mean(dim=0)  # [num_meta_paths]\n",
    "\n",
    "        # Apply softmax to get the final meta-path weights\n",
    "        meta_path_weights = self.softmax(meta_path_importance)  # [num_meta_paths]\n",
    "\n",
    "        # Expand meta_path_weights to match the dimensions of node_level_embeddings for broadcasting\n",
    "        meta_path_weights = meta_path_weights.view(1, -1, 1)  # Shape: [1, num_meta_paths, 1]\n",
    "\n",
    "        # Aggregate the final embeddings by weighting the node embeddings\n",
    "        z = torch.sum(meta_path_weights * node_level_embeddings, dim=1)  # [num_nodes, hidden_size]\n",
    "\n",
    "        z = self.final_proj(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        # extract source and target node indices\n",
    "        source_embeddings = z[edge_label_index[0]]\n",
    "        target_embeddings = z[edge_label_index[1]]\n",
    "\n",
    "        # dot product for the prediction\n",
    "        dot_product = torch.sum(source_embeddings * target_embeddings, dim=1)\n",
    "        return self.sigmoid(dot_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-rcLgkl0U43"
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5wkg7UNMFMh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def contrastive_loss(embeddings, pos_edge_index, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss without explicitly forming a large similarity matrix.\n",
    "\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    \"\"\"\n",
    "    num_tokens = embeddings.size(0)\n",
    "\n",
    "    # Normalize embeddings to compute cosine similarity efficiently\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "    losses = []\n",
    "    for src, tgt in pos_edge_index.t():  # Iterate over positive edges\n",
    "        # Compute positive similarity\n",
    "        pos_sim = torch.exp(torch.dot(embeddings[src], embeddings[tgt]) / temperature)\n",
    "\n",
    "        # Compute all negative similarities (excluding src itself)\n",
    "        neg_sims = torch.exp(embeddings[src] @ embeddings.T / temperature)  # [num_tokens]\n",
    "        neg_sims[src] = 0  # Mask self-similarity\n",
    "\n",
    "        # Compute denominator\n",
    "        denom = neg_sims.sum()\n",
    "\n",
    "        # Contrastive InfoNCE loss\n",
    "        loss = -torch.log(pos_sim / denom)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if losses:\n",
    "        return torch.stack(losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def triplet_loss(embeddings, pos_edge_index, margin=0.5, num_samples=10):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    margin: Margin hyperparameter for the triplet loss.\n",
    "    num_samples: Number of positive and negative samples per anchor.\n",
    "    \"\"\"\n",
    "    #embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "\n",
    "    num_tokens = embeddings.size(0)\n",
    "    triplet_losses = []\n",
    "\n",
    "    # Sample negative edges using torch_geometric's negative_sampling\n",
    "    neg_edge_index = negative_sampling(\n",
    "        pos_edge_index, num_nodes=num_tokens, num_neg_samples=num_samples * pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    # Convert positive edges to dictionary for fast lookup\n",
    "    pos_dict = {i: set() for i in range(num_tokens)}\n",
    "    for src, tgt in pos_edge_index.t().tolist():\n",
    "        pos_dict[src].add(tgt)\n",
    "        pos_dict[tgt].add(src)  # Assuming undirected edges\n",
    "\n",
    "    for anchor in range(num_tokens):\n",
    "        positives = list(pos_dict[anchor])\n",
    "        if not positives:\n",
    "            continue  # Skip tokens with no positive neighbors\n",
    "\n",
    "        # Sample up to `num_samples` positives\n",
    "        sampled_positives = random.sample(positives, min(num_samples, len(positives)))\n",
    "\n",
    "        # Find negatives corresponding to this anchor\n",
    "        neg_indices = (neg_edge_index[0] == anchor).nonzero(as_tuple=True)[0]\n",
    "        sampled_negatives = neg_edge_index[1, neg_indices].tolist()\n",
    "        sampled_negatives = random.sample(sampled_negatives, min(num_samples, len(sampled_negatives)))\n",
    "\n",
    "        if not sampled_negatives:\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings for all sampled positives and negatives\n",
    "        anchor_embed = embeddings[anchor].unsqueeze(0)  # [1, embedding_dim]\n",
    "        pos_embeds = embeddings[sampled_positives]  # [num_samples, embedding_dim]\n",
    "        neg_embeds = embeddings[sampled_negatives]  # [num_samples, embedding_dim]\n",
    "\n",
    "        # Compute cosine similarity (higher is better for positives)\n",
    "        pos_sim = F.cosine_similarity(anchor_embed, pos_embeds).mean()\n",
    "        neg_sim = F.cosine_similarity(anchor_embed, neg_embeds).mean()\n",
    "\n",
    "        # Compute triplet loss (maximize pos_sim, minimize neg_sim)\n",
    "        loss = F.relu(margin + neg_sim - pos_sim)  # Encourage higher pos_sim and lower neg_sim\n",
    "        triplet_losses.append(loss)\n",
    "\n",
    "    if triplet_losses:\n",
    "        return torch.stack(triplet_losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "'''\n",
    "def triplet_loss(embeddings, pos_edge_index, margin=0.5, num_samples=10):\n",
    "    \"\"\"\n",
    "    embeddings: Tensor of shape [num_tokens, embedding_dim] for token nodes.\n",
    "    pos_edge_index: LongTensor of shape [2, num_positive_edges] for linked token-token edges.\n",
    "    margin: Margin hyperparameter for the triplet loss.\n",
    "    num_samples: Number of positive and negative samples per anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    num_tokens = embeddings.size(0)\n",
    "    triplet_losses = []\n",
    "\n",
    "    # Sample negative edges using torch_geometric's negative_sampling\n",
    "    neg_edge_index = negative_sampling(\n",
    "        pos_edge_index, num_nodes=num_tokens, num_neg_samples=num_samples * pos_edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    # Convert positive edges to dictionary for fast lookup\n",
    "    pos_dict = {i: set() for i in range(num_tokens)}\n",
    "    for src, tgt in pos_edge_index.t().tolist():\n",
    "        pos_dict[src].add(tgt)\n",
    "        pos_dict[tgt].add(src)  # Assuming undirected edges\n",
    "\n",
    "    for anchor in range(num_tokens):\n",
    "        positives = list(pos_dict[anchor])\n",
    "        if not positives:\n",
    "            continue  # Skip tokens with no positive neighbors\n",
    "\n",
    "        # Sample up to `num_samples` positives\n",
    "        sampled_positives = random.sample(positives, min(num_samples, len(positives)))\n",
    "\n",
    "        # Find negatives corresponding to this anchor\n",
    "        neg_indices = (neg_edge_index[0] == anchor).nonzero(as_tuple=True)[0]\n",
    "        sampled_negatives = neg_edge_index[1, neg_indices].tolist()\n",
    "        sampled_negatives = random.sample(sampled_negatives, min(num_samples, len(sampled_negatives)))\n",
    "\n",
    "        if not sampled_negatives:\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings for all sampled positives and negatives\n",
    "        anchor_embed = embeddings[anchor].unsqueeze(0)  # [1, embedding_dim]\n",
    "        pos_embeds = embeddings[sampled_positives]  # [num_samples, embedding_dim]\n",
    "        neg_embeds = embeddings[sampled_negatives]  # [num_samples, embedding_dim]\n",
    "\n",
    "        # Compute cosine similarity (higher is better for positives)\n",
    "        pos_sim = F.pairwise_distance(anchor_embed, pos_embeds).mean()\n",
    "        neg_sim = F.pairwise_distance(anchor_embed, neg_embeds).mean()\n",
    "\n",
    "        # Compute triplet loss (maximize pos_sim, minimize neg_sim)\n",
    "        loss = F.relu(margin + neg_sim - pos_sim)  # Encourage higher pos_sim and lower neg_sim\n",
    "        triplet_losses.append(loss)\n",
    "\n",
    "    if triplet_losses:\n",
    "        return torch.stack(triplet_losses).mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "'''\n",
    "\n",
    "def graph_reconstruction_loss(token_emb, token_edge_index, neg_sample_ratio=1.0):\n",
    "    \"\"\"\n",
    "    For each tokentoken edge, predict a similarity score (dot product)\n",
    "    and use MSE loss: target 1 for positives and 0 for negatives.\n",
    "    \"\"\"\n",
    "    # Positive edges\n",
    "    pos_scores = (token_emb[token_edge_index[0]] * token_emb[token_edge_index[1]]).sum(dim=-1)\n",
    "    pos_loss = F.mse_loss(pos_scores, torch.ones_like(pos_scores))\n",
    "    # Negative sampling: sample random token pairs (not necessarily true negatives)\n",
    "    num_neg = int(neg_sample_ratio * token_edge_index.size(1))\n",
    "    num_tokens = token_emb.size(0)\n",
    "    neg_edge_index = torch.randint(0, num_tokens, (2, num_neg), device=token_emb.device)\n",
    "    neg_scores = (token_emb[neg_edge_index[0]] * token_emb[neg_edge_index[1]]).sum(dim=-1)\n",
    "    neg_loss = F.mse_loss(neg_scores, torch.zeros_like(neg_scores))\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "def graph_regularization_loss(embeddings, edge_index, edge_type, num_nodes, negative_ratio=1, num_negatives=10):\n",
    "    \"\"\"\n",
    "    Regularization term that encourages connected nodes to have similar embeddings\n",
    "    based on cosine similarity, and pushes unconnected nodes further apart.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to regularize.\n",
    "        edge_index: The edge index tensor.\n",
    "        edge_type: The edge types tensor.\n",
    "        num_nodes: The total number of nodes in the graph.\n",
    "        negative_ratio: The ratio of negative edges to positive edges for sampling.\n",
    "        num_negatives: The number of negative edges to sample for each token node.\n",
    "    \"\"\"\n",
    "    # Extract source and target nodes from edge_index\n",
    "    mask_token_token = (edge_type == 1)\n",
    "\n",
    "    # Get the token-token edge index (positive edges)\n",
    "    edge_index_token_token = edge_index[:, mask_token_token]\n",
    "\n",
    "    src, tgt = edge_index_token_token\n",
    "\n",
    "    # Get embeddings for the source and target nodes\n",
    "    src_embeddings = embeddings[src]\n",
    "    tgt_embeddings = embeddings[tgt]\n",
    "\n",
    "    # Compute cosine similarity between source and target embeddings\n",
    "    similarity = F.cosine_similarity(src_embeddings, tgt_embeddings, dim=-1)\n",
    "\n",
    "    # Positive loss: Minimize dissimilarity (maximize similarity)\n",
    "    positive_loss = 1 - similarity\n",
    "\n",
    "    # Negative sampling: Select random pairs of non-connected nodes\n",
    "    all_nodes = torch.arange(num_nodes)\n",
    "    non_edges = []\n",
    "\n",
    "    for i in range(len(src)):\n",
    "        # For each token (src), we will sample 'num_negatives' negative edges\n",
    "        src_node = src[i]\n",
    "\n",
    "        # Sample 'num_negatives' random non-connected nodes\n",
    "        for _ in range(num_negatives):\n",
    "            random_node = torch.randint(0, num_nodes, (1,)).item()\n",
    "            while random_node in edge_index_token_token[0, :] or random_node == src_node:\n",
    "                random_node = torch.randint(0, num_nodes, (1,)).item()\n",
    "\n",
    "            non_edges.append((src_node, random_node))\n",
    "\n",
    "    non_edges = torch.tensor(non_edges).T\n",
    "\n",
    "    # Get embeddings for negative pairs\n",
    "    src_negative = embeddings[non_edges[0]]\n",
    "    tgt_negative = embeddings[non_edges[1]]\n",
    "\n",
    "    # Compute cosine similarity for negative pairs (should be small)\n",
    "    negative_similarity = F.cosine_similarity(src_negative, tgt_negative, dim=-1)\n",
    "\n",
    "    # Negative loss: Maximize dissimilarity (minimize similarity)\n",
    "    negative_loss = torch.relu(negative_similarity)\n",
    "\n",
    "    # Combine positive and negative losses\n",
    "    loss = positive_loss.mean() + negative_loss.mean() * negative_ratio\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lhe0IBjteEy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def document_token_contrastive_loss(embeddings, edge_index, edge_types, num_samples=10, temperature=0.5, num_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss between tokens and documents with positive and negative sampling.\n",
    "\n",
    "    embeddings: Tensor of shape [num_nodes, embedding_dim] for all nodes (tokens and documents).\n",
    "    edge_index: LongTensor of shape [2, num_edges] for all edges.\n",
    "    edge_types: LongTensor of shape [num_edges] indicating edge types (0 for document-to-token).\n",
    "    num_samples: Number of positive and negative samples to use for contrastive loss.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    num_tokens: Number of token nodes (tokens are the first `num_tokens` nodes in `embeddings`).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = embeddings[:num_tokens]\n",
    "    documents = embeddings[num_tokens:]\n",
    "    # Normalize token and document embeddings\n",
    "    tokens_norm = F.normalize(tokens, p=2, dim=-1)\n",
    "    documents_norm = F.normalize(documents, p=2, dim=-1)\n",
    "\n",
    "    # Cosine similarity between tokens and documents\n",
    "    logits = torch.matmul(tokens_norm, documents_norm.t()) / temperature  # [N_tokens, N_documents]\n",
    "\n",
    "    # Extract document-to-token edges\n",
    "    doc_token_edges = edge_index[:, edge_types == 0]  # [2, num_positive_edges]\n",
    "\n",
    "    # Initialize the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Get the list of positive edges\n",
    "    positive_edges = set(zip(doc_token_edges[0].tolist(), doc_token_edges[1].tolist()))\n",
    "\n",
    "    for i in range(num_tokens):\n",
    "        # Get the positive edges for the current token\n",
    "        positive_edges_for_token = [edge for edge in positive_edges if edge[1] == i]\n",
    "\n",
    "        if len(positive_edges_for_token) == 0:\n",
    "            continue  # If there are no positive edges for this token, skip it.\n",
    "\n",
    "        # Sample num_samples positive edges (sampling randomly from the positive edges)\n",
    "        positive_sampled = random.sample(positive_edges_for_token, min(num_samples, len(positive_edges_for_token)))\n",
    "\n",
    "        # For negative sampling, we want to sample edges that do not already exist\n",
    "        # Generate valid negative document-token pairs\n",
    "        all_documents = set(range(num_tokens, embeddings.size(0)))  # Documents indices: [num_tokens, ...]\n",
    "\n",
    "        # Sample negative edges\n",
    "        negative_sampled = []\n",
    "        # Randomly sample a document and token\n",
    "        neg_docs = list(all_documents)\n",
    "        random.shuffle(neg_docs)\n",
    "\n",
    "        for neg_doc in neg_docs:\n",
    "            # Check if the sampled edge already exists as a positive edge\n",
    "            if (neg_doc, i) not in positive_edges:\n",
    "                negative_sampled.append((neg_doc, i))\n",
    "                if len(negative_sampled) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        # Now we have `positive_sampled` and `negative_sampled`\n",
    "        # Concatenate positive and negative logits\n",
    "        positive_logits = logits[i, [edge[0] - num_tokens for edge in positive_sampled]]  # directly index logits\n",
    "        negative_logits = logits[i, [edge[0] - num_tokens for edge in negative_sampled]]  # directly index logits\n",
    "\n",
    "        # Create the corresponding labels: 1 for positive, 0 for negative\n",
    "        pos_labels = torch.ones(len(positive_logits)).to(tokens.device)\n",
    "        neg_labels = torch.zeros(len(negative_logits)).to(tokens.device)\n",
    "\n",
    "        # Combine logits and labels\n",
    "        combined_logits = torch.cat([positive_logits, negative_logits], dim=0)\n",
    "        combined_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        # Compute contrastive loss using cross-entropy\n",
    "        loss = F.cross_entropy(combined_logits, combined_labels)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def token_token_contrastive_loss(embeddings, edge_index, edge_types, num_samples=10, temperature=0.5, num_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Compute contrastive loss between tokens using positive and negative sampling for token-token edges.\n",
    "\n",
    "    embeddings: Tensor of shape [num_nodes, embedding_dim] for all nodes (tokens and documents).\n",
    "    edge_index: LongTensor of shape [2, num_edges] for all edges.\n",
    "    edge_types: LongTensor of shape [num_edges] indicating edge types (1 for token-to-token).\n",
    "    num_samples: Number of positive and negative samples to use for contrastive loss.\n",
    "    temperature: Temperature parameter for scaling the similarities.\n",
    "    num_tokens: Number of token nodes (tokens are the first `num_tokens` nodes in `embeddings`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract token embeddings\n",
    "    tokens = embeddings[:num_tokens]\n",
    "\n",
    "    # Normalize token embeddings\n",
    "    tokens_norm = F.normalize(tokens, p=2, dim=-1)\n",
    "\n",
    "    # Cosine similarity between tokens (token-token similarity)\n",
    "    logits = torch.matmul(tokens_norm, tokens_norm.t()) / temperature  # [N_tokens, N_tokens]\n",
    "\n",
    "    # Extract token-to-token edges (positive edges for token-token pairs)\n",
    "    token_token_edges = edge_index[:, edge_types == 1]  # [2, num_token_token_edges]\n",
    "\n",
    "    # Initialize the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Get the list of positive edges\n",
    "    positive_edges = set(zip(token_token_edges[0].tolist(), token_token_edges[1].tolist()))\n",
    "\n",
    "    for i in range(num_tokens):\n",
    "        # Get the positive edges for the current token\n",
    "        positive_edges_for_token = [edge for edge in positive_edges if edge[0] == i]\n",
    "\n",
    "        if len(positive_edges_for_token) == 0:\n",
    "            continue  # If there are no positive edges for this token, skip it.\n",
    "\n",
    "        # Sample num_samples positive edges (sampling randomly from the positive edges)\n",
    "        positive_sampled = random.sample(positive_edges_for_token, min(num_samples, len(positive_edges_for_token)))\n",
    "\n",
    "        # For negative sampling, we want to sample token-token pairs that do not already exist\n",
    "        # Sample valid token-token pairs\n",
    "        all_tokens = set(range(num_tokens))  # Token indices: [0, ..., num_tokens-1]\n",
    "\n",
    "        # Sample negative edges\n",
    "        negative_sampled = []\n",
    "        # Randomly sample a token pair\n",
    "        neg_tokens = list(all_tokens)\n",
    "        random.shuffle(neg_tokens)\n",
    "\n",
    "\n",
    "        for neg_tok in neg_tokens:\n",
    "            # Check if the sampled edge already exists as a positive edge\n",
    "            if (i, neg_tok) not in positive_edges:\n",
    "                negative_sampled.append((i, neg_tok))\n",
    "                if len(negative_sampled) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        # Now we have `positive_sampled` and `negative_sampled`\n",
    "        # Concatenate positive and negative logits\n",
    "        positive_logits = logits[torch.tensor([edge[0] for edge in positive_sampled]), torch.tensor([edge[1] for edge in positive_sampled])].to(tokens.device)\n",
    "        negative_logits = logits[torch.tensor([edge[0] for edge in negative_sampled]), torch.tensor([edge[1] for edge in negative_sampled])].to(tokens.device)\n",
    "\n",
    "        # Create the corresponding labels: 1 for positive, 0 for negative\n",
    "        pos_labels = torch.ones(len(positive_logits)).to(tokens.device)\n",
    "        neg_labels = torch.zeros(len(negative_logits)).to(tokens.device)\n",
    "\n",
    "        # Combine logits and labels\n",
    "        combined_logits = torch.cat([positive_logits, negative_logits], dim=0)\n",
    "        combined_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        # Compute contrastive loss using cross-entropy\n",
    "        loss = F.cross_entropy(combined_logits, combined_labels)\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zrOYKhODu-N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(embeddings, positive_mask, temperature=0.5):\n",
    "    \"\"\"\n",
    "    NT-Xent loss for contrastive learning.\n",
    "\n",
    "    Args:\n",
    "      embeddings (Tensor): shape [N, D], where N is the number of nodes and D is the embedding dimension.\n",
    "      positive_mask (Tensor): Boolean tensor of shape [N, N] where positive_mask[i, j] is True if sample i and sample j form a positive pair.\n",
    "      temperature (float): Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "      loss (Tensor): A scalar tensor representing the NT-Xent loss.\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    norm_embeddings = F.normalize(embeddings, p=2, dim=1)  # [N, D]\n",
    "\n",
    "    # Compute cosine similarity matrix: [N, N]\n",
    "    sim_matrix = torch.matmul(norm_embeddings, norm_embeddings.t())  # cosine similarity\n",
    "\n",
    "    # Scale the similarities by temperature\n",
    "    sim_matrix = sim_matrix / temperature  # [N, N]\n",
    "\n",
    "    # To avoid trivial self-comparison, set the diagonal to a very negative value.\n",
    "    # This ensures that the self-similarity does not contribute to the softmax.\n",
    "    diag_mask = torch.eye(sim_matrix.size(0), dtype=torch.bool, device=sim_matrix.device)\n",
    "    sim_matrix = sim_matrix.masked_fill(diag_mask, -9e15)\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_sim = torch.exp(sim_matrix)  # [N, N]\n",
    "\n",
    "    # For each sample, compute numerator: sum of exp(similarity) over positive pairs.\n",
    "    # positive_mask is a Boolean mask [N, N]. Convert it to float.\n",
    "    pos_exp = exp_sim * positive_mask.float()  # [N, N]\n",
    "    positive_sum = pos_exp.sum(dim=1)  # [N]\n",
    "\n",
    "    # Denominator: sum over all (negative and positive) samples for each row.\n",
    "    negative_sum = exp_sim.sum(dim=1)  # [N]\n",
    "\n",
    "    # Avoid division by zero (if no positive exists, this should be handled appropriately)\n",
    "    eps = 1e-8\n",
    "    loss_per_sample = -torch.log((positive_sum + eps) / (negative_sum + eps))  # [N]\n",
    "\n",
    "    # Average over all samples\n",
    "    loss = loss_per_sample.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def nt_xent_loss_with_euclidean(embeddings, positive_mask, temperature=0.5):\n",
    "    \"\"\"\n",
    "    NT-Xent loss for contrastive learning using Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        embeddings (Tensor): shape [N, D], where N is the number of nodes and D is the embedding dimension.\n",
    "        positive_mask (Tensor): Boolean tensor of shape [N, N] where positive_mask[i, j] is True if sample i and sample j form a positive pair.\n",
    "        temperature (float): Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "        loss (Tensor): A scalar tensor representing the NT-Xent loss.\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings to unit vectors\n",
    "    norm_embeddings = F.normalize(embeddings, p=2, dim=1)  # [N, D]\n",
    "\n",
    "    # Compute pairwise Euclidean distance: [N, N]\n",
    "    # Euclidean distance: dist(x, y) = sqrt(sum((x_i - y_i)^2))\n",
    "    dist_matrix = torch.cdist(norm_embeddings, norm_embeddings, p=2)  # [N, N]\n",
    "\n",
    "    # Scale the distances by temperature\n",
    "    dist_matrix = dist_matrix / temperature  # [N, N]\n",
    "\n",
    "    # To avoid trivial self-comparison, set the diagonal to a very large value.\n",
    "    # This ensures that the self-similarity does not contribute to the softmax.\n",
    "    diag_mask = torch.eye(dist_matrix.size(0), dtype=torch.bool, device=dist_matrix.device)\n",
    "    dist_matrix = dist_matrix.masked_fill(diag_mask, 9e15)\n",
    "\n",
    "    # Compute the exponentials\n",
    "    exp_dist = torch.exp(-dist_matrix)  # [N, N] (negative because smaller distance means more similar)\n",
    "\n",
    "    # For each sample, compute numerator: sum of exp(-dist) over positive pairs.\n",
    "    pos_exp = exp_dist * positive_mask.float()  # [N, N]\n",
    "    positive_sum = pos_exp.sum(dim=1)  # [N]\n",
    "\n",
    "    # Denominator: sum over all (negative and positive) samples for each row.\n",
    "    negative_sum = exp_dist.sum(dim=1)  # [N]\n",
    "\n",
    "    # Avoid division by zero (if no positive exists, this should be handled appropriately)\n",
    "    eps = 1e-8\n",
    "    loss_per_sample = -torch.log((positive_sum + eps) / (negative_sum + eps))  # [N]\n",
    "\n",
    "    # Average over all samples\n",
    "    loss = loss_per_sample.mean()\n",
    "    return loss\n",
    "\n",
    "def create_positive_mask(num_nodes, edges_document_token, edges_token_token):\n",
    "    mask = torch.zeros((num_nodes, num_nodes), dtype=bool)\n",
    "    for i in range(edges_document_token.shape[1]):\n",
    "        e = edges_document_token[:, i]\n",
    "        if e[0] < num_nodes and e[1] < num_nodes:\n",
    "            mask[e[0]][e[1]] = True\n",
    "    for i in range(edges_document_token.shape[1]):\n",
    "        e = edges_document_token[:, i]\n",
    "        mask[e[0]][e[1]] = True\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPOwrmg6AKFj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "data['paper'].x = ... # [num_papers, num_features_paper]\n",
    "data['author'].x = ... # [num_authors, num_features_author]\n",
    "data['institution'].x = ... # [num_institutions, num_features_institution]\n",
    "data['field_of_study'].x = ... # [num_field, num_features_field]\n",
    "\n",
    "data['paper', 'cites', 'paper'].edge_index = ... # [2, num_edges_cites]\n",
    "data['author', 'writes', 'paper'].edge_index = ... # [2, num_edges_writes]\n",
    "data['author', 'affiliated_with', 'institution'].edge_index = ... # [2, num_edges_affiliated]\n",
    "data['paper', 'has_topic', 'field_of_study'].edge_index = ... # [2, num_edges_topic]\n",
    "\n",
    "data['paper', 'cites', 'paper'].edge_attr = ... # [num_edges_cites, num_features_cites]\n",
    "data['author', 'writes', 'paper'].edge_attr = ... # [num_edges_writes, num_features_writes]\n",
    "data['author', 'affiliated_with', 'institution'].edge_attr = ... # [num_edges_affiliated, num_features_affiliated]\n",
    "data['paper', 'has_topic', 'field_of_study'].edge_attr = ... # [num_edges_topic, num_features_topic]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnLnhISYWLZO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "#positive_mask = create_positive_mask(node_features.shape[0], edges_document_token, edges_token_token)\n",
    "\n",
    "def l1_reg(model, lambda_l1=0.01):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    return lambda_l1 * l1_norm\n",
    "\n",
    "def train(model, data, num_epochs=1500, learning_rate=1e-1, lambda_reg=1e-2, lambda_mse = 1e-4, num_input_tokens=len(input_tokens)):\n",
    "    \"\"\"\n",
    "    Trains the MultiLayerSeparateGAT model.\n",
    "\n",
    "    Arguments:\n",
    "        model: The MultiLayerSeparateGAT model.\n",
    "        data: A PyTorch Geometric Data object containing node features, edge_index, and edge_types.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        lambda_reg: Weight for the graph regularization loss.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    bce = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        # Forward pass\n",
    "        out = model.encode(data.x, data.edge_index, data.edge_type)\n",
    "        sampled_links, labels = sample_links(data.edge_index.cpu(), data.edge_type.cpu())\n",
    "\n",
    "        out = model.decode(out, sampled_links.to(out.device))\n",
    "\n",
    "        # Compute the final loss (here assuming no labels, just regularization)\n",
    "        loss = bce(out, labels.to(out.device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the training loss\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is a PyTorch Geometric Data object with the necessary fields: x, edge_index, edge_type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = DocTokenGNN(in_channels=node_features.shape[1], out_channels=128).to(device)\n",
    "#model = SeparateGAT(in_channels=node_features.shape[1], out_channels=8, heads=8, edge_types=edge_types).to(device)\n",
    "model = HAGN(in_channels=node_features.shape[1], out_channels=32, heads=8).to(device)\n",
    "trained_model = train(model, graph_data.to(device), num_epochs=150, learning_rate=1e-3, lambda_reg=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ5TKwJzVpWq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_embeddings(embeddings, labels=None, method='t-SNE', input_tokens=None):\n",
    "    \"\"\"\n",
    "    Visualizes the embeddings using either PCA or t-SNE, and optionally adds labels from `input_tokens`.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to visualize.\n",
    "        labels: The labels for coloring the embeddings (optional).\n",
    "        method: The dimensionality reduction method ('PCA' or 't-SNE').\n",
    "        input_tokens: A list of token labels (words) to display next to the points.\n",
    "    \"\"\"\n",
    "    # Apply dimensionality reduction (PCA or t-SNE)\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    elif method == 't-SNE':\n",
    "        tsne = TSNE(n_components=2)\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the reduced embeddings\n",
    "    plt.figure(figsize=(100, 80))\n",
    "\n",
    "    if labels is not None:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis', s=30)\n",
    "        plt.colorbar()\n",
    "    else:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=30)\n",
    "\n",
    "    # Add labels from input_tokens\n",
    "    if input_tokens is not None:\n",
    "        for i, token in enumerate(input_tokens):\n",
    "            plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], token, fontsize=9, ha='center', va='center')\n",
    "\n",
    "    plt.title(f\"Node Embeddings Visualization ({method})\")\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after training\n",
    "# Assuming `trained_model` and `graph_data` are defined\n",
    "# Extract the token embeddings (assuming the first 354 nodes are tokens)\n",
    "#embeddings = trained_model(graph_data.x, graph_data.edge_index, graph_data.edge_type)\n",
    "#embeddings = trained_model.get_embeddings()\n",
    "#token_embeddings = embeddings.cpu().detach().numpy()  # Extract token embeddings (first 354 nodes are tokens)\n",
    "\n",
    "token_embeddings = n2v.get_embeddings().cpu().detach().numpy()\n",
    "# Assuming `input_tokens` is a list of tokens (words) corresponding to the first 354 tokens\n",
    "visualize_embeddings(token_embeddings, method='t-SNE', input_tokens=input_tokens)  # Or method='t-SNE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxwAxj4xUf_-"
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cD4QDHBIJFsf"
   },
   "outputs": [],
   "source": [
    "class LearnableDistance(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(num_features))  # Learnable weights for each feature\n",
    "\n",
    "    def forward(self, node1_features, node2_features):\n",
    "        distance = torch.sum(self.weights * (node1_features - node2_features) ** 2, dim=-1)\n",
    "        return torch.exp(-distance)  # Convert to similarity score (or edge weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWLFe1YAClPM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reduce vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0ExKGZEJeVt"
   },
   "outputs": [],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPBrw4ITCjvY"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sequences)):\n",
    "    sequences[i] = sequences[i].replace(\"VINb\", \"VIN\")\n",
    "pattern = r'\\b(TELHA|TELA|OVIS|SUS|CAP|EQU|OLE|GRA|CYP|BOS|TUN|OLIV|AROM|ROTA)\\w*'\n",
    "sequences = [re.sub(pattern, r'\\1', seq) for seq in sequences]\n",
    "\n",
    "pattern2 = r'\\*\\d+VAS\\S*'\n",
    "sequences = [re.sub(pattern2, 'VAS', seq) for seq in sequences]\n",
    "\n",
    "pattern3 = r'\\b(Z|V|T|S|Q|P|N|M|L)\\b'\n",
    "sequences = [re.sub(pattern3, 'MEASURE_UNIT', seq) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWf5up5PGTaG"
   },
   "outputs": [],
   "source": [
    "for seq in sequences:\n",
    "  print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9y6ZIZPD-7K"
   },
   "outputs": [],
   "source": [
    "# choose values for the parameters\n",
    "wrong_per_sequence = 1\n",
    "wrong_per_word = 1\n",
    "\n",
    "#Create synthetic dataset with missing characters (syllables) marked as <?>\n",
    "missing = create_missing_dataset(sequences)\n",
    "sequences, missing = clean_datasets(sequences, missing)\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(missing, sequences)\n",
    "#Get input and output tokens to use for the encoding: input has all signs + <SOS> and <PAD>, output has <EOS> and <PAD>, all numbers different from 1 and 2 become  <NUM> for simplicity reasons.Space < > also needs to be encoded\n",
    "signs_path = os.path.join(prefix_path, \"processed_signs_LB.csv\")\n",
    "df_lb = pd.read_csv(signs_path)\n",
    "signs = df_lb['sign'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18HEQ-uaXmM8"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = set()\n",
    "for s in signs:\n",
    "    if not s.isdigit():\n",
    "        if not \"?\" in s:\n",
    "            if \"VIN\" in s:\n",
    "                tokens.add(\"VIN\")\n",
    "            elif \"VAS\" in s:\n",
    "                tokens.add(\"VAS\")\n",
    "            elif s == 'M' or s == 'N' or s == 'P' or s=='Q' or s=='L' or s=='Z' or s=='V' or s=='T' or s=='S':\n",
    "                tokens.add(\"MEASURE_UNIT\")\n",
    "            else:\n",
    "                tokens.add(s)\n",
    "    elif s == '1' or s == '2':\n",
    "      tokens.add(\"1\")\n",
    "      tokens.add(\"2\")\n",
    "    else:\n",
    "        tokens.add(\"NUM\")\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "input_tokens = [\" \"] + tokens\n",
    "output_tokens = [\" \"] + tokens\n",
    "input_tokens.sort()\n",
    "output_tokens.sort()\n",
    "# last one is the unknown symbol\n",
    "input_tokens = [\"PAD\", \"SOS\", \"EOS\"] + input_tokens\n",
    "output_tokens = [\"PAD\", \"SOS\", \"EOS\"] + output_tokens\n",
    "\n",
    "input_tokens.extend([\"SYL\", \"LOG\", \"NUMERAL\",\"?\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeshvmB2jRqU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Phonem2Vec\n",
    "To get phonetic embeddings we train in an unsupervised learning manner an lstm. We will use only \"encode\" method to get the latent embeddings. The key aspect is that we need this autoencoder to produce quality embeddings, so we don't really care about the result, more that similar sounding syllables have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "myhr-5l-jXAy"
   },
   "outputs": [],
   "source": [
    "class SyllableAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size=None, embedding_dim=None, hidden_dim=None, latent_dim=None, charsIndexes=None, verbose=False):\n",
    "        super(SyllableAutoencoder, self).__init__()\n",
    "        if not vocab_size or not embedding_dim or not hidden_dim or not latent_dim:\n",
    "            if verbose:\n",
    "                print(\"Missing parameters for autoencoder\")\n",
    "            return\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        #map hidden state to latent vector\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "        #output projection for each time step to vocabulary size (for reconstruction)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.charsIndexes = charsIndexes #vocabulary of the SyllableAutoencoder\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.encoder_rnn(packed)\n",
    "        if isinstance(hidden, tuple): #we don't need wider context\n",
    "            hidden = hidden[0]\n",
    "        last_hidden = hidden[-1]\n",
    "        latent = self.fc_mu(last_hidden)\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent, target_seq, lengths):\n",
    "        #prepare the initial hidden state for decoder from latent vector\n",
    "        hidden_init = self.fc_latent(latent).unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        #embed the target sequence (teacher forcing during training)\n",
    "        embedded = self.embedding(target_seq)\n",
    "\n",
    "        #initialize the cell state as zeros (or you could use hidden_init too)\n",
    "        cell_init = torch.zeros_like(hidden_init)\n",
    "        #combine into a tuple for\n",
    "        hidden_init = (hidden_init, cell_init)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.decoder_rnn(packed, hidden_init)\n",
    "        decoder_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        logits = self.output_projection(decoder_out)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        latent = self.encode(x, lengths)\n",
    "        #For training, use the input sequence as target (teacher forcing)\n",
    "        logits = self.decode(latent, x, lengths)\n",
    "        return logits, latent\n",
    "\n",
    "    def get_probabilities(self, logits):\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    #Given syllable, returns its tensor one-hot-encoded representation\n",
    "    def syl2tensor(self, syl):\n",
    "        return torch.tensor([self.charsIndexes[c] for c in syl], dtype=torch.long).unsqueeze(0), torch.tensor([len(syl)])\n",
    "\n",
    "    #Given list of syllables, produces dictionary <syllable> : <embedding>\n",
    "    def get_embeddings(self, syllables, vocab=None): #original vocabulary of transformer\n",
    "\n",
    "      #Get mapping <syllable> : <one-hot-encoded-syllable> from original vocabulary, positions in the list are the encodings\n",
    "      if vocab != None:\n",
    "        mapping = {}\n",
    "        count = 0\n",
    "        for tok in vocab:\n",
    "          mapping[tok] = count\n",
    "          count += 1\n",
    "\n",
    "      embeddings_per_syllable = {}\n",
    "      for syl in syllables:\n",
    "        syl_, length = self.syl2tensor(syl)\n",
    "        out = self.encode(syl_, length).squeeze(0).detach().numpy()\n",
    "        syl = syl.replace(\"0\", \"\") #remove padding when saving\n",
    "\n",
    "        #Returns representation <one-hot-encoded-syllable> : <embedding>\n",
    "        if vocab != None:\n",
    "          syl = mapping[syl]\n",
    "\n",
    "        embeddings_per_syllable[syl] = out\n",
    "\n",
    "      return embeddings_per_syllable\n",
    "\n",
    "    def save_embeddings(self, filename, syllables, prefix_path='./DMPROJECT/Embeddings/', tokenList=None):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      phonetic_embeddings = self.get_embeddings(syllables, tokenList)\n",
    "      np.save(embeddings_path, phonetic_embeddings)\n",
    "\n",
    "    def load_embeddings(self, filename, prefix_path='./DMPROJECT/Embeddings/'):\n",
    "      embeddings_path = os.path.join(prefix_path, filename)\n",
    "      return np.load(embeddings_path, allow_pickle=True).item()\n",
    "\n",
    "\n",
    "class SyllableDataset(Dataset):\n",
    "    def __init__(self, syllables, char2idx):\n",
    "        self.syllables = syllables\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.syllables)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        syll = self.syllables[idx]\n",
    "        #Convert each character to its corresponding index\n",
    "        token_indices = [self.char2idx[c] for c in syll]\n",
    "        length = len(token_indices)\n",
    "        return torch.tensor(token_indices, dtype=torch.long), length\n",
    "\n",
    "def collate_fn_syl(batch):\n",
    "    sequences, lengths = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    return sequences_padded, torch.tensor(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y3WyM7FIjhX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 3.2698\n",
      "Epoch 51/300, Loss: 0.1675\n",
      "Epoch 101/300, Loss: 0.0392\n",
      "Epoch 151/300, Loss: 0.0174\n",
      "Epoch 201/300, Loss: 0.0098\n",
      "Epoch 251/300, Loss: 0.0062\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "charsIndexes = {'0': 0,'a': 1,'b': 2,'c': 3,'d': 4,'e': 5,'f': 6,'g': 7,'h': 8,'i': 9,'j': 10,'k': 11,'l': 12,'m': 13,'n': 14,'o': 15,'p': 16,'q': 17,'r': 18,'s': 19,'t': 20,'u': 21,'v': 22,'w': 23,'x': 24,'y': 25,'z': 26}\n",
    "indexesChars = {v: k for k, v in charsIndexes.items()} #reverse mapping\n",
    "syllables = [] #dataset for unsupervised learning, '0' is padding\n",
    "\n",
    "#Get syllable tokens\n",
    "for tok in input_tokens:\n",
    "  if not tok.isupper() and not tok.isnumeric() and not '?' in tok and not '+' in tok and not '*' in tok and not 'separatum' in tok and not 'BOS' in tok and not 'CAP' in tok and not 'EQU' in tok and not 'OVIS' in tok and not 'SUS' in tok and not 'TELA' in tok and not 'VIN' in tok and not ' ' in tok:\n",
    "    syllables.append(tok)\n",
    "max_len_syl = max(len(syl) for syl in syllables)\n",
    "#Pad tokens to max length (3)\n",
    "for i in range(len(syllables)):\n",
    "  if len(syllables[i]) < max_len_syl:\n",
    "    syllables[i] += '0'*(max_len_syl-len(syllables[i]))\n",
    "\n",
    "#get dataset\n",
    "dataset = SyllableDataset(syllables, charsIndexes)\n",
    "data_loader_syl = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_syl)\n",
    "\n",
    "#define autoencoder\n",
    "vocab_size = len(charsIndexes)\n",
    "embedding_dim =  16\n",
    "hidden_dim = 32\n",
    "latent_dim = EMBEDDINGS_DIM #THIS IS THE SIZE THAT MUST BE EQUAL TO TRANSFORMER VECTOR, Dimension of encoder result\n",
    "\n",
    "syllable_autoencoder = SyllableAutoencoder(vocab_size, embedding_dim, hidden_dim, latent_dim, charsIndexes) ##IMPORTANT, this is the autoencoder model used later to create and save the embeddings\n",
    "optimizer = optim.Adam(syllable_autoencoder.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=charsIndexes['0'])\n",
    "\n",
    "#Train autoencoder\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch, lengths in data_loader_syl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, latent = syllable_autoencoder(batch, lengths)\n",
    "        logits_flat = logits.view(-1, vocab_size)  # shape: (batch_size*seq_length, vocab_size)\n",
    "        targets_flat = batch.view(-1)              # shape: (batch_size*seq_length,)\n",
    "\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader_syl)\n",
    "    if epoch % 50 == 0:\n",
    "      print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cCOLXMoi3m5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth & Predicted: \n",
      "[['m', '0', '0'], ['r', 'e', '0'], ['q', 's', '0'], ['w', 'o', '0'], ['t', 'u', '0'], ['m', 'i', '0'], ['n', 'a', '0'], ['n', 'e', '0']]\n",
      "[['m', 'o', 'o'], ['r', 'e', 'o'], ['q', 's', 'o'], ['w', 'o', 'o'], ['t', 'u', 'o'], ['m', 'i', 'i'], ['n', 'a', 'o'], ['n', 'e', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['t', 'o', '0'], ['j', 'a', '0'], ['s', 'i', '0'], ['d', 'a', '0'], ['k', 'a', '0'], ['k', 'o', '0'], ['t', 'a', '0'], ['q', 'i', '0']]\n",
      "[['t', 'o', 'o'], ['j', 'a', 'o'], ['s', 'i', 'i'], ['d', 'a', 'o'], ['k', 'a', 'o'], ['k', 'o', 'o'], ['t', 'a', 'o'], ['q', 'i', 'i']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['n', 'i', '0'], ['w', 'e', '0'], ['d', 'w', 'e'], ['m', 'e', '0'], ['t', 'e', '0'], ['f', '0', '0'], ['o', '0', '0'], ['r', 'i', '0']]\n",
      "[['n', 'i', 'i'], ['w', 'e', 'o'], ['d', 'w', 'e'], ['m', 'e', 'o'], ['t', 'e', 'o'], ['f', 'f', 'o'], ['o', 'o', 'o'], ['r', 'i', 'i']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['i', '0', '0'], ['z', 'o', '0'], ['a', '0', '0'], ['p', 'h', 'u'], ['d', 'e', '0'], ['m', 'u', '0'], ['p', 'e', '0'], ['s', 'e', '0']]\n",
      "[['i', 'i', 'i'], ['z', 'o', 'o'], ['a', 'o', 'o'], ['p', 'h', 'u'], ['d', 'e', 'o'], ['m', 'u', 'o'], ['p', 'e', 'o'], ['s', 'e', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['j', 'o', '0'], ['z', 'a', '0'], ['h', 'a', '0'], ['d', 'u', '0'], ['k', 'u', '0'], ['n', 'o', '0'], ['j', 'e', '0'], ['p', 't', 'e']]\n",
      "[['j', 'o', 'o'], ['z', 'a', 'o'], ['h', 'a', 'o'], ['d', 'u', 'o'], ['k', 'u', 'o'], ['n', 'o', 'o'], ['j', 'e', 'o'], ['p', 't', 'e']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['r', 'a', 'i'], ['s', 'u', '0'], ['p', 'a', '0'], ['n', 'u', '0'], ['e', '0', '0'], ['q', 'o', '0'], ['t', 'w', 'e'], ['p', 'u', '0']]\n",
      "[['r', 'a', 'i'], ['s', 'u', 'o'], ['p', 'a', 'o'], ['n', 'u', 'o'], ['e', 'o', 'o'], ['q', 'o', 'o'], ['t', 'w', 'e'], ['p', 'u', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['u', '0', '0'], ['q', 'e', '0'], ['k', 'e', '0'], ['r', 'y', 'o'], ['w', 'a', '0'], ['m', 'o', '0'], ['d', 'w', 'o'], ['s', 'o', '0']]\n",
      "[['u', 'o', 'o'], ['q', 'e', 'o'], ['k', 'e', 'o'], ['r', 'y', 'o'], ['w', 'a', 'o'], ['m', 'o', 'o'], ['d', 'w', 'o'], ['s', 'o', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['a', 'u', '0'], ['r', 'u', '0'], ['t', 'y', 'a'], ['t', 'i', '0'], ['r', 'y', 'a'], ['t', 'w', 'o'], ['z', 'e', '0'], ['p', 'o', '0']]\n",
      "[['a', 'u', 'o'], ['r', 'u', 'o'], ['t', 'y', 'a'], ['t', 'i', 'i'], ['r', 'y', 'a'], ['t', 'w', 'o'], ['z', 'e', 'o'], ['p', 'o', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['d', 'o', '0'], ['m', 'u', 't'], ['s', 'a', '0'], ['q', 'a', '0'], ['n', 'w', 'a'], ['k', 'i', '0'], ['r', 'o', '0'], ['m', 'a', '0']]\n",
      "[['d', 'o', 'o'], ['m', 'u', 't'], ['s', 'a', 'o'], ['q', 'a', 'o'], ['n', 'w', 'a'], ['k', 'i', 'i'], ['r', 'o', 'o'], ['m', 'a', 'o']]\n",
      "\n",
      "\n",
      "GroundTruth & Predicted: \n",
      "[['r', 'a', '0'], ['p', 'i', '0'], ['d', 'i', '0'], ['a', 'i', '0'], ['w', 'i', '0']]\n",
      "[['r', 'a', 'o'], ['p', 'i', 'i'], ['d', 'i', 'i'], ['a', 'i', 'i'], ['w', 'i', 'i']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test just to see if it works as autoencoder, ignores padding, input == output for good autoencoding\n",
    "with torch.no_grad():\n",
    "  for batch, lengths in data_loader_syl:\n",
    "    logits, latent = syllable_autoencoder(batch, lengths)\n",
    "    g_truth = list(batch.detach().numpy())\n",
    "    for el in range(len(g_truth)):\n",
    "      g_truth[el] = [indexesChars[i] for i in g_truth[el]]\n",
    "    probs = syllable_autoencoder.get_probabilities(logits) #(batch, syllable, char)\n",
    "    predicted_indices = list(torch.argmax(probs, dim=-1).detach().numpy())\n",
    "    for el in range(len(predicted_indices)):\n",
    "      predicted_indices[el] = [indexesChars[i] for i in predicted_indices[el]]\n",
    "    print(\"GroundTruth & Predicted: \")\n",
    "    print(g_truth)\n",
    "    print(predicted_indices)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TDQ_yo33w7KX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/beegfs/home/amaiola/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['da', 'de', 'di', 'do', 'du', 'dwe', 'dwo']\n",
      "Cluster 1: ['i']\n",
      "Cluster 2: ['m', 'ma', 'me', 'mi', 'mo', 'mu', 'mut']\n",
      "Cluster 3: ['rya', 'ryo']\n",
      "Cluster 4: ['pa', 'pe', 'phu', 'pi', 'po', 'pte', 'pu']\n",
      "Cluster 5: ['qa', 'qe', 'qi', 'qo']\n",
      "Cluster 6: ['sa', 'se', 'si', 'so', 'su']\n",
      "Cluster 7: ['na', 'ne', 'ni', 'no', 'nu', 'nwa']\n",
      "Cluster 8: ['ta', 'te', 'ti', 'to', 'tu', 'twe', 'two', 'tya']\n",
      "Cluster 9: ['ja', 'je', 'jo']\n",
      "Cluster 10: ['za', 'ze', 'zo']\n",
      "Cluster 11: ['ka', 'ke', 'ki', 'ko', 'ku']\n",
      "Cluster 12: ['f']\n",
      "Cluster 13: ['a', 'u']\n",
      "Cluster 14: ['ha']\n",
      "Cluster 15: ['wa', 'we', 'wi', 'wo']\n",
      "Cluster 16: ['e', 'o']\n",
      "Cluster 17: ['ai', 'au']\n",
      "Cluster 18: ['qs']\n",
      "Cluster 19: ['ra', 'rai', 're', 'ri', 'ro', 'ru']\n"
     ]
    }
   ],
   "source": [
    "#Assess quality of embeddings through clustering\n",
    "from sklearn.cluster import KMeans\n",
    "lengths = [3] #each syllable is always of len 3\n",
    "embeddings_per_syllable = syllable_autoencoder.get_embeddings(syllables)\n",
    "\n",
    "syllable_keys = list(embeddings_per_syllable.keys())\n",
    "embedding_list = [embeddings_per_syllable[s] for s in syllable_keys]\n",
    "\n",
    "\n",
    "num_clusters = 20\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(embedding_list)\n",
    "\n",
    "for cluster_id in range(num_clusters): #print clusters\n",
    "    cluster_syllables = [syllable_keys[i] for i in range(len(syllable_keys)) if clusters[i] == cluster_id]\n",
    "    print(f\"Cluster {cluster_id}: {cluster_syllables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "COyTr8eNCphK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1386787/2456792464.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#PLOT CLUSTERS PRINTED IN PREVIOUS CELL\n",
    "embedding_array = np.array(embedding_list)  #shape: (num_samples, embedding_dim)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embedding_2d = tsne.fit_transform(embedding_array)\n",
    "\n",
    "colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_id in range(num_clusters):\n",
    "    indices = [i for i, c in enumerate(clusters) if c == cluster_id]\n",
    "    plt.scatter(embedding_2d[indices, 0], embedding_2d[indices, 1],\n",
    "                color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"Clustering of Syllable Embeddings (t-SNE Projection)\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Jzl6SUAGJitw"
   },
   "outputs": [],
   "source": [
    "#Save embeddings on file with one-hot-encoded mapping\n",
    "syllable_autoencoder.save_embeddings(f\"syllable_embeddings_one-hot-encoded_{EMBEDDINGS_DIM}.npy\", syllables, tokenList=input_tokens)\n",
    "syllable_autoencoder.save_embeddings(f\"syllable_embeddings_{EMBEDDINGS_DIM}.npy\", syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_nZv6xMFPtCf"
   },
   "outputs": [],
   "source": [
    "#Load embeddings for further use\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(f\"syllable_embeddings_one-hot-encoded_{EMBEDDINGS_DIM}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9svVyxHZ8MRi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Frequences and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DrBdFVlW8U_M"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_tf_idf(sequences):\n",
    "    # Step 1: Initialize data structures\n",
    "    eps = 1e-10\n",
    "    words_tf_idf = defaultdict(lambda: {\"tf\": 0, \"df\": 0, \"tf_idf\": eps, \"unigram_freq\": eps})\n",
    "    tokens_tf_idf = defaultdict(lambda: {\"tf\": 0, \"df\": 0, \"tf_idf\": eps, \"unigram_freq\": eps})\n",
    "\n",
    "    total_sequences = len(sequences)  # Total number of sequences for IDF calculation\n",
    "    total_words = 0  # Total word count across all sequences\n",
    "    total_tokens = 0  # Total token count across all sequences\n",
    "\n",
    "    # Step 2: Count TF and DF\n",
    "    for sequence in sequences:\n",
    "        words = sequence.split()\n",
    "        seen_words = set()  # Track words seen in a single sequence\n",
    "        seen_tokens = set()  # Track tokens seen in a single sequence\n",
    "\n",
    "        for w in words:\n",
    "            words_tf_idf[w][\"tf\"] += 1  # Increment term frequency\n",
    "            total_words += 1  # Count total words\n",
    "\n",
    "            if w not in seen_words:\n",
    "                words_tf_idf[w][\"df\"] += 1  # Increment document frequency once per sequence\n",
    "                seen_words.add(w)\n",
    "\n",
    "            # Token-level processing\n",
    "            tokens = w.split(\"-\")\n",
    "            for tok in tokens:\n",
    "                tokens_tf_idf[tok][\"tf\"] += 1  # Increment token frequency\n",
    "                total_tokens += 1  # Count total tokens\n",
    "\n",
    "                if tok not in seen_tokens:\n",
    "                    tokens_tf_idf[tok][\"df\"] += 1  # Increment token document frequency once per sequence\n",
    "                    seen_tokens.add(tok)\n",
    "\n",
    "    # Step 3: Compute TF-IDF and Unigram Frequency\n",
    "    for w in words_tf_idf:\n",
    "        df = words_tf_idf[w][\"df\"]\n",
    "        idf = math.log((total_sequences + 1) / (df + 1))  # Smoothed IDF\n",
    "        words_tf_idf[w][\"tf_idf\"] = words_tf_idf[w][\"tf\"] * idf  # Compute TF-IDF\n",
    "        words_tf_idf[w][\"unigram_freq\"] = words_tf_idf[w][\"tf\"] / total_words  # Compute unigram probability\n",
    "\n",
    "    for tok in tokens_tf_idf:\n",
    "        df = tokens_tf_idf[tok][\"df\"]\n",
    "        idf = math.log((total_sequences + 1) / (df + 1))  # Smoothed IDF\n",
    "        tokens_tf_idf[tok][\"tf_idf\"] = tokens_tf_idf[tok][\"tf\"] * idf  # Compute TF-IDF\n",
    "        tokens_tf_idf[tok][\"unigram_freq\"] = tokens_tf_idf[tok][\"tf\"] / total_tokens  # Compute unigram probability\n",
    "\n",
    "    return words_tf_idf, tokens_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F1UaI-7pCsyF"
   },
   "outputs": [],
   "source": [
    "sequences = train_y + test_y\n",
    "word_results, token_results = compute_tf_idf(sequences)\n",
    "\n",
    "# Print results\n",
    "#print(\"Word TF-IDF Scores:\")\n",
    "#for word, values in word_results.items():\n",
    "#    print(f\"{word}: {values}\")\n",
    "\n",
    "#print(\"\\nToken TF-IDF Scores:\")\n",
    "#for token, values in token_results.items():\n",
    "#    print(f\"{token}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWT6GrV--8Rh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PJHEKgzKuQT7"
   },
   "outputs": [],
   "source": [
    "def inverse_mapping(tokens=output_tokens):\n",
    "    inv_map = {}\n",
    "    for i, t in enumerate(tokens):\n",
    "        inv_map[i] = t\n",
    "    return inv_map\n",
    "\n",
    "inv_map = inverse_mapping(output_tokens)\n",
    "inv_map_input = inverse_mapping(input_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8MXG-0zCP929"
   },
   "outputs": [],
   "source": [
    "k = 70\n",
    "top_k_frequent = sorted(token_results.keys(), key=lambda x: token_results[x][\"unigram_freq\"], reverse=True)[:k]\n",
    "\n",
    "def one_hot_encode_sequences(batch, vocab_size):\n",
    "    #takes lists of integer tokens and outputs batches of shape: (batch_size, seq_len)\n",
    "    tensor_batch = torch.tensor(batch, dtype=torch.long)\n",
    "    return torch.nn.functional.one_hot(tensor_batch, num_classes=vocab_size).float()\n",
    "\n",
    "\n",
    "# replace symbols with numerical labels to prepare one_hot_encoding\n",
    "def tokenize_sequences(dataset, mapping=input_mapping, use_SOS=True, use_EOS=False, distortion_prob=1, top_k_frequent=top_k_frequent):\n",
    "    assert distortion_prob >= 0 and distortion_prob <= 1, \"Distortion probability must be between 0 and 1\"\n",
    "    result = []\n",
    "    for i, subset in enumerate(dataset):\n",
    "        res = []\n",
    "        for seq in subset:\n",
    "            distorted_sequence = random.random() < distortion_prob\n",
    "            one_hot_encoded = [mapping[\"SOS\"]] if use_SOS else []\n",
    "            seq = seq.split(\" \")\n",
    "            for j, w in enumerate(seq):\n",
    "                if \"-\" in w:\n",
    "                    w = w.split(\"-\")\n",
    "                    for sign in w:\n",
    "                        if distorted_sequence and sign not in top_k_frequent and sign != \"?\":\n",
    "                            one_hot_encoded.append(mapping[\"SYL\"])\n",
    "                        else:\n",
    "                            one_hot_encoded.append(mapping[sign])\n",
    "                elif distorted_sequence and w not in top_k_frequent and w != \"?\":\n",
    "                    if w == \"1\" or w == \"2\" or w == \"NUM\":\n",
    "                        one_hot_encoded.append(mapping[\"NUMERAL\"])\n",
    "                    else:\n",
    "                        one_hot_encoded.append(mapping[\"LOG\"])\n",
    "                else:\n",
    "                    one_hot_encoded.append(mapping[w])\n",
    "\n",
    "                if j != len(seq) - 1:\n",
    "                    one_hot_encoded.append(mapping[\" \"])\n",
    "                elif use_EOS:\n",
    "                    one_hot_encoded.append(mapping[\"EOS\"])\n",
    "            res.append(one_hot_encoded)\n",
    "        result.append(res)\n",
    "    return result\n",
    "\n",
    "\n",
    "#Get labelsfor each sequence to get encoded dataset\n",
    "input_dim = len(input_tokens)\n",
    "output_dim = len(output_tokens)\n",
    "\n",
    "# lazy modification to avoid losing sequences\n",
    "seq_train_x, seq_train_y, seq_test_x, seq_test_y= train_x, train_y, test_x, test_y\n",
    "dataset = [seq_train_x, seq_test_x]\n",
    "# For BRNN: src starts with SOS\n",
    "train_x, test_x = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_X, use_EOS=USE_EOS_IN_X, distortion_prob=0)\n",
    "distorted_train_x, distorted_test_x = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_X, use_EOS=USE_EOS_IN_X, distortion_prob=1)\n",
    "dataset = [seq_train_y, seq_test_y]\n",
    "\n",
    "\n",
    "train_y, test_y = tokenize_sequences(dataset, use_SOS=USE_SOS_IN_Y, use_EOS=USE_EOS_IN_Y, distortion_prob=0)\n",
    "\n",
    "#Compute lengths before padding\n",
    "train_lengths = torch.tensor([len(seq) for seq in train_x])\n",
    "test_lengths = torch.tensor([len(seq) for seq in test_x])\n",
    "\n",
    "#Pad the dataset for the RNN (Transformers need sequences padded to same length, GPT says)\n",
    "train_x = pad_sequences(train_x)\n",
    "train_y = pad_sequences(train_y)\n",
    "test_x = pad_sequences(test_x)\n",
    "test_y = pad_sequences(test_y)\n",
    "distorted_train_x = pad_sequences(distorted_train_x)\n",
    "distorted_test_x = pad_sequences(distorted_test_x)\n",
    "\n",
    "if USE_ONE_HOT_ENCODING:\n",
    "    #Convert to one-hot encoded sequences\n",
    "    train_x = one_hot_encode_sequences(train_x, input_dim)\n",
    "    test_x = one_hot_encode_sequences(test_x, input_dim)\n",
    "    distorted_train_x = one_hot_encode_sequences(distorted_train_x, input_dim)\n",
    "    distorted_test_x = one_hot_encode_sequences(distorted_test_x, input_dim)\n",
    "    train_y = one_hot_encode_sequences(train_y, output_dim)\n",
    "    test_y = one_hot_encode_sequences(test_y, output_dim)\n",
    "\n",
    "p_unigram = np.zeros(len(output_tokens))\n",
    "tf_idf = np.zeros(len(output_tokens))\n",
    "for tok in token_results:\n",
    "    p_unigram[output_mapping[tok]] = token_results[tok][\"unigram_freq\"]\n",
    "    tf_idf[output_mapping[tok]] = token_results[tok][\"tf_idf\"]\n",
    "p_unigram = torch.tensor(p_unigram)\n",
    "tf_idf = torch.tensor(tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PbyMYI9vKaM_"
   },
   "outputs": [],
   "source": [
    "#seq_train_x[1], train_x[1], train_y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "S8OhIqILttVi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNMMKBIcIZwf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqTNdrIyIfUP"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sentences, split_sequences=False, remove_hyphen=True):\n",
    "    \"\"\"\n",
    "    Process Linear B text, handling numerals/logograms.\n",
    "\n",
    "    :param sentences: List of Linear B sentences (e.g., [\"we-we-si-jo OVISm 3 OVISf\"])\n",
    "    :return: Tokenized and normalized sentences\n",
    "        \"\"\"\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize by spaces (since Linear B syllabograms are typically space-separated)\n",
    "        if split_sequences:\n",
    "            tokens = sentence.split()\n",
    "            if remove_hyphen:\n",
    "                tokens = [token.replace(\"-\", \"\") for token in tokens]\n",
    "            processed_sentences.append(tokens)\n",
    "        else:\n",
    "            if remove_hyphen:\n",
    "                processed_sentences.append(sentence.replace(\"-\", \"\"))\n",
    "            else:\n",
    "                processed_sentences.append(sentence)\n",
    "    return processed_sentences\n",
    "\n",
    "\n",
    "sequences = preprocess_text(seq_train_y)\n",
    "test_sequences = preprocess_text(seq_test_y)\n",
    "sequences.extend(test_sequences)\n",
    "\n",
    "#convert the list of sequences into a single string with spaces\n",
    "text_data = \"\\n\".join(sequences)\n",
    "\n",
    "sp_file_name = os.path.join(prefix_path, \"linear_b_spm.model\")\n",
    "sp_input_dim = 500 #vocab size of sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COBurZ8xgPT9"
   },
   "outputs": [],
   "source": [
    "new_tokens = [\"?\", \"?\"]\n",
    "\n",
    "train_spm = True\n",
    "if train_spm:\n",
    "    seq_temp_file = \"temp_linear_b_text.txt\"\n",
    "    # Write the text data to a temporary file\n",
    "    with open(seq_temp_file, \"w\") as f:\n",
    "        f.write(text_data)\n",
    "\n",
    "    # Now train the SentencePiece model using this temporary file\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=seq_temp_file,  # Temporary file\n",
    "        model_prefix=sp_file_name[:-6],         # Output model name\n",
    "        vocab_size=sp_input_dim,   # Adjust based on your dataset size\n",
    "        model_type='unigram',             # Use 'unigram' or 'bpe'\n",
    "        pad_id=0,\n",
    "        unk_id=2,\n",
    "        bos_id=1, # we are forced to initialize this params\n",
    "        eos_id=3, # we are forced to initialize this params\n",
    "        pad_piece='<pad>',\n",
    "        unk_piece='<unk>',\n",
    "        bos_piece='<s>',\n",
    "        eos_piece='</s>',\n",
    "        user_defined_symbols=new_tokens\n",
    "    )\n",
    "\n",
    "    print(\"Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eEVmYWuI_gL"
   },
   "outputs": [],
   "source": [
    "def tokenize_sequences(sequences, sentence_piece_model):\n",
    "    tokenized_sequences = []\n",
    "    for seq in sequences:\n",
    "        tokens = sp.encode(seq)\n",
    "        tokenized_sequences.append(tokens)\n",
    "    return tokenized_sequences\n",
    "\n",
    "def tokenize_spaces(input):\n",
    "    res = []\n",
    "    for tok_list in input:\n",
    "        our_tok_list = [1] # SOS\n",
    "        for i, tok in enumerate(tok_list):\n",
    "            tok_str = sp.id_to_piece(tok)\n",
    "            if tok_str.startswith(\"\") and i != 0:\n",
    "                our_tok_list.append(3) # add spaces with unused token for eos!\n",
    "            our_tok_list.append(tok)\n",
    "        res.append(our_tok_list)\n",
    "    return res\n",
    "#load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor(model_file=sp_file_name)\n",
    "\n",
    "sequences = preprocess_text(seq_train_x)\n",
    "test_sequences = preprocess_text(seq_test_x)\n",
    "\n",
    "spm_train_x = tokenize_sequences(sequences, sp)\n",
    "spm_test_x = tokenize_sequences(test_sequences, sp)\n",
    "\n",
    "spm_train_x = tokenize_spaces(spm_train_x)\n",
    "spm_test_x = tokenize_spaces(spm_test_x)\n",
    "spm_lengths_train = torch.tensor([len(seq) for seq in spm_train_x])\n",
    "spm_lengths_test = torch.tensor([len(seq) for seq in spm_test_x])\n",
    "spm_train_x = pad_sequences(spm_train_x)\n",
    "spm_test_x = pad_sequences(spm_test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seo1_O3hf7VN"
   },
   "source": [
    "#### Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3hG9lpgZJCn"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "tokens = [tok.item() for tok in spm_train_x[idx] if tok != 0]\n",
    "tokens = [sp.id_to_piece(tok) for tok in tokens]\n",
    "print(sp.encode(\"OVISf\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHUtiVPPZeSo"
   },
   "outputs": [],
   "source": [
    "print(sp.id_to_piece(6))  # Get token corresponding to ID 5\n",
    "print(sp.id_to_piece(1))  # Get token corresponding to ID 0\n",
    "# Decode token IDs back into text\n",
    "decoded_text = sp.decode([286, 7, 4, 31, 4, 6, 1, 160, 10, 162, 38, 7, 9, 21, 7, 4])\n",
    "\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p3GM8JLqaQy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What the hellish graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLc8WQfJzx2X"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define the BRNN Model\n",
    "class BRNNGraphEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNGraphEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-3)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n",
    "\n",
    "    def tok_embeddings(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "    def rnn_embeddings(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhG9FTzZ0fhq"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, train_x, train_y, test_x, test_y, num_epochs=10, batch_size=32, lr=0.001, device=device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.NLLLoss(ignore_index=0)  # Ignore padding index\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Prepare DataLoader\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            lengths = (x_batch != 0).sum(dim=1)  # Compute sequence lengths\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch, lengths)\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                lengths = (x_batch != 0).sum(dim=1)\n",
    "\n",
    "                output = model(x_batch, lengths)\n",
    "                predictions = output.argmax(dim=-1)\n",
    "\n",
    "                mask = y_batch != 0  # Ignore padding\n",
    "                correct += (predictions[mask] == y_batch[mask]).sum().item()\n",
    "                total += mask.sum().item()\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfLjBg-n2mnH"
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, method='t-SNE'):\n",
    "    \"\"\"\n",
    "    Visualizes the embeddings using either PCA or t-SNE, and optionally adds labels from `input_tokens`.\n",
    "\n",
    "    Arguments:\n",
    "        embeddings: The node embeddings to visualize.\n",
    "        labels: The labels for coloring the embeddings (optional).\n",
    "        method: The dimensionality reduction method ('PCA' or 't-SNE').\n",
    "        input_tokens: A list of token labels (words) to display next to the points.\n",
    "    \"\"\"\n",
    "    # Apply dimensionality reduction (PCA or t-SNE)\n",
    "    if method == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    elif method == 't-SNE':\n",
    "        tsne = TSNE(n_components=2)\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the reduced embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=30)\n",
    "\n",
    "    plt.title(f\"Token Embeddings Visualization ({method})\")\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gupBNQoI16jQ"
   },
   "outputs": [],
   "source": [
    "model = BRNNGraphEmbedding(len(input_tokens)-1, EMBEDDINGS_DIM, 2*EMBEDDINGS_DIM, 8, 0.2)\n",
    "train_model(model, train_x, train_y, test_x, test_y, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYPETcvI4zCg"
   },
   "outputs": [],
   "source": [
    "seq = 'a-e-i-o-u-da-de-di-do-du-ja-je-jo-ka-ke-ki-ko-ku-ma-me-mi-mo-mu-na-ne-ni-no-nu-pa-pe-pi-po-pu-qa-qe-qi-qo-ra-re-ri-ro-ru-sa-se-si-so-su-ta-te-ti-to-tu-wa-we-wi-wo-za-ze-zo-ha-ai-au-dwo-nwa-phu-pte-rya-rai-ryo-tya'\n",
    "seq = (tokenize_sequences([[seq]], use_SOS=False, use_EOS=False))[0][0]\n",
    "length = torch.tensor([len(seq)]).to(device)\n",
    "seq += [0] * (train_x.shape[1] - len(seq))\n",
    "seq = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "emb = model.tok_embeddings(seq).cpu().detach()\n",
    "print(emb.shape)\n",
    "\n",
    "rnn_emb = model.rnn_embeddings(seq, length).cpu().detach()\n",
    "print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMHqPkbAAVIV"
   },
   "outputs": [],
   "source": [
    "emb = emb.squeeze(0)\n",
    "emb = emb[:length[0].item(), :]\n",
    "emb = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwftR5l_Bu42"
   },
   "outputs": [],
   "source": [
    "rnn_emb = rnn_emb.squeeze(0)\n",
    "rnn_emb = rnn_emb[:length[0].item(), :]\n",
    "rnn_emb = rnn_emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPmE7Rwe_kIf"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RH6b6kmBwia"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(rnn_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeqnih8Oqq8c"
   },
   "outputs": [],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK88rcyNqhxN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define node features (5 nodes, 3 features each)\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "    [10.0, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Define edge index (4 edges, where each column is a pair of (source, target) nodes)\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3],  # source nodes\n",
    "    [1, 2, 3, 4]   # target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Define edge attributes (optional)\n",
    "edge_attr = torch.tensor([\n",
    "    [0.5],  # weight of edge (0 -> 1)\n",
    "    [0.6],  # weight of edge (1 -> 2)\n",
    "    [0.7],  # weight of edge (2 -> 3)\n",
    "    [0.8]   # weight of edge (3 -> 4)\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Define node labels (optional, for node classification)\n",
    "y = torch.tensor([0, 1, 0, 1, 0], dtype=torch.long)\n",
    "\n",
    "# Create Data object\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMl4id-vGs_A"
   },
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er8DJ-hIxCZg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LmTusRDhJp4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fasttext in ./.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from fasttext) (1.23.5)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./.local/lib/python3.10/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./.local/lib/python3.10/site-packages (from fasttext) (80.9.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DO9WUvlb2JcP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'konoso': [(0.6594921350479126, 'kumonoso'), (0.6537270545959473, 'konosode'), (0.6365762948989868, 'konosija'), (0.5780808329582214, 'konosijo'), (0.5686375498771667, 'kononipi'), (0.5430461764335632, 'nosiro'), (0.49234628677368164, 'kono'), (0.4802187979221344, 'diso'), (0.47665509581565857, 'konija'), (0.47106119990348816, 'koso')]\n",
      "Nearest neighbors for 'potinija': [(0.7025516033172607, 'potinijawe'), (0.6799739003181458, 'potinijaweja'), (0.6756334900856018, 'potinijawijo'), (0.6643137335777283, 'sitopotinija'), (0.6455649137496948, 'upojopotinija'), (0.644006609916687, 'erewijopotinija'), (0.6003291010856628, 'potinijaweijo'), (0.5768953561782837, 'atanapotinija'), (0.5700084567070007, 'poti'), (0.5374864935874939, 'potinajo')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "class LinearBWordEmbedding:\n",
    "    def __init__(self,\n",
    "                 model_path=os.path.join(os.path.join(prefix_path, \"fasttext/\"), f\"linearb_fasttext_model_{EMBEDDINGS_DIM}.bin\"),\n",
    "                 train_data_path=\"linearb_train.txt\",\n",
    "                 model='skipgram',\n",
    "                 dim=EMBEDDINGS_DIM,\n",
    "                 ws=3,\n",
    "                 min_count=1,\n",
    "                 epoch=500,\n",
    "                 lr=0.1,\n",
    "                 minn=1,\n",
    "                 maxn=4,\n",
    "                 neg=5,\n",
    "                 loss='softmax',\n",
    "                 bucket=2000000,\n",
    "                 thread=4,\n",
    "                 t=0.0001,\n",
    "                 word_ngrams=1,\n",
    "                 verbose=2\n",
    "                ):\n",
    "        self.model_path = model_path\n",
    "        self.train_data_path = train_data_path\n",
    "        self.model = None\n",
    "\n",
    "        self.params = {\n",
    "            'model': model,\n",
    "            'dim': dim,\n",
    "            'ws': ws,\n",
    "            'minCount': min_count,\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'minn': minn,\n",
    "            'maxn': maxn,\n",
    "            'neg': neg,\n",
    "            'loss': loss,\n",
    "            'bucket': bucket,\n",
    "            'thread': thread,\n",
    "            't': t,\n",
    "            'wordNgrams': word_ngrams,\n",
    "            'verbose': verbose\n",
    "        }\n",
    "\n",
    "    def preprocess_text(self, sentences):\n",
    "        processed = []\n",
    "        uppercase_pattern = re.compile(r'[A-Z]')\n",
    "        digits_only_pattern = re.compile(r'^\\d+$')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split()\n",
    "            filtered_tokens = []\n",
    "            for token in tokens:\n",
    "                token_no_hyphen = token.replace('-', '')\n",
    "                if uppercase_pattern.search(token_no_hyphen):\n",
    "                    continue\n",
    "                if digits_only_pattern.match(token_no_hyphen):\n",
    "                    continue\n",
    "                filtered_tokens.append(token_no_hyphen)\n",
    "            processed.append(filtered_tokens)\n",
    "        return processed\n",
    "\n",
    "    def _write_train_data(self, sentences):\n",
    "        with open(self.train_data_path, 'w', encoding='utf-8') as f:\n",
    "            for tokens in sentences:\n",
    "                f.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "    def train(self, sentences):\n",
    "        tokenized = self.preprocess_text(sentences)\n",
    "        self._write_train_data(tokenized)\n",
    "        self.model = fasttext.train_unsupervised(\n",
    "            input=self.train_data_path,\n",
    "            model=self.params['model'],\n",
    "            dim=self.params['dim'],\n",
    "            ws=self.params['ws'],\n",
    "            minCount=self.params['minCount'],\n",
    "            epoch=self.params['epoch'],\n",
    "            lr=self.params['lr'],\n",
    "            minn=self.params['minn'],\n",
    "            maxn=self.params['maxn'],\n",
    "            neg=self.params['neg'],\n",
    "            loss=self.params['loss'],\n",
    "            bucket=self.params['bucket'],\n",
    "            thread=self.params['thread'],\n",
    "            t=self.params['t'],\n",
    "            wordNgrams=self.params['wordNgrams'],\n",
    "            verbose=self.params['verbose']\n",
    "        )\n",
    "        # Optional: remove training data file after training\n",
    "        os.remove(self.train_data_path)\n",
    "\n",
    "    def save_model(self, path=None):  #saves to default path 'linearb_fasttext_model.bin'\n",
    "        if self.model:\n",
    "            save_path = path if path else self.model_path\n",
    "            self.model.save_model(save_path)\n",
    "        else:\n",
    "            print(\"No model trained yet.\")\n",
    "\n",
    "    def load_model(self, path=None):  #loads from default path 'linearb_fasttext_model.bin'\n",
    "        load_path = path if path else self.model_path\n",
    "        self.model = fasttext.load_model(load_path)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if self.model:\n",
    "            return self.model.get_word_vector(word)\n",
    "        return None\n",
    "\n",
    "    def find_nearest_neighbors(self, word, k=5):\n",
    "        if self.model:\n",
    "            return self.model.get_nearest_neighbors(word, k)\n",
    "        return None\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "\n",
    "#traininig the model\n",
    "#embedding_model.train(sequences)\n",
    "#embedding_model.save_model()\n",
    "embedding_model.load_model() \n",
    "\n",
    "#get some nearest neighbors\n",
    "neighbors = embedding_model.find_nearest_neighbors(\"konoso\", k=10)\n",
    "print(f\"Nearest neighbors for 'konoso': {neighbors}\")\n",
    "\n",
    "neighbors = embedding_model.find_nearest_neighbors(\"potinija\", k=10) #get words similar to \"mistress\"\n",
    "print(f\"Nearest neighbors for 'potinija': {neighbors}\\n\")\n",
    "\n",
    "\n",
    "def nearest_neighbors_from_vector(model, vector, k=10):\n",
    "    all_words = model.model.get_words()\n",
    "    similarities = []\n",
    "    for w in all_words:\n",
    "        w_vec = model.get_vector(w)\n",
    "        sim = np.dot(vector, w_vec) / (np.linalg.norm(vector) * np.linalg.norm(w_vec))\n",
    "        similarities.append((sim, w))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words nearest to this v5 are:  [(0.65686536, 'wanaka'), (0.6448369, 'pawoko'), (0.6283691, 'dekutuwoko'), (0.6058625, 'tokosowoko'), (0.5709508, 'damoko'), (0.5689939, 'watuoko'), (0.56247073, 'ijerowoko'), (0.55798674, 'wanakate'), (0.55643743, 'kowirowoko'), (0.5383182, 'wanakatera')]\n"
     ]
    }
   ],
   "source": [
    "v2 = embedding_model.get_vector(\"wanaka\")\n",
    "v3 = embedding_model.get_vector(\"woko\")\n",
    "v4 = embedding_model.get_vector(\"qasireu\")\n",
    "v5 = v2 + v3\n",
    "\n",
    "def nearest_neighbors_from_vector(model, vector, k=10):\n",
    "    all_words = model.model.get_words()\n",
    "    similarities = []\n",
    "    for w in all_words:\n",
    "        w_vec = model.get_vector(w)\n",
    "        sim = np.dot(vector, w_vec) / (np.linalg.norm(vector) * np.linalg.norm(w_vec))\n",
    "        similarities.append((sim, w))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "print(\"\\n Words nearest to this v5 are: \",nearest_neighbors_from_vector(embedding_model, v5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearB2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx=0, max_seq_length=456):\n",
    "        super(LinearB2Vec, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.encoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # decoder\n",
    "\n",
    "        self.fc_enc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.encoder(packed)\n",
    "        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        latent = self.fc_enc(hidden_cat)\n",
    "\n",
    "        # decoder\n",
    "        decoder_init = latent.unsqueeze(0)\n",
    "        dec_out, _ = self.decoder(embedded, decoder_init)\n",
    "        logits = self.fc_out(dec_out)\n",
    "\n",
    "        # pad logits\n",
    "        seq_len = logits.size(1)\n",
    "        if seq_len < self.max_seq_length:\n",
    "            pad_size = self.max_seq_length - seq_len\n",
    "            padding = torch.zeros(logits.size(0), pad_size, logits.size(2), device=logits.device)\n",
    "            logits = torch.cat([logits, padding], dim=1)\n",
    "        else:\n",
    "            logits = logits[:, :self.max_seq_length, :]\n",
    "\n",
    "        return logits, latent\n",
    "\n",
    "    def encode(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden = self.encoder(packed)\n",
    "        hidden_cat = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        latent = self.fc_enc(hidden_cat)\n",
    "        return latent\n",
    "\n",
    "    def top_k_similar(self, query_embedding, all_embeddings, k, emb2word, metric='cosine'):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        query_embedding = query_embedding.to(device)\n",
    "        all_embeddings = all_embeddings.to(device)\n",
    "\n",
    "        if metric == 'cosine':\n",
    "            query_norm = query_embedding / query_embedding.norm(p=2)\n",
    "            all_norm = all_embeddings / all_embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "            similarities = torch.matmul(all_norm, query_norm.t()).squeeze(1)\n",
    "        elif metric == 'euclidean':\n",
    "            diff = all_embeddings - query_embedding.unsqueeze(0)\n",
    "            similarities = -torch.norm(diff, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "        topk_scores, topk_indices = torch.topk(similarities, k)\n",
    "        topk_indices = topk_indices.cpu()\n",
    "        topk_words = [emb2word[i.item()] for i in topk_indices]\n",
    "\n",
    "        return topk_words, topk_scores.tolist()\n",
    "\n",
    "    def get_all_embeddings(self, word_list, max_seq_length):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        all_embeddings = []\n",
    "        mapping = {}\n",
    "\n",
    "        for i, word in enumerate(word_list):\n",
    "            tokenized = fromWord2Vec(word, max_seq_length).to(device)\n",
    "            length = (tokenized != 0).sum().item()\n",
    "            tokenized = tokenized.unsqueeze(0)\n",
    "            lengths = torch.tensor([length], dtype=torch.long)\n",
    "            embedding = self.encode(tokenized, lengths).squeeze(0).cpu()\n",
    "            all_embeddings.append(embedding)\n",
    "            mapping[i] = word\n",
    "\n",
    "        return torch.stack(all_embeddings, dim=0), mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j0dVzaywr_2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a-ko-ra-jo', 'ma-ra', 'au-to', 'pi-ri-ja-o', 'o-du-we', 'po-ki-ro-nu-ka', 'tu-we-ta', 'sa-zo', 'da-i-wo-wo', 'we-je-ke-ha']\n"
     ]
    }
   ],
   "source": [
    "#Prepare data for LB2VEC\n",
    "def tokenize_sequences_LB2VEC(sequences, mapping=input_mapping, use_SOS=False, use_EOS=False):\n",
    "\n",
    "    tokenized_dataset = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        tokens = []\n",
    "        if use_SOS:\n",
    "            tokens.append(mapping[\"SOS\"])\n",
    "\n",
    "        words = seq.split(\" \")\n",
    "        for i, word in enumerate(words):\n",
    "            if \"-\" in word:\n",
    "                sub_words = word.split(\"-\")\n",
    "                for sub in sub_words:\n",
    "                    if sub not in mapping:\n",
    "                        raise KeyError(f\"Token '{sub}' not found in mapping!\")\n",
    "                    tokens.append(mapping[sub])\n",
    "            else:\n",
    "                if word not in mapping:\n",
    "                    raise KeyError(f\"Token '{word}' not found in mapping!\")\n",
    "                tokens.append(mapping[word])\n",
    "\n",
    "            # Add a space token between words (if not the last word)\n",
    "            if i != len(words) - 1:\n",
    "                tokens.append(mapping[\" \"])\n",
    "\n",
    "        if use_EOS:\n",
    "            tokens.append(mapping[\"EOS\"])\n",
    "\n",
    "        # Convert the list of token IDs to a PyTorch tensor.\n",
    "        tokenized_dataset.append(torch.tensor(tokens, dtype=torch.long))\n",
    "\n",
    "    return tokenized_dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences  # This is your list of tokenized tensors.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Custom collate_fn that pads sequences in a batch.\n",
    "def collate_fn_LB2VEC(max_seq_length, pad_token=0):\n",
    "    def collate_fn(batch):\n",
    "        padded_batch = []\n",
    "        lengths = []\n",
    "        for seq in batch:\n",
    "            lengths.append(seq.size(0))\n",
    "            # If the sequence is shorter than max_seq_length, pad it on the right.\n",
    "            pad_size = max_seq_length - seq.size(0)\n",
    "            if pad_size > 0:\n",
    "                padding = torch.full((pad_size,), pad_token, dtype=seq.dtype)\n",
    "                padded_seq = torch.cat([seq, padding], dim=0)\n",
    "            else:\n",
    "                padded_seq = seq\n",
    "            padded_batch.append(padded_seq)\n",
    "            lengths_t = torch.tensor(lengths)\n",
    "        return torch.stack(padded_batch, dim=0), lengths_t\n",
    "    return collate_fn\n",
    "\n",
    "def fromWord2Vec(word, max_seq_length):\n",
    "    res = tokenize_sequences_LB2VEC([word])\n",
    "    sequence = res[0]  # This is a tensor representing the tokenized word.\n",
    "\n",
    "    seq_len = sequence.size(0)\n",
    "    if seq_len < max_seq_length:\n",
    "        pad_size = max_seq_length - seq_len\n",
    "        padding = torch.zeros(pad_size, dtype=sequence.dtype)\n",
    "        sequence = torch.cat([sequence, padding], dim=0)\n",
    "    elif seq_len > max_seq_length:\n",
    "        sequence = sequence[:max_seq_length]\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def getAllWords(sequences):\n",
    "    words = set()\n",
    "    for seq in sequences:\n",
    "        seq = seq.split(\" \")\n",
    "        for w in seq:\n",
    "            words.add(w)\n",
    "    return list(words)\n",
    "\n",
    "\n",
    "tokenized = tokenize_sequences_LB2VEC(sequences)\n",
    "max_seq_length = max(seq.size(0) for seq in tokenized)\n",
    "dataset = SequenceDataset(tokenized)\n",
    "collate_fn = collate_fn_LB2VEC(max_seq_length)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "words = getAllWords(sequences)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xxnMF-STTvHI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5441\n",
      "Epoch 2/10, Loss: 0.0320\n",
      "Epoch 3/10, Loss: 0.0139\n",
      "Epoch 4/10, Loss: 0.0061\n",
      "Epoch 5/10, Loss: 0.0032\n",
      "Epoch 6/10, Loss: 0.0017\n",
      "Epoch 7/10, Loss: 0.0009\n",
      "Epoch 8/10, Loss: 0.0006\n",
      "Epoch 9/10, Loss: 0.0004\n",
      "Epoch 10/10, Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "LB2vec_model = LinearB2Vec(len(input_tokens), EMBEDDINGS_DIM, EMBEDDINGS_DIM)\n",
    "optimizer = optim.Adam(LB2vec_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LB2vec_model.to(device)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch, lengths in data_loader:\n",
    "        # Move data to the GPU\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = LB2vec_model(batch, lengths)\n",
    "        logits_flat = logits.view(-1, len(input_tokens))\n",
    "        targets_flat = batch.view(-1)\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po-ti-ni-ja 1.0000001192092896\n",
      "po-ti-ni-ja-we-ja 0.9680911302566528\n",
      "po-si-da-e-ja 0.9603016376495361\n",
      "po-so-re-ja 0.9535226225852966\n",
      "po-ni-ke-ja 0.9515949487686157\n",
      "po-pu-re-ja 0.9459639191627502\n",
      "po-re-no-zo-te-ri-ja 0.9432565569877625\n",
      "po-ro-e-ke-te-ri-ja 0.9404106140136719\n",
      "po-ni-ki-ja 0.9383034706115723\n",
      "po-qe-wi-ja 0.9265150427818298\n",
      "po-ni-ja-ja 0.916567862033844\n",
      "ko-no-so 0.9999999403953552\n",
      "ko-me-no 0.9324777126312256\n",
      "ko-to-na-no-no 0.930557131767273\n",
      "ko-to-no 0.9266769289970398\n",
      "ko-ro-tu-no 0.9254539608955383\n",
      "ko-i-no 0.9241381287574768\n",
      "ko-tu-ryo 0.9233684539794922\n",
      "ko-no-si-jo 0.9208793640136719\n",
      "ko-no-so-de 0.9206832051277161\n",
      "ko-to-i-na 0.9180614948272705\n",
      "ko-ri-jo 0.9174559116363525\n"
     ]
    }
   ],
   "source": [
    "# Use embeddings\n",
    "all_embeddings, emb2word = LB2vec_model.get_all_embeddings(words, max_seq_length)\n",
    "\n",
    "query = \"po-ti-ni-ja\"\n",
    "query_seq = fromWord2Vec(query, max_seq_length).unsqueeze(0).to(device)  #vectorize the word through tokenization\n",
    "query_len = torch.tensor([(query_seq != 0).sum().item()], dtype=torch.long).to(device)\n",
    "query_embedding = LB2vec_model.encode(query_seq, query_len) \n",
    "neighbors, scores = LB2vec_model.top_k_similar(query_embedding, all_embeddings, k=11, emb2word=emb2word, metric=\"cosine\")\n",
    "for n, s in zip(neighbors, scores):\n",
    "    print(n,s)\n",
    "\n",
    "query = \"ko-no-so\"\n",
    "query_seq = fromWord2Vec(query, max_seq_length).unsqueeze(0).to(device)  #vectorize the word through tokenization\n",
    "query_len = torch.tensor([(query_seq != 0).sum().item()], dtype=torch.long).to(device)\n",
    "query_embedding = LB2vec_model.encode(query_seq, query_len)\n",
    "neighbors, scores = LB2vec_model.top_k_similar(query_embedding, all_embeddings, k=11, emb2word=emb2word, metric=\"cosine\")\n",
    "for n, s in zip(neighbors, scores):\n",
    "    print(n,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPt6iYZyjy6I"
   },
   "source": [
    "#### TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ddRqPnXNLbxC"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LinearB2Vec.get_all_embeddings() missing 1 required positional argument: 'max_seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TEST4LB2VEC\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_embeddings, emb2word \u001b[38;5;241m=\u001b[39m \u001b[43mLB2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m456\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m aminisijo \u001b[38;5;241m=\u001b[39m fromWord2Vec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-mi-ni-si-jo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m456\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m aminisija \u001b[38;5;241m=\u001b[39m fromWord2Vec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma-mi-ni-si-ja\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m456\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: LinearB2Vec.get_all_embeddings() missing 1 required positional argument: 'max_seq_length'"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='linear_b.model')\n",
    "#TEST4LB2VEC\n",
    "all_embeddings, emb2word = LB2vec_model.get_all_embeddings(words, 456)\n",
    "aminisijo = fromWord2Vec(\"a-mi-ni-si-jo\", 456).unsqueeze(0)\n",
    "aminisija = fromWord2Vec(\"a-mi-ni-si-ja\", 456).unsqueeze(0)\n",
    "korisijo = fromWord2Vec(\"ko-ri-si-jo\", 456).unsqueeze(0)\n",
    "aminisijo = aminisijo.to(device)\n",
    "aminisija = aminisija.to(device)\n",
    "korisijo = korisijo.to(device)\n",
    "length = torch.tensor([aminisijo.size(0)])\n",
    "\n",
    "male_name = LB2vec_model.encode(aminisijo, length)\n",
    "female_name = LB2vec_model.encode(aminisija, length)\n",
    "king = LB2vec_model.encode(korisijo, length)\n",
    "\n",
    "hopefully_queen = male_name + king - female_name\n",
    "print(LB2vec_model.top_k_similar(king, all_embeddings, 150, emb2word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F3I_cwsVTTl"
   },
   "outputs": [],
   "source": [
    "embedding_model.find_nearest_neighbors(\"a-ta-ra-?-jo\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N82ZkEb51yFu"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"linear_b.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear_b.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m aminisijo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(embedding_model\u001b[38;5;241m.\u001b[39mget_vector(sp\u001b[38;5;241m.\u001b[39mencode_as_pieces(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maminisijo\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      4\u001b[0m aminisija \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(embedding_model\u001b[38;5;241m.\u001b[39mget_vector(sp\u001b[38;5;241m.\u001b[39mencode_as_pieces(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maminisija\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:468\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Init\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_threads \u001b[38;5;241m=\u001b[39m num_threads\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mor\u001b[39;00m model_proto:\n\u001b[0;32m--> 468\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_proto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentencepiece/__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"linear_b.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "aminisijo = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"aminisijo\")))\n",
    "aminisija = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"aminisija\")))\n",
    "korisijo = np.sum(embedding_model.get_vector(sp.encode_as_pieces(\"korisijo\")))\n",
    "\n",
    "print(korisijo, aminisijo.shape, aminisija.shape)\n",
    "\n",
    "hopefully_korisija = korisijo + aminisijo - aminisija\n",
    "embedding_model.find_nearest_neighbors_to_embedding(hopefully_queen, top_n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXZRUM3G8lpn"
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_model.check_meaningful_embeddings(\"_korisijo\", \"_aminisijo\", \"_aminisija\", top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuzMK1v6GCCN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def analyze_embedding_clusters(embedding_model, num_clusters=5, method=\"pca\"):\n",
    "    \"\"\"\n",
    "    Cluster and visualize Linear B word embeddings.\n",
    "\n",
    "    :param embedding_model: Trained Word2Vec/FastText model\n",
    "    :param num_clusters: Number of clusters for K-Means\n",
    "    :param method: \"pca\" or \"tsne\" for dimensionality reduction\n",
    "    \"\"\"\n",
    "    # Extract words and corresponding vectors\n",
    "    words = list(embedding_model.model.wv.index_to_key)\n",
    "    vectors = np.array([embedding_model.get_vector(word) for word in words if embedding_model.get_vector(word) is not None])\n",
    "\n",
    "    # Reduce dimensions (PCA or t-SNE)\n",
    "    if method == \"pca\":\n",
    "        reduced_vectors = PCA(n_components=2).fit_transform(vectors)\n",
    "    elif method == \"tsne\":\n",
    "        reduced_vectors = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(vectors)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Use 'pca' or 'tsne'.\")\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(vectors)\n",
    "\n",
    "    # Use a color map with as many colors as clusters\n",
    "    colors = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        indices = [i for i, c in enumerate(labels) if c == cluster_id]\n",
    "        plt.scatter(reduced_vectors[indices, 0], reduced_vectors[indices, 1],\n",
    "                    color=colors(cluster_id), label=f'Cluster {cluster_id}', alpha=0.7)\n",
    "\n",
    "    # Move the legend outside of the plot and remove the colorbar\n",
    "    plt.legend(title=\"Clusters\", bbox_to_anchor=(1.05, 0.5), loc='center left', fontsize=9)\n",
    "\n",
    "    # Annotate some points (adjust the number of labels)\n",
    "    for i, word in enumerate(words):\n",
    "        #if i % 10 == 0:  # Annotate every 10th word\n",
    "            plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=9)\n",
    "\n",
    "    # Add title and show plot\n",
    "    plt.title(f\"Word Embedding Clusters using {method.upper()}\")\n",
    "    plt.tight_layout()  # Adjust layout to accommodate the legend\n",
    "    plt.show()\n",
    "\n",
    "    # Print the words in each cluster\n",
    "    for cluster_id in range(num_clusters):\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        indices = [i for i, c in enumerate(labels) if c == cluster_id]\n",
    "        cluster_words = [words[i] for i in indices]\n",
    "        print(cluster_words)\n",
    "\n",
    "# Run the clustering visualization with desired number of clusters\n",
    "analyze_embedding_clusters(embedding_model, num_clusters=15, method=\"tsne\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12nNn5MGQkXY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IEfz2pEhMovx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SqueezeExcitationFusion(nn.Module):\n",
    "    def __init__(self, d_model, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)  #avg pooling (batch, d_model, 1)\n",
    "\n",
    "        #connected layers to model interactions\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // reduction_ratio),  # Reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model // reduction_ratio, d_model),  # Restore dimensionality\n",
    "            nn.Sigmoid()  # Normalize importance scores between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, token_emb, word_emb, phon_emb):\n",
    "        #Concatenate embeddings along the feature dimension\n",
    "        fusion_emb = token_emb + phon_emb #(batch, seq, d_model)\n",
    "        fusion_emb = fusion_emb + word_emb if USE_WORD_EMB else fusion_emb\n",
    "\n",
    "        #Squeeze: Compute global context (reduce across sequence dimension)\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1))  # (batch, d_model, 1)\n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  # Flatten to (batch, d_model)\n",
    "\n",
    "        #Excitation: Generate importance weights\n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  # (batch, 1, d_model)\n",
    "\n",
    "        #Scale embeddings by learned importance scores\n",
    "        fused_emb = fusion_emb * attention_weights  # Adaptive weighting\n",
    "\n",
    "        return fused_emb\n",
    "\n",
    "class TFIDF_SequeezeExcitationFusion(SqueezeExcitationFusion): #change forward function to use tf-idf and unigrams\n",
    "    def forward(self, token_emb, tfidf, unigram):\n",
    "        fusion_emb = token_emb + tfidf + unigram\n",
    "        squeeze_emb = self.squeeze(fusion_emb.permute(0, 2, 1)) \n",
    "        squeeze_emb = squeeze_emb.view(squeeze_emb.size(0), -1)  \n",
    "        attention_weights = self.excitation(squeeze_emb).unsqueeze(1)  \n",
    "        fused_emb = fusion_emb * attention_weights  \n",
    "        return fused_emb\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BRNN Allowing also for one-hot encoding\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.use_one_hot = USE_ONE_HOT_ENCODING\n",
    "\n",
    "        input_size = vocab_size if self.use_one_hot else embed_size\n",
    "\n",
    "        if not self.use_one_hot:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size - 4)  # output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        if not self.use_one_hot:\n",
    "            x = self.embedding(x)  # Shape: (batch, seq_len, embed_size)\n",
    "        # else: x is already one-hot: shape (batch, seq_len, vocab_size)\n",
    "\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Uzb5zvgof-3Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define the BRNN Model\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-4)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "FRFNCY0tvTmx"
   },
   "outputs": [],
   "source": [
    "#SE FUSION\n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-4)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "        packed_embedded = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "-Vq-GTgnOxVq"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model: Hidden dimension of the embeddings.\n",
    "            max_seq_len: Maximum sequence length supported.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a matrix (max_seq_len, d_model) with sinusoidal values\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)  # Shape: (max_seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Compute sinusoidal values\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
    "\n",
    "        # Register buffer so it's not a model parameter (no gradient updates needed)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # Shape: (1, max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            Tensor with positional encodings added.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_len=100, eos_token=2):\n",
    "        super(TransformerTextInfiller, self).__init__()\n",
    "        self.eos_token = eos_token  # EOS token value\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important! Now input shape is (batch, seq, features)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size - 4)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, tgt_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        - src: The input sequence with missing tokens (batch, seq_len).\n",
    "        - tgt: The sequence to decode (batch, seq_len).\n",
    "        - src_lengths: Lengths of the source sequences (for padding mask).\n",
    "        - tgt_lengths: Lengths of the target sequences (for padding mask).\n",
    "        \"\"\"\n",
    "\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        tgt_key_padding_mask = self.create_padding_mask(tgt)\n",
    "        tgt_mask = self.create_causal_mask(tgt, self.transformer.nhead)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_embedded,\n",
    "            tgt_embedded,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True\n",
    "        )\n",
    "\n",
    "        return self.softmax(self.fc(out))\n",
    "\n",
    "    def generate(self, src, src_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Autoregressive generation for inference.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence, shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence, shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        src_key_padding_mask = self.create_padding_mask(src)\n",
    "        tgt_key_padding_mask = self.tgt_key_padding_mask_from_src(src_key_padding_mask)\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        pos_indices = torch.arange(seq_length + 1, device=src.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=src.device)  # [SOS]\n",
    "        temp = torch.cat([src.clone(), torch.zeros((src.shape[0], 1), dtype=torch.int, device=src.device)], dim=1)\n",
    "        tgt_mask = self.create_causal_mask(temp)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        memory = self.transformer.encoder(src_embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "        result = []\n",
    "\n",
    "        for i in range(seq_length + 1):  # Generate up to input length\n",
    "            tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "            tgt_mask_cur = tgt_mask[:, :i + 1, :i + 1]  # Shape: [batch_size*num_heads, i+1, i+1]\n",
    "            tgt_key_padding_mask_cur = tgt_key_padding_mask[:, :i + 1]  # Shape: [batch_size, i+1]\n",
    "\n",
    "            out = self.transformer.decoder(\n",
    "                tgt_embedded,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask_cur,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask_cur,\n",
    "                tgt_is_causal=True\n",
    "            )\n",
    "\n",
    "            out = self.softmax(self.fc(out))[:, -1, :]\n",
    "            result.append(out)\n",
    "            next_token = out.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        result = torch.stack(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    # masks padding\n",
    "    def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "        \"\"\"\n",
    "        Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "            padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                          True = \"ignore this token\", False = \"keep it\".\n",
    "        \"\"\"\n",
    "        if unknown_token is None and mask_unknown:\n",
    "            unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "        return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "    # causal mask\n",
    "    def create_causal_mask(self, tgt, num_heads=1):\n",
    "        \"\"\"\n",
    "        Create a causal mask (upper triangular) for the target sequence.\n",
    "        - seq_len: The length of the target sequence.\n",
    "        - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "        - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tgt.shape\n",
    "        # Create the upper triangular mask (causal mask)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "        # Expand the mask for batch size and number of heads\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "        mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "        mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "        return mask.bool()\n",
    "\n",
    "    def tgt_key_padding_mask_from_src(self, src_key_padding_mask):\n",
    "        # Step 1: Find first True and change it to False\n",
    "        modified_mask = src_key_padding_mask.clone()\n",
    "        for i in range(modified_mask.shape[0]):\n",
    "            first_true = (modified_mask[i] == True).nonzero(as_tuple=True)[0]\n",
    "            if first_true.numel() > 0:  # If there is at least one True\n",
    "                modified_mask[i, first_true[0]] = False\n",
    "\n",
    "        # Step 2: Append False at the end of each sequence\n",
    "        modified_mask = torch.cat([modified_mask, torch.full((modified_mask.shape[0], 1), True, dtype=torch.bool, device=modified_mask.device)], dim=1)\n",
    "        return modified_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ilGHn-oZOAlR"
   },
   "outputs": [],
   "source": [
    "# SQUEEZE EXCITATION FUSION\n",
    "class TransformerTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_len=100, eos_token=2):\n",
    "        super(TransformerTextInfiller, self).__init__()\n",
    "        self.eos_token = eos_token  # EOS token value\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        self.se_fusion = SqueezeExcitationFusion(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important! Now input shape is (batch, seq, features)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size - 1)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, tgt_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        - src: The input sequence with missing tokens (batch, seq_len).\n",
    "        - tgt: The sequence to decode (batch, seq_len).\n",
    "        - src_lengths: Lengths of the source sequences (for padding mask).\n",
    "        - tgt_lengths: Lengths of the target sequences (for padding mask).\n",
    "        \"\"\"\n",
    "\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        tgt_key_padding_mask = self.create_padding_mask(tgt)\n",
    "        tgt_mask = self.create_causal_mask(tgt, self.transformer.nhead)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "        src_embedded = self.se_fusion(src_embedded, word_emb, phon_emb)\n",
    "        tgt_word_emb = torch.cat([torch.zeros((word_emb.size(0), 1, word_emb.size(2)), device=word_emb.device), word_emb], dim=1)\n",
    "        tgt_phon_emb = torch.cat([torch.zeros((phon_emb.size(0), 1, phon_emb.size(2)), device=phon_emb.device), phon_emb], dim=1)\n",
    "        tgt_embedded = self.se_fusion(tgt_embedded, tgt_word_emb, tgt_phon_emb)\n",
    "\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_embedded,\n",
    "            tgt_embedded,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True\n",
    "        )\n",
    "\n",
    "        return self.softmax(self.fc(out))\n",
    "\n",
    "    def generate(self, src, src_lengths, word_emb, phon_emb):\n",
    "        \"\"\"\n",
    "        Autoregressive generation for inference.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence, shape: [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence, shape: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = src.size()\n",
    "        src_key_padding_mask = self.create_padding_mask(src)\n",
    "        tgt_key_padding_mask = self.tgt_key_padding_mask_from_src(src_key_padding_mask)\n",
    "        src_key_padding_mask = self.create_padding_mask(src, mask_unknown=True)\n",
    "        pos_indices = torch.arange(seq_length + 1, device=src.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=src.device)  # [SOS]\n",
    "        temp = torch.cat([src.clone(), torch.zeros((src.shape[0], 1), dtype=torch.int, device=src.device)], dim=1)\n",
    "        tgt_mask = self.create_causal_mask(temp)\n",
    "\n",
    "        src_embedded = self.positional_encoding(self.embedding(src))\n",
    "        memory = self.transformer.encoder(src_embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "        result = []\n",
    "\n",
    "        src_embedded = self.se_fusion(src_embedded, word_emb, phon_emb)\n",
    "        tgt_word_emb = torch.cat([torch.zeros((word_emb.size(0), 1, word_emb.size(2)), device=word_emb.device), word_emb], dim=1)\n",
    "        tgt_phon_emb = torch.cat([torch.zeros((phon_emb.size(0), 1, phon_emb.size(2)), device=phon_emb.device), phon_emb], dim=1)\n",
    "\n",
    "\n",
    "        for i in range(seq_length + 1):  # Generate up to input length\n",
    "            tgt_embedded = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "            tgt_word_emb_cur = tgt_word_emb[:, :i+1, :]\n",
    "            tgt_phon_emb_cur = tgt_phon_emb[:, :i+1, :]\n",
    "            tgt_embedded = self.se_fusion(tgt_embedded, tgt_word_emb_cur, tgt_phon_emb_cur)\n",
    "\n",
    "            tgt_mask_cur = tgt_mask[:, :i + 1, :i + 1]  # Shape: [batch_size*num_heads, i+1, i+1]\n",
    "            tgt_key_padding_mask_cur = tgt_key_padding_mask[:, :i + 1]  # Shape: [batch_size, i+1]\n",
    "\n",
    "            out = self.transformer.decoder(\n",
    "                tgt_embedded,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask_cur,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask_cur,\n",
    "                tgt_is_causal=True\n",
    "            )\n",
    "\n",
    "            out = self.softmax(self.fc(out))[:, -1, :]\n",
    "            result.append(out)\n",
    "            next_token = out.argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        result = torch.stack(result, dim=1)\n",
    "        return result\n",
    "\n",
    "    # masks padding\n",
    "    def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "        \"\"\"\n",
    "        Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "            padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                          True = \"ignore this token\", False = \"keep it\".\n",
    "        \"\"\"\n",
    "        if unknown_token is None and mask_unknown:\n",
    "            unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "        return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "    # causal mask\n",
    "    def create_causal_mask(self, tgt, num_heads=1):\n",
    "        \"\"\"\n",
    "        Create a causal mask (upper triangular) for the target sequence.\n",
    "        - seq_len: The length of the target sequence.\n",
    "        - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "        - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tgt.shape\n",
    "        # Create the upper triangular mask (causal mask)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "        # Expand the mask for batch size and number of heads\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "        mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "        mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "        return mask.bool()\n",
    "\n",
    "    def tgt_key_padding_mask_from_src(self, src_key_padding_mask):\n",
    "        # Step 1: Find first True and change it to False\n",
    "        modified_mask = src_key_padding_mask.clone()\n",
    "        for i in range(modified_mask.shape[0]):\n",
    "            first_true = (modified_mask[i] == True).nonzero(as_tuple=True)[0]\n",
    "            if first_true.numel() > 0:  # If there is at least one True\n",
    "                modified_mask[i, first_true[0]] = False\n",
    "\n",
    "        # Step 2: Append False at the end of each sequence\n",
    "        modified_mask = torch.cat([modified_mask, torch.full((modified_mask.shape[0], 1), True, dtype=torch.bool, device=modified_mask.device)], dim=1)\n",
    "        return modified_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTRI TEXTINFILLER \n",
    "class BRNNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(BRNNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size-1)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb, phon_emb):\n",
    "        embedded = self.embedding(x)\n",
    "        fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "        packed_embedded = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFEKd7wnGIGp"
   },
   "source": [
    "#### MASK DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "pQwFuwJSaZd4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#POSSIBILI INCUSIONI NEL CODICE: formalizzato un po' meglio il discorso delle maschere\n",
    "def compute_src_mask(src, unknown_token=None):\n",
    "    \"\"\"\n",
    "    Compute a boolean mask for the source sequence.\n",
    "    Masks positions where the token equals the unknown token (default: max value in src).\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): The input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be masked. Defaults to max value in src.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    if unknown_token is None:\n",
    "        unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "    return src == unknown_token  # True where src equals unknown token\n",
    "\n",
    "# masks unknown token\n",
    "def create_attention_mask(src, unknown_token=None):\n",
    "    \"\"\"\n",
    "    Create a seq_len x seq_len mask for attention while ignoring unknown tokens.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be ignored.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len, seq_len).\n",
    "                      True means \"ignore\", False means \"attend\".\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = src.shape\n",
    "\n",
    "    # Compute unknown token mask\n",
    "    src_mask = compute_src_mask(src, unknown_token)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Create base attention mask (1s in upper triangle to mask future tokens in causal models)\n",
    "    attention_mask = torch.ones((seq_len, seq_len), dtype=torch.bool).triu(1)\n",
    "\n",
    "    # Expand unknown token mask to (batch_size, seq_len, seq_len)\n",
    "    src_mask = src_mask.unsqueeze(1).expand(-1, seq_len, -1)  # Broadcast across seq_len\n",
    "\n",
    "    # Combine masks: ignore future tokens and unknown tokens\n",
    "    final_mask = attention_mask.unsqueeze(0).expand(batch_size, -1, -1) | src_mask\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "# masks padding\n",
    "def create_padding_mask(sequence, pad_token=0):\n",
    "    \"\"\"\n",
    "    Create a padding mask for the given sequence and lengths.\n",
    "    - sequence: The sequence of input tokens (batch_size, seq_len).\n",
    "    - lengths: The lengths of the sequences (batch_size).\n",
    "    \"\"\"\n",
    "    mask = sequence == pad_token\n",
    "    return mask\n",
    "\n",
    "# causal mask\n",
    "def create_causal_mask(tgt, num_heads=1):\n",
    "    \"\"\"\n",
    "    Create a causal mask (upper triangular) for the target sequence.\n",
    "    - seq_len: The length of the target sequence.\n",
    "    - batch_size: The batch size (to repeat the mask for each instance in the batch).\n",
    "    - num_heads: The number of attention heads (to repeat the mask for each attention head).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = tgt.shape\n",
    "    # Create the upper triangular mask (causal mask)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=tgt.device), diagonal=1)\n",
    "\n",
    "    # Expand the mask for batch size and number of heads\n",
    "    mask = mask.unsqueeze(0).unsqueeze(1)  # Add batch and heads dimensions\n",
    "    mask = mask.expand(batch_size, num_heads, seq_len, seq_len)  # Repeat for batch and heads\n",
    "    mask = mask.reshape(batch_size * num_heads, seq_len, seq_len)\n",
    "    return mask.bool()\n",
    "import torch\n",
    "\n",
    "def create_padding_mask(self, src, padding_token=0, unknown_token=None, mask_unknown=False):\n",
    "    \"\"\"\n",
    "    Create a key padding mask that masks both padding and unknown tokens.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "        unknown_token (int, optional): Token value to be masked as \"ignored\".\n",
    "        padding_token (int, optional): Padding token value (default: 0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len), where\n",
    "                      True = \"ignore this token\", False = \"keep it\".\n",
    "    \"\"\"\n",
    "    if unknown_token is None and mask_unknown:\n",
    "        unknown_token = src.max()  # Default: assume max value is the unknown token\n",
    "\n",
    "    return (src == padding_token) | (src == unknown_token) if mask_unknown else (src == padding_token)# Mask both padding & unknowns\n",
    "\n",
    "# Example input\n",
    "src = torch.tensor([\n",
    "    [1, 2, 99, 0, 0],  # 99 and 0 should be ignored\n",
    "    [4, 99, 5, 6, 0]   # 99 and 0 should be ignored\n",
    "])\n",
    "\n",
    "# Example usage\n",
    "src = torch.tensor([[1, 2, 99, 0, 0], [4, 99, 5, 6, 0]])  # Assume 99 is the unknown token\n",
    "#mask = create_src_key_padding_mask(src, mask_unknown=True)\n",
    "#print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiDssS3AtLjc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "TTeJqPwwuY-E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"A single fully connected block with activation, dropout, and optional normalization.\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, normalize):\n",
    "        super(FCBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.normalize = normalize\n",
    "        if self.normalize:\n",
    "            self.layer_norm = nn.LayerNorm(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.normalize:\n",
    "            x = self.layer_norm(x)\n",
    "        x = self.activation(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class FCNTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, normalize=False):\n",
    "        super(FCNTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(FCBlock(embed_size, hidden_size, dropout, normalize))  # First layer\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.fc_layers.append(FCBlock(hidden_size, hidden_size, dropout, normalize))  # Hidden layers\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size - 4)  # Output layer\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        out = embedded\n",
    "        for layer in self.fc_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.output_layer(out)  # (batch_size, seq_len, vocab_size - 1)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8f-1ZUN5wH4d"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv1dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv1dLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.conv(x))\n",
    "\n",
    "\n",
    "class ConvTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, num_layers, dropout, normalize=False):\n",
    "        super(ConvTextInfiller, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers: each kernel size gets its own Conv1D layer\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = [FCBlock(num_filters * len(kernel_sizes), num_filters * len(kernel_sizes), dropout, normalize) for _ in range(num_layers-1)]\n",
    "        self.fc_layers.append(FCBlock(num_filters * len(kernel_sizes), num_filters, dropout, normalize))\n",
    "        self.fc_layers = nn.ModuleList(self.fc_layers)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters, vocab_size - 4)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Fully connected layers with normalization\n",
    "        for fc in self.fc_layers:\n",
    "            conv_out = fc(conv_out)\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(conv_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0fvN_Fv-TpZO"
   },
   "outputs": [],
   "source": [
    "class ConvTextInfillerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, hidden_size, num_layers, dropout):\n",
    "        super(ConvTextInfillerRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(embed_size, num_filters, k) for k in kernel_sizes])\n",
    "\n",
    "        # RNN Layer (replaces FC layers)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=num_filters * len(kernel_sizes),\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(hidden_size * 2, vocab_size - 4)  # *2 for bidirectional\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (B, E, L) for Conv1D\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(embedded) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Pack the sequence to ignore padding in RNN\n",
    "        packed_input = pack_padded_sequence(conv_out, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(rnn_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "xKQ3W9oJqTzy"
   },
   "outputs": [],
   "source": [
    "class ConvTextInfillerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, hidden_size, num_layers, dropout):\n",
    "        super(ConvTextInfillerRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Convolutional layers (after RNN)\n",
    "        self.convs = nn.ModuleList([Conv1dLayer(hidden_size * 2, num_filters, k) for k in kernel_sizes])  # Hidden size * 2 for bidirectional RNN\n",
    "\n",
    "        # RNN Layer (before convolutional layers)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(num_filters * len(kernel_sizes), vocab_size - 4)  # *2 for bidirectional\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        # Embedding lookup\n",
    "        embedded = self.embedding(x)  # (B, L, E)\n",
    "\n",
    "        # Pack the sequence to ignore padding in RNN\n",
    "        packed_input = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        # Now apply convolutions after RNN\n",
    "        rnn_out = rnn_out.permute(0, 2, 1)  # (B, H, L) -> (B, H, L) for Conv1D (H=hidden size)\n",
    "\n",
    "        # Apply convolutions\n",
    "        conv_features = [conv(rnn_out) for conv in self.convs]\n",
    "        conv_out = torch.cat(conv_features, dim=1)  # (B, C*k, L)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (B, L, C*k)\n",
    "\n",
    "        # Final output projection\n",
    "        logits = self.fc_out(conv_out)\n",
    "        return self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiConvTextInfiller(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, kernel_sizes=[1, 3, 5, 7], dropout=0.3):\n",
    "        super(BiConvTextInfiller, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        #multiple forward and backward conv layers (one per kernel size)\n",
    "        self.conv_fwd = nn.ModuleList([\n",
    "            nn.Conv1d(embed_size, hidden_size, k, padding=k // 2) for k in kernel_sizes\n",
    "        ])\n",
    "        self.conv_bwd = nn.ModuleList([\n",
    "            nn.Conv1d(embed_size, hidden_size, k, padding=k // 2) for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        total_hidden = hidden_size * len(kernel_sizes) * 2  # *2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(total_hidden, vocab_size - 4)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, word_emb=None, phon_emb=None):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)  # (B, E, L)\n",
    "\n",
    "        # Forward convolutions\n",
    "        out_fwd = [F.relu(conv(embedded)) for conv in self.conv_fwd]\n",
    "\n",
    "        # Backward convolutions (flip input)\n",
    "        reversed_emb = torch.flip(embedded, dims=[2])\n",
    "        out_bwd = [F.relu(conv(reversed_emb)) for conv in self.conv_bwd]\n",
    "        out_bwd = [torch.flip(o, dims=[2]) for o in out_bwd]\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        conv_outputs = out_fwd + out_bwd  # list of tensors: (B, H, L)\n",
    "        out = torch.cat(conv_outputs, dim=1)  # (B, H * 2 * K, L)\n",
    "\n",
    "        # Project back to vocabulary\n",
    "        out = self.dropout(out.permute(0, 2, 1))  # (B, L, H')\n",
    "        return self.softmax(self.fc(out))  # (B, L, V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdENsy1li13g",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Architectures for composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CNd3TxqJjAV2"
   },
   "outputs": [],
   "source": [
    "#Define the BRNN Model for both our tokenized input and for the distorted input\n",
    "class BRNNTokenizedInput(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNTokenizedInput, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FzH5lmnulomS"
   },
   "outputs": [],
   "source": [
    "#Define the BRNN Model for spm\n",
    "class BRNNSentencePiece(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(BRNNSentencePiece, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n",
    "\n",
    "        self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # *2 for bidirectional\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, lengths, y):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        out, _ = self.brnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, total_length=y.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "otMZcHPkLzSg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Composition(nn.Module):\n",
    "    def __init__(self, hyperparams, device):\n",
    "        super(Composition, self).__init__()\n",
    "\n",
    "        hyperparams[\"model\"] = \"BRNNTokenizedInput\"\n",
    "        self.original = initialize_model(hyperparams, device)\n",
    "        self.distorted = initialize_model(hyperparams, device)\n",
    "\n",
    "        hyperparams[\"model\"] = \"BRNNSentencePiece\"\n",
    "        self.sentence_piece = initialize_model(hyperparams, device)\n",
    "\n",
    "        #make weights learnable by converting them to torch.nn.Parameter\n",
    "        self.w_original = nn.Parameter(torch.tensor(0.8))  #learnable weight for original model\n",
    "        self.w_distorted = nn.Parameter(torch.tensor(0.1))  #learnable weight for distorted model\n",
    "        self.w_sentence_piece = nn.Parameter(torch.tensor(0.1))  #learnable weight for sentence piece model\n",
    "\n",
    "        #ensure that the weights sum up to 1 manually (soft constraint)\n",
    "        assert abs(self.w_original.item() + self.w_distorted.item() + self.w_sentence_piece.item() - 1.0) < 1e-5, \"Weights should sum to 1 initially.\"\n",
    "\n",
    "\n",
    "    def forward(self, x, y, lengths, x_distorted, x_sp, x_sp_length):\n",
    "        # Normalize weights (soft constraint)\n",
    "        with torch.no_grad():\n",
    "            total_weight = self.w_original + self.w_distorted + self.w_sentence_piece\n",
    "            self.w_original.data /= total_weight\n",
    "            self.w_distorted.data /= total_weight\n",
    "            self.w_sentence_piece.data /= total_weight\n",
    "    \n",
    "        #debugging\n",
    "        #print(self.w_original.item(), self.w_distorted.item(), self.w_sentence_piece.item())\n",
    "    \n",
    "        #get predictions (log-probabilities from each model)\n",
    "        orig_logp = self.original(x, lengths)  #[batch, seq, vocab]\n",
    "        distorted_logp = self.distorted(x_distorted, lengths)\n",
    "        sp_logp = self.sentence_piece(x_sp, x_sp_length, y)\n",
    "    \n",
    "        #exponentiate to convert to probs and use the weights\n",
    "        orig_p = torch.exp(orig_logp)\n",
    "        distorted_p = torch.exp(distorted_logp)\n",
    "        sp_p = torch.exp(sp_logp)\n",
    "    \n",
    "        #weighted sum in probability space\n",
    "        combined_p = (\n",
    "            self.w_original * orig_p +\n",
    "            self.w_distorted * distorted_p +\n",
    "            self.w_sentence_piece * sp_p\n",
    "        )\n",
    "    \n",
    "        #convert back to log-probabilities for the loss\n",
    "        combined_logp = torch.log(combined_p + 1e-12)  #use an epsilon to avoid log(0)\n",
    "    \n",
    "        return combined_logp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0guTkDZ7nI2C",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Recover Word and Phonetic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "SLpd49b-ntEZ"
   },
   "outputs": [],
   "source": [
    "file_name = f\"./fasttext/linearb_fasttext_model_{EMBEDDINGS_DIM}.bin\"\n",
    "\n",
    "embedding_model = LinearBWordEmbedding()\n",
    "embedding_model.load_model(os.path.join(prefix_path, file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_word_embeddings(embedding_model, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_embeddings = [] \n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  #split the sequence into words\n",
    "        seq_embeddings = []  #to store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            word_len = word.count(\"-\") + 1  #compute the length of the word (based on '-' count)\n",
    "            emb = torch.tensor(embedding_model.get_vector(word))  #get the word embedding\n",
    "            # Unsqueeze the embedding to match (word_len, embedding_dim)\n",
    "            emb_unsqueezed = emb.unsqueeze(0)\n",
    "\n",
    "            emb_expanded = emb_unsqueezed.expand(word_len, -1)  # Unsqueeze and expand\n",
    "            if i != len(seq) - 1:\n",
    "                space_embed = torch.zeros_like(emb_unsqueezed)\n",
    "                seq_embeddings.extend([emb_expanded, space_embed])  # Add to the sequence's embeddings\n",
    "            else:\n",
    "                seq_embeddings.append(emb_expanded)  # Add to the sequence's embeddings\n",
    "\n",
    "        # adding zeros for SOS at the beginning of the sequence\n",
    "        if use_sos_in_x:\n",
    "            seq_embeddings = [torch.zeros_like(emb_unsqueezed)] + seq_embeddings\n",
    "\n",
    "        # Concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = torch.cat(seq_embeddings, dim=0)\n",
    "        padding = torch.zeros((final_length - seq_embedding.shape[0], seq_embedding.shape[1]))\n",
    "        seq_embedding = torch.cat([seq_embedding, padding], dim=0)  # Add padding to the end\n",
    "\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.stack(all_embeddings)  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "## Example usage:\n",
    "word_embeddings_train_x = recover_word_embeddings(embedding_model, seq_train_x, train_x.shape[1])\n",
    "word_embeddings_test_x = recover_word_embeddings(embedding_model, seq_test_x, test_x.shape[1])\n",
    "\n",
    "# Free my ram please\n",
    "del embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5hcgTbxhcHmh"
   },
   "outputs": [],
   "source": [
    "syllable_autoencoder = SyllableAutoencoder()\n",
    "loaded_embeddings = syllable_autoencoder.load_embeddings(f\"syllable_embeddings_{EMBEDDINGS_DIM}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "52FluTPRZ0RV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([419, 256])\n",
      "419\n",
      "torch.Size([457, 256])\n",
      "457\n"
     ]
    }
   ],
   "source": [
    "def recover_phonetic_embeddings(embedding_dict, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_embeddings = []  # To store all sequence embeddings\n",
    "    embed_size = len(list(embedding_dict.values())[0])  # Assuming all embeddings have the same size\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_embeddings = []  # To store embeddings for the current sequence\n",
    "\n",
    "        for i, word in enumerate(seq):\n",
    "            for syllable in word.split(\"-\"):\n",
    "                if syllable in embedding_dict:\n",
    "                    emb = torch.from_numpy(embedding_dict[syllable])  # Get the word embedding\n",
    "                    seq_embeddings.append(emb)  # Add to the sequence's embeddings\n",
    "                else:\n",
    "                    seq_embeddings.append(torch.zeros(embed_size))\n",
    "            if i != len(seq) - 1:\n",
    "                space_embed = torch.zeros(embed_size)\n",
    "                seq_embeddings.append(space_embed)  # Add to the sequence's embeddings\n",
    "\n",
    "        #adding zeros for SOS at the beginning of the sequence\n",
    "        if use_sos_in_x:\n",
    "            seq_embeddings = [torch.zeros_like(seq_embeddings[0])] + seq_embeddings\n",
    "\n",
    "        #concatenate embeddings along the first axis (to form the final sequence embedding) and add padding\n",
    "        seq_embedding = torch.stack(seq_embeddings)\n",
    "        if seq_embedding.shape[0] > 400:\n",
    "            print(seq_embedding.shape)\n",
    "            print(len(seq_embeddings))\n",
    "        padding = torch.zeros((final_length - seq_embedding.shape[0], seq_embedding.shape[1]))\n",
    "        seq_embedding = torch.cat([seq_embedding, padding], dim=0)  # Add padding to the end\n",
    "\n",
    "        all_embeddings.append(seq_embedding)  # Add to the overall embeddings list\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.stack(all_embeddings)  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "phonetic_embeddings_train_x = recover_phonetic_embeddings(loaded_embeddings, seq_train_x, train_x.shape[1])\n",
    "phonetic_embeddings_test_x = recover_phonetic_embeddings(loaded_embeddings, seq_test_x, test_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VcCrJUmbpjsA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3318, 419, 256]), torch.Size([3318, 419, 256]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings_train_x.shape, phonetic_embeddings_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_lchCPdO3lK",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Recover Tf-idf and Unigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "dW2_51ANO2a8"
   },
   "outputs": [],
   "source": [
    "def recover_words_features(word_info, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_tf_idf= []  # To store all sequence embeddings\n",
    "    all_unigram= []\n",
    "    lookup = word_info.copy()\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_tf_idf = [-2] if use_sos_in_x else []  # To store embeddings for the current sequence\n",
    "        seq_unigram = [-2] if use_sos_in_x else []\n",
    "        for i, word in enumerate(seq):\n",
    "            seq_tf_idf.extend([lookup[word][\"tf_idf\"]]*(word.count(\"-\") + 1))\n",
    "            seq_unigram.extend([lookup[word][\"unigram_freq\"]]*(word.count(\"-\") + 1))\n",
    "            if i != len(seq) - 1:\n",
    "                seq_tf_idf.append(-1)  # Add to the sequence's embeddings\n",
    "                seq_unigram.append(-1)\n",
    "        seq_tf_idf.extend([0]*(final_length - len(seq_tf_idf)))\n",
    "        seq_unigram.extend([0]*(final_length - len(seq_unigram)))\n",
    "\n",
    "        all_tf_idf.append(seq_tf_idf)  # Add to the overall embeddings list\n",
    "        all_unigram.append(seq_unigram)\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.tensor(all_tf_idf), torch.tensor(all_unigram),  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "tf_idf_words_train_x, unigram_words_train_x = recover_words_features(word_results, seq_train_x, train_x.shape[1])\n",
    "tf_idf_words_test_x, unigram_words_test_x = recover_words_features(word_results, seq_test_x, test_x.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CCksMGI8Vjc5"
   },
   "outputs": [],
   "source": [
    "def recover_tokens_features(tokens_info, dataset, final_length, use_sos_in_x=USE_SOS_IN_X):\n",
    "    all_tf_idf= []  # To store all sequence embeddings\n",
    "    all_unigram= []\n",
    "    lookup = tokens_info.copy()\n",
    "\n",
    "    for seq in dataset:\n",
    "        seq = seq.split()  # Split the sequence into words\n",
    "        seq_tf_idf = [-2] if use_sos_in_x else []  # To store embeddings for the current sequence\n",
    "        seq_unigram = [-2] if use_sos_in_x else []\n",
    "        for i, word in enumerate(seq):\n",
    "            for tok in word.split(\"-\"):\n",
    "                seq_tf_idf.append(lookup[tok][\"tf_idf\"])\n",
    "                seq_unigram.append(lookup[tok][\"unigram_freq\"])\n",
    "            if i != len(seq) - 1:\n",
    "                seq_tf_idf.append(-1)  # Add to the sequence's embeddings\n",
    "                seq_unigram.append(-1)\n",
    "        seq_tf_idf.extend([0]*(final_length - len(seq_tf_idf)))\n",
    "        seq_unigram.extend([0]*(final_length - len(seq_unigram)))\n",
    "\n",
    "        all_tf_idf.append(seq_tf_idf)  # Add to the overall embeddings list\n",
    "        all_unigram.append(seq_unigram)\n",
    "\n",
    "    # Convert list of sequences to a tensor\n",
    "    return torch.tensor(all_tf_idf), torch.tensor(all_unigram),  # Stacks along a new dimension (batch dimension)\n",
    "\n",
    "tf_idf_tokens_train_x, unigram_tokens_train_x = recover_tokens_features(token_results, seq_train_x, train_x.shape[1])\n",
    "tf_idf_tokens_test_x, unigram_tokens_test_x = recover_tokens_features(token_results, seq_test_x, test_x.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Kz3MMtgj5K-y"
   },
   "outputs": [],
   "source": [
    "#Tf-Idf BRNN\n",
    "if USE_TFIDF:\n",
    "    print(\"Chaning to tf-idf BRNNTextInfiller\")\n",
    "    class BRNNTextInfiller(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, unigram_probs, tfidf_scores):\n",
    "            super(BRNNTextInfiller, self).__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "            self.brnn = nn.RNN(embed_size, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "            self.fc = nn.Linear(hidden_size * 2, vocab_size - 4)  # *2 for bidirectional\n",
    "            self.softmax = nn.LogSoftmax(dim=-1)\n",
    "            self.se_fusion = SqueezeExcitationFusion(embed_size)\n",
    "    \n",
    "            #convert unigram and TF-IDF scores to log-space\n",
    "            self.unigram_log_probs = torch.log(unigram_probs + 1e-10)\n",
    "            self.tfidf_scores = tfidf_scores\n",
    "    \n",
    "            self.mask = torch.ones(vocab_size-4)\n",
    "            self.mask[0:4] = 0  #zero out first 4 tokens\n",
    "            self.mask = self.mask\n",
    "            self.se_out = TFIDF_SequeezeExcitationFusion(vocab_size-4)\n",
    "    \n",
    "        def forward(self, x, lengths, word_emb, phon_emb):\n",
    "            embedded = self.embedding(x)\n",
    "            fused = self.se_fusion(embedded, word_emb, phon_emb)\n",
    "            packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            out, _ = self.brnn(packed_embedded)\n",
    "            out, _ = pad_packed_sequence(out, batch_first=True, total_length=x.shape[1])\n",
    "            out = self.fc(out)\n",
    "    \n",
    "            # Integrate unigram bias + TF-IDF weighting\n",
    "            #print(out.shape, self.unigram_log_probs.shape, self.tfidf_scores.shape, (self.unigram_log_probs.to(x.device) * self.mask.to(x.device)).unsqueeze(0).unsqueeze(1).expand_as(out).shape)\n",
    "            out = self.se_out(out, self.tfidf_scores.to(x.device).float(), (self.unigram_log_probs.to(x.device).float() * self.mask.to(x.device)).unsqueeze(0).unsqueeze(1).expand_as(out).float())\n",
    "            #out = out + self.unigram_log_probs.to(x.device) * self.mask.to(x.device)\n",
    "            return self.softmax(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItxZ4t0HgO0n",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions to reconstruct sequences, create DataLoaders and collect metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "EEH6yNGfoMdE"
   },
   "outputs": [],
   "source": [
    "class NLLWithKL(nn.Module):\n",
    "    def __init__(self, unigram_probs, kl_excluded_tokens=[i for i in range(4)], pad_idx=0, lambda_kld=1):\n",
    "        \"\"\"\n",
    "        Computes Negative Log-Likelihood (NLL) loss + KL-Divergence regularization.\n",
    "\n",
    "        Parameters:\n",
    "        - unigram_probs: Tensor of shape (vocab_size,) with unigram probabilities\n",
    "        - kl_excluded_tokens: List of token indices to exclude from KL-divergence computation\n",
    "        - lambda_kld: Weight for KL-divergence term\n",
    "        \"\"\"\n",
    "        super(NLLWithKL, self).__init__()\n",
    "        self.register_buffer(\"unigram_probs\", unigram_probs)\n",
    "        self.excluded_tokens = kl_excluded_tokens\n",
    "        self.lambda_kld = lambda_kld\n",
    "\n",
    "        #define loss functions\n",
    "        self.nll_loss = nn.NLLLoss(ignore_index=0, reduction=\"mean\")\n",
    "        self.kl_div_loss = nn.KLDivLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        \"\"\"\n",
    "        Compute the combined NLL + KL loss.\n",
    "\n",
    "        Parameters:\n",
    "        - predictions: Log-softmax outputs (batch_size, seq_len, vocab_size)\n",
    "        - targets: Ground-truth token indices (batch_size, seq_len)\n",
    "        - lengths: Sequence lengths for each batch element (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "        - Total loss (scalar)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, vocab_size = predictions.shape\n",
    "\n",
    "        # Compute Negative Log-Likelihood (NLL) Loss\n",
    "        ce_loss = self.nll_loss(predictions.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        # Expand ungram probabilities to match batch size and sequence length\n",
    "        p_unigram = self.unigram_probs.unsqueeze(0).unsqueeze(0).expand_as(predictions).to(predictions.device)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        # Compute KL-divergence (token-wise)\n",
    "        kl_div = self.kl_div_loss(predictions, p_unigram)  # KL divergence loss\n",
    "\n",
    "        # Mask out special tokens in KL computation\n",
    "        mask = torch.ones(vocab_size, dtype=torch.bool, device=predictions.device)\n",
    "        mask[self.excluded_tokens] = False\n",
    "\n",
    "        # Normalize by sequence length\n",
    "        kl_div = (kl_div * mask.float().unsqueeze(0).unsqueeze(1)).sum(dim=1) / lengths.float().unsqueeze(1)\n",
    "        kl_div = kl_div.sum(dim=-1)\n",
    "        kl_div = kl_div.mean()  # Mean over batch\n",
    "\n",
    "        # Final loss\n",
    "        return ce_loss + self.lambda_kld * kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5lVL9KqI6P4a"
   },
   "outputs": [],
   "source": [
    "def extract_label(t):\n",
    "    if t.ndim == 0:\n",
    "        return t.item()\n",
    "    elif t.ndim == 1:\n",
    "        return t.argmax().item()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid label shape: expected scalar or 1D vector\")\n",
    "\n",
    "#def reconstruct_sequences(model_output, test_y, test_x, inv_map=inv_map, inv_map_input=inv_map_input):\n",
    "#    input_seq = []\n",
    "#    gt_seq = []\n",
    "#    out_seq = []\n",
    "\n",
    "#    for i in range(test_y.shape[0]):\n",
    "#        test_seq = []\n",
    "  #      sequence = []\n",
    "#        result = []\n",
    "#        for j in range(test_y.shape[1]):\n",
    "#            if test_y[i][j] == 0:\n",
    "#                break\n",
    "#            sequence.append(inv_map[test_y[i][j].item()])\n",
    "#            result.append(inv_map[model_output[i][j].argmax().item()])\n",
    "\n",
    "            # src is one character shorter!\n",
    "#            if j != test_y.shape[1] - 1:\n",
    "#                test_seq.append(inv_map_input[test_x[i][j].item()])\n",
    "#\n",
    "#        input_seq.append(\" \".join(test_seq))\n",
    "#        gt_seq.append(\" \".join(sequence))\n",
    "#        out_seq.append(\" \".join(result))\n",
    "#    return input_seq, gt_seq, out_seq\n",
    "\n",
    "def reconstruct_sequences(model_output, test_y, test_x, inv_map=inv_map, inv_map_input=inv_map_input):\n",
    "    input_seq = []\n",
    "    gt_seq = []\n",
    "    out_seq = []\n",
    "\n",
    "    for i in range(test_y.shape[0]):\n",
    "        test_seq = []\n",
    "        sequence = []\n",
    "        result = []\n",
    "        for j in range(test_y.shape[1]):\n",
    "            y_idx = extract_label(test_y[i][j])\n",
    "            if y_idx == 0: \n",
    "                break\n",
    "\n",
    "            sequence.append(inv_map[y_idx])\n",
    "            result.append(inv_map[model_output[i][j].argmax().item()])\n",
    "\n",
    "            # src is one character shorter!\n",
    "            if j != test_y.shape[1] - 1:\n",
    "                test_seq.append(inv_map_input[extract_label(test_x[i][j])])\n",
    "\n",
    "        input_seq.append(\" \".join(test_seq))\n",
    "        gt_seq.append(\" \".join(sequence))\n",
    "        out_seq.append(\" \".join(result))\n",
    "\n",
    "    return input_seq, gt_seq, out_seq\n",
    "\n",
    "def collect_batch_metrics(model_output, test_y, test_x, top_k=(20, 15, 10, 5, 1), use_sos_in_x=USE_SOS_IN_X):\n",
    "    \"\"\"\n",
    "    Collects batch-level metrics without computing final scores.\n",
    "    This allows aggregation over multiple batches.\n",
    "\n",
    "    Args:\n",
    "        model_output (Tensor): Model's output log probabilities (batch_size, seq_len, num_classes).\n",
    "        test_x (Tensor): Input sequence (batch_size, seq_len), contains '?' locations.\n",
    "        test_y (Tensor): Ground truth labels (batch_size, seq_len).\n",
    "        top_k (tuple): List of top-k values to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with raw counts for aggregation.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len, num_classes = model_output.shape\n",
    "    question_mark_id = input_mapping[\"?\"] #test_x.max().item()  # '?' is the highest label in test_x\n",
    "\n",
    "    batch_metrics = {\n",
    "        \"correct_sequences\": 0,\n",
    "        \"total_sequences\": batch_size,\n",
    "        \"total_question_marks\": 0,\n",
    "        \"top_k_correct\": {k: 0 for k in top_k},\n",
    "        \"mrr_sum\": 0.0,  # Mean Reciprocal Rank numerator\n",
    "    }\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        correct_sequence = True\n",
    "\n",
    "        for j in range(1 if use_sos_in_x else 0, seq_len if use_sos_in_x else seq_len-1):  # Start from 1 to check i-1\n",
    "            index_to_check = j - 1 if use_sos_in_x else j\n",
    "\n",
    "            # Check full sequence accuracy\n",
    "            \n",
    "            true_label = extract_label(test_y[i, index_to_check])\n",
    "            predicted_label = model_output[i, index_to_check].argmax().item()\n",
    "\n",
    "            if true_label == 0:  # Padding reached\n",
    "                break\n",
    "\n",
    "            if (use_sos_in_x or j != seq_len - 1) and extract_label(test_x[i, j]) == question_mark_id:  # '?' in input\n",
    "                batch_metrics[\"total_question_marks\"] += 1\n",
    "\n",
    "                sorted_indices = model_output[i, index_to_check].argsort(descending=True)\n",
    "\n",
    "                # Rank of the correct answer (1-based index)\n",
    "                rank = (sorted_indices == true_label).nonzero(as_tuple=True)[0].item() + 1\n",
    "                batch_metrics[\"mrr_sum\"] += 1 / rank  # Add reciprocal rank\n",
    "\n",
    "                # Check top-K\n",
    "                for k in top_k:\n",
    "                    if rank <= k:\n",
    "                        batch_metrics[\"top_k_correct\"][k] += 1\n",
    "\n",
    "            if predicted_label != true_label:\n",
    "                correct_sequence = False\n",
    "\n",
    "        if correct_sequence:\n",
    "            batch_metrics[\"correct_sequences\"] += 1\n",
    "\n",
    "    return batch_metrics\n",
    "\n",
    "def aggregate_metrics(metrics, batch):\n",
    "    if metrics is None:\n",
    "        return batch\n",
    "    for key, value in batch.items():\n",
    "        if key != \"top_k_correct\":\n",
    "            metrics[key] += value\n",
    "        else:\n",
    "            for k, v in metrics[\"top_k_correct\"].items():\n",
    "                metrics[\"top_k_correct\"][k] += value[k]\n",
    "    return metrics\n",
    "\n",
    "def compute_final_metrics(aggregated):\n",
    "    \"\"\"\n",
    "    Computes final metrics from aggregated batch metrics.\n",
    "\n",
    "    Args:\n",
    "        aggregated (dict): Aggregated counts from all batches.\n",
    "\n",
    "    Returns:\n",
    "        dict: Final computed metric values.\n",
    "    \"\"\"\n",
    "    final_metrics = {}\n",
    "\n",
    "    # Sequence accuracy\n",
    "    final_metrics[\"sequence_accuracy\"] = (\n",
    "        aggregated[\"correct_sequences\"] / aggregated[\"total_sequences\"]\n",
    "        if aggregated[\"total_sequences\"] > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Mean Reciprocal Rank (MRR)\n",
    "    final_metrics[\"mrr\"] = (\n",
    "        aggregated[\"mrr_sum\"] / aggregated[\"total_question_marks\"]\n",
    "        if aggregated[\"total_question_marks\"] > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Top-k accuracies\n",
    "    final_metrics[\"top_k_accuracy\"] = {\n",
    "        k: (aggregated[\"top_k_correct\"][k] / aggregated[\"total_question_marks\"])\n",
    "        if aggregated[\"total_question_marks\"] > 0 else 0\n",
    "        for k in aggregated[\"top_k_correct\"]\n",
    "    }\n",
    "\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "#Takes self.history with is < k_numfold : (avg_loss, test_metrics) > dictionary, every tuple is an epoch\n",
    "#Returns couple [avg_metric_epoch_1, avg_metric_epoch_2, ...] [num_folds_epoch_1, num_folds_epoch_2, ...]\n",
    "def aggregate_k_fold_metrics(history):\n",
    "    \"\"\"\n",
    "    Aggregates the metrics from k-fold cross-validation, accounting for different early stopping points.\n",
    "    \"\"\"\n",
    "    max_epochs = max(len(history[k]) for k in range(len(history)))  # Longest training\n",
    "    avg_metrics_per_epoch = []\n",
    "    num_folds_used = []\n",
    "\n",
    "    for epoch in range(max_epochs): #for each epoch get averages of parameters between the different folds\n",
    "        losses = []\n",
    "        seq_accs = []\n",
    "        mrrs = []\n",
    "        top_k_accs = {20: [], 15: [], 10: [], 5: [], 1: []}\n",
    "        folds_used = 0\n",
    "\n",
    "        for k in range(len(history)): #iterate through every fold\n",
    "            if epoch < len(history[k]):  # If fold has this epoch\n",
    "                loss, metrics = history[k][epoch] #extract avg loss and val metrics\n",
    "                losses.append(loss)\n",
    "                seq_accs.append(metrics[\"sequence_accuracy\"])\n",
    "                mrrs.append(metrics[\"mrr\"])\n",
    "                for k_top in top_k_accs.keys():\n",
    "                    top_k_accs[k_top].append(metrics[\"top_k_accuracy\"][k_top]) #populate also top_k parameters\n",
    "                folds_used += 1\n",
    "\n",
    "        if folds_used > 0: #compute final avg metrics\n",
    "            # same format as the input: i.e. a tuple with loss as first element and metrics dict as the second\n",
    "            # this ensures compatibility of the unpack function with this format as well\n",
    "            avg_metrics = (\n",
    "                np.mean(losses),\n",
    "                {\n",
    "                    \"sequence_accuracy\": np.mean(seq_accs),\n",
    "                    \"mrr\": np.mean(mrrs),\n",
    "                    \"top_k_accuracy\": {k: np.mean(v) for k, v in top_k_accs.items()}\n",
    "                }\n",
    "            )\n",
    "            avg_metrics_per_epoch.append(avg_metrics)\n",
    "            num_folds_used.append(folds_used)\n",
    "\n",
    "    return avg_metrics_per_epoch, num_folds_used\n",
    "\n",
    "#Takes input [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2...]\n",
    "def unpack_history(history):\n",
    "    epochs = []\n",
    "    losses = []\n",
    "    top_k_accuracies = defaultdict(lambda: [])\n",
    "    sequence_accuracies = []\n",
    "    mrrs = []\n",
    "    #Unpacks for every epoch its aggregated metrics and returns them as sorted lists\n",
    "    for i, (loss, metrics) in enumerate(history):\n",
    "       epoch = i + 1\n",
    "       epochs.append(epoch)\n",
    "       losses.append(loss)\n",
    "\n",
    "       sequence_accuracies.append(metrics[\"sequence_accuracy\"])\n",
    "       mrrs.append(metrics[\"mrr\"])\n",
    "\n",
    "       for k in metrics[\"top_k_accuracy\"].keys():\n",
    "           top_k_accuracies[k].append(metrics[\"top_k_accuracy\"][k])\n",
    "\n",
    "    return epochs, losses, sequence_accuracies, mrrs, top_k_accuracies\n",
    "\n",
    "def create_loader(x, y, lengths, word_emb, phon_emb, batch_size):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    dataset = TensorDataset(x, y, lengths, word_emb, phon_emb)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    return loader\n",
    "\n",
    "def create_loader_with_sp(x, y, lengths, x_distorted, x_sp, lengths_sp, batch_size):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    dataset = TensorDataset(x, y, lengths, x_distorted, x_sp, lengths_sp)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "    return loader\n",
    "\n",
    "#\n",
    "def replace_eos_with_sos(batch_y, sos_token = 1, eos_token = 2, pad_token = 0):\n",
    "    inp = batch_y.clone()\n",
    "    for i in range(inp.shape[0]):\n",
    "        for j in range(inp.shape[1]):\n",
    "            if inp[i][j] == eos_token and (j == inp.shape[1] - 1 or inp[i][j+1] == pad_token):\n",
    "                inp[i][j] = pad_token # replace EOS with padding\n",
    "                break\n",
    "    return torch.cat([torch.ones((inp.shape[0], 1), dtype=torch.long, device=batch_y.device) * sos_token, inp[:, :-1]], dim=1)\n",
    "\n",
    "def convert_keys(d):\n",
    "    \"\"\"Recursively converts numeric string keys to integers in a dictionary.\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: convert_keys(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_keys(i) for i in d]\n",
    "    else:\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8tKyaEpgZlB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initialization of Model, Optimizer, Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JYHgre_7adfD"
   },
   "outputs": [],
   "source": [
    "CLASS_REGISTRY = {\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"NLLLoss\": nn.NLLLoss,\n",
    "    \"BRNNTextInfiller\": BRNNTextInfiller,\n",
    "    \"TransformerTextInfiller\": TransformerTextInfiller,\n",
    "    \"FCNTextInfiller\": FCNTextInfiller,\n",
    "    \"ConvTextInfiller\": ConvTextInfiller,\n",
    "    \"ConvTextInfillerRNN\": ConvTextInfillerRNN,\n",
    "    \"BRNNTokenizedInput\": BRNNTokenizedInput,\n",
    "    \"BRNNSentencePiece\": BRNNSentencePiece,\n",
    "    \"BiConvTextInfiller\": BiConvTextInfiller,\n",
    "    \"NLLWithKL\": NLLWithKL,\n",
    "    \"KLDivLoss\" : nn.KLDivLoss,  \n",
    "}\n",
    "\n",
    "def get_class(class_name):\n",
    "    return CLASS_REGISTRY.get(class_name, None)\n",
    "\n",
    "# General hyperparameters (shared across models)\n",
    "general_hyperparams = {\n",
    "    # general\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 200,\n",
    "    \"num_folds\": 7,\n",
    "\n",
    "    # optimizer\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.0001,\n",
    "\n",
    "    # criterion\n",
    "    \"criterion\": \"NLLWithKL\" if USE_NLLWithKL else \"NLLLoss\",\n",
    "    \"pad_idx\": 0,\n",
    "\n",
    "    \"ranked_based_loss_contribution\": USE_RANKED_LOSS_CONTRIB,\n",
    "    \"ranked_based_loss_contribution_weight\": 100,\n",
    "    \"kl_loss_contribution_weight\": 0.1\n",
    "\n",
    "}\n",
    "\n",
    "# Model-specific hyperparameters\n",
    "if USE_COMPOSITION:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"Composition\",\n",
    "        \"input_dim\": input_dim,\n",
    "        \"sp_input_dim\": sp_input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_BRNN and USE_CONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"ConvTextInfillerRNN\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"num_filters\": EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1, 3, 5, 7],  # Example kernel sizes\n",
    "        \"hidden_size\": 2 * EMBEDDINGS_DIM,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_BRNN:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"BRNNTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        #\"norms_or_ratios\":  [1.0, 0.8],\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "elif USE_TRANSFORMER:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"TransformerTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"d_model\": EMBEDDINGS_DIM,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 5,\n",
    "        \"dim_feedforward\": 512,\n",
    "        \"max_seq_len\": max([train_lengths.max().item(), test_lengths.max().item()]) + 1,\n",
    "        \"dropout\": 0.1\n",
    "    }\n",
    "elif USE_FCN:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"FCNTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "        \"normalize\": False\n",
    "    }\n",
    "elif USE_CONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"ConvTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"num_filters\": 2*EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1, 3, 5, 7], \n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.2,\n",
    "        \"normalize\": False\n",
    "    }\n",
    "elif USE_BICONV:\n",
    "    hyperparams = {\n",
    "        **general_hyperparams,\n",
    "        \"model\": \"BiConvTextInfiller\",\n",
    "        \"vocab_size\": input_dim,\n",
    "        \"embed_size\": EMBEDDINGS_DIM,\n",
    "        \"kernel_sizes\": [1,3,5,7],\n",
    "        \"hidden_size\": 2*EMBEDDINGS_DIM,\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "def initialize_model(hyperparams, device):\n",
    "    model_name = hyperparams[\"model\"]\n",
    "\n",
    "    # Initialize the BRNNTextInfiller model\n",
    "    if model_name == \"BRNNTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            #norms_or_ratios=hyperparams[\"norms_or_ratios\"], #if SE\n",
    "            # DEBUG\n",
    "            #unigram_probs=p_unigram,\n",
    "            #tfidf_scores=tf_idf\n",
    "        )\n",
    "\n",
    "    elif model_name == \"BRNNTokenizedInput\":\n",
    "        model = get_class(model_name)(\n",
    "            input_dim=hyperparams[\"input_dim\"],\n",
    "            output_dim=hyperparams[\"output_dim\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "\n",
    "    elif model_name == \"BRNNSentencePiece\":\n",
    "        model = get_class(model_name)(\n",
    "            input_dim=hyperparams[\"sp_input_dim\"],\n",
    "            output_dim=hyperparams[\"output_dim\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the TransformerTextInfiller model\n",
    "    elif model_name == \"TransformerTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            d_model=hyperparams[\"d_model\"],\n",
    "            num_heads=hyperparams[\"num_heads\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dim_feedforward=hyperparams[\"dim_feedforward\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            max_seq_len=hyperparams[\"max_seq_len\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the FCNTextInfiller model\n",
    "    elif model_name == \"FCNTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            normalize=hyperparams[\"normalize\"],\n",
    "        )\n",
    "\n",
    "    # Initialize the ConvTextInfiller model\n",
    "    elif model_name == \"ConvTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "            normalize=hyperparams[\"normalize\"],\n",
    "        )\n",
    "    elif model_name == \"ConvTextInfillerRNN\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            num_filters=hyperparams[\"num_filters\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            num_layers=hyperparams[\"num_layers\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "    elif model_name == \"BiConvTextInfiller\":\n",
    "        model = get_class(model_name)(\n",
    "            vocab_size=hyperparams[\"vocab_size\"],\n",
    "            embed_size=hyperparams[\"embed_size\"],\n",
    "            kernel_sizes=hyperparams[\"kernel_sizes\"],\n",
    "            hidden_size=hyperparams[\"hidden_size\"],\n",
    "            dropout=hyperparams[\"dropout\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "def initialize_optimizer(hyperparams, model):\n",
    "    optimizer = get_class(hyperparams[\"optimizer\"])(model.parameters(), lr=hyperparams[\"lr\"])\n",
    "    return optimizer\n",
    "\n",
    "def initialize_criterion(hyperparams):\n",
    "    if hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(p_unigram, kl_excluded_tokens=[0,1,2,3], pad_idx=hyperparams[\"pad_idx\"], lambda_kld=hyperparams[\"kl_loss_contribution_weight\"])\n",
    "    elif hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(ignore_index=hyperparams[\"pad_idx\"])\n",
    "    elif hyperparams[\"criterion\"] == \"KLDivLoss\":\n",
    "        criterion = get_class(hyperparams[\"criterion\"])(reduction='batchmean')\n",
    "    return criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "TA036xcf-Gjq"
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, hyperparams, save_dir=prefix_path, experiment_name=None, verbose=True, use_gpu=True):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.hyperparams = hyperparams\n",
    "        if not self.hyperparams is None:\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "        if not experiment_name:\n",
    "            self.experiment_name = \"experiment\"\n",
    "        else:\n",
    "            self.experiment_name = experiment_name\n",
    "\n",
    "        results_dir = os.path.join(save_dir, \"Models Results\")\n",
    "        #create directory if it doesn't exist\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        self.save_dir = os.path.join(results_dir, self.experiment_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.model_path = os.path.join(self.save_dir, \"model.pt\")\n",
    "        self.optimizer_path = os.path.join(self.save_dir, \"optimizer.pt\")\n",
    "        self.hypers_path = os.path.join(self.save_dir, \"hypers.json\")\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def log_specifics(self):\n",
    "\n",
    "        with open(os.path.join(self.save_dir, \"model_specifics.txt\"), 'w') as f:\n",
    "\n",
    "            f.write(\"\\n=== Initialized Model ===\")\n",
    "            f.write(f\"{self.model}\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"\\n=== Optimizer ===\")\n",
    "            f.write(f\"{self.optimizer}\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "            f.write(\"\\n=== Loss Function (Criterion) ===\")\n",
    "            f.write(f\"{self.criterion}\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    def ping(self, epoch):\n",
    "        with open(os.path.join(self.save_dir, \"HeartBeat.txt\"), 'a') as f:\n",
    "            f.write(f\"PING: Current hour = {datetime.now()}, epoch {epoch}\\n\")\n",
    "            \n",
    "\n",
    "    def initialize_from_hyperparams(self):\n",
    "        self.batch_size = self.hyperparams[\"batch_size\"]\n",
    "        self.model = initialize_model(self.hyperparams, self.device)\n",
    "        self.optimizer = initialize_optimizer(self.hyperparams, self.model)\n",
    "        self.criterion = initialize_criterion(self.hyperparams)\n",
    "\n",
    "    #If validating, it saves all aggregated validation metrics and loss per epoch, else it saves only testing metrics\n",
    "    def save_experiment(self, history, folds_involved=None):\n",
    "\n",
    "        is_validation = folds_involved is not None\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_{'val' if is_validation else 'test'}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        # create dictionary to save JSON file\n",
    "        for epoch, (loss, metrics_dictionary) in enumerate(history):\n",
    "            metadata[f\"epoch_{epoch+1}\"] = {\n",
    "                \"loss\": loss,\n",
    "                \"extra_info\": metrics_dictionary\n",
    "            }\n",
    "            # add also number of folds from which the information derives\n",
    "            if is_validation:\n",
    "                metadata[f\"epoch_{epoch+1}\"][\"folds aggregated\"] = folds_involved[epoch]\n",
    "\n",
    "\n",
    "        if is_validation:\n",
    "            # this code has been put here to avoid saving the metadata twice\n",
    "            with open(self.hypers_path, \"w\") as f:\n",
    "                json.dump(self.hyperparams, f, indent=4)\n",
    "\n",
    "        else:\n",
    "            #save model only when training on whole dataset and testing on test set\n",
    "            torch.save(self.model.state_dict(), self.model_path)\n",
    "            torch.save(self.optimizer.state_dict(), self.optimizer_path)\n",
    "\n",
    "        #Dump metadata (test metrics or validation aggregated data)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metrics(self, epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Plots and saves loss, sequence accuracy, MRR, and Top-K accuracy metrics.\n",
    "        Ensures points are connected by a default-colored line, while points themselves\n",
    "        are color-coded based on the number of folds involved in training (if available).\n",
    "        \"\"\"\n",
    "        is_validation = folds_involved is not None\n",
    "\n",
    "        if is_validation:\n",
    "            # If folds are provided, create a colormap\n",
    "            unique_folds = sorted(set(folds_involved))\n",
    "            colormap = plt.colormaps[\"tab20\"]  # Switch to 'tab10' for distinct colors\n",
    "            fold_color_map = {fold: colormap(i+2) for i, fold in enumerate(unique_folds)} # i+2 skips too bluish colors for better visualization\n",
    "\n",
    "\n",
    "        def plot_metric(metric_values, ylabel, title, filename):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "            # Plot a continuous gray line through all points\n",
    "            plt.plot(epochs, metric_values, zorder=1)\n",
    "\n",
    "            # Overlay scatter points: Use fold colors if available, otherwise default color\n",
    "            if is_validation:\n",
    "                for fold in unique_folds[::-1]:\n",
    "                    fold_epochs = [e for e, f in zip(epochs, folds_involved) if f == fold]\n",
    "                    fold_values = [v for v, f in zip(metric_values, folds_involved) if f == fold]\n",
    "                    plt.scatter(fold_epochs, fold_values, color=fold_color_map[fold], label=f\"{fold} Folds\", s=50, zorder=2)\n",
    "            else:\n",
    "                plt.scatter(epochs, metric_values, s=50, zorder=2)\n",
    "\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"{title} ({'Validation' if is_validation else 'Test'})\")\n",
    "            if is_validation:\n",
    "                plt.legend(title=\"Folds Used\", loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(self.save_dir, filename))\n",
    "            if verbose:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "        # Plot Loss\n",
    "        plot_metric(losses, \"Loss\", \"Loss Over Epochs\", f\"loss_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Sequence Accuracy\n",
    "        plot_metric(sequence_accuracies, \"Sequence Accuracy\", \"Sequence Accuracy Over Epochs\",\n",
    "                    f\"sequence_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot MRR\n",
    "        plot_metric(mrrs, \"MRR\", \"Mean Reciprocal Rank Over Epochs\",\n",
    "                    f\"mrr_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Top-K Accuracy\n",
    "        for k, values in top_k_accuracies.items():\n",
    "            plot_metric(values, \"Accuracy\", f\"Top-{k} Accuracy Over Epochs\",\n",
    "                        f\"top_{k}_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "    def load_experiment(self):\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_test.json\")\n",
    "        assert os.path.exists(self.hypers_path) and os.path.exists(self.model_path) and os.path.exists(self.optimizer_path) and os.path.exists(metadata_path), \" No saved experiment found!\"\n",
    "\n",
    "        with open(self.hypers_path, \"r\") as f:\n",
    "            self.hyperparams = json.load(f)\n",
    "\n",
    "        #load model with hyperparams and optimizer\n",
    "        self.initialize_from_hyperparams()\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))\n",
    "        self.optimizer.load_state_dict(torch.load(self.optimizer_path, map_location=self.device, weights_only=True))\n",
    "\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        i = 0\n",
    "        # recover history of past iterations\n",
    "        while f\"epoch_{i+1}\" in metadata:\n",
    "            loss = metadata[f\"epoch_{i+1}\"][\"loss\"]\n",
    "            extra_info = metadata[f\"epoch_{i+1}\"][\"extra_info\"]\n",
    "            # converts integers key of top_k accuracies back to int (by default they become strings once in the JSON file)\n",
    "            extra_info = convert_keys(extra_info)\n",
    "            self.history.append((loss, extra_info))\n",
    "            i += 1\n",
    "\n",
    "    def test(self, test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, verbose=True):\n",
    "        if verbose:\n",
    "            print(\"TESTING MODEL!\")\n",
    "            num_sequences = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        test_loader = create_loader(test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_corrects = 0\n",
    "            metrics = None\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, word_emb, phon_emb in test_loader:\n",
    "                batch_x, batch_y, batch_lengths, word_emb, phon_emb = batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), word_emb.to(self.device), phon_emb.to(self.device)\n",
    "\n",
    "                if self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfillerRNN\" or self.hyperparams[\"model\"] == \"BiConvTextInfiller\":\n",
    "                    # For BRNNTextInfiller, use the model's forward pass\n",
    "                    model_output = self.model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "                elif self.hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                    # Generate the sequence autoregressively using the model's generate function\n",
    "                    model_output = self.model.generate(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "                if verbose:\n",
    "                    # Reconstruct sequences (input, ground truth, generated output)\n",
    "                    inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "\n",
    "                    for (input, gt_seq, out_seq) in zip(inp, gt, out):\n",
    "                        print(f\"TEST n. {num_sequences + 1}\")\n",
    "                        print(f\"Input: {input}\")\n",
    "                        print(f\"Ground Truth: {gt_seq}\")\n",
    "                        print(f\"Output: {out_seq}\")\n",
    "                        num_sequences += 1\n",
    "\n",
    "                # Collect batch metrics (e.g., accuracy, loss, etc.)\n",
    "                batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "                metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "            # Compute final metrics\n",
    "            metrics = compute_final_metrics(metrics)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"TEST FINISHED\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    #Train model, returns history if early stopping is true, else it returns empty tuple\n",
    "    def train(self, train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x, val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x,\n",
    "              num_epochs=10, verbose=False, early_stop=False, max_mrr_init=None, max_top1_init=None, max_seq_acc_init=None, best_epoch_init=None):\n",
    "\n",
    "        train_loader = create_loader(train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x, self.batch_size)\n",
    "\n",
    "        if early_stop:\n",
    "            max_mrr = max_mrr_init\n",
    "            max_top1 = max_top1_init\n",
    "            max_seq_acc = max_seq_acc_init\n",
    "            best_epoch = best_epoch_init\n",
    "            patience = 50\n",
    "            counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            self.ping(epoch)\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, word_emb, phon_emb in train_loader:  # batch processing to not have RAM usage explode\n",
    "                batch_x, batch_y, batch_lengths, word_emb, phon_emb = batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), word_emb.to(self.device), phon_emb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                if self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfillerRNN\" or self.hyperparams[\"model\"] == \"BiConvTextInfiller\":\n",
    "                    # accounts for SOS beggining the sequence\n",
    "                    model_output = self.model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "                    #reshape for loss calculation\n",
    "                    output = model_output.view(-1, model_output.shape[-1])\n",
    "                    batch_y_reshaped = batch_y.view(-1)\n",
    "                    if USE_ONE_HOT_ENCODING:\n",
    "                        output = output.view(-1)\n",
    "                        \n",
    "                elif self.hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                    tgt_input = replace_eos_with_sos(batch_y)\n",
    "                    #accounting for the fact that target sequence is always one token longer (SOS)\n",
    "                    tgt_lengths = batch_lengths + torch.ones_like(batch_lengths, device=batch_lengths.device)\n",
    "                    model_output = self.model(batch_x, batch_lengths, tgt_input, tgt_lengths, word_emb, phon_emb)\n",
    "                    output = model_output.view(-1, model_output.shape[-1])\n",
    "                    batch_y_reshaped = batch_y.view(-1)\n",
    "\n",
    "                if self.hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "                elif self.hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "                    loss = self.criterion(model_output, batch_y, batch_lengths)\n",
    "                elif self.hyperparams[\"criterion\"] == \"KLDivLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "\n",
    "                if self.hyperparams[\"ranked_based_loss_contribution\"]:\n",
    "                    # Step 1: Find the maximum indices along the sequence dimension for each batch (along axis 1)\n",
    "                    max_indices = batch_x.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                    # Step 2: Compute the previous index for the comparison (i, j-1)\n",
    "                    # Ensure that j-1 is not less than 0 (i.e., for the first token in the sequence)\n",
    "                    prev_indices = max_indices - 1  if (self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\") else max_indices #clamp to ensure no negative indices\n",
    "                    # Step 3: Select the corresponding predictions and ground truth for the prev indices (i, j-1)\n",
    "                    # Get the log probabilities from model_output at (i, j-1)\n",
    "                    predictions_at_prev_idx = model_output[torch.arange(model_output.size(0)), prev_indices, :]  # (batch_size, vocab_size)\n",
    "                    # Turn log probabilities into probabilities\n",
    "                    predictions_at_prev_idx = torch.exp(predictions_at_prev_idx)\n",
    "                    # gt values\n",
    "                    target_at_prev_idx = batch_y[torch.arange(batch_y.size(0)), prev_indices] # (batch_size)\n",
    "                    # Step 2: Extract log probability of the correct token\n",
    "                    correct_probs = predictions_at_prev_idx.gather(dim=-1, index=target_at_prev_idx.unsqueeze(-1)).squeeze(-1)  # (batch_size,)\n",
    "                    # Step 3: Find the max predicted token probability\n",
    "                    max_predicted_probs, _ = predictions_at_prev_idx.max(dim=-1)  # (batch_size,)\n",
    "                    # Step 4: Compute the loss term comparing max-predicted probability vs ground-truth probability\n",
    "                    prob_diff_loss = (max_predicted_probs - correct_probs).mean()  # Penalize if gt prob is lower than max prob\n",
    "                    loss_contrib = prob_diff_loss * self.hyperparams[\"ranked_based_loss_contribution_weight\"]\n",
    "                    loss += loss_contrib\n",
    "\n",
    "                # Compute loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            metrics = self.test(val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x, verbose=False)  # validation\n",
    "            history.append((avg_loss, metrics))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{len(self.history) + epoch+1}/{len(self.history) + num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "                print(\"Validation metrics are: \", metrics)\n",
    "\n",
    "            # Save validation after each epoch\n",
    "            if early_stop:\n",
    "                if not max_mrr or metrics[\"mrr\"] > max_mrr or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] > max_top1) or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] == max_top1 and max_seq_acc > metrics[\"sequence_accuracy\"]):\n",
    "                    max_mrr = metrics[\"mrr\"]\n",
    "                    max_top1 = metrics[\"top_k_accuracy\"][1]\n",
    "                    max_seq_acc = metrics[\"sequence_accuracy\"]\n",
    "                    best_epoch = epoch\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience and epoch >= 150: #avoids \"too\" early stopping\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "        if early_stop:\n",
    "            return history, best_epoch\n",
    "        else:\n",
    "            # when training on the whole dataset, or when continuing train after loading the model, I want to store the new updated history always\n",
    "            self.history.extend(history)\n",
    "            self.save_experiment(self.history)\n",
    "            epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(self.history)\n",
    "\n",
    "            self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies)\n",
    "\n",
    "            return\n",
    "\n",
    "    def k_fold_cross_validation(self, train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x,\n",
    "                                test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, num_epochs, num_folds=2, verbose=True):\n",
    "        assert num_folds >= 2, \"Cannot use less than 2 folds\"\n",
    "\n",
    "        self.log_specifics()\n",
    "        folds_history = []\n",
    "        best_epochs = []\n",
    "        if verbose:\n",
    "            print(f\"Performing {num_folds}-fold cross validation\")\n",
    "\n",
    "        for k in range(num_folds):\n",
    "            # define validation set indices\n",
    "            val_start = int(k * (train_x.shape[0] // num_folds))\n",
    "            val_end = min(int((k+1) * (train_x.shape[0] // num_folds)), train_x.shape[0])\n",
    "\n",
    "            # validation set\n",
    "            val_x = train_x[val_start:val_end]\n",
    "            val_y = train_y[val_start:val_end]\n",
    "            val_lengths = train_lengths[val_start:val_end]\n",
    "            word_emb_val_x = word_emb_train_x[val_start:val_end]\n",
    "            phon_emb_val_x = phon_emb_train_x[val_start:val_end]\n",
    "\n",
    "            # train set\n",
    "            train_x_fold = torch.cat([train_x[:val_start], train_x[val_end:]])\n",
    "            train_y_fold = torch.cat([train_y[:val_start], train_y[val_end:]])\n",
    "            train_lengths_fold = torch.cat([train_lengths[:val_start], train_lengths[val_end:]])\n",
    "            word_emb_train_x_fold = torch.cat([word_emb_train_x[:val_start], word_emb_train_x[val_end:]])\n",
    "            phon_emb_train_x_fold = torch.cat([phon_emb_train_x[:val_start], phon_emb_train_x[val_end:]])\n",
    "\n",
    "            # reinitialize the model, criterion, optimizer\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "            # training returns history as (avg_loss, val_metrics) pair\n",
    "            history, best_epoch = self.train(train_x_fold, train_y_fold, train_lengths_fold, word_emb_train_x_fold, phon_emb_train_x_fold,\n",
    "                                             val_x, val_y, val_lengths, word_emb_val_x, phon_emb_val_x, num_epochs=num_epochs, early_stop=True)\n",
    "\n",
    "            folds_history.append(history) #self.history is dictioanry < num_kfold : (avg_loss, val_metrics) >\n",
    "            best_epochs.append(best_epoch)\n",
    "            if verbose:\n",
    "                print(f\"Trained fold {k+1} for {len(history)} epochs\")\n",
    "                print(f\"Best results at epoch {best_epoch+1}: {history[best_epoch]}\")\n",
    "\n",
    "        aggregated_history, folds_considered = aggregate_k_fold_metrics(folds_history) #aggregated history is [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2, ...]\n",
    "        epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(aggregated_history) #sorted lists of different aggregated metrics per epoch\n",
    "\n",
    "        self.save_experiment(aggregated_history, folds_involved=folds_considered)\n",
    "        self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=folds_considered)\n",
    "\n",
    "        avg_epochs = int(round(np.mean(best_epochs))) + 1 #get average best number of epochs to train the model on the whole dataset\n",
    "        if verbose:\n",
    "            print(f\"Average best number of epochs: {avg_epochs} (max: {np.max(best_epochs) + 1}, min: {np.min(best_epochs) + 1})\")\n",
    "\n",
    "        self.initialize_from_hyperparams()\n",
    "        #We do training on the whole training set and test data directly on the test set\n",
    "        self.train(train_x, train_y, train_lengths, word_emb_train_x, phon_emb_train_x,\n",
    "                   test_x, test_y, test_lengths, word_emb_test_x, phon_emb_test_x, num_epochs=avg_epochs, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i_N_Z-Mghu7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment class for composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Z7spxWUnWTtT"
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, hyperparams, save_dir=prefix_path, experiment_name=None, verbose=True, use_gpu=True):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.hyperparams = hyperparams\n",
    "        if not self.hyperparams is None:\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "        if not experiment_name:\n",
    "            self.experiment_name = \"experiment\"\n",
    "        else:\n",
    "            self.experiment_name = experiment_name\n",
    "\n",
    "        results_dir = os.path.join(save_dir, \"Models Results\")\n",
    "        #create directory if it doesn't exist\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        self.save_dir = os.path.join(results_dir, self.experiment_name)\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        self.model_path = os.path.join(self.save_dir, \"model.pt\")\n",
    "        self.optimizer_path = os.path.join(self.save_dir, \"optimizer.pt\")\n",
    "        self.hypers_path = os.path.join(self.save_dir, \"hypers.json\")\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def initialize_from_hyperparams(self):\n",
    "        self.batch_size = self.hyperparams[\"batch_size\"]\n",
    "        self.model = Composition(self.hyperparams, self.device)\n",
    "        self.optimizer = initialize_optimizer(self.hyperparams, self.model)\n",
    "        self.criterion = initialize_criterion(self.hyperparams)\n",
    "\n",
    "    #If validating, it saves all aggregated validation metrics and loss per epoch, else it saves only testing metrics\n",
    "    def save_experiment(self, history, folds_involved=None):\n",
    "\n",
    "        is_validation = folds_involved is not None\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_{'val' if is_validation else 'test'}.json\")\n",
    "        metadata = {}\n",
    "\n",
    "        # create dictionary to save JSON file\n",
    "        for epoch, (loss, metrics_dictionary) in enumerate(history):\n",
    "            metadata[f\"epoch_{epoch+1}\"] = {\n",
    "                \"loss\": loss,\n",
    "                \"extra_info\": metrics_dictionary\n",
    "            }\n",
    "            # add also number of folds from which the information derives\n",
    "            if is_validation:\n",
    "                metadata[f\"epoch_{epoch+1}\"][\"folds aggregated\"] = folds_involved[epoch]\n",
    "\n",
    "\n",
    "        if is_validation:\n",
    "            # this code has been put here to avoid saving the metadata twice\n",
    "            with open(self.hypers_path, \"w\") as f:\n",
    "                json.dump(self.hyperparams, f, indent=4)\n",
    "\n",
    "        else:\n",
    "            #save model only when training on whole dataset and testing on test set\n",
    "            torch.save(self.model.state_dict(), self.model_path)\n",
    "            torch.save(self.optimizer.state_dict(), self.optimizer_path)\n",
    "\n",
    "        #Dump metadata (test metrics or validation aggregated data)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "                json.dump(metadata, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metrics(self, epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Plots and saves loss, sequence accuracy, MRR, and Top-K accuracy metrics.\n",
    "        Ensures points are connected by a default-colored line, while points themselves\n",
    "        are color-coded based on the number of folds involved in training (if available).\n",
    "        \"\"\"\n",
    "        is_validation = folds_involved is not None\n",
    "\n",
    "        if is_validation:\n",
    "            # If folds are provided, create a colormap\n",
    "            unique_folds = sorted(set(folds_involved))\n",
    "            colormap = plt.colormaps[\"tab20\"]  # Switch to 'tab10' for distinct colors\n",
    "            fold_color_map = {fold: colormap(i+2) for i, fold in enumerate(unique_folds)} # i+2 skips too bluish colors for better visualization\n",
    "\n",
    "\n",
    "        def plot_metric(metric_values, ylabel, title, filename):\n",
    "            plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "            # Plot a continuous gray line through all points\n",
    "            plt.plot(epochs, metric_values, zorder=1)\n",
    "\n",
    "            # Overlay scatter points: Use fold colors if available, otherwise default color\n",
    "            if is_validation:\n",
    "                for fold in unique_folds[::-1]:\n",
    "                    fold_epochs = [e for e, f in zip(epochs, folds_involved) if f == fold]\n",
    "                    fold_values = [v for v, f in zip(metric_values, folds_involved) if f == fold]\n",
    "                    plt.scatter(fold_epochs, fold_values, color=fold_color_map[fold], label=f\"{fold} Folds\", s=50, zorder=2)\n",
    "            else:\n",
    "                plt.scatter(epochs, metric_values, s=50, zorder=2)\n",
    "\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(ylabel)\n",
    "            plt.title(f\"{title} ({'Validation' if is_validation else 'Test'})\")\n",
    "            if is_validation:\n",
    "                plt.legend(title=\"Folds Used\", loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.savefig(os.path.join(self.save_dir, filename))\n",
    "            if verbose:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "        # Plot Loss\n",
    "        plot_metric(losses, \"Loss\", \"Loss Over Epochs\", f\"loss_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Sequence Accuracy\n",
    "        plot_metric(sequence_accuracies, \"Sequence Accuracy\", \"Sequence Accuracy Over Epochs\",\n",
    "                    f\"sequence_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot MRR\n",
    "        plot_metric(mrrs, \"MRR\", \"Mean Reciprocal Rank Over Epochs\",\n",
    "                    f\"mrr_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "        # Plot Top-K Accuracy\n",
    "        for k, values in top_k_accuracies.items():\n",
    "            plot_metric(values, \"Accuracy\", f\"Top-{k} Accuracy Over Epochs\",\n",
    "                        f\"top_{k}_accuracy_plot_{'val' if is_validation else 'test'}.png\")\n",
    "\n",
    "    def load_experiment(self):\n",
    "        metadata_path = os.path.join(self.save_dir, f\"metadata_test.json\")\n",
    "        assert os.path.exists(self.hypers_path) and os.path.exists(self.model_path) and os.path.exists(self.optimizer_path) and os.path.exists(metadata_path), \" No saved experiment found!\"\n",
    "\n",
    "        with open(self.hypers_path, \"r\") as f:\n",
    "            self.hyperparams = json.load(f)\n",
    "\n",
    "        #load model with hyperparams and optimizer\n",
    "        self.initialize_from_hyperparams()\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device, weights_only=True))\n",
    "        self.optimizer.load_state_dict(torch.load(self.optimizer_path, map_location=self.device, weights_only=True))\n",
    "\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        i = 0\n",
    "        # recover history of past iterations\n",
    "        while f\"epoch_{i+1}\" in metadata:\n",
    "            loss = metadata[f\"epoch_{i+1}\"][\"loss\"]\n",
    "            extra_info = metadata[f\"epoch_{i+1}\"][\"extra_info\"]\n",
    "            # converts integers key of top_k accuracies back to int (by default they become strings once in the JSON file)\n",
    "            extra_info = convert_keys(extra_info)\n",
    "            self.history.append((loss, extra_info))\n",
    "            i += 1\n",
    "\n",
    "    def test(self, test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, verbose=True):\n",
    "        if verbose:\n",
    "            print(\"TESTING MODEL!\")\n",
    "            num_sequences = 0\n",
    "\n",
    "        self.model.eval()\n",
    "        test_loader = create_loader_with_sp(test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            total_corrects = 0\n",
    "            metrics = None\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp in test_loader:\n",
    "                batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp = \\\n",
    "                batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), batch_x_distorted.to(self.device), \\\n",
    "                batch_x_sp.to(self.device), batch_lengths_sp.to(self.device)\n",
    "\n",
    "                model_output = self.model(batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp)\n",
    "\n",
    "                if verbose:\n",
    "                    # Reconstruct sequences (input, ground truth, generated output)\n",
    "                    inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "\n",
    "                    for (input, gt_seq, out_seq) in zip(inp, gt, out):\n",
    "                        print(f\"TEST n. {num_sequences + 1}\")\n",
    "                        print(f\"Input: {input}\")\n",
    "                        print(f\"Ground Truth: {gt_seq}\")\n",
    "                        print(f\"Output: {out_seq}\")\n",
    "                        num_sequences += 1\n",
    "\n",
    "                # Collect batch metrics (e.g., accuracy, loss, etc.)\n",
    "                batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "                metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "            # Compute final metrics\n",
    "            metrics = compute_final_metrics(metrics)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"TEST FINISHED\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    #Train model, returns history if early stopping is true, else it returns empty tuple\n",
    "    def train(self, train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "              val_x, val_y, val_lengths, x_distorted_val_x, x_sp_val_x, batch_lengths_sp_val_x,\n",
    "              num_epochs=10, verbose=False, early_stop=False, max_mrr_init=None, max_top1_init=None, max_seq_acc_init=None, best_epoch_init=None):\n",
    "\n",
    "        train_loader = create_loader_with_sp(train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x, self.batch_size)\n",
    "\n",
    "        if early_stop:\n",
    "            max_mrr = max_mrr_init\n",
    "            max_top1 = max_top1_init\n",
    "            max_seq_acc = max_seq_acc_init\n",
    "            best_epoch = best_epoch_init\n",
    "            patience = 50\n",
    "            counter = 0\n",
    "\n",
    "        # Training loop\n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "\n",
    "\n",
    "            for batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp in train_loader:\n",
    "                batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp = \\\n",
    "                batch_x.to(self.device), batch_y.to(self.device), batch_lengths.to(self.device), batch_x_distorted.to(self.device), \\\n",
    "                batch_x_sp.to(self.device), batch_lengths_sp.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                model_output = self.model(batch_x, batch_y, batch_lengths, batch_x_distorted, batch_x_sp, batch_lengths_sp)\n",
    "                output = model_output.view(-1, model_output.shape[-1])\n",
    "                batch_y_reshaped = batch_y.view(-1)\n",
    "\n",
    "\n",
    "                if self.hyperparams[\"criterion\"] == \"NLLLoss\":\n",
    "                    loss = self.criterion(output, batch_y_reshaped)\n",
    "\n",
    "                elif self.hyperparams[\"criterion\"] == \"NLLWithKL\":\n",
    "                    loss = self.criterion(model_output, batch_y, batch_lengths)\n",
    "\n",
    "                if self.hyperparams[\"ranked_based_loss_contribution\"]:\n",
    "                    # Step 1: Find the maximum indices along the sequence dimension for each batch (along axis 1)\n",
    "                    max_indices = batch_x.argmax(dim=1)  # Shape: (batch_size,)\n",
    "                    # Step 2: Compute the previous index for the comparison (i, j-1)\n",
    "                    # Ensure that j-1 is not less than 0 (i.e., for the first token in the sequence)\n",
    "                    prev_indices = max_indices - 1  if (self.hyperparams[\"model\"] == \"BRNNTextInfiller\" or self.hyperparams[\"model\"] == \"FCNTextInfiller\" or self.hyperparams[\"model\"] == \"ConvTextInfiller\") else max_indices # Clamp to ensure no negative indices\n",
    "                    # Step 3: Select the corresponding predictions and ground truth for the prev indices (i, j-1)\n",
    "                    # Get the log probabilities from model_output at (i, j-1)\n",
    "                    predictions_at_prev_idx = model_output[torch.arange(model_output.size(0)), prev_indices, :]  # (batch_size, vocab_size)\n",
    "                    # Turn log probabilities into probabilities\n",
    "                    predictions_at_prev_idx = torch.exp(predictions_at_prev_idx)\n",
    "                    # gt values\n",
    "                    target_at_prev_idx = batch_y[torch.arange(batch_y.size(0)), prev_indices] # (batch_size)\n",
    "                    # Step 2: Extract log probability of the correct token\n",
    "                    correct_probs = predictions_at_prev_idx.gather(dim=-1, index=target_at_prev_idx.unsqueeze(-1)).squeeze(-1)  # (batch_size,)\n",
    "                    # Step 3: Find the max predicted token probability\n",
    "                    max_predicted_probs, _ = predictions_at_prev_idx.max(dim=-1)  # (batch_size,)\n",
    "\n",
    "                    # Step 4: Compute the loss term comparing max-predicted probability vs ground-truth probability\n",
    "                    prob_diff_loss = (max_predicted_probs - correct_probs).mean()  # Penalize if gt prob is lower than max prob\n",
    "\n",
    "                    loss_contrib = prob_diff_loss * self.hyperparams[\"ranked_based_loss_contribution_weight\"]\n",
    "\n",
    "                    loss += loss_contrib\n",
    "\n",
    "                # Compute loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            metrics = self.test(val_x, val_y, val_lengths, x_distorted_val_x, x_sp_val_x, batch_lengths_sp_val_x, verbose=False)  # validation\n",
    "            history.append((avg_loss, metrics))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch [{len(self.history) + epoch+1}/{len(self.history) + num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "                print(\"Validation metrics are: \", metrics)\n",
    "\n",
    "            # Save validation after each epoch\n",
    "            if early_stop:\n",
    "                if not max_mrr or metrics[\"mrr\"] > max_mrr or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] > max_top1) or (metrics[\"mrr\"] == max_mrr and metrics[\"top_k_accuracy\"][1] == max_top1 and max_seq_acc > metrics[\"sequence_accuracy\"]):\n",
    "                    max_mrr = metrics[\"mrr\"]\n",
    "                    max_top1 = metrics[\"top_k_accuracy\"][1]\n",
    "                    max_seq_acc = metrics[\"sequence_accuracy\"]\n",
    "                    best_epoch = epoch\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience and epoch >= 150: #avoids \"too\" early stopping\n",
    "                        if verbose:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "        if early_stop:\n",
    "            return history, best_epoch\n",
    "        else:\n",
    "            # when training on the whole dataset, or when continuing train after loading the model, I want to store the new updated history always\n",
    "            self.history.extend(history)\n",
    "            self.save_experiment(self.history)\n",
    "            epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(self.history)\n",
    "\n",
    "            self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies)\n",
    "\n",
    "            return\n",
    "\n",
    "    def k_fold_cross_validation(self, train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "                                test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, num_epochs, num_folds=2, verbose=True):\n",
    "        assert num_folds >= 2, \"Cannot use less than 2 folds\"\n",
    "\n",
    "        folds_history = []\n",
    "        best_epochs = []\n",
    "        if verbose:\n",
    "            print(f\"Performing {num_folds}-fold cross validation\")\n",
    "\n",
    "        for k in range(num_folds):\n",
    "            # define validation set indices\n",
    "            val_start = int(k * (train_x.shape[0] // num_folds))\n",
    "            val_end = min(int((k+1) * (train_x.shape[0] // num_folds)), train_x.shape[0])\n",
    "\n",
    "            # validation set\n",
    "            val_x = train_x[val_start:val_end]\n",
    "            val_y = train_y[val_start:val_end]\n",
    "            val_lengths = train_lengths[val_start:val_end]\n",
    "            val_x_distorted = x_distorted_train_x[val_start:val_end]\n",
    "            val_x_sp = x_sp_train_x[val_start:val_end]\n",
    "            val_lengths_sp = lengths_sp_train_x[val_start:val_end]\n",
    "\n",
    "            # train set\n",
    "            train_x_fold = torch.cat([train_x[:val_start], train_x[val_end:]])\n",
    "            train_y_fold = torch.cat([train_y[:val_start], train_y[val_end:]])\n",
    "            train_lengths_fold = torch.cat([train_lengths[:val_start], train_lengths[val_end:]])\n",
    "            x_distorted_train_x_fold = torch.cat([x_distorted_train_x[:val_start], x_distorted_train_x[val_end:]])\n",
    "            x_sp_train_x_fold = torch.cat([x_sp_train_x[:val_start], x_sp_train_x[val_end:]])\n",
    "            lengths_sp_train_x_fold = torch.cat([lengths_sp_train_x[:val_start], lengths_sp_train_x[val_end:]])\n",
    "\n",
    "            # reinitialize the model, criterion, optimizer\n",
    "            self.initialize_from_hyperparams()\n",
    "\n",
    "            # training returns history as (avg_loss, val_metrics) pair\n",
    "            history, best_epoch = self.train(train_x_fold, train_y_fold, train_lengths_fold, x_distorted_train_x_fold, x_sp_train_x_fold, lengths_sp_train_x_fold,\n",
    "                                             val_x, val_y, val_lengths, val_x_distorted, val_x_sp, val_lengths_sp, num_epochs=num_epochs, early_stop=True)\n",
    "\n",
    "            folds_history.append(history) #self.history is dictioanry < num_kfold : (avg_loss, val_metrics) >\n",
    "            best_epochs.append(best_epoch)\n",
    "            if verbose:\n",
    "                print(f\"Trained fold {k+1} for {len(history)} epochs\")\n",
    "                print(f\"Best results at epoch {best_epoch+1}: {history[best_epoch]}\")\n",
    "\n",
    "        aggregated_history, folds_considered = aggregate_k_fold_metrics(folds_history) #aggregated history is [avg_metrics_across_folds_epoch1, avg_metrics_across_folds_epoch2, ...]\n",
    "        epochs, losses, sequence_accuracies, mrrs, top_k_accuracies = unpack_history(aggregated_history) #sorted lists of different aggregated metrics per epoch\n",
    "\n",
    "        self.save_experiment(aggregated_history, folds_involved=folds_considered)\n",
    "        self.plot_metrics(epochs, losses, sequence_accuracies, mrrs, top_k_accuracies, folds_involved=folds_considered)\n",
    "\n",
    "        avg_epochs = int(round(np.mean(best_epochs))) + 1 #get average best number of epochs to train the model on the whole dataset\n",
    "        if verbose:\n",
    "            print(f\"Average best number of epochs: {avg_epochs} (max: {np.max(best_epochs) + 1}, min: {np.min(best_epochs) + 1})\")\n",
    "\n",
    "        self.initialize_from_hyperparams()\n",
    "        #We do training on the whole training set and test data directly on the test set\n",
    "        self.train(train_x, train_y, train_lengths, x_distorted_train_x, x_sp_train_x, lengths_sp_train_x,\n",
    "                   test_x, test_y, test_lengths, test_x_distorted, test_x_sp, test_lengths_sp, num_epochs=avg_epochs, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D32xz_Oz_SU0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnG6AVEGyLcp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 3.0989\n",
      "Validation metrics are:  {'sequence_accuracy': 0.0, 'mrr': 0.22803581962258992, 'top_k_accuracy': {20: 0.6856368563685636, 15: 0.5772357723577236, 10: 0.4444444444444444, 5: 0.3062330623306233, 1: 0.12466124661246612}}\n",
      "Epoch [2/200], Loss: 1.5737\n",
      "Validation metrics are:  {'sequence_accuracy': 0.04065040650406504, 'mrr': 0.282040369578462, 'top_k_accuracy': {20: 0.6964769647696477, 15: 0.6287262872628726, 10: 0.5040650406504065, 5: 0.37398373983739835, 1: 0.16260162601626016}}\n",
      "Epoch [3/200], Loss: 0.7199\n",
      "Validation metrics are:  {'sequence_accuracy': 0.13550135501355012, 'mrr': 0.34395015144253915, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6504065040650406, 10: 0.5799457994579946, 5: 0.4742547425474255, 1: 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"BRNNTextInfillerSE_batchsize64_{EMBEDDINGS_DIM}\" + (\"withKLLoss\" if hyperparams[\"criterion\"] == 'NLLWithKL' else \"\")\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "#experiment.k_fold_cross_validation(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "#                                   test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x,\n",
    "#                                   #num_epochs=2, num_folds=2) # debug\n",
    "#                                   num_epochs=hyperparams[\"num_epochs\"], num_folds=hyperparams[\"num_folds\"])\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                   test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=200, early_stop=False, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "CnvKLPhGhy51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.2501\n",
      "Validation metrics are:  {'sequence_accuracy': 0.08672086720867209, 'mrr': 0.31904275837959123, 'top_k_accuracy': {20: 0.7046070460704607, 15: 0.6422764227642277, 10: 0.5311653116531165, 5: 0.41192411924119243, 1: 0.2140921409214092}}\n",
      "Epoch [2/200], Loss: 0.7124\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2222222222222222, 'mrr': 0.4020212776449771, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6233062330623306, 5: 0.5040650406504065, 1: 0.28184281842818426}}\n",
      "Epoch [3/200], Loss: 0.4714\n",
      "Validation metrics are:  {'sequence_accuracy': 0.23577235772357724, 'mrr': 0.38481559260073317, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.6964769647696477, 10: 0.6043360433604336, 5: 0.4905149051490515, 1: 0.27371273712737126}}\n",
      "Epoch [4/200], Loss: 0.3641\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2899728997289973, 'mrr': 0.4231922749361374, 'top_k_accuracy': {20: 0.7804878048780488, 15: 0.7371273712737128, 10: 0.6531165311653117, 5: 0.5121951219512195, 1: 0.3224932249322493}}\n",
      "Epoch [5/200], Loss: 0.3264\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.41904985801835193, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7181571815718157, 10: 0.6504065040650406, 5: 0.5230352303523035, 1: 0.3116531165311653}}\n",
      "Epoch [6/200], Loss: 0.3087\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3062330623306233, 'mrr': 0.4384755217432945, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7289972899728997, 10: 0.6639566395663956, 5: 0.5528455284552846, 1: 0.3252032520325203}}\n",
      "Epoch [7/200], Loss: 0.2909\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.42854488133868895, 'top_k_accuracy': {20: 0.7831978319783198, 15: 0.7289972899728997, 10: 0.6802168021680217, 5: 0.5392953929539296, 1: 0.3170731707317073}}\n",
      "Epoch [8/200], Loss: 0.2779\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.43381710493166575, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7181571815718157, 10: 0.6531165311653117, 5: 0.5420054200542005, 1: 0.32791327913279134}}\n",
      "Epoch [9/200], Loss: 0.2676\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.43141428319480996, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7181571815718157, 10: 0.6476964769647696, 5: 0.5447154471544715, 1: 0.33062330623306235}}\n",
      "Epoch [10/200], Loss: 0.2605\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4299303589378406, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5311653116531165, 1: 0.33604336043360433}}\n",
      "Epoch [11/200], Loss: 0.2547\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4502514238098019, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.3523035230352303}}\n",
      "Epoch [17/200], Loss: 0.2272\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4408817856531943, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5447154471544715, 1: 0.34146341463414637}}\n",
      "Epoch [18/200], Loss: 0.2299\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.42493274351745364, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7073170731707317, 10: 0.6504065040650406, 5: 0.5474254742547425, 1: 0.3224932249322493}}\n",
      "Epoch [19/200], Loss: 0.2302\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44239373635989315, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5474254742547425, 1: 0.34959349593495936}}\n",
      "Epoch [20/200], Loss: 0.2272\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4330922676789172, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7262872628726287, 10: 0.6639566395663956, 5: 0.5257452574525745, 1: 0.33604336043360433}}\n",
      "Epoch [21/200], Loss: 0.2268\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4389570475581161, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7018970189701897, 10: 0.6449864498644986, 5: 0.5392953929539296, 1: 0.33875338753387535}}\n",
      "Epoch [22/200], Loss: 0.2297\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.43754467143950937, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6178861788617886, 5: 0.5392953929539296, 1: 0.34688346883468835}}\n",
      "Epoch [23/200], Loss: 0.2346\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34959349593495936, 'mrr': 0.4490792392327166, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7046070460704607, 10: 0.6341463414634146, 5: 0.5284552845528455, 1: 0.3604336043360434}}\n",
      "Epoch [24/200], Loss: 0.2348\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.44161653245479887, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7262872628726287, 10: 0.6476964769647696, 5: 0.5257452574525745, 1: 0.3523035230352303}}\n",
      "Epoch [25/200], Loss: 0.2334\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.4409051377123803, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7018970189701897, 10: 0.6368563685636857, 5: 0.5284552845528455, 1: 0.3523035230352303}}\n",
      "Epoch [26/200], Loss: 0.2338\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4446920078986743, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7208672086720868, 10: 0.6504065040650406, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [27/200], Loss: 0.2334\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45814145179599447, 'top_k_accuracy': {20: 0.7886178861788617, 15: 0.7371273712737128, 10: 0.6666666666666666, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [28/200], Loss: 0.2319\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4396215168828754, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7127371273712737, 10: 0.6395663956639567, 5: 0.5501355013550135, 1: 0.34688346883468835}}\n",
      "Epoch [29/200], Loss: 0.2345\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33604336043360433, 'mrr': 0.4442130823744582, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7398373983739838, 10: 0.6693766937669376, 5: 0.5420054200542005, 1: 0.34417344173441733}}\n",
      "Epoch [30/200], Loss: 0.2357\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4452027973028896, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5284552845528455, 1: 0.34959349593495936}}\n",
      "Epoch [31/200], Loss: 0.2401\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44124415615695767, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [32/200], Loss: 0.2431\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4477954054104646, 'top_k_accuracy': {20: 0.7886178861788617, 15: 0.7262872628726287, 10: 0.6639566395663956, 5: 0.5447154471544715, 1: 0.34688346883468835}}\n",
      "Epoch [33/200], Loss: 0.2441\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.45011128594701433, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7208672086720868, 10: 0.6531165311653117, 5: 0.5365853658536586, 1: 0.3604336043360434}}\n",
      "Epoch [34/200], Loss: 0.2451\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45286317287110134, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [35/200], Loss: 0.2453\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4423560957555479, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7100271002710027, 10: 0.6504065040650406, 5: 0.5555555555555556, 1: 0.34959349593495936}}\n",
      "Epoch [36/200], Loss: 0.2470\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45334476248108935, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [37/200], Loss: 0.2499\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4460265414168531, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7262872628726287, 10: 0.6666666666666666, 5: 0.5528455284552846, 1: 0.34688346883468835}}\n",
      "Epoch [38/200], Loss: 0.2504\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.43901860228162615, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6964769647696477, 10: 0.6260162601626016, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [39/200], Loss: 0.2504\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.452644474950744, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7018970189701897, 10: 0.6341463414634146, 5: 0.5338753387533876, 1: 0.36314363143631434}}\n",
      "Epoch [40/200], Loss: 0.2542\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.43640420432453064, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.6937669376693767, 10: 0.5989159891598916, 5: 0.5230352303523035, 1: 0.33875338753387535}}\n",
      "Epoch [41/200], Loss: 0.2603\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.44110185096654086, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5338753387533876, 1: 0.34688346883468835}}\n",
      "Epoch [42/200], Loss: 0.2622\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44047549643510403, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6775067750677507, 10: 0.6287262872628726, 5: 0.5121951219512195, 1: 0.35501355013550134}}\n",
      "Epoch [43/200], Loss: 0.2620\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.449508976574878, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5338753387533876, 1: 0.35772357723577236}}\n",
      "Epoch [44/200], Loss: 0.2621\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4484498859537215, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.3523035230352303}}\n",
      "Epoch [45/200], Loss: 0.2651\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.43941892780092834, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7018970189701897, 10: 0.6422764227642277, 5: 0.5392953929539296, 1: 0.33875338753387535}}\n",
      "Epoch [46/200], Loss: 0.2668\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4452757534773057, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5555555555555556, 1: 0.34417344173441733}}\n",
      "Epoch [47/200], Loss: 0.2643\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4486637152077359, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5609756097560976, 1: 0.34959349593495936}}\n",
      "Epoch [48/200], Loss: 0.2663\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.449016139927795, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6666666666666666, 10: 0.6151761517615176, 5: 0.5420054200542005, 1: 0.3604336043360434}}\n",
      "Epoch [49/200], Loss: 0.2670\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4481805459179039, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6991869918699187, 10: 0.6531165311653117, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [50/200], Loss: 0.2689\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4366992891495713, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7181571815718157, 10: 0.6504065040650406, 5: 0.5582655826558266, 1: 0.3333333333333333}}\n",
      "Epoch [51/200], Loss: 0.2723\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.4474115782360186, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5474254742547425, 1: 0.34688346883468835}}\n",
      "Epoch [52/200], Loss: 0.2727\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3333333333333333, 'mrr': 0.4438913995680929, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.6964769647696477, 10: 0.6531165311653117, 5: 0.5636856368563685, 1: 0.34688346883468835}}\n",
      "Epoch [53/200], Loss: 0.2747\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4381303767875076, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7181571815718157, 10: 0.6368563685636857, 5: 0.5203252032520326, 1: 0.34959349593495936}}\n",
      "Epoch [54/200], Loss: 0.2719\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4410541193914098, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6964769647696477, 10: 0.6314363143631436, 5: 0.5230352303523035, 1: 0.34688346883468835}}\n",
      "Epoch [55/200], Loss: 0.2728\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3333333333333333, 'mrr': 0.43711066633080103, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5528455284552846, 1: 0.33875338753387535}}\n",
      "Epoch [56/200], Loss: 0.2737\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4606568166090221, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5636856368563685, 1: 0.36314363143631434}}\n",
      "Epoch [57/200], Loss: 0.2747\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.46428464795586216, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7127371273712737, 10: 0.6476964769647696, 5: 0.5501355013550135, 1: 0.3794037940379404}}\n",
      "Epoch [58/200], Loss: 0.2746\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45392196809838803, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6910569105691057, 10: 0.6531165311653117, 5: 0.5447154471544715, 1: 0.36314363143631434}}\n",
      "Epoch [59/200], Loss: 0.2763\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.44358372349089303, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6585365853658537, 5: 0.5528455284552846, 1: 0.34417344173441733}}\n",
      "Epoch [60/200], Loss: 0.2790\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44491182382556294, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5311653116531165, 1: 0.35501355013550134}}\n",
      "Epoch [61/200], Loss: 0.2791\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4572562344599672, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6422764227642277, 5: 0.5555555555555556, 1: 0.3604336043360434}}\n",
      "Epoch [62/200], Loss: 0.2791\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46526350596481103, 'top_k_accuracy': {20: 0.7289972899728997, 15: 0.7046070460704607, 10: 0.6612466124661247, 5: 0.5772357723577236, 1: 0.3712737127371274}}\n",
      "Epoch [63/200], Loss: 0.2797\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4401641481085608, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5365853658536586, 1: 0.34146341463414637}}\n",
      "Epoch [64/200], Loss: 0.2800\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.44850164770702444, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7127371273712737, 10: 0.6558265582655827, 5: 0.5609756097560976, 1: 0.34417344173441733}}\n",
      "Epoch [65/200], Loss: 0.2825\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4528797399439216, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5420054200542005, 1: 0.36314363143631434}}\n",
      "Epoch [66/200], Loss: 0.2836\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45373171562891795, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.35772357723577236}}\n",
      "Epoch [67/200], Loss: 0.2863\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4451531519174158, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6205962059620597, 5: 0.5392953929539296, 1: 0.35501355013550134}}\n",
      "Epoch [68/200], Loss: 0.2869\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4426973680273395, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7018970189701897, 10: 0.6395663956639567, 5: 0.5284552845528455, 1: 0.34959349593495936}}\n",
      "Epoch [69/200], Loss: 0.2864\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.4396047498419022, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6856368563685636, 10: 0.6205962059620597, 5: 0.5149051490514905, 1: 0.3604336043360434}}\n",
      "Epoch [70/200], Loss: 0.2877\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33875338753387535, 'mrr': 0.4373748197212212, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6856368563685636, 10: 0.6233062330623306, 5: 0.5365853658536586, 1: 0.34146341463414637}}\n",
      "Epoch [71/200], Loss: 0.2878\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.4584843485084116, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7127371273712737, 10: 0.6476964769647696, 5: 0.5447154471544715, 1: 0.3712737127371274}}\n",
      "Epoch [72/200], Loss: 0.2885\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45561422564017223, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5528455284552846, 1: 0.3604336043360434}}\n",
      "Epoch [73/200], Loss: 0.2873\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35501355013550134, 'mrr': 0.45438714194353935, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5501355013550135, 1: 0.35772357723577236}}\n",
      "Epoch [74/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.45863579775099655, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6314363143631436, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [75/200], Loss: 0.2877\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45045470850641695, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7018970189701897, 10: 0.6395663956639567, 5: 0.5284552845528455, 1: 0.3604336043360434}}\n",
      "Epoch [76/200], Loss: 0.2872\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.463616740990832, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.7100271002710027, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.37398373983739835}}\n",
      "Epoch [77/200], Loss: 0.2872\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4552566541036695, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6829268292682927, 10: 0.6341463414634146, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [78/200], Loss: 0.2873\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46503824893407786, 'top_k_accuracy': {20: 0.7777777777777778, 15: 0.7154471544715447, 10: 0.6368563685636857, 5: 0.5365853658536586, 1: 0.3712737127371274}}\n",
      "Epoch [79/200], Loss: 0.2880\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4595507122530175, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5420054200542005, 1: 0.3712737127371274}}\n",
      "Epoch [80/200], Loss: 0.2887\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.4536546027872326, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [81/200], Loss: 0.2880\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46497671317504596, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7018970189701897, 10: 0.6314363143631436, 5: 0.5474254742547425, 1: 0.37398373983739835}}\n",
      "Epoch [82/200], Loss: 0.2883\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45506001980256044, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.6964769647696477, 10: 0.6368563685636857, 5: 0.5338753387533876, 1: 0.36585365853658536}}\n",
      "Epoch [83/200], Loss: 0.2885\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46425725042611443, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5582655826558266, 1: 0.3712737127371274}}\n",
      "Epoch [84/200], Loss: 0.2893\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.464261832241127, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.6964769647696477, 10: 0.6368563685636857, 5: 0.5501355013550135, 1: 0.37669376693766937}}\n",
      "Epoch [85/200], Loss: 0.2893\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.46487404582557545, 'top_k_accuracy': {20: 0.7831978319783198, 15: 0.7235772357723578, 10: 0.6368563685636857, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [86/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45550676301921034, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5474254742547425, 1: 0.36585365853658536}}\n",
      "Epoch [87/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.46156870541996725, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7073170731707317, 10: 0.6422764227642277, 5: 0.5230352303523035, 1: 0.38482384823848237}}\n",
      "Epoch [88/200], Loss: 0.2891\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.44514453457340963, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6856368563685636, 10: 0.6368563685636857, 5: 0.5420054200542005, 1: 0.3523035230352303}}\n",
      "Epoch [89/200], Loss: 0.2937\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4243036601968377, 'top_k_accuracy': {20: 0.7127371273712737, 15: 0.6802168021680217, 10: 0.5989159891598916, 5: 0.5230352303523035, 1: 0.3170731707317073}}\n",
      "Epoch [90/200], Loss: 0.3204\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4361627359819182, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6747967479674797, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.33604336043360433}}\n",
      "Epoch [91/200], Loss: 0.3121\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.44779816331151745, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7208672086720868, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.3523035230352303}}\n",
      "Epoch [92/200], Loss: 0.2966\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.45627573825756723, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [93/200], Loss: 0.2930\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.45935212946851817, 'top_k_accuracy': {20: 0.7750677506775068, 15: 0.7235772357723578, 10: 0.6504065040650406, 5: 0.5392953929539296, 1: 0.37398373983739835}}\n",
      "Epoch [94/200], Loss: 0.2932\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46067941686686964, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7073170731707317, 10: 0.6504065040650406, 5: 0.5501355013550135, 1: 0.3685636856368564}}\n",
      "Epoch [95/200], Loss: 0.2932\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.46344854907719035, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7073170731707317, 10: 0.6395663956639567, 5: 0.5392953929539296, 1: 0.37669376693766937}}\n",
      "Epoch [96/200], Loss: 0.2924\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4629573532746183, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7100271002710027, 10: 0.6585365853658537, 5: 0.5528455284552846, 1: 0.3685636856368564}}\n",
      "Epoch [97/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.45741754570535686, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7154471544715447, 10: 0.6558265582655827, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [98/200], Loss: 0.2919\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.4626203173051575, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7344173441734417, 10: 0.6639566395663956, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [99/200], Loss: 0.2912\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46282311770693574, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7127371273712737, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [100/200], Loss: 0.2914\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3604336043360434, 'mrr': 0.4545235178827429, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7262872628726287, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.3604336043360434}}\n",
      "Epoch [101/200], Loss: 0.2922\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46440914766041475, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [102/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.46293717260450395, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.7154471544715447, 10: 0.6531165311653117, 5: 0.5528455284552846, 1: 0.3794037940379404}}\n",
      "Epoch [103/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4655027486309156, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7073170731707317, 10: 0.6558265582655827, 5: 0.5555555555555556, 1: 0.37669376693766937}}\n",
      "Epoch [104/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4679704090489491, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7181571815718157, 10: 0.6531165311653117, 5: 0.5555555555555556, 1: 0.37669376693766937}}\n",
      "Epoch [105/200], Loss: 0.2913\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4709616137340876, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6937669376693767, 10: 0.6449864498644986, 5: 0.5555555555555556, 1: 0.3821138211382114}}\n",
      "Epoch [106/200], Loss: 0.2918\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4692364382590738, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7100271002710027, 10: 0.6449864498644986, 5: 0.5636856368563685, 1: 0.3821138211382114}}\n",
      "Epoch [107/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.45868482212765954, 'top_k_accuracy': {20: 0.7696476964769647, 15: 0.7208672086720868, 10: 0.6395663956639567, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [108/200], Loss: 0.2916\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37669376693766937, 'mrr': 0.4669940725107325, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7100271002710027, 10: 0.6287262872628726, 5: 0.5528455284552846, 1: 0.3821138211382114}}\n",
      "Epoch [109/200], Loss: 0.2917\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46389051981235874, 'top_k_accuracy': {20: 0.7642276422764228, 15: 0.7208672086720868, 10: 0.6395663956639567, 5: 0.5447154471544715, 1: 0.37398373983739835}}\n",
      "Epoch [110/200], Loss: 0.2907\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46302960206462485, 'top_k_accuracy': {20: 0.7723577235772358, 15: 0.7235772357723578, 10: 0.6395663956639567, 5: 0.5636856368563685, 1: 0.3712737127371274}}\n",
      "Epoch [111/200], Loss: 0.2921\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34417344173441733, 'mrr': 0.4438622952702845, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7154471544715447, 10: 0.6368563685636857, 5: 0.5528455284552846, 1: 0.34688346883468835}}\n",
      "Epoch [112/200], Loss: 0.2974\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.42428110964705973, 'top_k_accuracy': {20: 0.7127371273712737, 15: 0.6639566395663956, 10: 0.6205962059620597, 5: 0.5474254742547425, 1: 0.3224932249322493}}\n",
      "Epoch [113/200], Loss: 0.3090\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4333492918492197, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5176151761517616, 1: 0.34417344173441733}}\n",
      "Epoch [114/200], Loss: 0.2971\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4304097228474006, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6775067750677507, 10: 0.6260162601626016, 5: 0.5420054200542005, 1: 0.3333333333333333}}\n",
      "Epoch [115/200], Loss: 0.2915\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45418556922915765, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7046070460704607, 10: 0.6314363143631436, 5: 0.5447154471544715, 1: 0.36314363143631434}}\n",
      "Epoch [116/200], Loss: 0.2887\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.4642191947086885, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7100271002710027, 10: 0.6476964769647696, 5: 0.5691056910569106, 1: 0.37398373983739835}}\n",
      "Epoch [117/200], Loss: 0.2875\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4621405100683741, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5609756097560976, 1: 0.3712737127371274}}\n",
      "Epoch [118/200], Loss: 0.2864\n",
      "Validation metrics are:  {'sequence_accuracy': 0.35772357723577236, 'mrr': 0.45220769156532115, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6937669376693767, 10: 0.6476964769647696, 5: 0.5609756097560976, 1: 0.3604336043360434}}\n",
      "Epoch [119/200], Loss: 0.2866\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.45951090020996443, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6287262872628726, 5: 0.5420054200542005, 1: 0.37669376693766937}}\n",
      "Epoch [120/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3875338753387534, 'mrr': 0.4697874963119685, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5474254742547425, 1: 0.3902439024390244}}\n",
      "Epoch [121/200], Loss: 0.2868\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.46262998219135415, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6775067750677507, 10: 0.6205962059620597, 5: 0.5311653116531165, 1: 0.37669376693766937}}\n",
      "Epoch [122/200], Loss: 0.2853\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46456358220770927, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6205962059620597, 5: 0.5555555555555556, 1: 0.37398373983739835}}\n",
      "Epoch [123/200], Loss: 0.2849\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3821138211382114, 'mrr': 0.4686444223665247, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6341463414634146, 5: 0.5528455284552846, 1: 0.38482384823848237}}\n",
      "Epoch [124/200], Loss: 0.2838\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.4603247611457416, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5420054200542005, 1: 0.37669376693766937}}\n",
      "Epoch [125/200], Loss: 0.2828\n",
      "Validation metrics are:  {'sequence_accuracy': 0.37398373983739835, 'mrr': 0.4638946410339267, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6287262872628726, 5: 0.5447154471544715, 1: 0.3794037940379404}}\n",
      "Epoch [126/200], Loss: 0.2797\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3685636856368564, 'mrr': 0.4672819080623839, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6910569105691057, 10: 0.6422764227642277, 5: 0.5691056910569106, 1: 0.37398373983739835}}\n",
      "Epoch [127/200], Loss: 0.2767\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.46616985211755035, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7046070460704607, 10: 0.6341463414634146, 5: 0.5447154471544715, 1: 0.38482384823848237}}\n",
      "Epoch [128/200], Loss: 0.2764\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3794037940379404, 'mrr': 0.4664670351309928, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6829268292682927, 10: 0.6233062330623306, 5: 0.5582655826558266, 1: 0.38482384823848237}}\n",
      "Epoch [129/200], Loss: 0.2763\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3712737127371274, 'mrr': 0.46488401065221024, 'top_k_accuracy': {20: 0.7615176151761518, 15: 0.6991869918699187, 10: 0.6422764227642277, 5: 0.5582655826558266, 1: 0.3794037940379404}}\n",
      "Epoch [130/200], Loss: 0.2800\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.44296364220539863, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6016260162601627, 5: 0.5257452574525745, 1: 0.35772357723577236}}\n",
      "Epoch [131/200], Loss: 0.2884\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4395108553273903, 'top_k_accuracy': {20: 0.7100271002710027, 15: 0.6693766937669376, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.34417344173441733}}\n",
      "Epoch [132/200], Loss: 0.2892\n",
      "Validation metrics are:  {'sequence_accuracy': 0.33062330623306235, 'mrr': 0.45401009035844564, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7127371273712737, 10: 0.6720867208672087, 5: 0.5691056910569106, 1: 0.34959349593495936}}\n",
      "Epoch [133/200], Loss: 0.2815\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36585365853658536, 'mrr': 0.46860616878282196, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7127371273712737, 10: 0.6504065040650406, 5: 0.5555555555555556, 1: 0.37398373983739835}}\n",
      "Epoch [134/200], Loss: 0.2776\n",
      "Validation metrics are:  {'sequence_accuracy': 0.36314363143631434, 'mrr': 0.47180780040387127, 'top_k_accuracy': {20: 0.7560975609756098, 15: 0.7154471544715447, 10: 0.6476964769647696, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [135/200], Loss: 0.2768\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4513649949671808, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7018970189701897, 10: 0.6531165311653117, 5: 0.5501355013550135, 1: 0.35501355013550134}}\n",
      "Epoch [136/200], Loss: 0.2767\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3523035230352303, 'mrr': 0.4633528139564754, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7127371273712737, 10: 0.6341463414634146, 5: 0.5474254742547425, 1: 0.3712737127371274}}\n",
      "Epoch [137/200], Loss: 0.2754\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34146341463414637, 'mrr': 0.4607757988006485, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7073170731707317, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.36314363143631434}}\n",
      "Epoch [138/200], Loss: 0.2757\n",
      "Validation metrics are:  {'sequence_accuracy': 0.34688346883468835, 'mrr': 0.4742670112331803, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7073170731707317, 10: 0.6476964769647696, 5: 0.5528455284552846, 1: 0.38482384823848237}}\n",
      "Epoch [139/200], Loss: 0.2749\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4617676379444355, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6964769647696477, 10: 0.6504065040650406, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [140/200], Loss: 0.2739\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.45608613125988895, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [141/200], Loss: 0.2743\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4578526941737763, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6476964769647696, 5: 0.5582655826558266, 1: 0.3604336043360434}}\n",
      "Epoch [142/200], Loss: 0.2726\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4650080511439994, 'top_k_accuracy': {20: 0.7669376693766937, 15: 0.7127371273712737, 10: 0.6368563685636857, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [143/200], Loss: 0.2736\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.4673115251377811, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7018970189701897, 10: 0.6422764227642277, 5: 0.5582655826558266, 1: 0.37398373983739835}}\n",
      "Epoch [144/200], Loss: 0.2720\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4508073467454164, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6964769647696477, 10: 0.6449864498644986, 5: 0.5420054200542005, 1: 0.34688346883468835}}\n",
      "Epoch [145/200], Loss: 0.2707\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.45666022075335433, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6991869918699187, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.3604336043360434}}\n",
      "Epoch [146/200], Loss: 0.2693\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4618287668991878, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5311653116531165, 1: 0.37398373983739835}}\n",
      "Epoch [147/200], Loss: 0.2710\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45872231292325333, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.6964769647696477, 10: 0.6504065040650406, 5: 0.5447154471544715, 1: 0.36585365853658536}}\n",
      "Epoch [148/200], Loss: 0.2697\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.465488202109093, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7046070460704607, 10: 0.6449864498644986, 5: 0.5718157181571816, 1: 0.3685636856368564}}\n",
      "Epoch [149/200], Loss: 0.2682\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2899728997289973, 'mrr': 0.46119451090406105, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.7100271002710027, 10: 0.6531165311653117, 5: 0.5528455284552846, 1: 0.36585365853658536}}\n",
      "Epoch [150/200], Loss: 0.2691\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4570452947297486, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6910569105691057, 10: 0.6341463414634146, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [151/200], Loss: 0.2701\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.4350934257337224, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6747967479674797, 10: 0.6178861788617886, 5: 0.5338753387533876, 1: 0.33875338753387535}}\n",
      "Epoch [152/200], Loss: 0.2713\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2764227642276423, 'mrr': 0.4315611328371936, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6802168021680217, 10: 0.6178861788617886, 5: 0.5338753387533876, 1: 0.3252032520325203}}\n",
      "Epoch [153/200], Loss: 0.2758\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2764227642276423, 'mrr': 0.4283285937888515, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6802168021680217, 10: 0.6287262872628726, 5: 0.5284552845528455, 1: 0.3252032520325203}}\n",
      "Epoch [154/200], Loss: 0.2753\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2845528455284553, 'mrr': 0.43556799386748896, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6910569105691057, 10: 0.6124661246612466, 5: 0.5311653116531165, 1: 0.33604336043360433}}\n",
      "Epoch [155/200], Loss: 0.2686\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.44686943872661816, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.6856368563685636, 10: 0.6395663956639567, 5: 0.5392953929539296, 1: 0.3523035230352303}}\n",
      "Epoch [156/200], Loss: 0.2676\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.45431181341914245, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6720867208672087, 10: 0.6205962059620597, 5: 0.5284552845528455, 1: 0.36585365853658536}}\n",
      "Epoch [157/200], Loss: 0.2668\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45118108871436546, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7018970189701897, 10: 0.6476964769647696, 5: 0.5555555555555556, 1: 0.3523035230352303}}\n",
      "Epoch [158/200], Loss: 0.2658\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4514607856612823, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6937669376693767, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.35772357723577236}}\n",
      "Epoch [159/200], Loss: 0.2653\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4581246851127785, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6883468834688347, 10: 0.6341463414634146, 5: 0.5609756097560976, 1: 0.36314363143631434}}\n",
      "Epoch [160/200], Loss: 0.2637\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4629825841148014, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6964769647696477, 10: 0.6341463414634146, 5: 0.5555555555555556, 1: 0.36585365853658536}}\n",
      "Epoch [161/200], Loss: 0.2629\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.4569612357172983, 'top_k_accuracy': {20: 0.7262872628726287, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5718157181571816, 1: 0.34959349593495936}}\n",
      "Epoch [162/200], Loss: 0.2625\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4555257288805811, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6937669376693767, 10: 0.6287262872628726, 5: 0.5718157181571816, 1: 0.3604336043360434}}\n",
      "Epoch [163/200], Loss: 0.2642\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2953929539295393, 'mrr': 0.45448114352109614, 'top_k_accuracy': {20: 0.7371273712737128, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5663956639566395, 1: 0.3523035230352303}}\n",
      "Epoch [164/200], Loss: 0.2628\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45326810381729443, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6829268292682927, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3523035230352303}}\n",
      "Epoch [165/200], Loss: 0.2610\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4455058333573052, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6775067750677507, 10: 0.6314363143631436, 5: 0.5365853658536586, 1: 0.34688346883468835}}\n",
      "Epoch [166/200], Loss: 0.2592\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2926829268292683, 'mrr': 0.45204368551373797, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6883468834688347, 10: 0.6558265582655827, 5: 0.5745257452574526, 1: 0.3523035230352303}}\n",
      "Epoch [167/200], Loss: 0.2601\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3062330623306233, 'mrr': 0.45728987024667106, 'top_k_accuracy': {20: 0.7506775067750677, 15: 0.7100271002710027, 10: 0.6585365853658537, 5: 0.5663956639566395, 1: 0.35772357723577236}}\n",
      "Epoch [168/200], Loss: 0.2580\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4588113846907427, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7046070460704607, 10: 0.6368563685636857, 5: 0.5501355013550135, 1: 0.36314363143631434}}\n",
      "Epoch [169/200], Loss: 0.2589\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4575990584940088, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6205962059620597, 5: 0.5663956639566395, 1: 0.36314363143631434}}\n",
      "Epoch [170/200], Loss: 0.2570\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.46010217098814354, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.7046070460704607, 10: 0.6422764227642277, 5: 0.5447154471544715, 1: 0.3685636856368564}}\n",
      "Epoch [171/200], Loss: 0.2578\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4523370801301299, 'top_k_accuracy': {20: 0.7289972899728997, 15: 0.6937669376693767, 10: 0.6422764227642277, 5: 0.5718157181571816, 1: 0.34959349593495936}}\n",
      "Epoch [172/200], Loss: 0.2568\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4573129476884344, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6910569105691057, 10: 0.6504065040650406, 5: 0.5691056910569106, 1: 0.35772357723577236}}\n",
      "Epoch [173/200], Loss: 0.2559\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.45414588991376037, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.7018970189701897, 10: 0.6504065040650406, 5: 0.5582655826558266, 1: 0.35501355013550134}}\n",
      "Epoch [174/200], Loss: 0.2573\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.4584533363719266, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6693766937669376, 10: 0.6043360433604336, 5: 0.5582655826558266, 1: 0.36585365853658536}}\n",
      "Epoch [175/200], Loss: 0.2595\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.44906217080383043, 'top_k_accuracy': {20: 0.7046070460704607, 15: 0.6504065040650406, 10: 0.6124661246612466, 5: 0.5365853658536586, 1: 0.3685636856368564}}\n",
      "Epoch [176/200], Loss: 0.2670\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2953929539295393, 'mrr': 0.44753093117474324, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.7018970189701897, 10: 0.6476964769647696, 5: 0.5582655826558266, 1: 0.34688346883468835}}\n",
      "Epoch [177/200], Loss: 0.2597\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.4478107320474357, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6883468834688347, 10: 0.6287262872628726, 5: 0.5501355013550135, 1: 0.35501355013550134}}\n",
      "Epoch [178/200], Loss: 0.2565\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.45319018899575875, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6937669376693767, 10: 0.6260162601626016, 5: 0.5501355013550135, 1: 0.3604336043360434}}\n",
      "Epoch [179/200], Loss: 0.2558\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.44773484580582446, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6775067750677507, 10: 0.6097560975609756, 5: 0.5203252032520326, 1: 0.35772357723577236}}\n",
      "Epoch [180/200], Loss: 0.2558\n",
      "Validation metrics are:  {'sequence_accuracy': 0.2981029810298103, 'mrr': 0.4509427198381609, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6910569105691057, 10: 0.6449864498644986, 5: 0.5528455284552846, 1: 0.3523035230352303}}\n",
      "Epoch [181/200], Loss: 0.2544\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4611879229417281, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6422764227642277, 5: 0.5609756097560976, 1: 0.36585365853658536}}\n",
      "Epoch [182/200], Loss: 0.2538\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45721623401693046, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6964769647696477, 10: 0.6233062330623306, 5: 0.5474254742547425, 1: 0.3604336043360434}}\n",
      "Epoch [183/200], Loss: 0.2536\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.462072019520301, 'top_k_accuracy': {20: 0.7344173441734417, 15: 0.6856368563685636, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3685636856368564}}\n",
      "Epoch [184/200], Loss: 0.2550\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4608661583339003, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5365853658536586, 1: 0.3712737127371274}}\n",
      "Epoch [185/200], Loss: 0.2534\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4595884521932542, 'top_k_accuracy': {20: 0.7208672086720868, 15: 0.6829268292682927, 10: 0.6233062330623306, 5: 0.5528455284552846, 1: 0.3712737127371274}}\n",
      "Epoch [186/200], Loss: 0.2532\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.4710635127838755, 'top_k_accuracy': {20: 0.7154471544715447, 15: 0.6937669376693767, 10: 0.6449864498644986, 5: 0.5582655826558266, 1: 0.38482384823848237}}\n",
      "Epoch [187/200], Loss: 0.2518\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3035230352303523, 'mrr': 0.45529030683135435, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6910569105691057, 10: 0.6287262872628726, 5: 0.5555555555555556, 1: 0.3604336043360434}}\n",
      "Epoch [188/200], Loss: 0.2501\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4655191689219137, 'top_k_accuracy': {20: 0.7533875338753387, 15: 0.7073170731707317, 10: 0.6368563685636857, 5: 0.5528455284552846, 1: 0.37398373983739835}}\n",
      "Epoch [189/200], Loss: 0.2513\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3008130081300813, 'mrr': 0.4596683667629578, 'top_k_accuracy': {20: 0.7398373983739838, 15: 0.6856368563685636, 10: 0.6368563685636857, 5: 0.5392953929539296, 1: 0.3685636856368564}}\n",
      "Epoch [190/200], Loss: 0.2505\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3143631436314363, 'mrr': 0.47090591368530355, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6856368563685636, 10: 0.6314363143631436, 5: 0.5609756097560976, 1: 0.38482384823848237}}\n",
      "Epoch [191/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.46470029908336796, 'top_k_accuracy': {20: 0.7479674796747967, 15: 0.6991869918699187, 10: 0.6476964769647696, 5: 0.5636856368563685, 1: 0.37398373983739835}}\n",
      "Epoch [192/200], Loss: 0.2502\n",
      "Validation metrics are:  {'sequence_accuracy': 0.32791327913279134, 'mrr': 0.4663084351444116, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6856368563685636, 10: 0.6395663956639567, 5: 0.5609756097560976, 1: 0.37669376693766937}}\n",
      "Epoch [193/200], Loss: 0.2506\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3089430894308943, 'mrr': 0.4628840132018393, 'top_k_accuracy': {20: 0.7588075880758808, 15: 0.7208672086720868, 10: 0.6558265582655827, 5: 0.5501355013550135, 1: 0.3712737127371274}}\n",
      "Epoch [194/200], Loss: 0.2509\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46567543903605446, 'top_k_accuracy': {20: 0.7317073170731707, 15: 0.6883468834688347, 10: 0.6341463414634146, 5: 0.5582655826558266, 1: 0.3794037940379404}}\n",
      "Epoch [195/200], Loss: 0.2528\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3116531165311653, 'mrr': 0.46772718678932385, 'top_k_accuracy': {20: 0.7425474254742548, 15: 0.6802168021680217, 10: 0.6341463414634146, 5: 0.5745257452574526, 1: 0.37398373983739835}}\n",
      "Epoch [196/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46481936632351417, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6883468834688347, 10: 0.6422764227642277, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [197/200], Loss: 0.2495\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3252032520325203, 'mrr': 0.4657506171634889, 'top_k_accuracy': {20: 0.7181571815718157, 15: 0.6883468834688347, 10: 0.6205962059620597, 5: 0.5528455284552846, 1: 0.37669376693766937}}\n",
      "Epoch [198/200], Loss: 0.2503\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3170731707317073, 'mrr': 0.4726922084028645, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6964769647696477, 10: 0.6260162601626016, 5: 0.5636856368563685, 1: 0.38482384823848237}}\n",
      "Epoch [199/200], Loss: 0.2486\n",
      "Validation metrics are:  {'sequence_accuracy': 0.3224932249322493, 'mrr': 0.46677484676798425, 'top_k_accuracy': {20: 0.7235772357723578, 15: 0.6829268292682927, 10: 0.6368563685636857, 5: 0.5636856368563685, 1: 0.37669376693766937}}\n",
      "Epoch [200/200], Loss: 0.2487\n",
      "Validation metrics are:  {'sequence_accuracy': 0.31978319783197834, 'mrr': 0.46909402036662573, 'top_k_accuracy': {20: 0.7452574525745257, 15: 0.6991869918699187, 10: 0.6395663956639567, 5: 0.5636856368563685, 1: 0.3821138211382114}}\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"CompositionTextInfiller_{EMBEDDINGS_DIM}\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "#experiment.k_fold_cross_validation(train_x, train_y, train_lengths, distorted_train_x, spm_train_x, spm_lengths_train,\n",
    "#                                   test_x, test_y, test_lengths, distorted_test_x, spm_test_x, spm_lengths_test,\n",
    "                                   #num_epochs=2, num_folds=2) # debug\n",
    "#                                   num_epochs=hyperparams[\"num_epochs\"], num_folds=hyperparams[\"num_folds\"])\n",
    "experiment.train(train_x, train_y, train_lengths, distorted_train_x, spm_train_x, spm_lengths_train,\n",
    "                   test_x, test_y, test_lengths, distorted_test_x, spm_test_x, spm_lengths_test, num_epochs=200, early_stop=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLTJjMsidp9J"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"test_learnable_composition\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "experiment.test(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOdVE6BAXxjt"
   },
   "outputs": [],
   "source": [
    "train_x.shape, test_x.shape, len(seq_train_x), len(seq_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv1BBzcHZhiY"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"brnn_se_with_kl\"\n",
    "\n",
    "experiment = Experiment(None, experiment_name=experiment_name)\n",
    "experiment.load_experiment()\n",
    "\n",
    "# train additional epochs\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                 test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=1, verbose=True)\n",
    "\n",
    "# test the model\n",
    "experiment.test(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3vW2Hg2Wqsx"
   },
   "outputs": [],
   "source": [
    "experiment_name = f\"test_gemini\"\n",
    "experiment = Experiment(hyperparams, experiment_name=experiment_name)\n",
    "experiment.train(train_x, train_y, train_lengths, word_embeddings_train_x, phonetic_embeddings_train_x,\n",
    "                 test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, num_epochs=50, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OUp0TYTMdHC"
   },
   "outputs": [],
   "source": [
    "def predict_linear_b_token(input_sequence, token_predictions):\n",
    "    \"\"\"\n",
    "    Creates a prompt for an LLM to predict the next token in a Linear B sequence.\n",
    "\n",
    "    Args:\n",
    "        input_sequence (str): The partial Linear B sequence\n",
    "        token_predictions (list): List of tuples (token, probability) of possible next tokens\n",
    "\n",
    "    Returns:\n",
    "        str: XML-formatted prompt for an LLM\n",
    "    \"\"\"\n",
    "    # Extract the context (everything before the missing token)\n",
    "    context = input_sequence.split(\"?\")[0].strip()\n",
    "\n",
    "    # Extract the top 20 token predictions\n",
    "    top_predictions = token_predictions[:20]\n",
    "\n",
    "    # Create the XML-formatted prompt\n",
    "    prompt = f\"\"\"<linear_b_prediction>\n",
    "  <context>{context}</context>\n",
    "  <possible_tokens>\n",
    "\"\"\"\n",
    "\n",
    "    # Add each candidate token with its probability\n",
    "    for token, probability in top_predictions:\n",
    "        prompt += f\"    <token probability=\\\"{probability:.4f}\\\">{token}</token>\\n\"\n",
    "\n",
    "    prompt += \"\"\"  </possible_tokens>\n",
    "  <instructions>\n",
    "    You are helping to predict the next token in a Linear B sequence. Linear B was used to write Mycenaean Greek around 1400-1200 BCE.\n",
    "\n",
    "    Based ONLY on:\n",
    "    1. The context provided above\n",
    "    2. Your understanding of Linear B syllabic patterns\n",
    "    3. Transliterate input into ancient greek to get more context information from the incomplete tokens sequence\n",
    "    4. Only select a syllabogram, do not choose logograms or numbers\n",
    "    5. Space is both word separator and token separator\n",
    "\n",
    "    Do not choose simply the most probable next token from the list of possible tokens: consider all possible options as possible replacements.\n",
    "    Compare all possible options, also using Chain of Thoughts.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - Do NOT use knowledge of actual Linear B texts or fragments\n",
    "    - Do NOT access or reference any external sources\n",
    "    - Do NOT explain your reasoning\n",
    "    - Respond ONLY with the single token you predict, nothing else\n",
    "  </instructions>\n",
    "</linear_b_prediction>\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Sample data from the example\n",
    "input_sequence = \"SOS a ja me no   e re pa te ja pi   ka ru ?   *220   1   ta ra nu   a ja me no   e re pa te ja pi   ka ru pi   *220   1\"\n",
    "token_predictions = [('ro', 0.14198297262191772), ('no', 0.12454625219106674), ('si', 0.09385930001735687), ('te', 0.08285455405712128), ('tya', 0.04424416646361351), ('mo', 0.03839782625436783), ('wo', 0.029186910018324852), ('de', 0.027668841183185577), ('qo', 0.027635548263788223), ('jo', 0.02637413516640663), ('i', 0.025987736880779266), ('ko', 0.01489566545933485), ('NUM', 0.013128552585840225), ('se', 0.010486927814781666), ('ta', 0.010461430996656418), ('we', 0.010421977378427982), ('o', 0.01035719271749258), ('qe', 0.010198087431490421), ('sa', 0.010087325237691402), ('na', 0.009956919588148594)]\n",
    "prompt = predict_linear_b_token(input_sequence, token_predictions)\n",
    "print(prompt)\n",
    "GOOGLE_API_KEY=\"[redacted]\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "response = gemini_model.generate_content(prompt)\n",
    "pred = response.text.strip()\n",
    "print(output_mapping[pred], inv_map[output_mapping[pred]], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzFv607n-yhh"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def evaluate_model(model, test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, batch_size, hyperparams, verbose=False):\n",
    "    model.eval()\n",
    "    test_loader = create_loader(test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, batch_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        total_corrects = 0\n",
    "        gemini_correct = 0\n",
    "        metrics = None\n",
    "        num_sequences = 0\n",
    "\n",
    "        for batch_x, batch_y, batch_lengths, word_emb, phon_emb in test_loader:\n",
    "            batch_x, batch_y, batch_lengths, word_emb, phon_emb = (\n",
    "                batch_x.to(device), batch_y.to(device), batch_lengths.to(device),\n",
    "                word_emb.to(device), phon_emb.to(device)\n",
    "            )\n",
    "\n",
    "            if hyperparams[\"model\"] in {\"BRNNTextInfiller\", \"FCNTextInfiller\", \"ConvTextInfiller\", \"ConvTextInfillerRNN\", \"BiConvTextInfiller\"}:\n",
    "                model_output = model(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "            elif hyperparams[\"model\"] == \"TransformerTextInfiller\":\n",
    "                model_output = model.generate(batch_x, batch_lengths, word_emb, phon_emb)\n",
    "\n",
    "            if verbose:\n",
    "                inp, gt, out = reconstruct_sequences(model_output, batch_y, batch_x)\n",
    "                for i, (input_seq, gt_seq, out_seq) in enumerate(zip(inp, gt, out)):\n",
    "                    print(f\"TEST n. {num_sequences + 1}\")\n",
    "                    print(f\"Input: {input_seq}\")\n",
    "                    print(f\"Ground Truth: {gt_seq}\")\n",
    "                    print(f\"Output: {out_seq}\")\n",
    "\n",
    "                    # Find the index j where batch_x has the max value\n",
    "                    j = torch.argmax(batch_x[i]).item()\n",
    "                    if j > 0:\n",
    "                        top_preds = torch.topk(model_output[i, j-1], 20)\n",
    "                        indices = top_preds.indices.tolist()\n",
    "                        tokens = [inv_map[i] for i in indices]\n",
    "                        probabilities = torch.exp(top_preds.values).tolist()  # Convert log probabilities to actual probabilities\n",
    "                        print(f\"Top 20 predictions for token index {j-1}: {list(zip(tokens, probabilities))}\")\n",
    "                        prompt = predict_linear_b_token(input_seq, list(zip(tokens, probabilities)))\n",
    "                        gemini_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "                        response = gemini_model.generate_content(prompt)\n",
    "                        pred = response.text.strip()\n",
    "                        print(pred, inv_map[batch_y[i, j-1].item()])\n",
    "                        if output_mapping[pred] ==  batch_y[i, j-1]:\n",
    "                            gemini_correct += 1\n",
    "                        time.sleep(5) # Avoid Gemini blocking\n",
    "                    num_sequences += 1\n",
    "\n",
    "            batch_metrics = collect_batch_metrics(model_output, batch_y, batch_x)\n",
    "            metrics = aggregate_metrics(metrics, batch_metrics)\n",
    "\n",
    "        metrics = compute_final_metrics(metrics)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"TEST FINISHED\")\n",
    "        print(f\"GEMINI CORRECT: {gemini_correct} / {num_sequences}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "evaluate_model(experiment.model, test_x, test_y, test_lengths, word_embeddings_test_x, phonetic_embeddings_test_x, experiment.batch_size, experiment.hyperparams, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ASfy4nGDQ3ZA",
    "Bzm3_1MAa915",
    "-Ca5j5iS0qat",
    "-CM84L7pqoj0",
    "PArR6E3SElyC",
    "NMzlfXi5ZBXi",
    "VByz0YR6e0wl",
    "LIa0iX34efK-",
    "-qubc_YVfkZ2",
    "THVkwEMUesYG",
    "Qf9HEKgIa0aQ",
    "EAqaPwXaAHNE",
    "QKVUfb_Ks5wT",
    "sLgdHwYEJqXm",
    "orwm78mS7Q9c",
    "7elfqidiFx9z",
    "TYrpaOIu2_lQ",
    "M5yEV0dkIah7",
    "6labFmFVLx0k",
    "mtjQWbLwLviQ",
    "pwkouvbR5sUg",
    "gTN0seM0ZeMS",
    "nKGKQINpiOjf",
    "tO0zgTaV3uiY",
    "xQtEmYC3pEcV",
    "obuLB4rasOdi",
    "nI3Ul8KCs1hv",
    "vYmgonqwtAOv",
    "vu9ifIcOvJ34",
    "6NNyKMajwQ0z",
    "7SDXTbmly8wm",
    "Sb1XkEqWk5uG",
    "3VzJkbCQlBDu",
    "r45qGiVOlEQJ",
    "mvAvEDpclHQR",
    "DS9G66palJnO",
    "8dU1iu5QQH-O",
    "JaTDgRdkQBOl",
    "Ez_-5lhQkHBD",
    "Gisi9B669oNQ",
    "dk0tqyCsxdsb",
    "ZJyuylWgxX58",
    "a4g4oCsQPwJy",
    "xW33X_JoQi-y",
    "_3o3VulFSA6a",
    "7O_exCdBTx85",
    "J6her1ZpgLaS",
    "fYpB3YBqVla4",
    "O5-PzMfDGWWA",
    "PiF0WQ1UYbxS",
    "q2mD8AD1X89w",
    "TYEuAhe5ahUG",
    "B0nPynQ3avWF",
    "ZUW1774iLcV3",
    "9eZVPovYqqVT",
    "0_JGjDSQQcRJ",
    "jqSJBCKr3g3Q",
    "IWLFe1YAClPM",
    "MeshvmB2jRqU",
    "9svVyxHZ8MRi",
    "hWT6GrV--8Rh",
    "yNMMKBIcIZwf",
    "9p3GM8JLqaQy",
    "Er8DJ-hIxCZg",
    "12nNn5MGQkXY",
    "RFEKd7wnGIGp",
    "uiDssS3AtLjc",
    "rdENsy1li13g",
    "0guTkDZ7nI2C",
    "O_lchCPdO3lK",
    "ItxZ4t0HgO0n",
    "D8tKyaEpgZlB",
    "-i_N_Z-Mghu7",
    "D32xz_Oz_SU0"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
